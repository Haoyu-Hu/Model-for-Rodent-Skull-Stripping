{"cells":[{"cell_type":"markdown","metadata":{"id":"sM9eTP0xbLO5"},"source":["# Model Procedure"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30684,"status":"ok","timestamp":1647707416074,"user":{"displayName":"胡浩宇","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06402011815971394657"},"user_tz":-480},"id":"IhqBLExsbYxa","outputId":"428d4c5a-f971-47ab-9bcb-98a354f349be"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"aGUpyYQpbLO9"},"source":["## 1) Train part for the model\n","## It's a necessary part for using these pre-trained model to different spieces"]},{"cell_type":"markdown","metadata":{"id":"78lQaK0ibLO-"},"source":["In this part, you can train model to match your dataset.<br /> For example, the pre-trained model is based on the human brain, if you want to use it on mice brains,\n"," you should train it using your own dataset"]},{"cell_type":"markdown","metadata":{"id":"uOqelWLVbLPA"},"source":["### Part 1: import necessary packages"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9158,"status":"ok","timestamp":1647707425198,"user":{"displayName":"胡浩宇","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06402011815971394657"},"user_tz":-480},"id":"K2ajUWDtczRv","outputId":"9a2d131a-4a31-4d86-ef04-79bdd9d09387"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting nilearn\n","  Downloading nilearn-0.9.0-py3-none-any.whl (10.1 MB)\n","\u001b[K     |████████████████████████████████| 10.1 MB 4.1 MB/s \n","\u001b[?25hRequirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.7/dist-packages (from nilearn) (1.1.0)\n","Requirement already satisfied: nibabel>=2.5 in /usr/local/lib/python3.7/dist-packages (from nilearn) (3.0.2)\n","Requirement already satisfied: scipy>=1.2 in /usr/local/lib/python3.7/dist-packages (from nilearn) (1.4.1)\n","Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from nilearn) (1.3.5)\n","Requirement already satisfied: requests>=2 in /usr/local/lib/python3.7/dist-packages (from nilearn) (2.23.0)\n","Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from nilearn) (1.21.5)\n","Requirement already satisfied: scikit-learn>=0.21 in /usr/local/lib/python3.7/dist-packages (from nilearn) (1.0.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->nilearn) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->nilearn) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24.0->nilearn) (1.15.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2->nilearn) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2->nilearn) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2->nilearn) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2->nilearn) (2.10)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21->nilearn) (3.1.0)\n","Installing collected packages: nilearn\n","Successfully installed nilearn-0.9.0\n","Collecting SimpleITK\n","  Downloading SimpleITK-2.1.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (48.4 MB)\n","\u001b[K     |████████████████████████████████| 48.4 MB 1.7 MB/s \n","\u001b[?25hInstalling collected packages: SimpleITK\n","Successfully installed SimpleITK-2.1.1\n"]}],"source":["!pip install nilearn\n","!pip install SimpleITK"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F1caVj__bLPB"},"outputs":[],"source":["import os, sys, pickle\n","sys.path.append('/content/drive/MyDrive/UNet_Model')\n","from arguments import Args, Args_test\n","from orientation import orientation\n","from Train import Train\n","from Test import Test\n","import numpy as np\n","import nilearn as nil\n","import nibabel as nib\n","from nilearn import plotting\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd"]},{"cell_type":"markdown","metadata":{"id":"1SLo6dBpbLPF"},"source":["### Part 2: set the parameters we need"]},{"cell_type":"markdown","metadata":{"id":"3SUb26qXbLPG"},"source":["#### The followings are the parameters we can set:\n","|Parameters|Meaning|Default value|Whether the input is required|\n","| :----: | :------: | :----: | :----:|\n","|train_t1w|Train T1w Directory|None|Yes|\n","|train_msk|Train Mask Directory|None|Yes|\n","|out_dir|Output Directory|None|Yes|\n","|validate_t1w|Validation T1w Directory|None|Yes|\n","|validate_msk|Validation Mask Directory|None|Yes|\n","|init_model|Initial Model Directory or Filename|None|Yes|\n","|conv_block|Num of UNet Blocks|5|Optional|\n","|input_slice|Num of Slices for Model Input|3|Optional|\n","|kernel_root|Num of the Root of Kernel|16|Optional|\n","|rescale_dim|Dimension to Rescale|256|Optional|\n","|num_epoch|Num of Epoch|40|Optional|\n","|learning_rate|Learning Rate of the Model|0.0001|Optional|\n","|optimizerSs|Optimizer|Adam|Can't be set|"]},{"cell_type":"markdown","metadata":{"id":"sQDqbgEFbLPI"},"source":["#### Also, we can change some parameters individually\n","|Parameters|Function to Change it|\n","| :----: | :------: |\n","|train_t1w|change_train_t1w|\n","|train_msk|change_train_msk|\n","|out_dir|change_out_dir|\n","|validate_t1w|change_validate_t1w|\n","|validate_msk|change_validate_msk|\n","|init_model|change_init_model|"]},{"cell_type":"markdown","metadata":{"id":"o6rLF472bLPK"},"source":["### Here is a brief introduction for each pretrained model we provide\n","1. **Site-All-T-epoch_36.model**: Trained on 12 macaques across 6 sites (2 macaques per site) from PRIME-DE. Six sites include newcastle, ucdavis, oxford, ion, ecnu-chen, and sbri.\n","\n","2. **Site-All-T-epoch_36_update_with_Site_6_plus_7-epoch_09.model**: Trained on 19 macaques across 13 sites from PRIME-DE (12 macaques across 6 sites used in the first model and 7 macaques across 7 additional sites) Seven sites include NIMH, ecnu-k, nin, rockefeller, uwo, mountsinai-S, and lyon.\n","\n","3. **Site-All-T-epoch_36_update_with_Site_*.model**: Site-specific model for NIMH, ecnu-k, nin, rockefeller, uwo, mountsinai-S, and lyon.\n","\n","4. **Site-All-T-epoch_36_update_with_Site_Pigs_09.model**: The **pig** model - Trained on 12 macaques and updated with the pig data (N=3).\n","\n","#### [Download the models](https://github.com/HumanBrainED/NHP-BrainExtraction/tree/master/UNet_Model)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"86MuobvybLPM","executionInfo":{"status":"ok","timestamp":1647707781511,"user_tz":-480,"elapsed":633,"user":{"displayName":"胡浩宇","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06402011815971394657"}}},"outputs":[],"source":["args = Args(train_t1w=\"/content/drive/MyDrive/Train_3/Train_data\"\n","           , train_msk=\"/content/drive/MyDrive/Train_Val_Test_model_3/Train_data.zip (Unzipped Files)/Train_msk_2\"\n","           , out_dir=\"/content/drive/MyDrive/Train_Val_Test_model_3/Train_data.zip (Unzipped Files)/Out_Dir/model_noba_model\"\n","           , validate_t1w=\"/content/drive/MyDrive/Train_3/Val_data\"\n","           , validate_msk=\"/content/drive/MyDrive/Train_Val_Test_model_3/Train_data.zip (Unzipped Files)/Val_msk_2\"\n","           , init_model=\"/content/drive/MyDrive/UNet_Model/models/Site-Human-epoch_08.model\"\n","           , num_epoch=50\n","           , rotation=True)"]},{"cell_type":"markdown","metadata":{"id":"MkNQ7rkbbLPN"},"source":["### Part 3: Check the Orientation & Resample the Image"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"XImrHB2XbLPO","outputId":"864c7381-4b3b-44ae-f204-3da3cb873240","scrolled":true},"outputs":[{"output_type":"stream","name":"stdout","text":["image : 4_6_d_163930.nii\n","original shape: (256, 256, 30) resample shape: (256, 108, 256)\n","image : 4_12_d_183819.nii\n","original shape: (256, 256, 23) resample shape: (256, 82, 256)\n","image : 1_1_d.nii\n","original shape: (256, 256, 30) resample shape: (256, 187, 256)\n","image : 2_1_d.nii\n","original shape: (256, 256, 30) resample shape: (256, 187, 256)\n","image : 3_d (1).nii\n","original shape: (360, 320, 36) resample shape: (360, 351, 360)\n","image : 3_d (4).nii\n","original shape: (360, 320, 36) resample shape: (360, 351, 360)\n","mask : 4_12_m_183819.nii.gz\n","original shape: (256, 256, 34) resample shape: (256, 122, 257)\n","mask : 4_6_m_163930.nii.gz\n","original shape: (256, 256, 30) resample shape: (256, 108, 257)\n","mask : 1_1_m.nii\n","original shape: (256, 256, 30) resample shape: (256, 187, 257)\n","mask : 2_1_m.nii\n","original shape: (256, 256, 30) resample shape: (256, 187, 257)\n","mask : 3_m (1).nii.gz\n","original shape: (360, 320, 36) resample shape: (360, 352, 360)\n","mask : 3_m (4).nii\n"]}],"source":["ort = orientation(t1_path = args.train_t1w)\n","if ort.check() == False:\n","    args.train_t1w = ort.orient()\n","\n","ort.path = args.train_msk\n","if ort.check() == False:\n","    args.train_msk = ort.orient(mode='mask')\n","\n","ort.path = args.validate_t1w\n","if ort.check() == False:\n","    args.validate_t1w = ort.orient()\n","    \n","ort.path = args.validate_msk\n","if ort.check() == False:\n","    args.validate_msk = ort.orient(mode='mask')"]},{"cell_type":"markdown","metadata":{"id":"SST9-R2NbLPQ"},"source":["### Part 4: Train the Model & Save it"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"adgScnWzbLPQ","scrolled":true,"outputId":"22a2e07e-28a5-487a-a2c9-fc61322cba9c","executionInfo":{"status":"ok","timestamp":1647723334232,"user_tz":-480,"elapsed":15550343,"user":{"displayName":"胡浩宇","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06402011815971394657"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["===================================Training Model===================================\n","new_1_6_d.nii\n","new_1_6_m.nii\n","new_1_7_d.nii\n","new_1_7_m.nii\n","new_2_5_d.nii\n","new_2_5_m.nii\n","new_2_6_d.nii\n","new_2_6_m.nii\n","new_2_7_d.nii\n","new_2_7_m.nii\n","new_3_d (2).nii\n","new_3_m (2).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (3).nii\n","new_3_m (3).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (7).nii\n","new_3_m (7).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (8).nii\n","new_3_m (8).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (9).nii\n","new_3_m (9).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_4_10_d_163526.nii\n","new_4_10_m_163526.nii.gz\n","new_4_11_d_084336.nii\n","new_4_11_m_084336.nii.gz\n","new_4_7_d_184422.nii\n","new_4_7_m_184422.nii.gz\n","new_4_8_d_143412.nii\n","new_4_8_m_143412.nii.gz\n","new_4_9_d_155347.nii\n","new_4_9_m_155347.nii.gz\n","Origin Dice: 0.5081 +/- 0.2542\n","Begin Epoch 0\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tEpoch:00 [000/005 (0000/0693)]\tLoss Ss: 0.972990\n","\tEpoch:00 [000/005 (0020/0693)]\tLoss Ss: 0.424823\n","\tEpoch:00 [000/005 (0040/0693)]\tLoss Ss: 0.327961\n","\tEpoch:00 [000/005 (0060/0693)]\tLoss Ss: 0.140530\n","\tEpoch:00 [000/005 (0080/0693)]\tLoss Ss: 0.182452\n","\tEpoch:00 [000/005 (0100/0693)]\tLoss Ss: 0.074634\n","\tEpoch:00 [000/005 (0120/0693)]\tLoss Ss: 0.057483\n","\tEpoch:00 [000/005 (0140/0693)]\tLoss Ss: 0.125943\n","\tEpoch:00 [000/005 (0160/0693)]\tLoss Ss: 0.068048\n","\tEpoch:00 [000/005 (0180/0693)]\tLoss Ss: 0.082251\n","\tEpoch:00 [000/005 (0200/0693)]\tLoss Ss: 0.050789\n","\tEpoch:00 [000/005 (0220/0693)]\tLoss Ss: 0.061721\n","\tEpoch:00 [000/005 (0240/0693)]\tLoss Ss: 0.050418\n","\tEpoch:00 [000/005 (0260/0693)]\tLoss Ss: 0.090374\n","\tEpoch:00 [000/005 (0280/0693)]\tLoss Ss: 0.119776\n","\tEpoch:00 [000/005 (0300/0693)]\tLoss Ss: 0.063973\n","\tEpoch:00 [000/005 (0320/0693)]\tLoss Ss: 0.088713\n","\tEpoch:00 [000/005 (0340/0693)]\tLoss Ss: 0.044480\n","\tEpoch:00 [000/005 (0360/0693)]\tLoss Ss: 0.072348\n","\tEpoch:00 [000/005 (0380/0693)]\tLoss Ss: 0.046488\n","\tEpoch:00 [000/005 (0400/0693)]\tLoss Ss: 0.077564\n","\tEpoch:00 [000/005 (0420/0693)]\tLoss Ss: 0.072613\n","\tEpoch:00 [000/005 (0440/0693)]\tLoss Ss: 0.035644\n","\tEpoch:00 [000/005 (0460/0693)]\tLoss Ss: 0.040900\n","\tEpoch:00 [000/005 (0480/0693)]\tLoss Ss: 0.059917\n","\tEpoch:00 [000/005 (0500/0693)]\tLoss Ss: 0.029409\n","\tEpoch:00 [000/005 (0520/0693)]\tLoss Ss: 0.093564\n","\tEpoch:00 [000/005 (0540/0693)]\tLoss Ss: 0.047278\n","\tEpoch:00 [000/005 (0560/0693)]\tLoss Ss: 0.056854\n","\tEpoch:00 [000/005 (0580/0693)]\tLoss Ss: 0.035817\n","\tEpoch:00 [000/005 (0600/0693)]\tLoss Ss: 0.049790\n","\tEpoch:00 [000/005 (0620/0693)]\tLoss Ss: 0.050635\n","\tEpoch:00 [000/005 (0640/0693)]\tLoss Ss: 0.052126\n","\tEpoch:00 [000/005 (0660/0693)]\tLoss Ss: 0.040017\n","\tEpoch:00 [000/005 (0680/0693)]\tLoss Ss: 0.038912\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:00 [001/005 (0000/0755)]\tLoss Ss: 0.275101\n","\tEpoch:00 [001/005 (0020/0755)]\tLoss Ss: 0.253684\n","\tEpoch:00 [001/005 (0040/0755)]\tLoss Ss: 0.222353\n","\tEpoch:00 [001/005 (0060/0755)]\tLoss Ss: 0.417945\n","\tEpoch:00 [001/005 (0080/0755)]\tLoss Ss: 0.201162\n","\tEpoch:00 [001/005 (0100/0755)]\tLoss Ss: 0.158406\n","\tEpoch:00 [001/005 (0120/0755)]\tLoss Ss: 0.091589\n","\tEpoch:00 [001/005 (0140/0755)]\tLoss Ss: 0.089605\n","\tEpoch:00 [001/005 (0160/0755)]\tLoss Ss: 0.156790\n","\tEpoch:00 [001/005 (0180/0755)]\tLoss Ss: 0.095152\n","\tEpoch:00 [001/005 (0200/0755)]\tLoss Ss: 0.077092\n","\tEpoch:00 [001/005 (0220/0755)]\tLoss Ss: 0.081601\n","\tEpoch:00 [001/005 (0240/0755)]\tLoss Ss: 0.060678\n","\tEpoch:00 [001/005 (0260/0755)]\tLoss Ss: 0.047732\n","\tEpoch:00 [001/005 (0280/0755)]\tLoss Ss: 0.059225\n","\tEpoch:00 [001/005 (0300/0755)]\tLoss Ss: 0.065364\n","\tEpoch:00 [001/005 (0320/0755)]\tLoss Ss: 0.114882\n","\tEpoch:00 [001/005 (0340/0755)]\tLoss Ss: 0.066255\n","\tEpoch:00 [001/005 (0360/0755)]\tLoss Ss: 0.076075\n","\tEpoch:00 [001/005 (0380/0755)]\tLoss Ss: 0.069229\n","\tEpoch:00 [001/005 (0400/0755)]\tLoss Ss: 0.056048\n","\tEpoch:00 [001/005 (0420/0755)]\tLoss Ss: 0.065238\n","\tEpoch:00 [001/005 (0440/0755)]\tLoss Ss: 0.047385\n","\tEpoch:00 [001/005 (0460/0755)]\tLoss Ss: 0.027812\n","\tEpoch:00 [001/005 (0480/0755)]\tLoss Ss: 0.066075\n","\tEpoch:00 [001/005 (0500/0755)]\tLoss Ss: 0.043274\n","\tEpoch:00 [001/005 (0520/0755)]\tLoss Ss: 0.057977\n","\tEpoch:00 [001/005 (0540/0755)]\tLoss Ss: 0.028012\n","\tEpoch:00 [001/005 (0560/0755)]\tLoss Ss: 0.064886\n","\tEpoch:00 [001/005 (0580/0755)]\tLoss Ss: 0.029784\n","\tEpoch:00 [001/005 (0600/0755)]\tLoss Ss: 0.058988\n","\tEpoch:00 [001/005 (0620/0755)]\tLoss Ss: 0.041301\n","\tEpoch:00 [001/005 (0640/0755)]\tLoss Ss: 0.056738\n","\tEpoch:00 [001/005 (0660/0755)]\tLoss Ss: 0.047614\n","\tEpoch:00 [001/005 (0680/0755)]\tLoss Ss: 0.070247\n","\tEpoch:00 [001/005 (0700/0755)]\tLoss Ss: 0.037050\n","\tEpoch:00 [001/005 (0720/0755)]\tLoss Ss: 0.033403\n","\tEpoch:00 [001/005 (0740/0755)]\tLoss Ss: 0.043692\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:00 [002/005 (0000/0755)]\tLoss Ss: 0.034449\n","\tEpoch:00 [002/005 (0020/0755)]\tLoss Ss: 0.014454\n","\tEpoch:00 [002/005 (0040/0755)]\tLoss Ss: 0.030710\n","\tEpoch:00 [002/005 (0060/0755)]\tLoss Ss: 0.037479\n","\tEpoch:00 [002/005 (0080/0755)]\tLoss Ss: 0.033224\n","\tEpoch:00 [002/005 (0100/0755)]\tLoss Ss: 0.028228\n","\tEpoch:00 [002/005 (0120/0755)]\tLoss Ss: 0.033560\n","\tEpoch:00 [002/005 (0140/0755)]\tLoss Ss: 0.029662\n","\tEpoch:00 [002/005 (0160/0755)]\tLoss Ss: 0.041732\n","\tEpoch:00 [002/005 (0180/0755)]\tLoss Ss: 0.037844\n","\tEpoch:00 [002/005 (0200/0755)]\tLoss Ss: 0.057560\n","\tEpoch:00 [002/005 (0220/0755)]\tLoss Ss: 0.035967\n","\tEpoch:00 [002/005 (0240/0755)]\tLoss Ss: 0.040015\n","\tEpoch:00 [002/005 (0260/0755)]\tLoss Ss: 0.104989\n","\tEpoch:00 [002/005 (0280/0755)]\tLoss Ss: 0.022709\n","\tEpoch:00 [002/005 (0300/0755)]\tLoss Ss: 0.026297\n","\tEpoch:00 [002/005 (0320/0755)]\tLoss Ss: 0.033492\n","\tEpoch:00 [002/005 (0340/0755)]\tLoss Ss: 0.035266\n","\tEpoch:00 [002/005 (0360/0755)]\tLoss Ss: 0.028664\n","\tEpoch:00 [002/005 (0380/0755)]\tLoss Ss: 0.055237\n","\tEpoch:00 [002/005 (0400/0755)]\tLoss Ss: 0.047499\n","\tEpoch:00 [002/005 (0420/0755)]\tLoss Ss: 0.027432\n","\tEpoch:00 [002/005 (0440/0755)]\tLoss Ss: 0.042781\n","\tEpoch:00 [002/005 (0460/0755)]\tLoss Ss: 0.034401\n","\tEpoch:00 [002/005 (0480/0755)]\tLoss Ss: 0.030105\n","\tEpoch:00 [002/005 (0500/0755)]\tLoss Ss: 0.033786\n","\tEpoch:00 [002/005 (0520/0755)]\tLoss Ss: 0.020674\n","\tEpoch:00 [002/005 (0540/0755)]\tLoss Ss: 0.038270\n","\tEpoch:00 [002/005 (0560/0755)]\tLoss Ss: 0.024872\n","\tEpoch:00 [002/005 (0580/0755)]\tLoss Ss: 0.024738\n","\tEpoch:00 [002/005 (0600/0755)]\tLoss Ss: 0.025018\n","\tEpoch:00 [002/005 (0620/0755)]\tLoss Ss: 0.031371\n","\tEpoch:00 [002/005 (0640/0755)]\tLoss Ss: 0.025721\n","\tEpoch:00 [002/005 (0660/0755)]\tLoss Ss: 0.025971\n","\tEpoch:00 [002/005 (0680/0755)]\tLoss Ss: 0.020592\n","\tEpoch:00 [002/005 (0700/0755)]\tLoss Ss: 0.031218\n","\tEpoch:00 [002/005 (0720/0755)]\tLoss Ss: 0.024206\n","\tEpoch:00 [002/005 (0740/0755)]\tLoss Ss: 0.037788\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tEpoch:00 [003/005 (0000/0614)]\tLoss Ss: 0.938895\n","\tEpoch:00 [003/005 (0020/0614)]\tLoss Ss: 0.266882\n","\tEpoch:00 [003/005 (0040/0614)]\tLoss Ss: 0.096222\n","\tEpoch:00 [003/005 (0060/0614)]\tLoss Ss: 0.107576\n","\tEpoch:00 [003/005 (0080/0614)]\tLoss Ss: 0.082866\n","\tEpoch:00 [003/005 (0100/0614)]\tLoss Ss: 0.044947\n","\tEpoch:00 [003/005 (0120/0614)]\tLoss Ss: 0.028593\n","\tEpoch:00 [003/005 (0140/0614)]\tLoss Ss: 0.050171\n","\tEpoch:00 [003/005 (0160/0614)]\tLoss Ss: 0.026315\n","\tEpoch:00 [003/005 (0180/0614)]\tLoss Ss: 0.031175\n","\tEpoch:00 [003/005 (0200/0614)]\tLoss Ss: 0.030997\n","\tEpoch:00 [003/005 (0220/0614)]\tLoss Ss: 0.041467\n","\tEpoch:00 [003/005 (0240/0614)]\tLoss Ss: 0.052770\n","\tEpoch:00 [003/005 (0260/0614)]\tLoss Ss: 0.044750\n","\tEpoch:00 [003/005 (0280/0614)]\tLoss Ss: 0.027313\n","\tEpoch:00 [003/005 (0300/0614)]\tLoss Ss: 0.031910\n","\tEpoch:00 [003/005 (0320/0614)]\tLoss Ss: 0.024517\n","\tEpoch:00 [003/005 (0340/0614)]\tLoss Ss: 0.021855\n","\tEpoch:00 [003/005 (0360/0614)]\tLoss Ss: 0.027341\n","\tEpoch:00 [003/005 (0380/0614)]\tLoss Ss: 0.045594\n","\tEpoch:00 [003/005 (0400/0614)]\tLoss Ss: 0.030975\n","\tEpoch:00 [003/005 (0420/0614)]\tLoss Ss: 0.027851\n","\tEpoch:00 [003/005 (0440/0614)]\tLoss Ss: 0.027945\n","\tEpoch:00 [003/005 (0460/0614)]\tLoss Ss: 0.023114\n","\tEpoch:00 [003/005 (0480/0614)]\tLoss Ss: 0.034555\n","\tEpoch:00 [003/005 (0500/0614)]\tLoss Ss: 0.018721\n","\tEpoch:00 [003/005 (0520/0614)]\tLoss Ss: 0.024027\n","\tEpoch:00 [003/005 (0540/0614)]\tLoss Ss: 0.014558\n","\tEpoch:00 [003/005 (0560/0614)]\tLoss Ss: 0.020329\n","\tEpoch:00 [003/005 (0580/0614)]\tLoss Ss: 0.021526\n","\tEpoch:00 [003/005 (0600/0614)]\tLoss Ss: 0.019221\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tEpoch:00 [004/005 (0000/0588)]\tLoss Ss: 0.020780\n","\tEpoch:00 [004/005 (0020/0588)]\tLoss Ss: 0.024363\n","\tEpoch:00 [004/005 (0040/0588)]\tLoss Ss: 0.015781\n","\tEpoch:00 [004/005 (0060/0588)]\tLoss Ss: 0.011648\n","\tEpoch:00 [004/005 (0080/0588)]\tLoss Ss: 0.023014\n","\tEpoch:00 [004/005 (0100/0588)]\tLoss Ss: 0.021685\n","\tEpoch:00 [004/005 (0120/0588)]\tLoss Ss: 0.023879\n","\tEpoch:00 [004/005 (0140/0588)]\tLoss Ss: 0.016222\n","\tEpoch:00 [004/005 (0160/0588)]\tLoss Ss: 0.014649\n","\tEpoch:00 [004/005 (0180/0588)]\tLoss Ss: 0.013059\n","\tEpoch:00 [004/005 (0200/0588)]\tLoss Ss: 0.020804\n","\tEpoch:00 [004/005 (0220/0588)]\tLoss Ss: 0.009509\n","\tEpoch:00 [004/005 (0240/0588)]\tLoss Ss: 0.019449\n","\tEpoch:00 [004/005 (0260/0588)]\tLoss Ss: 0.026718\n","\tEpoch:00 [004/005 (0280/0588)]\tLoss Ss: 0.011549\n","\tEpoch:00 [004/005 (0300/0588)]\tLoss Ss: 0.010910\n","\tEpoch:00 [004/005 (0320/0588)]\tLoss Ss: 0.014592\n","\tEpoch:00 [004/005 (0340/0588)]\tLoss Ss: 0.007727\n","\tEpoch:00 [004/005 (0360/0588)]\tLoss Ss: 0.013390\n","\tEpoch:00 [004/005 (0380/0588)]\tLoss Ss: 0.013356\n","\tEpoch:00 [004/005 (0400/0588)]\tLoss Ss: 0.009193\n","\tEpoch:00 [004/005 (0420/0588)]\tLoss Ss: 0.013532\n","\tEpoch:00 [004/005 (0440/0588)]\tLoss Ss: 0.012741\n","\tEpoch:00 [004/005 (0460/0588)]\tLoss Ss: 0.011824\n","\tEpoch:00 [004/005 (0480/0588)]\tLoss Ss: 0.007667\n","\tEpoch:00 [004/005 (0500/0588)]\tLoss Ss: 0.016717\n","\tEpoch:00 [004/005 (0520/0588)]\tLoss Ss: 0.014564\n","\tEpoch:00 [004/005 (0540/0588)]\tLoss Ss: 0.012639\n","\tEpoch:00 [004/005 (0560/0588)]\tLoss Ss: 0.015525\n","\tEpoch:00 [004/005 (0580/0588)]\tLoss Ss: 0.013595\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tEpoch:00 [005/005 (0000/0693)]\tLoss Ss: 0.076418\n","\tEpoch:00 [005/005 (0020/0693)]\tLoss Ss: 0.067726\n","\tEpoch:00 [005/005 (0040/0693)]\tLoss Ss: 0.028208\n","\tEpoch:00 [005/005 (0060/0693)]\tLoss Ss: 0.054847\n","\tEpoch:00 [005/005 (0080/0693)]\tLoss Ss: 0.088394\n","\tEpoch:00 [005/005 (0100/0693)]\tLoss Ss: 0.041720\n","\tEpoch:00 [005/005 (0120/0693)]\tLoss Ss: 0.033906\n","\tEpoch:00 [005/005 (0140/0693)]\tLoss Ss: 0.067639\n","\tEpoch:00 [005/005 (0160/0693)]\tLoss Ss: 0.033058\n","\tEpoch:00 [005/005 (0180/0693)]\tLoss Ss: 0.025509\n","\tEpoch:00 [005/005 (0200/0693)]\tLoss Ss: 0.075138\n","\tEpoch:00 [005/005 (0220/0693)]\tLoss Ss: 0.019477\n","\tEpoch:00 [005/005 (0240/0693)]\tLoss Ss: 0.037068\n","\tEpoch:00 [005/005 (0260/0693)]\tLoss Ss: 0.122304\n","\tEpoch:00 [005/005 (0280/0693)]\tLoss Ss: 0.021594\n","\tEpoch:00 [005/005 (0300/0693)]\tLoss Ss: 0.041250\n","\tEpoch:00 [005/005 (0320/0693)]\tLoss Ss: 0.030953\n","\tEpoch:00 [005/005 (0340/0693)]\tLoss Ss: 0.040247\n","\tEpoch:00 [005/005 (0360/0693)]\tLoss Ss: 0.072081\n","\tEpoch:00 [005/005 (0380/0693)]\tLoss Ss: 0.032678\n","\tEpoch:00 [005/005 (0400/0693)]\tLoss Ss: 0.031878\n","\tEpoch:00 [005/005 (0420/0693)]\tLoss Ss: 0.025689\n","\tEpoch:00 [005/005 (0440/0693)]\tLoss Ss: 0.027863\n","\tEpoch:00 [005/005 (0460/0693)]\tLoss Ss: 0.035078\n","\tEpoch:00 [005/005 (0480/0693)]\tLoss Ss: 0.027180\n","\tEpoch:00 [005/005 (0500/0693)]\tLoss Ss: 0.024892\n","\tEpoch:00 [005/005 (0520/0693)]\tLoss Ss: 0.023963\n","\tEpoch:00 [005/005 (0540/0693)]\tLoss Ss: 0.017745\n","\tEpoch:00 [005/005 (0560/0693)]\tLoss Ss: 0.023164\n","\tEpoch:00 [005/005 (0580/0693)]\tLoss Ss: 0.025179\n","\tEpoch:00 [005/005 (0600/0693)]\tLoss Ss: 0.023823\n","\tEpoch:00 [005/005 (0620/0693)]\tLoss Ss: 0.026034\n","\tEpoch:00 [005/005 (0640/0693)]\tLoss Ss: 0.018569\n","\tEpoch:00 [005/005 (0660/0693)]\tLoss Ss: 0.024823\n","\tEpoch:00 [005/005 (0680/0693)]\tLoss Ss: 0.043200\n","Now train the rotated image\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:00 [000/005 (0000/0755)]\tLoss Ss: 0.068972\n","\tRotated_Epoch:00 [000/005 (0020/0755)]\tLoss Ss: 0.058909\n","\tRotated_Epoch:00 [000/005 (0040/0755)]\tLoss Ss: 0.049610\n","\tRotated_Epoch:00 [000/005 (0060/0755)]\tLoss Ss: 0.031299\n","\tRotated_Epoch:00 [000/005 (0080/0755)]\tLoss Ss: 0.105010\n","\tRotated_Epoch:00 [000/005 (0100/0755)]\tLoss Ss: 0.065514\n","\tRotated_Epoch:00 [000/005 (0120/0755)]\tLoss Ss: 0.034577\n","\tRotated_Epoch:00 [000/005 (0140/0755)]\tLoss Ss: 0.050136\n","\tRotated_Epoch:00 [000/005 (0160/0755)]\tLoss Ss: 0.043690\n","\tRotated_Epoch:00 [000/005 (0180/0755)]\tLoss Ss: 0.042354\n","\tRotated_Epoch:00 [000/005 (0200/0755)]\tLoss Ss: 0.036586\n","\tRotated_Epoch:00 [000/005 (0220/0755)]\tLoss Ss: 0.053314\n","\tRotated_Epoch:00 [000/005 (0240/0755)]\tLoss Ss: 0.037285\n","\tRotated_Epoch:00 [000/005 (0260/0755)]\tLoss Ss: 0.127033\n","\tRotated_Epoch:00 [000/005 (0280/0755)]\tLoss Ss: 0.036934\n","\tRotated_Epoch:00 [000/005 (0300/0755)]\tLoss Ss: 0.043608\n","\tRotated_Epoch:00 [000/005 (0320/0755)]\tLoss Ss: 0.038084\n","\tRotated_Epoch:00 [000/005 (0340/0755)]\tLoss Ss: 0.037115\n","\tRotated_Epoch:00 [000/005 (0360/0755)]\tLoss Ss: 0.043880\n","\tRotated_Epoch:00 [000/005 (0380/0755)]\tLoss Ss: 0.038125\n","\tRotated_Epoch:00 [000/005 (0400/0755)]\tLoss Ss: 0.027872\n","\tRotated_Epoch:00 [000/005 (0420/0755)]\tLoss Ss: 0.057282\n","\tRotated_Epoch:00 [000/005 (0440/0755)]\tLoss Ss: 0.023770\n","\tRotated_Epoch:00 [000/005 (0460/0755)]\tLoss Ss: 0.044231\n","\tRotated_Epoch:00 [000/005 (0480/0755)]\tLoss Ss: 0.041048\n","\tRotated_Epoch:00 [000/005 (0500/0755)]\tLoss Ss: 0.035816\n","\tRotated_Epoch:00 [000/005 (0520/0755)]\tLoss Ss: 0.020914\n","\tRotated_Epoch:00 [000/005 (0540/0755)]\tLoss Ss: 0.020617\n","\tRotated_Epoch:00 [000/005 (0560/0755)]\tLoss Ss: 0.039994\n","\tRotated_Epoch:00 [000/005 (0580/0755)]\tLoss Ss: 0.029693\n","\tRotated_Epoch:00 [000/005 (0600/0755)]\tLoss Ss: 0.045040\n","\tRotated_Epoch:00 [000/005 (0620/0755)]\tLoss Ss: 0.028190\n","\tRotated_Epoch:00 [000/005 (0640/0755)]\tLoss Ss: 0.040264\n","\tRotated_Epoch:00 [000/005 (0660/0755)]\tLoss Ss: 0.045512\n","\tRotated_Epoch:00 [000/005 (0680/0755)]\tLoss Ss: 0.032987\n","\tRotated_Epoch:00 [000/005 (0700/0755)]\tLoss Ss: 0.027984\n","\tRotated_Epoch:00 [000/005 (0720/0755)]\tLoss Ss: 0.028816\n","\tRotated_Epoch:00 [000/005 (0740/0755)]\tLoss Ss: 0.033945\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tRotated_Epoch:00 [001/005 (0000/0693)]\tLoss Ss: 0.044405\n","\tRotated_Epoch:00 [001/005 (0020/0693)]\tLoss Ss: 0.086633\n","\tRotated_Epoch:00 [001/005 (0040/0693)]\tLoss Ss: 0.041953\n","\tRotated_Epoch:00 [001/005 (0060/0693)]\tLoss Ss: 0.043995\n","\tRotated_Epoch:00 [001/005 (0080/0693)]\tLoss Ss: 0.043456\n","\tRotated_Epoch:00 [001/005 (0100/0693)]\tLoss Ss: 0.047152\n","\tRotated_Epoch:00 [001/005 (0120/0693)]\tLoss Ss: 0.075608\n","\tRotated_Epoch:00 [001/005 (0140/0693)]\tLoss Ss: 0.024665\n","\tRotated_Epoch:00 [001/005 (0160/0693)]\tLoss Ss: 0.045947\n","\tRotated_Epoch:00 [001/005 (0180/0693)]\tLoss Ss: 0.041981\n","\tRotated_Epoch:00 [001/005 (0200/0693)]\tLoss Ss: 0.036551\n","\tRotated_Epoch:00 [001/005 (0220/0693)]\tLoss Ss: 0.033753\n","\tRotated_Epoch:00 [001/005 (0240/0693)]\tLoss Ss: 0.025086\n","\tRotated_Epoch:00 [001/005 (0260/0693)]\tLoss Ss: 0.039990\n","\tRotated_Epoch:00 [001/005 (0280/0693)]\tLoss Ss: 0.029302\n","\tRotated_Epoch:00 [001/005 (0300/0693)]\tLoss Ss: 0.028271\n","\tRotated_Epoch:00 [001/005 (0320/0693)]\tLoss Ss: 0.041233\n","\tRotated_Epoch:00 [001/005 (0340/0693)]\tLoss Ss: 0.029596\n","\tRotated_Epoch:00 [001/005 (0360/0693)]\tLoss Ss: 0.035368\n","\tRotated_Epoch:00 [001/005 (0380/0693)]\tLoss Ss: 0.032123\n","\tRotated_Epoch:00 [001/005 (0400/0693)]\tLoss Ss: 0.033761\n","\tRotated_Epoch:00 [001/005 (0420/0693)]\tLoss Ss: 0.028678\n","\tRotated_Epoch:00 [001/005 (0440/0693)]\tLoss Ss: 0.028599\n","\tRotated_Epoch:00 [001/005 (0460/0693)]\tLoss Ss: 0.035548\n","\tRotated_Epoch:00 [001/005 (0480/0693)]\tLoss Ss: 0.024269\n","\tRotated_Epoch:00 [001/005 (0500/0693)]\tLoss Ss: 0.034260\n","\tRotated_Epoch:00 [001/005 (0520/0693)]\tLoss Ss: 0.027574\n","\tRotated_Epoch:00 [001/005 (0540/0693)]\tLoss Ss: 0.047273\n","\tRotated_Epoch:00 [001/005 (0560/0693)]\tLoss Ss: 0.046821\n","\tRotated_Epoch:00 [001/005 (0580/0693)]\tLoss Ss: 0.058961\n","\tRotated_Epoch:00 [001/005 (0600/0693)]\tLoss Ss: 0.024172\n","\tRotated_Epoch:00 [001/005 (0620/0693)]\tLoss Ss: 0.026283\n","\tRotated_Epoch:00 [001/005 (0640/0693)]\tLoss Ss: 0.029992\n","\tRotated_Epoch:00 [001/005 (0660/0693)]\tLoss Ss: 0.026189\n","\tRotated_Epoch:00 [001/005 (0680/0693)]\tLoss Ss: 0.039711\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tRotated_Epoch:00 [002/005 (0000/0693)]\tLoss Ss: 0.033939\n","\tRotated_Epoch:00 [002/005 (0020/0693)]\tLoss Ss: 0.024846\n","\tRotated_Epoch:00 [002/005 (0040/0693)]\tLoss Ss: 0.028837\n","\tRotated_Epoch:00 [002/005 (0060/0693)]\tLoss Ss: 0.042601\n","\tRotated_Epoch:00 [002/005 (0080/0693)]\tLoss Ss: 0.026024\n","\tRotated_Epoch:00 [002/005 (0100/0693)]\tLoss Ss: 0.020076\n","\tRotated_Epoch:00 [002/005 (0120/0693)]\tLoss Ss: 0.029690\n","\tRotated_Epoch:00 [002/005 (0140/0693)]\tLoss Ss: 0.026880\n","\tRotated_Epoch:00 [002/005 (0160/0693)]\tLoss Ss: 0.023695\n","\tRotated_Epoch:00 [002/005 (0180/0693)]\tLoss Ss: 0.054576\n","\tRotated_Epoch:00 [002/005 (0200/0693)]\tLoss Ss: 0.032447\n","\tRotated_Epoch:00 [002/005 (0220/0693)]\tLoss Ss: 0.022759\n","\tRotated_Epoch:00 [002/005 (0240/0693)]\tLoss Ss: 0.031606\n","\tRotated_Epoch:00 [002/005 (0260/0693)]\tLoss Ss: 0.020608\n","\tRotated_Epoch:00 [002/005 (0280/0693)]\tLoss Ss: 0.029699\n","\tRotated_Epoch:00 [002/005 (0300/0693)]\tLoss Ss: 0.040465\n","\tRotated_Epoch:00 [002/005 (0320/0693)]\tLoss Ss: 0.033988\n","\tRotated_Epoch:00 [002/005 (0340/0693)]\tLoss Ss: 0.032327\n","\tRotated_Epoch:00 [002/005 (0360/0693)]\tLoss Ss: 0.020755\n","\tRotated_Epoch:00 [002/005 (0380/0693)]\tLoss Ss: 0.029125\n","\tRotated_Epoch:00 [002/005 (0400/0693)]\tLoss Ss: 0.021949\n","\tRotated_Epoch:00 [002/005 (0420/0693)]\tLoss Ss: 0.020968\n","\tRotated_Epoch:00 [002/005 (0440/0693)]\tLoss Ss: 0.029248\n","\tRotated_Epoch:00 [002/005 (0460/0693)]\tLoss Ss: 0.026603\n","\tRotated_Epoch:00 [002/005 (0480/0693)]\tLoss Ss: 0.020594\n","\tRotated_Epoch:00 [002/005 (0500/0693)]\tLoss Ss: 0.026378\n","\tRotated_Epoch:00 [002/005 (0520/0693)]\tLoss Ss: 0.025448\n","\tRotated_Epoch:00 [002/005 (0540/0693)]\tLoss Ss: 0.023485\n","\tRotated_Epoch:00 [002/005 (0560/0693)]\tLoss Ss: 0.026941\n","\tRotated_Epoch:00 [002/005 (0580/0693)]\tLoss Ss: 0.028014\n","\tRotated_Epoch:00 [002/005 (0600/0693)]\tLoss Ss: 0.026853\n","\tRotated_Epoch:00 [002/005 (0620/0693)]\tLoss Ss: 0.020954\n","\tRotated_Epoch:00 [002/005 (0640/0693)]\tLoss Ss: 0.020460\n","\tRotated_Epoch:00 [002/005 (0660/0693)]\tLoss Ss: 0.024845\n","\tRotated_Epoch:00 [002/005 (0680/0693)]\tLoss Ss: 0.026476\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tRotated_Epoch:00 [003/005 (0000/0588)]\tLoss Ss: 0.729872\n","\tRotated_Epoch:00 [003/005 (0020/0588)]\tLoss Ss: 0.721670\n","\tRotated_Epoch:00 [003/005 (0040/0588)]\tLoss Ss: 0.318266\n","\tRotated_Epoch:00 [003/005 (0060/0588)]\tLoss Ss: 0.448476\n","\tRotated_Epoch:00 [003/005 (0080/0588)]\tLoss Ss: 0.333419\n","\tRotated_Epoch:00 [003/005 (0100/0588)]\tLoss Ss: 0.229745\n","\tRotated_Epoch:00 [003/005 (0120/0588)]\tLoss Ss: 0.330401\n","\tRotated_Epoch:00 [003/005 (0140/0588)]\tLoss Ss: 0.212954\n","\tRotated_Epoch:00 [003/005 (0160/0588)]\tLoss Ss: 0.146446\n","\tRotated_Epoch:00 [003/005 (0180/0588)]\tLoss Ss: 0.167828\n","\tRotated_Epoch:00 [003/005 (0200/0588)]\tLoss Ss: 0.204316\n","\tRotated_Epoch:00 [003/005 (0220/0588)]\tLoss Ss: 0.168052\n","\tRotated_Epoch:00 [003/005 (0240/0588)]\tLoss Ss: 0.146978\n","\tRotated_Epoch:00 [003/005 (0260/0588)]\tLoss Ss: 0.155078\n","\tRotated_Epoch:00 [003/005 (0280/0588)]\tLoss Ss: 0.198372\n","\tRotated_Epoch:00 [003/005 (0300/0588)]\tLoss Ss: 0.145926\n","\tRotated_Epoch:00 [003/005 (0320/0588)]\tLoss Ss: 0.139750\n","\tRotated_Epoch:00 [003/005 (0340/0588)]\tLoss Ss: 0.203808\n","\tRotated_Epoch:00 [003/005 (0360/0588)]\tLoss Ss: 0.230296\n","\tRotated_Epoch:00 [003/005 (0380/0588)]\tLoss Ss: 0.166127\n","\tRotated_Epoch:00 [003/005 (0400/0588)]\tLoss Ss: 0.132139\n","\tRotated_Epoch:00 [003/005 (0420/0588)]\tLoss Ss: 0.209263\n","\tRotated_Epoch:00 [003/005 (0440/0588)]\tLoss Ss: 0.138226\n","\tRotated_Epoch:00 [003/005 (0460/0588)]\tLoss Ss: 0.117625\n","\tRotated_Epoch:00 [003/005 (0480/0588)]\tLoss Ss: 0.150652\n","\tRotated_Epoch:00 [003/005 (0500/0588)]\tLoss Ss: 0.095308\n","\tRotated_Epoch:00 [003/005 (0520/0588)]\tLoss Ss: 0.154527\n","\tRotated_Epoch:00 [003/005 (0540/0588)]\tLoss Ss: 0.109428\n","\tRotated_Epoch:00 [003/005 (0560/0588)]\tLoss Ss: 0.098715\n","\tRotated_Epoch:00 [003/005 (0580/0588)]\tLoss Ss: 0.122239\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tRotated_Epoch:00 [004/005 (0000/0614)]\tLoss Ss: 0.091278\n","\tRotated_Epoch:00 [004/005 (0020/0614)]\tLoss Ss: 0.092726\n","\tRotated_Epoch:00 [004/005 (0040/0614)]\tLoss Ss: 0.067919\n","\tRotated_Epoch:00 [004/005 (0060/0614)]\tLoss Ss: 0.059287\n","\tRotated_Epoch:00 [004/005 (0080/0614)]\tLoss Ss: 0.052517\n","\tRotated_Epoch:00 [004/005 (0100/0614)]\tLoss Ss: 0.062124\n","\tRotated_Epoch:00 [004/005 (0120/0614)]\tLoss Ss: 0.059196\n","\tRotated_Epoch:00 [004/005 (0140/0614)]\tLoss Ss: 0.058790\n","\tRotated_Epoch:00 [004/005 (0160/0614)]\tLoss Ss: 0.047227\n","\tRotated_Epoch:00 [004/005 (0180/0614)]\tLoss Ss: 0.028492\n","\tRotated_Epoch:00 [004/005 (0200/0614)]\tLoss Ss: 0.048561\n","\tRotated_Epoch:00 [004/005 (0220/0614)]\tLoss Ss: 0.035410\n","\tRotated_Epoch:00 [004/005 (0240/0614)]\tLoss Ss: 0.030254\n","\tRotated_Epoch:00 [004/005 (0260/0614)]\tLoss Ss: 0.043063\n","\tRotated_Epoch:00 [004/005 (0280/0614)]\tLoss Ss: 0.044904\n","\tRotated_Epoch:00 [004/005 (0300/0614)]\tLoss Ss: 0.028501\n","\tRotated_Epoch:00 [004/005 (0320/0614)]\tLoss Ss: 0.026868\n","\tRotated_Epoch:00 [004/005 (0340/0614)]\tLoss Ss: 0.028466\n","\tRotated_Epoch:00 [004/005 (0360/0614)]\tLoss Ss: 0.028979\n","\tRotated_Epoch:00 [004/005 (0380/0614)]\tLoss Ss: 0.031692\n","\tRotated_Epoch:00 [004/005 (0400/0614)]\tLoss Ss: 0.031940\n","\tRotated_Epoch:00 [004/005 (0420/0614)]\tLoss Ss: 0.023565\n","\tRotated_Epoch:00 [004/005 (0440/0614)]\tLoss Ss: 0.027423\n","\tRotated_Epoch:00 [004/005 (0460/0614)]\tLoss Ss: 0.020558\n","\tRotated_Epoch:00 [004/005 (0480/0614)]\tLoss Ss: 0.027532\n","\tRotated_Epoch:00 [004/005 (0500/0614)]\tLoss Ss: 0.027048\n","\tRotated_Epoch:00 [004/005 (0520/0614)]\tLoss Ss: 0.025671\n","\tRotated_Epoch:00 [004/005 (0540/0614)]\tLoss Ss: 0.026010\n","\tRotated_Epoch:00 [004/005 (0560/0614)]\tLoss Ss: 0.024781\n","\tRotated_Epoch:00 [004/005 (0580/0614)]\tLoss Ss: 0.019903\n","\tRotated_Epoch:00 [004/005 (0600/0614)]\tLoss Ss: 0.021388\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:00 [005/005 (0000/0755)]\tLoss Ss: 0.440295\n","\tRotated_Epoch:00 [005/005 (0020/0755)]\tLoss Ss: 0.503108\n","\tRotated_Epoch:00 [005/005 (0040/0755)]\tLoss Ss: 0.593960\n","\tRotated_Epoch:00 [005/005 (0060/0755)]\tLoss Ss: 0.407852\n","\tRotated_Epoch:00 [005/005 (0080/0755)]\tLoss Ss: 0.209275\n","\tRotated_Epoch:00 [005/005 (0100/0755)]\tLoss Ss: 0.182411\n","\tRotated_Epoch:00 [005/005 (0120/0755)]\tLoss Ss: 0.386112\n","\tRotated_Epoch:00 [005/005 (0140/0755)]\tLoss Ss: 0.288808\n","\tRotated_Epoch:00 [005/005 (0160/0755)]\tLoss Ss: 0.223070\n","\tRotated_Epoch:00 [005/005 (0180/0755)]\tLoss Ss: 0.210161\n","\tRotated_Epoch:00 [005/005 (0200/0755)]\tLoss Ss: 0.197671\n","\tRotated_Epoch:00 [005/005 (0220/0755)]\tLoss Ss: 0.295651\n","\tRotated_Epoch:00 [005/005 (0240/0755)]\tLoss Ss: 0.148927\n","\tRotated_Epoch:00 [005/005 (0260/0755)]\tLoss Ss: 0.216510\n","\tRotated_Epoch:00 [005/005 (0280/0755)]\tLoss Ss: 0.160649\n","\tRotated_Epoch:00 [005/005 (0300/0755)]\tLoss Ss: 0.192338\n","\tRotated_Epoch:00 [005/005 (0320/0755)]\tLoss Ss: 0.165619\n","\tRotated_Epoch:00 [005/005 (0340/0755)]\tLoss Ss: 0.130941\n","\tRotated_Epoch:00 [005/005 (0360/0755)]\tLoss Ss: 0.158252\n","\tRotated_Epoch:00 [005/005 (0380/0755)]\tLoss Ss: 0.173494\n","\tRotated_Epoch:00 [005/005 (0400/0755)]\tLoss Ss: 0.160051\n","\tRotated_Epoch:00 [005/005 (0420/0755)]\tLoss Ss: 0.152457\n","\tRotated_Epoch:00 [005/005 (0440/0755)]\tLoss Ss: 0.143811\n","\tRotated_Epoch:00 [005/005 (0460/0755)]\tLoss Ss: 0.133287\n","\tRotated_Epoch:00 [005/005 (0480/0755)]\tLoss Ss: 0.130059\n","\tRotated_Epoch:00 [005/005 (0500/0755)]\tLoss Ss: 0.249546\n","\tRotated_Epoch:00 [005/005 (0520/0755)]\tLoss Ss: 0.247935\n","\tRotated_Epoch:00 [005/005 (0540/0755)]\tLoss Ss: 0.143804\n","\tRotated_Epoch:00 [005/005 (0560/0755)]\tLoss Ss: 0.150413\n","\tRotated_Epoch:00 [005/005 (0580/0755)]\tLoss Ss: 0.151094\n","\tRotated_Epoch:00 [005/005 (0600/0755)]\tLoss Ss: 0.126586\n","\tRotated_Epoch:00 [005/005 (0620/0755)]\tLoss Ss: 0.125823\n","\tRotated_Epoch:00 [005/005 (0640/0755)]\tLoss Ss: 0.166487\n","\tRotated_Epoch:00 [005/005 (0660/0755)]\tLoss Ss: 0.153795\n","\tRotated_Epoch:00 [005/005 (0680/0755)]\tLoss Ss: 0.101476\n","\tRotated_Epoch:00 [005/005 (0700/0755)]\tLoss Ss: 0.169548\n","\tRotated_Epoch:00 [005/005 (0720/0755)]\tLoss Ss: 0.142022\n","\tRotated_Epoch:00 [005/005 (0740/0755)]\tLoss Ss: 0.136715\n","new_1_6_d.nii\n","new_1_6_m.nii\n","new_1_7_d.nii\n","new_1_7_m.nii\n","new_2_5_d.nii\n","new_2_5_m.nii\n","new_2_6_d.nii\n","new_2_6_m.nii\n","new_2_7_d.nii\n","new_2_7_m.nii\n","new_3_d (2).nii\n","new_3_m (2).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (3).nii\n","new_3_m (3).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (7).nii\n","new_3_m (7).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (8).nii\n","new_3_m (8).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (9).nii\n","new_3_m (9).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_4_10_d_163526.nii\n","new_4_10_m_163526.nii.gz\n","new_4_11_d_084336.nii\n","new_4_11_m_084336.nii.gz\n","new_4_7_d_184422.nii\n","new_4_7_m_184422.nii.gz\n","new_4_8_d_143412.nii\n","new_4_8_m_143412.nii.gz\n","new_4_9_d_155347.nii\n","new_4_9_m_155347.nii.gz\n","\tEpoch: 0; Dice: 0.7926 +/- 0.2155; Loss: 32.9633\n","Begin Epoch 1\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:01 [000/005 (0000/0755)]\tLoss Ss: 0.084965\n","\tEpoch:01 [000/005 (0020/0755)]\tLoss Ss: 0.087523\n","\tEpoch:01 [000/005 (0040/0755)]\tLoss Ss: 0.097166\n","\tEpoch:01 [000/005 (0060/0755)]\tLoss Ss: 0.070243\n","\tEpoch:01 [000/005 (0080/0755)]\tLoss Ss: 0.076473\n","\tEpoch:01 [000/005 (0100/0755)]\tLoss Ss: 0.065090\n","\tEpoch:01 [000/005 (0120/0755)]\tLoss Ss: 0.069663\n","\tEpoch:01 [000/005 (0140/0755)]\tLoss Ss: 0.072625\n","\tEpoch:01 [000/005 (0160/0755)]\tLoss Ss: 0.068130\n","\tEpoch:01 [000/005 (0180/0755)]\tLoss Ss: 0.057659\n","\tEpoch:01 [000/005 (0200/0755)]\tLoss Ss: 0.074874\n","\tEpoch:01 [000/005 (0220/0755)]\tLoss Ss: 0.075423\n","\tEpoch:01 [000/005 (0240/0755)]\tLoss Ss: 0.069045\n","\tEpoch:01 [000/005 (0260/0755)]\tLoss Ss: 0.059034\n","\tEpoch:01 [000/005 (0280/0755)]\tLoss Ss: 0.066422\n","\tEpoch:01 [000/005 (0300/0755)]\tLoss Ss: 0.058130\n","\tEpoch:01 [000/005 (0320/0755)]\tLoss Ss: 0.060860\n","\tEpoch:01 [000/005 (0340/0755)]\tLoss Ss: 0.062619\n","\tEpoch:01 [000/005 (0360/0755)]\tLoss Ss: 0.069419\n","\tEpoch:01 [000/005 (0380/0755)]\tLoss Ss: 0.054722\n","\tEpoch:01 [000/005 (0400/0755)]\tLoss Ss: 0.054111\n","\tEpoch:01 [000/005 (0420/0755)]\tLoss Ss: 0.043157\n","\tEpoch:01 [000/005 (0440/0755)]\tLoss Ss: 0.045333\n","\tEpoch:01 [000/005 (0460/0755)]\tLoss Ss: 0.052112\n","\tEpoch:01 [000/005 (0480/0755)]\tLoss Ss: 0.056705\n","\tEpoch:01 [000/005 (0500/0755)]\tLoss Ss: 0.045778\n","\tEpoch:01 [000/005 (0520/0755)]\tLoss Ss: 0.038807\n","\tEpoch:01 [000/005 (0540/0755)]\tLoss Ss: 0.069511\n","\tEpoch:01 [000/005 (0560/0755)]\tLoss Ss: 0.047348\n","\tEpoch:01 [000/005 (0580/0755)]\tLoss Ss: 0.034886\n","\tEpoch:01 [000/005 (0600/0755)]\tLoss Ss: 0.042914\n","\tEpoch:01 [000/005 (0620/0755)]\tLoss Ss: 0.044446\n","\tEpoch:01 [000/005 (0640/0755)]\tLoss Ss: 0.040046\n","\tEpoch:01 [000/005 (0660/0755)]\tLoss Ss: 0.044464\n","\tEpoch:01 [000/005 (0680/0755)]\tLoss Ss: 0.054229\n","\tEpoch:01 [000/005 (0700/0755)]\tLoss Ss: 0.036013\n","\tEpoch:01 [000/005 (0720/0755)]\tLoss Ss: 0.034420\n","\tEpoch:01 [000/005 (0740/0755)]\tLoss Ss: 0.026569\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tEpoch:01 [001/005 (0000/0693)]\tLoss Ss: 0.045801\n","\tEpoch:01 [001/005 (0020/0693)]\tLoss Ss: 0.026517\n","\tEpoch:01 [001/005 (0040/0693)]\tLoss Ss: 0.032080\n","\tEpoch:01 [001/005 (0060/0693)]\tLoss Ss: 0.032701\n","\tEpoch:01 [001/005 (0080/0693)]\tLoss Ss: 0.028777\n","\tEpoch:01 [001/005 (0100/0693)]\tLoss Ss: 0.026819\n","\tEpoch:01 [001/005 (0120/0693)]\tLoss Ss: 0.041141\n","\tEpoch:01 [001/005 (0140/0693)]\tLoss Ss: 0.035226\n","\tEpoch:01 [001/005 (0160/0693)]\tLoss Ss: 0.026358\n","\tEpoch:01 [001/005 (0180/0693)]\tLoss Ss: 0.033396\n","\tEpoch:01 [001/005 (0200/0693)]\tLoss Ss: 0.040039\n","\tEpoch:01 [001/005 (0220/0693)]\tLoss Ss: 0.024430\n","\tEpoch:01 [001/005 (0240/0693)]\tLoss Ss: 0.028619\n","\tEpoch:01 [001/005 (0260/0693)]\tLoss Ss: 0.030462\n","\tEpoch:01 [001/005 (0280/0693)]\tLoss Ss: 0.021340\n","\tEpoch:01 [001/005 (0300/0693)]\tLoss Ss: 0.027280\n","\tEpoch:01 [001/005 (0320/0693)]\tLoss Ss: 0.027991\n","\tEpoch:01 [001/005 (0340/0693)]\tLoss Ss: 0.036869\n","\tEpoch:01 [001/005 (0360/0693)]\tLoss Ss: 0.027984\n","\tEpoch:01 [001/005 (0380/0693)]\tLoss Ss: 0.030469\n","\tEpoch:01 [001/005 (0400/0693)]\tLoss Ss: 0.032941\n","\tEpoch:01 [001/005 (0420/0693)]\tLoss Ss: 0.024450\n","\tEpoch:01 [001/005 (0440/0693)]\tLoss Ss: 0.029780\n","\tEpoch:01 [001/005 (0460/0693)]\tLoss Ss: 0.019193\n","\tEpoch:01 [001/005 (0480/0693)]\tLoss Ss: 0.034574\n","\tEpoch:01 [001/005 (0500/0693)]\tLoss Ss: 0.020800\n","\tEpoch:01 [001/005 (0520/0693)]\tLoss Ss: 0.024611\n","\tEpoch:01 [001/005 (0540/0693)]\tLoss Ss: 0.027795\n","\tEpoch:01 [001/005 (0560/0693)]\tLoss Ss: 0.034692\n","\tEpoch:01 [001/005 (0580/0693)]\tLoss Ss: 0.024584\n","\tEpoch:01 [001/005 (0600/0693)]\tLoss Ss: 0.020249\n","\tEpoch:01 [001/005 (0620/0693)]\tLoss Ss: 0.025560\n","\tEpoch:01 [001/005 (0640/0693)]\tLoss Ss: 0.024795\n","\tEpoch:01 [001/005 (0660/0693)]\tLoss Ss: 0.033853\n","\tEpoch:01 [001/005 (0680/0693)]\tLoss Ss: 0.027342\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tEpoch:01 [002/005 (0000/0614)]\tLoss Ss: 0.034209\n","\tEpoch:01 [002/005 (0020/0614)]\tLoss Ss: 0.042810\n","\tEpoch:01 [002/005 (0040/0614)]\tLoss Ss: 0.033164\n","\tEpoch:01 [002/005 (0060/0614)]\tLoss Ss: 0.026852\n","\tEpoch:01 [002/005 (0080/0614)]\tLoss Ss: 0.028284\n","\tEpoch:01 [002/005 (0100/0614)]\tLoss Ss: 0.033969\n","\tEpoch:01 [002/005 (0120/0614)]\tLoss Ss: 0.031818\n","\tEpoch:01 [002/005 (0140/0614)]\tLoss Ss: 0.034994\n","\tEpoch:01 [002/005 (0160/0614)]\tLoss Ss: 0.035187\n","\tEpoch:01 [002/005 (0180/0614)]\tLoss Ss: 0.020362\n","\tEpoch:01 [002/005 (0200/0614)]\tLoss Ss: 0.027213\n","\tEpoch:01 [002/005 (0220/0614)]\tLoss Ss: 0.025425\n","\tEpoch:01 [002/005 (0240/0614)]\tLoss Ss: 0.025230\n","\tEpoch:01 [002/005 (0260/0614)]\tLoss Ss: 0.021102\n","\tEpoch:01 [002/005 (0280/0614)]\tLoss Ss: 0.015815\n","\tEpoch:01 [002/005 (0300/0614)]\tLoss Ss: 0.016126\n","\tEpoch:01 [002/005 (0320/0614)]\tLoss Ss: 0.016596\n","\tEpoch:01 [002/005 (0340/0614)]\tLoss Ss: 0.021796\n","\tEpoch:01 [002/005 (0360/0614)]\tLoss Ss: 0.018141\n","\tEpoch:01 [002/005 (0380/0614)]\tLoss Ss: 0.022282\n","\tEpoch:01 [002/005 (0400/0614)]\tLoss Ss: 0.014021\n","\tEpoch:01 [002/005 (0420/0614)]\tLoss Ss: 0.015855\n","\tEpoch:01 [002/005 (0440/0614)]\tLoss Ss: 0.017572\n","\tEpoch:01 [002/005 (0460/0614)]\tLoss Ss: 0.022342\n","\tEpoch:01 [002/005 (0480/0614)]\tLoss Ss: 0.010917\n","\tEpoch:01 [002/005 (0500/0614)]\tLoss Ss: 0.015549\n","\tEpoch:01 [002/005 (0520/0614)]\tLoss Ss: 0.014881\n","\tEpoch:01 [002/005 (0540/0614)]\tLoss Ss: 0.010915\n","\tEpoch:01 [002/005 (0560/0614)]\tLoss Ss: 0.015268\n","\tEpoch:01 [002/005 (0580/0614)]\tLoss Ss: 0.014187\n","\tEpoch:01 [002/005 (0600/0614)]\tLoss Ss: 0.016366\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tEpoch:01 [003/005 (0000/0693)]\tLoss Ss: 0.038230\n","\tEpoch:01 [003/005 (0020/0693)]\tLoss Ss: 0.037711\n","\tEpoch:01 [003/005 (0040/0693)]\tLoss Ss: 0.025175\n","\tEpoch:01 [003/005 (0060/0693)]\tLoss Ss: 0.068281\n","\tEpoch:01 [003/005 (0080/0693)]\tLoss Ss: 0.033310\n","\tEpoch:01 [003/005 (0100/0693)]\tLoss Ss: 0.058921\n","\tEpoch:01 [003/005 (0120/0693)]\tLoss Ss: 0.020887\n","\tEpoch:01 [003/005 (0140/0693)]\tLoss Ss: 0.028284\n","\tEpoch:01 [003/005 (0160/0693)]\tLoss Ss: 0.041706\n","\tEpoch:01 [003/005 (0180/0693)]\tLoss Ss: 0.019136\n","\tEpoch:01 [003/005 (0200/0693)]\tLoss Ss: 0.030696\n","\tEpoch:01 [003/005 (0220/0693)]\tLoss Ss: 0.032911\n","\tEpoch:01 [003/005 (0240/0693)]\tLoss Ss: 0.022725\n","\tEpoch:01 [003/005 (0260/0693)]\tLoss Ss: 0.023401\n","\tEpoch:01 [003/005 (0280/0693)]\tLoss Ss: 0.021423\n","\tEpoch:01 [003/005 (0300/0693)]\tLoss Ss: 0.022804\n","\tEpoch:01 [003/005 (0320/0693)]\tLoss Ss: 0.031335\n","\tEpoch:01 [003/005 (0340/0693)]\tLoss Ss: 0.023977\n","\tEpoch:01 [003/005 (0360/0693)]\tLoss Ss: 0.023402\n","\tEpoch:01 [003/005 (0380/0693)]\tLoss Ss: 0.027962\n","\tEpoch:01 [003/005 (0400/0693)]\tLoss Ss: 0.033850\n","\tEpoch:01 [003/005 (0420/0693)]\tLoss Ss: 0.031923\n","\tEpoch:01 [003/005 (0440/0693)]\tLoss Ss: 0.022929\n","\tEpoch:01 [003/005 (0460/0693)]\tLoss Ss: 0.034841\n","\tEpoch:01 [003/005 (0480/0693)]\tLoss Ss: 0.025793\n","\tEpoch:01 [003/005 (0500/0693)]\tLoss Ss: 0.027997\n","\tEpoch:01 [003/005 (0520/0693)]\tLoss Ss: 0.030202\n","\tEpoch:01 [003/005 (0540/0693)]\tLoss Ss: 0.017934\n","\tEpoch:01 [003/005 (0560/0693)]\tLoss Ss: 0.023755\n","\tEpoch:01 [003/005 (0580/0693)]\tLoss Ss: 0.023966\n","\tEpoch:01 [003/005 (0600/0693)]\tLoss Ss: 0.024744\n","\tEpoch:01 [003/005 (0620/0693)]\tLoss Ss: 0.020381\n","\tEpoch:01 [003/005 (0640/0693)]\tLoss Ss: 0.019487\n","\tEpoch:01 [003/005 (0660/0693)]\tLoss Ss: 0.026343\n","\tEpoch:01 [003/005 (0680/0693)]\tLoss Ss: 0.028135\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tEpoch:01 [004/005 (0000/0588)]\tLoss Ss: 0.031786\n","\tEpoch:01 [004/005 (0020/0588)]\tLoss Ss: 0.016124\n","\tEpoch:01 [004/005 (0040/0588)]\tLoss Ss: 0.012447\n","\tEpoch:01 [004/005 (0060/0588)]\tLoss Ss: 0.011456\n","\tEpoch:01 [004/005 (0080/0588)]\tLoss Ss: 0.018935\n","\tEpoch:01 [004/005 (0100/0588)]\tLoss Ss: 0.014806\n","\tEpoch:01 [004/005 (0120/0588)]\tLoss Ss: 0.013781\n","\tEpoch:01 [004/005 (0140/0588)]\tLoss Ss: 0.014570\n","\tEpoch:01 [004/005 (0160/0588)]\tLoss Ss: 0.025731\n","\tEpoch:01 [004/005 (0180/0588)]\tLoss Ss: 0.011858\n","\tEpoch:01 [004/005 (0200/0588)]\tLoss Ss: 0.010952\n","\tEpoch:01 [004/005 (0220/0588)]\tLoss Ss: 0.010433\n","\tEpoch:01 [004/005 (0240/0588)]\tLoss Ss: 0.009490\n","\tEpoch:01 [004/005 (0260/0588)]\tLoss Ss: 0.011395\n","\tEpoch:01 [004/005 (0280/0588)]\tLoss Ss: 0.011237\n","\tEpoch:01 [004/005 (0300/0588)]\tLoss Ss: 0.012399\n","\tEpoch:01 [004/005 (0320/0588)]\tLoss Ss: 0.013570\n","\tEpoch:01 [004/005 (0340/0588)]\tLoss Ss: 0.015178\n","\tEpoch:01 [004/005 (0360/0588)]\tLoss Ss: 0.011709\n","\tEpoch:01 [004/005 (0380/0588)]\tLoss Ss: 0.021555\n","\tEpoch:01 [004/005 (0400/0588)]\tLoss Ss: 0.012991\n","\tEpoch:01 [004/005 (0420/0588)]\tLoss Ss: 0.009575\n","\tEpoch:01 [004/005 (0440/0588)]\tLoss Ss: 0.010120\n","\tEpoch:01 [004/005 (0460/0588)]\tLoss Ss: 0.008895\n","\tEpoch:01 [004/005 (0480/0588)]\tLoss Ss: 0.008405\n","\tEpoch:01 [004/005 (0500/0588)]\tLoss Ss: 0.014208\n","\tEpoch:01 [004/005 (0520/0588)]\tLoss Ss: 0.012421\n","\tEpoch:01 [004/005 (0540/0588)]\tLoss Ss: 0.011658\n","\tEpoch:01 [004/005 (0560/0588)]\tLoss Ss: 0.016108\n","\tEpoch:01 [004/005 (0580/0588)]\tLoss Ss: 0.011470\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:01 [005/005 (0000/0755)]\tLoss Ss: 0.034017\n","\tEpoch:01 [005/005 (0020/0755)]\tLoss Ss: 0.023779\n","\tEpoch:01 [005/005 (0040/0755)]\tLoss Ss: 0.036133\n","\tEpoch:01 [005/005 (0060/0755)]\tLoss Ss: 0.025340\n","\tEpoch:01 [005/005 (0080/0755)]\tLoss Ss: 0.032544\n","\tEpoch:01 [005/005 (0100/0755)]\tLoss Ss: 0.040800\n","\tEpoch:01 [005/005 (0120/0755)]\tLoss Ss: 0.031472\n","\tEpoch:01 [005/005 (0140/0755)]\tLoss Ss: 0.046597\n","\tEpoch:01 [005/005 (0160/0755)]\tLoss Ss: 0.031461\n","\tEpoch:01 [005/005 (0180/0755)]\tLoss Ss: 0.029202\n","\tEpoch:01 [005/005 (0200/0755)]\tLoss Ss: 0.031117\n","\tEpoch:01 [005/005 (0220/0755)]\tLoss Ss: 0.025454\n","\tEpoch:01 [005/005 (0240/0755)]\tLoss Ss: 0.021087\n","\tEpoch:01 [005/005 (0260/0755)]\tLoss Ss: 0.024815\n","\tEpoch:01 [005/005 (0280/0755)]\tLoss Ss: 0.061506\n","\tEpoch:01 [005/005 (0300/0755)]\tLoss Ss: 0.023406\n","\tEpoch:01 [005/005 (0320/0755)]\tLoss Ss: 0.033038\n","\tEpoch:01 [005/005 (0340/0755)]\tLoss Ss: 0.030813\n","\tEpoch:01 [005/005 (0360/0755)]\tLoss Ss: 0.050697\n","\tEpoch:01 [005/005 (0380/0755)]\tLoss Ss: 0.019536\n","\tEpoch:01 [005/005 (0400/0755)]\tLoss Ss: 0.039365\n","\tEpoch:01 [005/005 (0420/0755)]\tLoss Ss: 0.023440\n","\tEpoch:01 [005/005 (0440/0755)]\tLoss Ss: 0.027218\n","\tEpoch:01 [005/005 (0460/0755)]\tLoss Ss: 0.028529\n","\tEpoch:01 [005/005 (0480/0755)]\tLoss Ss: 0.025637\n","\tEpoch:01 [005/005 (0500/0755)]\tLoss Ss: 0.019975\n","\tEpoch:01 [005/005 (0520/0755)]\tLoss Ss: 0.039802\n","\tEpoch:01 [005/005 (0540/0755)]\tLoss Ss: 0.038899\n","\tEpoch:01 [005/005 (0560/0755)]\tLoss Ss: 0.022793\n","\tEpoch:01 [005/005 (0580/0755)]\tLoss Ss: 0.023642\n","\tEpoch:01 [005/005 (0600/0755)]\tLoss Ss: 0.024001\n","\tEpoch:01 [005/005 (0620/0755)]\tLoss Ss: 0.021744\n","\tEpoch:01 [005/005 (0640/0755)]\tLoss Ss: 0.019402\n","\tEpoch:01 [005/005 (0660/0755)]\tLoss Ss: 0.026648\n","\tEpoch:01 [005/005 (0680/0755)]\tLoss Ss: 0.027904\n","\tEpoch:01 [005/005 (0700/0755)]\tLoss Ss: 0.022427\n","\tEpoch:01 [005/005 (0720/0755)]\tLoss Ss: 0.038752\n","\tEpoch:01 [005/005 (0740/0755)]\tLoss Ss: 0.028072\n","Now train the rotated image\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:01 [000/005 (0000/0755)]\tLoss Ss: 0.277876\n","\tRotated_Epoch:01 [000/005 (0020/0755)]\tLoss Ss: 0.528603\n","\tRotated_Epoch:01 [000/005 (0040/0755)]\tLoss Ss: 0.222731\n","\tRotated_Epoch:01 [000/005 (0060/0755)]\tLoss Ss: 0.459409\n","\tRotated_Epoch:01 [000/005 (0080/0755)]\tLoss Ss: 0.405973\n","\tRotated_Epoch:01 [000/005 (0100/0755)]\tLoss Ss: 0.230387\n","\tRotated_Epoch:01 [000/005 (0120/0755)]\tLoss Ss: 0.234586\n","\tRotated_Epoch:01 [000/005 (0140/0755)]\tLoss Ss: 0.451956\n","\tRotated_Epoch:01 [000/005 (0160/0755)]\tLoss Ss: 0.353657\n","\tRotated_Epoch:01 [000/005 (0180/0755)]\tLoss Ss: 0.159420\n","\tRotated_Epoch:01 [000/005 (0200/0755)]\tLoss Ss: 0.163847\n","\tRotated_Epoch:01 [000/005 (0220/0755)]\tLoss Ss: 0.167679\n","\tRotated_Epoch:01 [000/005 (0240/0755)]\tLoss Ss: 0.235315\n","\tRotated_Epoch:01 [000/005 (0260/0755)]\tLoss Ss: 0.133822\n","\tRotated_Epoch:01 [000/005 (0280/0755)]\tLoss Ss: 0.100060\n","\tRotated_Epoch:01 [000/005 (0300/0755)]\tLoss Ss: 0.088574\n","\tRotated_Epoch:01 [000/005 (0320/0755)]\tLoss Ss: 0.141347\n","\tRotated_Epoch:01 [000/005 (0340/0755)]\tLoss Ss: 0.100551\n","\tRotated_Epoch:01 [000/005 (0360/0755)]\tLoss Ss: 0.137600\n","\tRotated_Epoch:01 [000/005 (0380/0755)]\tLoss Ss: 0.137622\n","\tRotated_Epoch:01 [000/005 (0400/0755)]\tLoss Ss: 0.193256\n","\tRotated_Epoch:01 [000/005 (0420/0755)]\tLoss Ss: 0.142600\n","\tRotated_Epoch:01 [000/005 (0440/0755)]\tLoss Ss: 0.161951\n","\tRotated_Epoch:01 [000/005 (0460/0755)]\tLoss Ss: 0.187507\n","\tRotated_Epoch:01 [000/005 (0480/0755)]\tLoss Ss: 0.137302\n","\tRotated_Epoch:01 [000/005 (0500/0755)]\tLoss Ss: 0.117723\n","\tRotated_Epoch:01 [000/005 (0520/0755)]\tLoss Ss: 0.133745\n","\tRotated_Epoch:01 [000/005 (0540/0755)]\tLoss Ss: 0.096401\n","\tRotated_Epoch:01 [000/005 (0560/0755)]\tLoss Ss: 0.123259\n","\tRotated_Epoch:01 [000/005 (0580/0755)]\tLoss Ss: 0.122909\n","\tRotated_Epoch:01 [000/005 (0600/0755)]\tLoss Ss: 0.102267\n","\tRotated_Epoch:01 [000/005 (0620/0755)]\tLoss Ss: 0.093441\n","\tRotated_Epoch:01 [000/005 (0640/0755)]\tLoss Ss: 0.133746\n","\tRotated_Epoch:01 [000/005 (0660/0755)]\tLoss Ss: 0.130964\n","\tRotated_Epoch:01 [000/005 (0680/0755)]\tLoss Ss: 0.125359\n","\tRotated_Epoch:01 [000/005 (0700/0755)]\tLoss Ss: 0.103786\n","\tRotated_Epoch:01 [000/005 (0720/0755)]\tLoss Ss: 0.102473\n","\tRotated_Epoch:01 [000/005 (0740/0755)]\tLoss Ss: 0.115276\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tRotated_Epoch:01 [001/005 (0000/0614)]\tLoss Ss: 0.030755\n","\tRotated_Epoch:01 [001/005 (0020/0614)]\tLoss Ss: 0.033045\n","\tRotated_Epoch:01 [001/005 (0040/0614)]\tLoss Ss: 0.041700\n","\tRotated_Epoch:01 [001/005 (0060/0614)]\tLoss Ss: 0.040921\n","\tRotated_Epoch:01 [001/005 (0080/0614)]\tLoss Ss: 0.044251\n","\tRotated_Epoch:01 [001/005 (0100/0614)]\tLoss Ss: 0.026100\n","\tRotated_Epoch:01 [001/005 (0120/0614)]\tLoss Ss: 0.034118\n","\tRotated_Epoch:01 [001/005 (0140/0614)]\tLoss Ss: 0.044694\n","\tRotated_Epoch:01 [001/005 (0160/0614)]\tLoss Ss: 0.028541\n","\tRotated_Epoch:01 [001/005 (0180/0614)]\tLoss Ss: 0.028301\n","\tRotated_Epoch:01 [001/005 (0200/0614)]\tLoss Ss: 0.030217\n","\tRotated_Epoch:01 [001/005 (0220/0614)]\tLoss Ss: 0.032509\n","\tRotated_Epoch:01 [001/005 (0240/0614)]\tLoss Ss: 0.032135\n","\tRotated_Epoch:01 [001/005 (0260/0614)]\tLoss Ss: 0.025379\n","\tRotated_Epoch:01 [001/005 (0280/0614)]\tLoss Ss: 0.029072\n","\tRotated_Epoch:01 [001/005 (0300/0614)]\tLoss Ss: 0.025950\n","\tRotated_Epoch:01 [001/005 (0320/0614)]\tLoss Ss: 0.027444\n","\tRotated_Epoch:01 [001/005 (0340/0614)]\tLoss Ss: 0.028471\n","\tRotated_Epoch:01 [001/005 (0360/0614)]\tLoss Ss: 0.021938\n","\tRotated_Epoch:01 [001/005 (0380/0614)]\tLoss Ss: 0.017128\n","\tRotated_Epoch:01 [001/005 (0400/0614)]\tLoss Ss: 0.028851\n","\tRotated_Epoch:01 [001/005 (0420/0614)]\tLoss Ss: 0.020429\n","\tRotated_Epoch:01 [001/005 (0440/0614)]\tLoss Ss: 0.019440\n","\tRotated_Epoch:01 [001/005 (0460/0614)]\tLoss Ss: 0.020854\n","\tRotated_Epoch:01 [001/005 (0480/0614)]\tLoss Ss: 0.023971\n","\tRotated_Epoch:01 [001/005 (0500/0614)]\tLoss Ss: 0.022407\n","\tRotated_Epoch:01 [001/005 (0520/0614)]\tLoss Ss: 0.021410\n","\tRotated_Epoch:01 [001/005 (0540/0614)]\tLoss Ss: 0.018891\n","\tRotated_Epoch:01 [001/005 (0560/0614)]\tLoss Ss: 0.012756\n","\tRotated_Epoch:01 [001/005 (0580/0614)]\tLoss Ss: 0.013136\n","\tRotated_Epoch:01 [001/005 (0600/0614)]\tLoss Ss: 0.026291\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tRotated_Epoch:01 [002/005 (0000/0693)]\tLoss Ss: 0.057014\n","\tRotated_Epoch:01 [002/005 (0020/0693)]\tLoss Ss: 0.041448\n","\tRotated_Epoch:01 [002/005 (0040/0693)]\tLoss Ss: 0.031484\n","\tRotated_Epoch:01 [002/005 (0060/0693)]\tLoss Ss: 0.058540\n","\tRotated_Epoch:01 [002/005 (0080/0693)]\tLoss Ss: 0.045881\n","\tRotated_Epoch:01 [002/005 (0100/0693)]\tLoss Ss: 0.041455\n","\tRotated_Epoch:01 [002/005 (0120/0693)]\tLoss Ss: 0.069258\n","\tRotated_Epoch:01 [002/005 (0140/0693)]\tLoss Ss: 0.042419\n","\tRotated_Epoch:01 [002/005 (0160/0693)]\tLoss Ss: 0.042484\n","\tRotated_Epoch:01 [002/005 (0180/0693)]\tLoss Ss: 0.042640\n","\tRotated_Epoch:01 [002/005 (0200/0693)]\tLoss Ss: 0.036831\n","\tRotated_Epoch:01 [002/005 (0220/0693)]\tLoss Ss: 0.047552\n","\tRotated_Epoch:01 [002/005 (0240/0693)]\tLoss Ss: 0.031971\n","\tRotated_Epoch:01 [002/005 (0260/0693)]\tLoss Ss: 0.055652\n","\tRotated_Epoch:01 [002/005 (0280/0693)]\tLoss Ss: 0.037179\n","\tRotated_Epoch:01 [002/005 (0300/0693)]\tLoss Ss: 0.031101\n","\tRotated_Epoch:01 [002/005 (0320/0693)]\tLoss Ss: 0.026692\n","\tRotated_Epoch:01 [002/005 (0340/0693)]\tLoss Ss: 0.054493\n","\tRotated_Epoch:01 [002/005 (0360/0693)]\tLoss Ss: 0.033328\n","\tRotated_Epoch:01 [002/005 (0380/0693)]\tLoss Ss: 0.038216\n","\tRotated_Epoch:01 [002/005 (0400/0693)]\tLoss Ss: 0.030611\n","\tRotated_Epoch:01 [002/005 (0420/0693)]\tLoss Ss: 0.034629\n","\tRotated_Epoch:01 [002/005 (0440/0693)]\tLoss Ss: 0.028543\n","\tRotated_Epoch:01 [002/005 (0460/0693)]\tLoss Ss: 0.029967\n","\tRotated_Epoch:01 [002/005 (0480/0693)]\tLoss Ss: 0.029606\n","\tRotated_Epoch:01 [002/005 (0500/0693)]\tLoss Ss: 0.039842\n","\tRotated_Epoch:01 [002/005 (0520/0693)]\tLoss Ss: 0.021437\n","\tRotated_Epoch:01 [002/005 (0540/0693)]\tLoss Ss: 0.029492\n","\tRotated_Epoch:01 [002/005 (0560/0693)]\tLoss Ss: 0.032787\n","\tRotated_Epoch:01 [002/005 (0580/0693)]\tLoss Ss: 0.036672\n","\tRotated_Epoch:01 [002/005 (0600/0693)]\tLoss Ss: 0.020691\n","\tRotated_Epoch:01 [002/005 (0620/0693)]\tLoss Ss: 0.028845\n","\tRotated_Epoch:01 [002/005 (0640/0693)]\tLoss Ss: 0.020393\n","\tRotated_Epoch:01 [002/005 (0660/0693)]\tLoss Ss: 0.029178\n","\tRotated_Epoch:01 [002/005 (0680/0693)]\tLoss Ss: 0.028774\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tRotated_Epoch:01 [003/005 (0000/0693)]\tLoss Ss: 0.020547\n","\tRotated_Epoch:01 [003/005 (0020/0693)]\tLoss Ss: 0.021248\n","\tRotated_Epoch:01 [003/005 (0040/0693)]\tLoss Ss: 0.018991\n","\tRotated_Epoch:01 [003/005 (0060/0693)]\tLoss Ss: 0.015499\n","\tRotated_Epoch:01 [003/005 (0080/0693)]\tLoss Ss: 0.027284\n","\tRotated_Epoch:01 [003/005 (0100/0693)]\tLoss Ss: 0.036122\n","\tRotated_Epoch:01 [003/005 (0120/0693)]\tLoss Ss: 0.024372\n","\tRotated_Epoch:01 [003/005 (0140/0693)]\tLoss Ss: 0.023136\n","\tRotated_Epoch:01 [003/005 (0160/0693)]\tLoss Ss: 0.026739\n","\tRotated_Epoch:01 [003/005 (0180/0693)]\tLoss Ss: 0.036583\n","\tRotated_Epoch:01 [003/005 (0200/0693)]\tLoss Ss: 0.031550\n","\tRotated_Epoch:01 [003/005 (0220/0693)]\tLoss Ss: 0.024573\n","\tRotated_Epoch:01 [003/005 (0240/0693)]\tLoss Ss: 0.022767\n","\tRotated_Epoch:01 [003/005 (0260/0693)]\tLoss Ss: 0.023370\n","\tRotated_Epoch:01 [003/005 (0280/0693)]\tLoss Ss: 0.023675\n","\tRotated_Epoch:01 [003/005 (0300/0693)]\tLoss Ss: 0.033888\n","\tRotated_Epoch:01 [003/005 (0320/0693)]\tLoss Ss: 0.022213\n","\tRotated_Epoch:01 [003/005 (0340/0693)]\tLoss Ss: 0.037417\n","\tRotated_Epoch:01 [003/005 (0360/0693)]\tLoss Ss: 0.036241\n","\tRotated_Epoch:01 [003/005 (0380/0693)]\tLoss Ss: 0.023363\n","\tRotated_Epoch:01 [003/005 (0400/0693)]\tLoss Ss: 0.027705\n","\tRotated_Epoch:01 [003/005 (0420/0693)]\tLoss Ss: 0.022377\n","\tRotated_Epoch:01 [003/005 (0440/0693)]\tLoss Ss: 0.023469\n","\tRotated_Epoch:01 [003/005 (0460/0693)]\tLoss Ss: 0.030589\n","\tRotated_Epoch:01 [003/005 (0480/0693)]\tLoss Ss: 0.027560\n","\tRotated_Epoch:01 [003/005 (0500/0693)]\tLoss Ss: 0.023936\n","\tRotated_Epoch:01 [003/005 (0520/0693)]\tLoss Ss: 0.021576\n","\tRotated_Epoch:01 [003/005 (0540/0693)]\tLoss Ss: 0.017466\n","\tRotated_Epoch:01 [003/005 (0560/0693)]\tLoss Ss: 0.023854\n","\tRotated_Epoch:01 [003/005 (0580/0693)]\tLoss Ss: 0.019684\n","\tRotated_Epoch:01 [003/005 (0600/0693)]\tLoss Ss: 0.023532\n","\tRotated_Epoch:01 [003/005 (0620/0693)]\tLoss Ss: 0.020173\n","\tRotated_Epoch:01 [003/005 (0640/0693)]\tLoss Ss: 0.018996\n","\tRotated_Epoch:01 [003/005 (0660/0693)]\tLoss Ss: 0.021147\n","\tRotated_Epoch:01 [003/005 (0680/0693)]\tLoss Ss: 0.019046\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:01 [004/005 (0000/0755)]\tLoss Ss: 0.040132\n","\tRotated_Epoch:01 [004/005 (0020/0755)]\tLoss Ss: 0.043102\n","\tRotated_Epoch:01 [004/005 (0040/0755)]\tLoss Ss: 0.043484\n","\tRotated_Epoch:01 [004/005 (0060/0755)]\tLoss Ss: 0.035714\n","\tRotated_Epoch:01 [004/005 (0080/0755)]\tLoss Ss: 0.033648\n","\tRotated_Epoch:01 [004/005 (0100/0755)]\tLoss Ss: 0.037588\n","\tRotated_Epoch:01 [004/005 (0120/0755)]\tLoss Ss: 0.030292\n","\tRotated_Epoch:01 [004/005 (0140/0755)]\tLoss Ss: 0.031022\n","\tRotated_Epoch:01 [004/005 (0160/0755)]\tLoss Ss: 0.029171\n","\tRotated_Epoch:01 [004/005 (0180/0755)]\tLoss Ss: 0.065150\n","\tRotated_Epoch:01 [004/005 (0200/0755)]\tLoss Ss: 0.037372\n","\tRotated_Epoch:01 [004/005 (0220/0755)]\tLoss Ss: 0.031786\n","\tRotated_Epoch:01 [004/005 (0240/0755)]\tLoss Ss: 0.031149\n","\tRotated_Epoch:01 [004/005 (0260/0755)]\tLoss Ss: 0.035772\n","\tRotated_Epoch:01 [004/005 (0280/0755)]\tLoss Ss: 0.029867\n","\tRotated_Epoch:01 [004/005 (0300/0755)]\tLoss Ss: 0.031852\n","\tRotated_Epoch:01 [004/005 (0320/0755)]\tLoss Ss: 0.024035\n","\tRotated_Epoch:01 [004/005 (0340/0755)]\tLoss Ss: 0.033010\n","\tRotated_Epoch:01 [004/005 (0360/0755)]\tLoss Ss: 0.025935\n","\tRotated_Epoch:01 [004/005 (0380/0755)]\tLoss Ss: 0.023833\n","\tRotated_Epoch:01 [004/005 (0400/0755)]\tLoss Ss: 0.026344\n","\tRotated_Epoch:01 [004/005 (0420/0755)]\tLoss Ss: 0.029368\n","\tRotated_Epoch:01 [004/005 (0440/0755)]\tLoss Ss: 0.032096\n","\tRotated_Epoch:01 [004/005 (0460/0755)]\tLoss Ss: 0.031596\n","\tRotated_Epoch:01 [004/005 (0480/0755)]\tLoss Ss: 0.024728\n","\tRotated_Epoch:01 [004/005 (0500/0755)]\tLoss Ss: 0.064140\n","\tRotated_Epoch:01 [004/005 (0520/0755)]\tLoss Ss: 0.024071\n","\tRotated_Epoch:01 [004/005 (0540/0755)]\tLoss Ss: 0.035443\n","\tRotated_Epoch:01 [004/005 (0560/0755)]\tLoss Ss: 0.023445\n","\tRotated_Epoch:01 [004/005 (0580/0755)]\tLoss Ss: 0.020919\n","\tRotated_Epoch:01 [004/005 (0600/0755)]\tLoss Ss: 0.034838\n","\tRotated_Epoch:01 [004/005 (0620/0755)]\tLoss Ss: 0.018109\n","\tRotated_Epoch:01 [004/005 (0640/0755)]\tLoss Ss: 0.024127\n","\tRotated_Epoch:01 [004/005 (0660/0755)]\tLoss Ss: 0.020860\n","\tRotated_Epoch:01 [004/005 (0680/0755)]\tLoss Ss: 0.030851\n","\tRotated_Epoch:01 [004/005 (0700/0755)]\tLoss Ss: 0.019574\n","\tRotated_Epoch:01 [004/005 (0720/0755)]\tLoss Ss: 0.021142\n","\tRotated_Epoch:01 [004/005 (0740/0755)]\tLoss Ss: 0.025995\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tRotated_Epoch:01 [005/005 (0000/0588)]\tLoss Ss: 0.317683\n","\tRotated_Epoch:01 [005/005 (0020/0588)]\tLoss Ss: 0.227647\n","\tRotated_Epoch:01 [005/005 (0040/0588)]\tLoss Ss: 0.296307\n","\tRotated_Epoch:01 [005/005 (0060/0588)]\tLoss Ss: 0.129503\n","\tRotated_Epoch:01 [005/005 (0080/0588)]\tLoss Ss: 0.145105\n","\tRotated_Epoch:01 [005/005 (0100/0588)]\tLoss Ss: 0.300194\n","\tRotated_Epoch:01 [005/005 (0120/0588)]\tLoss Ss: 0.407831\n","\tRotated_Epoch:01 [005/005 (0140/0588)]\tLoss Ss: 0.181047\n","\tRotated_Epoch:01 [005/005 (0160/0588)]\tLoss Ss: 0.181506\n","\tRotated_Epoch:01 [005/005 (0180/0588)]\tLoss Ss: 0.165674\n","\tRotated_Epoch:01 [005/005 (0200/0588)]\tLoss Ss: 0.114808\n","\tRotated_Epoch:01 [005/005 (0220/0588)]\tLoss Ss: 0.173170\n","\tRotated_Epoch:01 [005/005 (0240/0588)]\tLoss Ss: 0.112018\n","\tRotated_Epoch:01 [005/005 (0260/0588)]\tLoss Ss: 0.147831\n","\tRotated_Epoch:01 [005/005 (0280/0588)]\tLoss Ss: 0.149868\n","\tRotated_Epoch:01 [005/005 (0300/0588)]\tLoss Ss: 0.215798\n","\tRotated_Epoch:01 [005/005 (0320/0588)]\tLoss Ss: 0.075809\n","\tRotated_Epoch:01 [005/005 (0340/0588)]\tLoss Ss: 0.126955\n","\tRotated_Epoch:01 [005/005 (0360/0588)]\tLoss Ss: 0.063484\n","\tRotated_Epoch:01 [005/005 (0380/0588)]\tLoss Ss: 0.085638\n","\tRotated_Epoch:01 [005/005 (0400/0588)]\tLoss Ss: 0.137015\n","\tRotated_Epoch:01 [005/005 (0420/0588)]\tLoss Ss: 0.170170\n","\tRotated_Epoch:01 [005/005 (0440/0588)]\tLoss Ss: 0.084654\n","\tRotated_Epoch:01 [005/005 (0460/0588)]\tLoss Ss: 0.141997\n","\tRotated_Epoch:01 [005/005 (0480/0588)]\tLoss Ss: 0.116913\n","\tRotated_Epoch:01 [005/005 (0500/0588)]\tLoss Ss: 0.091311\n","\tRotated_Epoch:01 [005/005 (0520/0588)]\tLoss Ss: 0.078460\n","\tRotated_Epoch:01 [005/005 (0540/0588)]\tLoss Ss: 0.108644\n","\tRotated_Epoch:01 [005/005 (0560/0588)]\tLoss Ss: 0.107753\n","\tRotated_Epoch:01 [005/005 (0580/0588)]\tLoss Ss: 0.071936\n","new_1_6_d.nii\n","new_1_6_m.nii\n","new_1_7_d.nii\n","new_1_7_m.nii\n","new_2_5_d.nii\n","new_2_5_m.nii\n","new_2_6_d.nii\n","new_2_6_m.nii\n","new_2_7_d.nii\n","new_2_7_m.nii\n","new_3_d (2).nii\n","new_3_m (2).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (3).nii\n","new_3_m (3).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (7).nii\n","new_3_m (7).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (8).nii\n","new_3_m (8).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (9).nii\n","new_3_m (9).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_4_10_d_163526.nii\n","new_4_10_m_163526.nii.gz\n","new_4_11_d_084336.nii\n","new_4_11_m_084336.nii.gz\n","new_4_7_d_184422.nii\n","new_4_7_m_184422.nii.gz\n","new_4_8_d_143412.nii\n","new_4_8_m_143412.nii.gz\n","new_4_9_d_155347.nii\n","new_4_9_m_155347.nii.gz\n","\tEpoch: 1; Dice: 0.5056 +/- 0.1539; Loss: 22.5428\n","Begin Epoch 2\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tEpoch:02 [000/005 (0000/0614)]\tLoss Ss: 0.191276\n","\tEpoch:02 [000/005 (0020/0614)]\tLoss Ss: 0.249881\n","\tEpoch:02 [000/005 (0040/0614)]\tLoss Ss: 0.262955\n","\tEpoch:02 [000/005 (0060/0614)]\tLoss Ss: 0.149846\n","\tEpoch:02 [000/005 (0080/0614)]\tLoss Ss: 0.171352\n","\tEpoch:02 [000/005 (0100/0614)]\tLoss Ss: 0.079413\n","\tEpoch:02 [000/005 (0120/0614)]\tLoss Ss: 0.081043\n","\tEpoch:02 [000/005 (0140/0614)]\tLoss Ss: 0.085971\n","\tEpoch:02 [000/005 (0160/0614)]\tLoss Ss: 0.067792\n","\tEpoch:02 [000/005 (0180/0614)]\tLoss Ss: 0.045995\n","\tEpoch:02 [000/005 (0200/0614)]\tLoss Ss: 0.053791\n","\tEpoch:02 [000/005 (0220/0614)]\tLoss Ss: 0.048556\n","\tEpoch:02 [000/005 (0240/0614)]\tLoss Ss: 0.052290\n","\tEpoch:02 [000/005 (0260/0614)]\tLoss Ss: 0.054359\n","\tEpoch:02 [000/005 (0280/0614)]\tLoss Ss: 0.045180\n","\tEpoch:02 [000/005 (0300/0614)]\tLoss Ss: 0.052780\n","\tEpoch:02 [000/005 (0320/0614)]\tLoss Ss: 0.056735\n","\tEpoch:02 [000/005 (0340/0614)]\tLoss Ss: 0.063836\n","\tEpoch:02 [000/005 (0360/0614)]\tLoss Ss: 0.050291\n","\tEpoch:02 [000/005 (0380/0614)]\tLoss Ss: 0.059086\n","\tEpoch:02 [000/005 (0400/0614)]\tLoss Ss: 0.056717\n","\tEpoch:02 [000/005 (0420/0614)]\tLoss Ss: 0.033994\n","\tEpoch:02 [000/005 (0440/0614)]\tLoss Ss: 0.049205\n","\tEpoch:02 [000/005 (0460/0614)]\tLoss Ss: 0.036904\n","\tEpoch:02 [000/005 (0480/0614)]\tLoss Ss: 0.034134\n","\tEpoch:02 [000/005 (0500/0614)]\tLoss Ss: 0.038675\n","\tEpoch:02 [000/005 (0520/0614)]\tLoss Ss: 0.032831\n","\tEpoch:02 [000/005 (0540/0614)]\tLoss Ss: 0.032443\n","\tEpoch:02 [000/005 (0560/0614)]\tLoss Ss: 0.033928\n","\tEpoch:02 [000/005 (0580/0614)]\tLoss Ss: 0.036942\n","\tEpoch:02 [000/005 (0600/0614)]\tLoss Ss: 0.030084\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tEpoch:02 [001/005 (0000/0693)]\tLoss Ss: 0.057263\n","\tEpoch:02 [001/005 (0020/0693)]\tLoss Ss: 0.072174\n","\tEpoch:02 [001/005 (0040/0693)]\tLoss Ss: 0.044606\n","\tEpoch:02 [001/005 (0060/0693)]\tLoss Ss: 0.256169\n","\tEpoch:02 [001/005 (0080/0693)]\tLoss Ss: 0.051499\n","\tEpoch:02 [001/005 (0100/0693)]\tLoss Ss: 0.050472\n","\tEpoch:02 [001/005 (0120/0693)]\tLoss Ss: 0.068875\n","\tEpoch:02 [001/005 (0140/0693)]\tLoss Ss: 0.061535\n","\tEpoch:02 [001/005 (0160/0693)]\tLoss Ss: 0.063481\n","\tEpoch:02 [001/005 (0180/0693)]\tLoss Ss: 0.045026\n","\tEpoch:02 [001/005 (0200/0693)]\tLoss Ss: 0.057370\n","\tEpoch:02 [001/005 (0220/0693)]\tLoss Ss: 0.042592\n","\tEpoch:02 [001/005 (0240/0693)]\tLoss Ss: 0.046884\n","\tEpoch:02 [001/005 (0260/0693)]\tLoss Ss: 0.044558\n","\tEpoch:02 [001/005 (0280/0693)]\tLoss Ss: 0.048044\n","\tEpoch:02 [001/005 (0300/0693)]\tLoss Ss: 0.045953\n","\tEpoch:02 [001/005 (0320/0693)]\tLoss Ss: 0.037217\n","\tEpoch:02 [001/005 (0340/0693)]\tLoss Ss: 0.035123\n","\tEpoch:02 [001/005 (0360/0693)]\tLoss Ss: 0.040736\n","\tEpoch:02 [001/005 (0380/0693)]\tLoss Ss: 0.033049\n","\tEpoch:02 [001/005 (0400/0693)]\tLoss Ss: 0.039027\n","\tEpoch:02 [001/005 (0420/0693)]\tLoss Ss: 0.030117\n","\tEpoch:02 [001/005 (0440/0693)]\tLoss Ss: 0.041349\n","\tEpoch:02 [001/005 (0460/0693)]\tLoss Ss: 0.033575\n","\tEpoch:02 [001/005 (0480/0693)]\tLoss Ss: 0.042084\n","\tEpoch:02 [001/005 (0500/0693)]\tLoss Ss: 0.027553\n","\tEpoch:02 [001/005 (0520/0693)]\tLoss Ss: 0.034124\n","\tEpoch:02 [001/005 (0540/0693)]\tLoss Ss: 0.032412\n","\tEpoch:02 [001/005 (0560/0693)]\tLoss Ss: 0.034212\n","\tEpoch:02 [001/005 (0580/0693)]\tLoss Ss: 0.027624\n","\tEpoch:02 [001/005 (0600/0693)]\tLoss Ss: 0.030607\n","\tEpoch:02 [001/005 (0620/0693)]\tLoss Ss: 0.040791\n","\tEpoch:02 [001/005 (0640/0693)]\tLoss Ss: 0.038962\n","\tEpoch:02 [001/005 (0660/0693)]\tLoss Ss: 0.031919\n","\tEpoch:02 [001/005 (0680/0693)]\tLoss Ss: 0.033800\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tEpoch:02 [002/005 (0000/0693)]\tLoss Ss: 0.038329\n","\tEpoch:02 [002/005 (0020/0693)]\tLoss Ss: 0.049406\n","\tEpoch:02 [002/005 (0040/0693)]\tLoss Ss: 0.046064\n","\tEpoch:02 [002/005 (0060/0693)]\tLoss Ss: 0.026125\n","\tEpoch:02 [002/005 (0080/0693)]\tLoss Ss: 0.033948\n","\tEpoch:02 [002/005 (0100/0693)]\tLoss Ss: 0.036897\n","\tEpoch:02 [002/005 (0120/0693)]\tLoss Ss: 0.029598\n","\tEpoch:02 [002/005 (0140/0693)]\tLoss Ss: 0.036162\n","\tEpoch:02 [002/005 (0160/0693)]\tLoss Ss: 0.027828\n","\tEpoch:02 [002/005 (0180/0693)]\tLoss Ss: 0.035502\n","\tEpoch:02 [002/005 (0200/0693)]\tLoss Ss: 0.042340\n","\tEpoch:02 [002/005 (0220/0693)]\tLoss Ss: 0.026672\n","\tEpoch:02 [002/005 (0240/0693)]\tLoss Ss: 0.034606\n","\tEpoch:02 [002/005 (0260/0693)]\tLoss Ss: 0.022591\n","\tEpoch:02 [002/005 (0280/0693)]\tLoss Ss: 0.033230\n","\tEpoch:02 [002/005 (0300/0693)]\tLoss Ss: 0.028702\n","\tEpoch:02 [002/005 (0320/0693)]\tLoss Ss: 0.034390\n","\tEpoch:02 [002/005 (0340/0693)]\tLoss Ss: 0.025499\n","\tEpoch:02 [002/005 (0360/0693)]\tLoss Ss: 0.037426\n","\tEpoch:02 [002/005 (0380/0693)]\tLoss Ss: 0.028540\n","\tEpoch:02 [002/005 (0400/0693)]\tLoss Ss: 0.036774\n","\tEpoch:02 [002/005 (0420/0693)]\tLoss Ss: 0.023113\n","\tEpoch:02 [002/005 (0440/0693)]\tLoss Ss: 0.022954\n","\tEpoch:02 [002/005 (0460/0693)]\tLoss Ss: 0.029969\n","\tEpoch:02 [002/005 (0480/0693)]\tLoss Ss: 0.035636\n","\tEpoch:02 [002/005 (0500/0693)]\tLoss Ss: 0.034062\n","\tEpoch:02 [002/005 (0520/0693)]\tLoss Ss: 0.021410\n","\tEpoch:02 [002/005 (0540/0693)]\tLoss Ss: 0.030390\n","\tEpoch:02 [002/005 (0560/0693)]\tLoss Ss: 0.027211\n","\tEpoch:02 [002/005 (0580/0693)]\tLoss Ss: 0.026782\n","\tEpoch:02 [002/005 (0600/0693)]\tLoss Ss: 0.024739\n","\tEpoch:02 [002/005 (0620/0693)]\tLoss Ss: 0.026932\n","\tEpoch:02 [002/005 (0640/0693)]\tLoss Ss: 0.024724\n","\tEpoch:02 [002/005 (0660/0693)]\tLoss Ss: 0.019371\n","\tEpoch:02 [002/005 (0680/0693)]\tLoss Ss: 0.025175\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tEpoch:02 [003/005 (0000/0588)]\tLoss Ss: 0.029736\n","\tEpoch:02 [003/005 (0020/0588)]\tLoss Ss: 0.026545\n","\tEpoch:02 [003/005 (0040/0588)]\tLoss Ss: 0.032943\n","\tEpoch:02 [003/005 (0060/0588)]\tLoss Ss: 0.027207\n","\tEpoch:02 [003/005 (0080/0588)]\tLoss Ss: 0.024664\n","\tEpoch:02 [003/005 (0100/0588)]\tLoss Ss: 0.038516\n","\tEpoch:02 [003/005 (0120/0588)]\tLoss Ss: 0.024695\n","\tEpoch:02 [003/005 (0140/0588)]\tLoss Ss: 0.043926\n","\tEpoch:02 [003/005 (0160/0588)]\tLoss Ss: 0.025944\n","\tEpoch:02 [003/005 (0180/0588)]\tLoss Ss: 0.025523\n","\tEpoch:02 [003/005 (0200/0588)]\tLoss Ss: 0.019046\n","\tEpoch:02 [003/005 (0220/0588)]\tLoss Ss: 0.021710\n","\tEpoch:02 [003/005 (0240/0588)]\tLoss Ss: 0.017027\n","\tEpoch:02 [003/005 (0260/0588)]\tLoss Ss: 0.013066\n","\tEpoch:02 [003/005 (0280/0588)]\tLoss Ss: 0.021779\n","\tEpoch:02 [003/005 (0300/0588)]\tLoss Ss: 0.015487\n","\tEpoch:02 [003/005 (0320/0588)]\tLoss Ss: 0.013266\n","\tEpoch:02 [003/005 (0340/0588)]\tLoss Ss: 0.021627\n","\tEpoch:02 [003/005 (0360/0588)]\tLoss Ss: 0.012865\n","\tEpoch:02 [003/005 (0380/0588)]\tLoss Ss: 0.015349\n","\tEpoch:02 [003/005 (0400/0588)]\tLoss Ss: 0.023080\n","\tEpoch:02 [003/005 (0420/0588)]\tLoss Ss: 0.020274\n","\tEpoch:02 [003/005 (0440/0588)]\tLoss Ss: 0.024792\n","\tEpoch:02 [003/005 (0460/0588)]\tLoss Ss: 0.020192\n","\tEpoch:02 [003/005 (0480/0588)]\tLoss Ss: 0.015132\n","\tEpoch:02 [003/005 (0500/0588)]\tLoss Ss: 0.025877\n","\tEpoch:02 [003/005 (0520/0588)]\tLoss Ss: 0.013895\n","\tEpoch:02 [003/005 (0540/0588)]\tLoss Ss: 0.012672\n","\tEpoch:02 [003/005 (0560/0588)]\tLoss Ss: 0.021160\n","\tEpoch:02 [003/005 (0580/0588)]\tLoss Ss: 0.010261\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:02 [004/005 (0000/0755)]\tLoss Ss: 0.034396\n","\tEpoch:02 [004/005 (0020/0755)]\tLoss Ss: 0.035534\n","\tEpoch:02 [004/005 (0040/0755)]\tLoss Ss: 0.024228\n","\tEpoch:02 [004/005 (0060/0755)]\tLoss Ss: 0.030068\n","\tEpoch:02 [004/005 (0080/0755)]\tLoss Ss: 0.031902\n","\tEpoch:02 [004/005 (0100/0755)]\tLoss Ss: 0.028801\n","\tEpoch:02 [004/005 (0120/0755)]\tLoss Ss: 0.037485\n","\tEpoch:02 [004/005 (0140/0755)]\tLoss Ss: 0.032371\n","\tEpoch:02 [004/005 (0160/0755)]\tLoss Ss: 0.055877\n","\tEpoch:02 [004/005 (0180/0755)]\tLoss Ss: 0.041007\n","\tEpoch:02 [004/005 (0200/0755)]\tLoss Ss: 0.031352\n","\tEpoch:02 [004/005 (0220/0755)]\tLoss Ss: 0.025480\n","\tEpoch:02 [004/005 (0240/0755)]\tLoss Ss: 0.025577\n","\tEpoch:02 [004/005 (0260/0755)]\tLoss Ss: 0.026581\n","\tEpoch:02 [004/005 (0280/0755)]\tLoss Ss: 0.028543\n","\tEpoch:02 [004/005 (0300/0755)]\tLoss Ss: 0.052887\n","\tEpoch:02 [004/005 (0320/0755)]\tLoss Ss: 0.044888\n","\tEpoch:02 [004/005 (0340/0755)]\tLoss Ss: 0.029117\n","\tEpoch:02 [004/005 (0360/0755)]\tLoss Ss: 0.028902\n","\tEpoch:02 [004/005 (0380/0755)]\tLoss Ss: 0.040421\n","\tEpoch:02 [004/005 (0400/0755)]\tLoss Ss: 0.018534\n","\tEpoch:02 [004/005 (0420/0755)]\tLoss Ss: 0.028232\n","\tEpoch:02 [004/005 (0440/0755)]\tLoss Ss: 0.028932\n","\tEpoch:02 [004/005 (0460/0755)]\tLoss Ss: 0.020167\n","\tEpoch:02 [004/005 (0480/0755)]\tLoss Ss: 0.020070\n","\tEpoch:02 [004/005 (0500/0755)]\tLoss Ss: 0.032220\n","\tEpoch:02 [004/005 (0520/0755)]\tLoss Ss: 0.026072\n","\tEpoch:02 [004/005 (0540/0755)]\tLoss Ss: 0.031662\n","\tEpoch:02 [004/005 (0560/0755)]\tLoss Ss: 0.038107\n","\tEpoch:02 [004/005 (0580/0755)]\tLoss Ss: 0.023623\n","\tEpoch:02 [004/005 (0600/0755)]\tLoss Ss: 0.027583\n","\tEpoch:02 [004/005 (0620/0755)]\tLoss Ss: 0.026026\n","\tEpoch:02 [004/005 (0640/0755)]\tLoss Ss: 0.025160\n","\tEpoch:02 [004/005 (0660/0755)]\tLoss Ss: 0.021175\n","\tEpoch:02 [004/005 (0680/0755)]\tLoss Ss: 0.024614\n","\tEpoch:02 [004/005 (0700/0755)]\tLoss Ss: 0.027909\n","\tEpoch:02 [004/005 (0720/0755)]\tLoss Ss: 0.035484\n","\tEpoch:02 [004/005 (0740/0755)]\tLoss Ss: 0.025361\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:02 [005/005 (0000/0755)]\tLoss Ss: 0.043436\n","\tEpoch:02 [005/005 (0020/0755)]\tLoss Ss: 0.020794\n","\tEpoch:02 [005/005 (0040/0755)]\tLoss Ss: 0.030892\n","\tEpoch:02 [005/005 (0060/0755)]\tLoss Ss: 0.023781\n","\tEpoch:02 [005/005 (0080/0755)]\tLoss Ss: 0.027954\n","\tEpoch:02 [005/005 (0100/0755)]\tLoss Ss: 0.028811\n","\tEpoch:02 [005/005 (0120/0755)]\tLoss Ss: 0.039709\n","\tEpoch:02 [005/005 (0140/0755)]\tLoss Ss: 0.032546\n","\tEpoch:02 [005/005 (0160/0755)]\tLoss Ss: 0.026945\n","\tEpoch:02 [005/005 (0180/0755)]\tLoss Ss: 0.015755\n","\tEpoch:02 [005/005 (0200/0755)]\tLoss Ss: 0.077026\n","\tEpoch:02 [005/005 (0220/0755)]\tLoss Ss: 0.033536\n","\tEpoch:02 [005/005 (0240/0755)]\tLoss Ss: 0.018421\n","\tEpoch:02 [005/005 (0260/0755)]\tLoss Ss: 0.036519\n","\tEpoch:02 [005/005 (0280/0755)]\tLoss Ss: 0.049480\n","\tEpoch:02 [005/005 (0300/0755)]\tLoss Ss: 0.028745\n","\tEpoch:02 [005/005 (0320/0755)]\tLoss Ss: 0.025262\n","\tEpoch:02 [005/005 (0340/0755)]\tLoss Ss: 0.029181\n","\tEpoch:02 [005/005 (0360/0755)]\tLoss Ss: 0.028520\n","\tEpoch:02 [005/005 (0380/0755)]\tLoss Ss: 0.026421\n","\tEpoch:02 [005/005 (0400/0755)]\tLoss Ss: 0.035231\n","\tEpoch:02 [005/005 (0420/0755)]\tLoss Ss: 0.020482\n","\tEpoch:02 [005/005 (0440/0755)]\tLoss Ss: 0.034273\n","\tEpoch:02 [005/005 (0460/0755)]\tLoss Ss: 0.024174\n","\tEpoch:02 [005/005 (0480/0755)]\tLoss Ss: 0.024259\n","\tEpoch:02 [005/005 (0500/0755)]\tLoss Ss: 0.016627\n","\tEpoch:02 [005/005 (0520/0755)]\tLoss Ss: 0.029351\n","\tEpoch:02 [005/005 (0540/0755)]\tLoss Ss: 0.019593\n","\tEpoch:02 [005/005 (0560/0755)]\tLoss Ss: 0.020406\n","\tEpoch:02 [005/005 (0580/0755)]\tLoss Ss: 0.027696\n","\tEpoch:02 [005/005 (0600/0755)]\tLoss Ss: 0.020506\n","\tEpoch:02 [005/005 (0620/0755)]\tLoss Ss: 0.018399\n","\tEpoch:02 [005/005 (0640/0755)]\tLoss Ss: 0.023156\n","\tEpoch:02 [005/005 (0660/0755)]\tLoss Ss: 0.018028\n","\tEpoch:02 [005/005 (0680/0755)]\tLoss Ss: 0.028759\n","\tEpoch:02 [005/005 (0700/0755)]\tLoss Ss: 0.026578\n","\tEpoch:02 [005/005 (0720/0755)]\tLoss Ss: 0.034558\n","\tEpoch:02 [005/005 (0740/0755)]\tLoss Ss: 0.017558\n","Now train the rotated image\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tRotated_Epoch:02 [000/005 (0000/0693)]\tLoss Ss: 0.050649\n","\tRotated_Epoch:02 [000/005 (0020/0693)]\tLoss Ss: 0.038218\n","\tRotated_Epoch:02 [000/005 (0040/0693)]\tLoss Ss: 0.031492\n","\tRotated_Epoch:02 [000/005 (0060/0693)]\tLoss Ss: 0.037892\n","\tRotated_Epoch:02 [000/005 (0080/0693)]\tLoss Ss: 0.037818\n","\tRotated_Epoch:02 [000/005 (0100/0693)]\tLoss Ss: 0.038361\n","\tRotated_Epoch:02 [000/005 (0120/0693)]\tLoss Ss: 0.036724\n","\tRotated_Epoch:02 [000/005 (0140/0693)]\tLoss Ss: 0.027224\n","\tRotated_Epoch:02 [000/005 (0160/0693)]\tLoss Ss: 0.031275\n","\tRotated_Epoch:02 [000/005 (0180/0693)]\tLoss Ss: 0.037036\n","\tRotated_Epoch:02 [000/005 (0200/0693)]\tLoss Ss: 0.019655\n","\tRotated_Epoch:02 [000/005 (0220/0693)]\tLoss Ss: 0.027742\n","\tRotated_Epoch:02 [000/005 (0240/0693)]\tLoss Ss: 0.025480\n","\tRotated_Epoch:02 [000/005 (0260/0693)]\tLoss Ss: 0.039507\n","\tRotated_Epoch:02 [000/005 (0280/0693)]\tLoss Ss: 0.022723\n","\tRotated_Epoch:02 [000/005 (0300/0693)]\tLoss Ss: 0.034608\n","\tRotated_Epoch:02 [000/005 (0320/0693)]\tLoss Ss: 0.032739\n","\tRotated_Epoch:02 [000/005 (0340/0693)]\tLoss Ss: 0.033866\n","\tRotated_Epoch:02 [000/005 (0360/0693)]\tLoss Ss: 0.027867\n","\tRotated_Epoch:02 [000/005 (0380/0693)]\tLoss Ss: 0.024127\n","\tRotated_Epoch:02 [000/005 (0400/0693)]\tLoss Ss: 0.023679\n","\tRotated_Epoch:02 [000/005 (0420/0693)]\tLoss Ss: 0.024706\n","\tRotated_Epoch:02 [000/005 (0440/0693)]\tLoss Ss: 0.031377\n","\tRotated_Epoch:02 [000/005 (0460/0693)]\tLoss Ss: 0.018436\n","\tRotated_Epoch:02 [000/005 (0480/0693)]\tLoss Ss: 0.021525\n","\tRotated_Epoch:02 [000/005 (0500/0693)]\tLoss Ss: 0.028535\n","\tRotated_Epoch:02 [000/005 (0520/0693)]\tLoss Ss: 0.040313\n","\tRotated_Epoch:02 [000/005 (0540/0693)]\tLoss Ss: 0.022279\n","\tRotated_Epoch:02 [000/005 (0560/0693)]\tLoss Ss: 0.026482\n","\tRotated_Epoch:02 [000/005 (0580/0693)]\tLoss Ss: 0.025209\n","\tRotated_Epoch:02 [000/005 (0600/0693)]\tLoss Ss: 0.028845\n","\tRotated_Epoch:02 [000/005 (0620/0693)]\tLoss Ss: 0.023654\n","\tRotated_Epoch:02 [000/005 (0640/0693)]\tLoss Ss: 0.023110\n","\tRotated_Epoch:02 [000/005 (0660/0693)]\tLoss Ss: 0.017205\n","\tRotated_Epoch:02 [000/005 (0680/0693)]\tLoss Ss: 0.028501\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tRotated_Epoch:02 [001/005 (0000/0588)]\tLoss Ss: 0.341483\n","\tRotated_Epoch:02 [001/005 (0020/0588)]\tLoss Ss: 0.323876\n","\tRotated_Epoch:02 [001/005 (0040/0588)]\tLoss Ss: 0.255025\n","\tRotated_Epoch:02 [001/005 (0060/0588)]\tLoss Ss: 0.259664\n","\tRotated_Epoch:02 [001/005 (0080/0588)]\tLoss Ss: 0.251007\n","\tRotated_Epoch:02 [001/005 (0100/0588)]\tLoss Ss: 0.269897\n","\tRotated_Epoch:02 [001/005 (0120/0588)]\tLoss Ss: 0.236502\n","\tRotated_Epoch:02 [001/005 (0140/0588)]\tLoss Ss: 0.250281\n","\tRotated_Epoch:02 [001/005 (0160/0588)]\tLoss Ss: 0.160137\n","\tRotated_Epoch:02 [001/005 (0180/0588)]\tLoss Ss: 0.191020\n","\tRotated_Epoch:02 [001/005 (0200/0588)]\tLoss Ss: 0.131499\n","\tRotated_Epoch:02 [001/005 (0220/0588)]\tLoss Ss: 0.185235\n","\tRotated_Epoch:02 [001/005 (0240/0588)]\tLoss Ss: 0.270608\n","\tRotated_Epoch:02 [001/005 (0260/0588)]\tLoss Ss: 0.129806\n","\tRotated_Epoch:02 [001/005 (0280/0588)]\tLoss Ss: 0.160778\n","\tRotated_Epoch:02 [001/005 (0300/0588)]\tLoss Ss: 0.164234\n","\tRotated_Epoch:02 [001/005 (0320/0588)]\tLoss Ss: 0.172933\n","\tRotated_Epoch:02 [001/005 (0340/0588)]\tLoss Ss: 0.100857\n","\tRotated_Epoch:02 [001/005 (0360/0588)]\tLoss Ss: 0.182793\n","\tRotated_Epoch:02 [001/005 (0380/0588)]\tLoss Ss: 0.166258\n","\tRotated_Epoch:02 [001/005 (0400/0588)]\tLoss Ss: 0.162244\n","\tRotated_Epoch:02 [001/005 (0420/0588)]\tLoss Ss: 0.091070\n","\tRotated_Epoch:02 [001/005 (0440/0588)]\tLoss Ss: 0.112757\n","\tRotated_Epoch:02 [001/005 (0460/0588)]\tLoss Ss: 0.108176\n","\tRotated_Epoch:02 [001/005 (0480/0588)]\tLoss Ss: 0.100289\n","\tRotated_Epoch:02 [001/005 (0500/0588)]\tLoss Ss: 0.095140\n","\tRotated_Epoch:02 [001/005 (0520/0588)]\tLoss Ss: 0.117489\n","\tRotated_Epoch:02 [001/005 (0540/0588)]\tLoss Ss: 0.127425\n","\tRotated_Epoch:02 [001/005 (0560/0588)]\tLoss Ss: 0.128460\n","\tRotated_Epoch:02 [001/005 (0580/0588)]\tLoss Ss: 0.152890\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tRotated_Epoch:02 [002/005 (0000/0693)]\tLoss Ss: 0.057935\n","\tRotated_Epoch:02 [002/005 (0020/0693)]\tLoss Ss: 0.055544\n","\tRotated_Epoch:02 [002/005 (0040/0693)]\tLoss Ss: 0.072424\n","\tRotated_Epoch:02 [002/005 (0060/0693)]\tLoss Ss: 0.067457\n","\tRotated_Epoch:02 [002/005 (0080/0693)]\tLoss Ss: 0.064795\n","\tRotated_Epoch:02 [002/005 (0100/0693)]\tLoss Ss: 0.049321\n","\tRotated_Epoch:02 [002/005 (0120/0693)]\tLoss Ss: 0.047237\n","\tRotated_Epoch:02 [002/005 (0140/0693)]\tLoss Ss: 0.044144\n","\tRotated_Epoch:02 [002/005 (0160/0693)]\tLoss Ss: 0.041711\n","\tRotated_Epoch:02 [002/005 (0180/0693)]\tLoss Ss: 0.043250\n","\tRotated_Epoch:02 [002/005 (0200/0693)]\tLoss Ss: 0.052167\n","\tRotated_Epoch:02 [002/005 (0220/0693)]\tLoss Ss: 0.042765\n","\tRotated_Epoch:02 [002/005 (0240/0693)]\tLoss Ss: 0.040541\n","\tRotated_Epoch:02 [002/005 (0260/0693)]\tLoss Ss: 0.050527\n","\tRotated_Epoch:02 [002/005 (0280/0693)]\tLoss Ss: 0.028251\n","\tRotated_Epoch:02 [002/005 (0300/0693)]\tLoss Ss: 0.037105\n","\tRotated_Epoch:02 [002/005 (0320/0693)]\tLoss Ss: 0.030193\n","\tRotated_Epoch:02 [002/005 (0340/0693)]\tLoss Ss: 0.034344\n","\tRotated_Epoch:02 [002/005 (0360/0693)]\tLoss Ss: 0.026401\n","\tRotated_Epoch:02 [002/005 (0380/0693)]\tLoss Ss: 0.036650\n","\tRotated_Epoch:02 [002/005 (0400/0693)]\tLoss Ss: 0.042700\n","\tRotated_Epoch:02 [002/005 (0420/0693)]\tLoss Ss: 0.036544\n","\tRotated_Epoch:02 [002/005 (0440/0693)]\tLoss Ss: 0.027338\n","\tRotated_Epoch:02 [002/005 (0460/0693)]\tLoss Ss: 0.033656\n","\tRotated_Epoch:02 [002/005 (0480/0693)]\tLoss Ss: 0.032792\n","\tRotated_Epoch:02 [002/005 (0500/0693)]\tLoss Ss: 0.024584\n","\tRotated_Epoch:02 [002/005 (0520/0693)]\tLoss Ss: 0.023513\n","\tRotated_Epoch:02 [002/005 (0540/0693)]\tLoss Ss: 0.027797\n","\tRotated_Epoch:02 [002/005 (0560/0693)]\tLoss Ss: 0.040126\n","\tRotated_Epoch:02 [002/005 (0580/0693)]\tLoss Ss: 0.032213\n","\tRotated_Epoch:02 [002/005 (0600/0693)]\tLoss Ss: 0.027921\n","\tRotated_Epoch:02 [002/005 (0620/0693)]\tLoss Ss: 0.024321\n","\tRotated_Epoch:02 [002/005 (0640/0693)]\tLoss Ss: 0.024433\n","\tRotated_Epoch:02 [002/005 (0660/0693)]\tLoss Ss: 0.019059\n","\tRotated_Epoch:02 [002/005 (0680/0693)]\tLoss Ss: 0.027458\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:02 [003/005 (0000/0755)]\tLoss Ss: 0.269111\n","\tRotated_Epoch:02 [003/005 (0020/0755)]\tLoss Ss: 0.336437\n","\tRotated_Epoch:02 [003/005 (0040/0755)]\tLoss Ss: 0.285619\n","\tRotated_Epoch:02 [003/005 (0060/0755)]\tLoss Ss: 0.350029\n","\tRotated_Epoch:02 [003/005 (0080/0755)]\tLoss Ss: 0.246960\n","\tRotated_Epoch:02 [003/005 (0100/0755)]\tLoss Ss: 0.199704\n","\tRotated_Epoch:02 [003/005 (0120/0755)]\tLoss Ss: 0.225829\n","\tRotated_Epoch:02 [003/005 (0140/0755)]\tLoss Ss: 0.183069\n","\tRotated_Epoch:02 [003/005 (0160/0755)]\tLoss Ss: 0.264443\n","\tRotated_Epoch:02 [003/005 (0180/0755)]\tLoss Ss: 0.209766\n","\tRotated_Epoch:02 [003/005 (0200/0755)]\tLoss Ss: 0.196235\n","\tRotated_Epoch:02 [003/005 (0220/0755)]\tLoss Ss: 0.162530\n","\tRotated_Epoch:02 [003/005 (0240/0755)]\tLoss Ss: 0.245902\n","\tRotated_Epoch:02 [003/005 (0260/0755)]\tLoss Ss: 0.185453\n","\tRotated_Epoch:02 [003/005 (0280/0755)]\tLoss Ss: 0.195055\n","\tRotated_Epoch:02 [003/005 (0300/0755)]\tLoss Ss: 0.225137\n","\tRotated_Epoch:02 [003/005 (0320/0755)]\tLoss Ss: 0.201520\n","\tRotated_Epoch:02 [003/005 (0340/0755)]\tLoss Ss: 0.174458\n","\tRotated_Epoch:02 [003/005 (0360/0755)]\tLoss Ss: 0.138288\n","\tRotated_Epoch:02 [003/005 (0380/0755)]\tLoss Ss: 0.187005\n","\tRotated_Epoch:02 [003/005 (0400/0755)]\tLoss Ss: 0.148797\n","\tRotated_Epoch:02 [003/005 (0420/0755)]\tLoss Ss: 0.165457\n","\tRotated_Epoch:02 [003/005 (0440/0755)]\tLoss Ss: 0.120363\n","\tRotated_Epoch:02 [003/005 (0460/0755)]\tLoss Ss: 0.147677\n","\tRotated_Epoch:02 [003/005 (0480/0755)]\tLoss Ss: 0.173871\n","\tRotated_Epoch:02 [003/005 (0500/0755)]\tLoss Ss: 0.156425\n","\tRotated_Epoch:02 [003/005 (0520/0755)]\tLoss Ss: 0.119106\n","\tRotated_Epoch:02 [003/005 (0540/0755)]\tLoss Ss: 0.124164\n","\tRotated_Epoch:02 [003/005 (0560/0755)]\tLoss Ss: 0.139235\n","\tRotated_Epoch:02 [003/005 (0580/0755)]\tLoss Ss: 0.132961\n","\tRotated_Epoch:02 [003/005 (0600/0755)]\tLoss Ss: 0.192111\n","\tRotated_Epoch:02 [003/005 (0620/0755)]\tLoss Ss: 0.120926\n","\tRotated_Epoch:02 [003/005 (0640/0755)]\tLoss Ss: 0.129343\n","\tRotated_Epoch:02 [003/005 (0660/0755)]\tLoss Ss: 0.183107\n","\tRotated_Epoch:02 [003/005 (0680/0755)]\tLoss Ss: 0.127861\n","\tRotated_Epoch:02 [003/005 (0700/0755)]\tLoss Ss: 0.145991\n","\tRotated_Epoch:02 [003/005 (0720/0755)]\tLoss Ss: 0.143391\n","\tRotated_Epoch:02 [003/005 (0740/0755)]\tLoss Ss: 0.153596\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:02 [004/005 (0000/0755)]\tLoss Ss: 0.088216\n","\tRotated_Epoch:02 [004/005 (0020/0755)]\tLoss Ss: 0.097484\n","\tRotated_Epoch:02 [004/005 (0040/0755)]\tLoss Ss: 0.115555\n","\tRotated_Epoch:02 [004/005 (0060/0755)]\tLoss Ss: 0.106137\n","\tRotated_Epoch:02 [004/005 (0080/0755)]\tLoss Ss: 0.119681\n","\tRotated_Epoch:02 [004/005 (0100/0755)]\tLoss Ss: 0.071064\n","\tRotated_Epoch:02 [004/005 (0120/0755)]\tLoss Ss: 0.089255\n","\tRotated_Epoch:02 [004/005 (0140/0755)]\tLoss Ss: 0.072168\n","\tRotated_Epoch:02 [004/005 (0160/0755)]\tLoss Ss: 0.072330\n","\tRotated_Epoch:02 [004/005 (0180/0755)]\tLoss Ss: 0.056501\n","\tRotated_Epoch:02 [004/005 (0200/0755)]\tLoss Ss: 0.084711\n","\tRotated_Epoch:02 [004/005 (0220/0755)]\tLoss Ss: 0.051376\n","\tRotated_Epoch:02 [004/005 (0240/0755)]\tLoss Ss: 0.058478\n","\tRotated_Epoch:02 [004/005 (0260/0755)]\tLoss Ss: 0.072241\n","\tRotated_Epoch:02 [004/005 (0280/0755)]\tLoss Ss: 0.059696\n","\tRotated_Epoch:02 [004/005 (0300/0755)]\tLoss Ss: 0.051179\n","\tRotated_Epoch:02 [004/005 (0320/0755)]\tLoss Ss: 0.063220\n","\tRotated_Epoch:02 [004/005 (0340/0755)]\tLoss Ss: 0.050930\n","\tRotated_Epoch:02 [004/005 (0360/0755)]\tLoss Ss: 0.061604\n","\tRotated_Epoch:02 [004/005 (0380/0755)]\tLoss Ss: 0.071381\n","\tRotated_Epoch:02 [004/005 (0400/0755)]\tLoss Ss: 0.045617\n","\tRotated_Epoch:02 [004/005 (0420/0755)]\tLoss Ss: 0.049372\n","\tRotated_Epoch:02 [004/005 (0440/0755)]\tLoss Ss: 0.041356\n","\tRotated_Epoch:02 [004/005 (0460/0755)]\tLoss Ss: 0.044181\n","\tRotated_Epoch:02 [004/005 (0480/0755)]\tLoss Ss: 0.044929\n","\tRotated_Epoch:02 [004/005 (0500/0755)]\tLoss Ss: 0.040388\n","\tRotated_Epoch:02 [004/005 (0520/0755)]\tLoss Ss: 0.032959\n","\tRotated_Epoch:02 [004/005 (0540/0755)]\tLoss Ss: 0.043745\n","\tRotated_Epoch:02 [004/005 (0560/0755)]\tLoss Ss: 0.035265\n","\tRotated_Epoch:02 [004/005 (0580/0755)]\tLoss Ss: 0.046510\n","\tRotated_Epoch:02 [004/005 (0600/0755)]\tLoss Ss: 0.031928\n","\tRotated_Epoch:02 [004/005 (0620/0755)]\tLoss Ss: 0.040293\n","\tRotated_Epoch:02 [004/005 (0640/0755)]\tLoss Ss: 0.028679\n","\tRotated_Epoch:02 [004/005 (0660/0755)]\tLoss Ss: 0.043822\n","\tRotated_Epoch:02 [004/005 (0680/0755)]\tLoss Ss: 0.049313\n","\tRotated_Epoch:02 [004/005 (0700/0755)]\tLoss Ss: 0.028356\n","\tRotated_Epoch:02 [004/005 (0720/0755)]\tLoss Ss: 0.024956\n","\tRotated_Epoch:02 [004/005 (0740/0755)]\tLoss Ss: 0.027216\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tRotated_Epoch:02 [005/005 (0000/0614)]\tLoss Ss: 0.074431\n","\tRotated_Epoch:02 [005/005 (0020/0614)]\tLoss Ss: 0.039560\n","\tRotated_Epoch:02 [005/005 (0040/0614)]\tLoss Ss: 0.035196\n","\tRotated_Epoch:02 [005/005 (0060/0614)]\tLoss Ss: 0.045857\n","\tRotated_Epoch:02 [005/005 (0080/0614)]\tLoss Ss: 0.032721\n","\tRotated_Epoch:02 [005/005 (0100/0614)]\tLoss Ss: 0.044481\n","\tRotated_Epoch:02 [005/005 (0120/0614)]\tLoss Ss: 0.027972\n","\tRotated_Epoch:02 [005/005 (0140/0614)]\tLoss Ss: 0.042076\n","\tRotated_Epoch:02 [005/005 (0160/0614)]\tLoss Ss: 0.031529\n","\tRotated_Epoch:02 [005/005 (0180/0614)]\tLoss Ss: 0.031276\n","\tRotated_Epoch:02 [005/005 (0200/0614)]\tLoss Ss: 0.030890\n","\tRotated_Epoch:02 [005/005 (0220/0614)]\tLoss Ss: 0.031927\n","\tRotated_Epoch:02 [005/005 (0240/0614)]\tLoss Ss: 0.023377\n","\tRotated_Epoch:02 [005/005 (0260/0614)]\tLoss Ss: 0.027440\n","\tRotated_Epoch:02 [005/005 (0280/0614)]\tLoss Ss: 0.019724\n","\tRotated_Epoch:02 [005/005 (0300/0614)]\tLoss Ss: 0.017201\n","\tRotated_Epoch:02 [005/005 (0320/0614)]\tLoss Ss: 0.018990\n","\tRotated_Epoch:02 [005/005 (0340/0614)]\tLoss Ss: 0.020536\n","\tRotated_Epoch:02 [005/005 (0360/0614)]\tLoss Ss: 0.017816\n","\tRotated_Epoch:02 [005/005 (0380/0614)]\tLoss Ss: 0.013184\n","\tRotated_Epoch:02 [005/005 (0400/0614)]\tLoss Ss: 0.013765\n","\tRotated_Epoch:02 [005/005 (0420/0614)]\tLoss Ss: 0.017837\n","\tRotated_Epoch:02 [005/005 (0440/0614)]\tLoss Ss: 0.015346\n","\tRotated_Epoch:02 [005/005 (0460/0614)]\tLoss Ss: 0.013188\n","\tRotated_Epoch:02 [005/005 (0480/0614)]\tLoss Ss: 0.009427\n","\tRotated_Epoch:02 [005/005 (0500/0614)]\tLoss Ss: 0.016745\n","\tRotated_Epoch:02 [005/005 (0520/0614)]\tLoss Ss: 0.012834\n","\tRotated_Epoch:02 [005/005 (0540/0614)]\tLoss Ss: 0.014739\n","\tRotated_Epoch:02 [005/005 (0560/0614)]\tLoss Ss: 0.014633\n","\tRotated_Epoch:02 [005/005 (0580/0614)]\tLoss Ss: 0.016268\n","\tRotated_Epoch:02 [005/005 (0600/0614)]\tLoss Ss: 0.017905\n","new_1_6_d.nii\n","new_1_6_m.nii\n","new_1_7_d.nii\n","new_1_7_m.nii\n","new_2_5_d.nii\n","new_2_5_m.nii\n","new_2_6_d.nii\n","new_2_6_m.nii\n","new_2_7_d.nii\n","new_2_7_m.nii\n","new_3_d (2).nii\n","new_3_m (2).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (3).nii\n","new_3_m (3).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (7).nii\n","new_3_m (7).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (8).nii\n","new_3_m (8).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (9).nii\n","new_3_m (9).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_4_10_d_163526.nii\n","new_4_10_m_163526.nii.gz\n","new_4_11_d_084336.nii\n","new_4_11_m_084336.nii.gz\n","new_4_7_d_184422.nii\n","new_4_7_m_184422.nii.gz\n","new_4_8_d_143412.nii\n","new_4_8_m_143412.nii.gz\n","new_4_9_d_155347.nii\n","new_4_9_m_155347.nii.gz\n","\tEpoch: 2; Dice: 0.9169 +/- 0.0226; Loss: 25.9639\n","Begin Epoch 3\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tEpoch:03 [000/005 (0000/0693)]\tLoss Ss: 0.039038\n","\tEpoch:03 [000/005 (0020/0693)]\tLoss Ss: 0.027413\n","\tEpoch:03 [000/005 (0040/0693)]\tLoss Ss: 0.035364\n","\tEpoch:03 [000/005 (0060/0693)]\tLoss Ss: 0.028951\n","\tEpoch:03 [000/005 (0080/0693)]\tLoss Ss: 0.040354\n","\tEpoch:03 [000/005 (0100/0693)]\tLoss Ss: 0.022117\n","\tEpoch:03 [000/005 (0120/0693)]\tLoss Ss: 0.026432\n","\tEpoch:03 [000/005 (0140/0693)]\tLoss Ss: 0.024359\n","\tEpoch:03 [000/005 (0160/0693)]\tLoss Ss: 0.024247\n","\tEpoch:03 [000/005 (0180/0693)]\tLoss Ss: 0.019088\n","\tEpoch:03 [000/005 (0200/0693)]\tLoss Ss: 0.024932\n","\tEpoch:03 [000/005 (0220/0693)]\tLoss Ss: 0.035516\n","\tEpoch:03 [000/005 (0240/0693)]\tLoss Ss: 0.028675\n","\tEpoch:03 [000/005 (0260/0693)]\tLoss Ss: 0.019565\n","\tEpoch:03 [000/005 (0280/0693)]\tLoss Ss: 0.022537\n","\tEpoch:03 [000/005 (0300/0693)]\tLoss Ss: 0.023952\n","\tEpoch:03 [000/005 (0320/0693)]\tLoss Ss: 0.022920\n","\tEpoch:03 [000/005 (0340/0693)]\tLoss Ss: 0.018611\n","\tEpoch:03 [000/005 (0360/0693)]\tLoss Ss: 0.033431\n","\tEpoch:03 [000/005 (0380/0693)]\tLoss Ss: 0.022812\n","\tEpoch:03 [000/005 (0400/0693)]\tLoss Ss: 0.030997\n","\tEpoch:03 [000/005 (0420/0693)]\tLoss Ss: 0.023613\n","\tEpoch:03 [000/005 (0440/0693)]\tLoss Ss: 0.017868\n","\tEpoch:03 [000/005 (0460/0693)]\tLoss Ss: 0.020762\n","\tEpoch:03 [000/005 (0480/0693)]\tLoss Ss: 0.017717\n","\tEpoch:03 [000/005 (0500/0693)]\tLoss Ss: 0.021367\n","\tEpoch:03 [000/005 (0520/0693)]\tLoss Ss: 0.027708\n","\tEpoch:03 [000/005 (0540/0693)]\tLoss Ss: 0.015589\n","\tEpoch:03 [000/005 (0560/0693)]\tLoss Ss: 0.018921\n","\tEpoch:03 [000/005 (0580/0693)]\tLoss Ss: 0.025970\n","\tEpoch:03 [000/005 (0600/0693)]\tLoss Ss: 0.018811\n","\tEpoch:03 [000/005 (0620/0693)]\tLoss Ss: 0.025669\n","\tEpoch:03 [000/005 (0640/0693)]\tLoss Ss: 0.020764\n","\tEpoch:03 [000/005 (0660/0693)]\tLoss Ss: 0.024244\n","\tEpoch:03 [000/005 (0680/0693)]\tLoss Ss: 0.019566\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:03 [001/005 (0000/0755)]\tLoss Ss: 0.045454\n","\tEpoch:03 [001/005 (0020/0755)]\tLoss Ss: 0.044771\n","\tEpoch:03 [001/005 (0040/0755)]\tLoss Ss: 0.036055\n","\tEpoch:03 [001/005 (0060/0755)]\tLoss Ss: 0.036101\n","\tEpoch:03 [001/005 (0080/0755)]\tLoss Ss: 0.037596\n","\tEpoch:03 [001/005 (0100/0755)]\tLoss Ss: 0.029963\n","\tEpoch:03 [001/005 (0120/0755)]\tLoss Ss: 0.038997\n","\tEpoch:03 [001/005 (0140/0755)]\tLoss Ss: 0.031414\n","\tEpoch:03 [001/005 (0160/0755)]\tLoss Ss: 0.030430\n","\tEpoch:03 [001/005 (0180/0755)]\tLoss Ss: 0.029460\n","\tEpoch:03 [001/005 (0200/0755)]\tLoss Ss: 0.035996\n","\tEpoch:03 [001/005 (0220/0755)]\tLoss Ss: 0.026663\n","\tEpoch:03 [001/005 (0240/0755)]\tLoss Ss: 0.034172\n","\tEpoch:03 [001/005 (0260/0755)]\tLoss Ss: 0.038024\n","\tEpoch:03 [001/005 (0280/0755)]\tLoss Ss: 0.026732\n","\tEpoch:03 [001/005 (0300/0755)]\tLoss Ss: 0.016904\n","\tEpoch:03 [001/005 (0320/0755)]\tLoss Ss: 0.035768\n","\tEpoch:03 [001/005 (0340/0755)]\tLoss Ss: 0.027243\n","\tEpoch:03 [001/005 (0360/0755)]\tLoss Ss: 0.021765\n","\tEpoch:03 [001/005 (0380/0755)]\tLoss Ss: 0.023319\n","\tEpoch:03 [001/005 (0400/0755)]\tLoss Ss: 0.039317\n","\tEpoch:03 [001/005 (0420/0755)]\tLoss Ss: 0.040379\n","\tEpoch:03 [001/005 (0440/0755)]\tLoss Ss: 0.023907\n","\tEpoch:03 [001/005 (0460/0755)]\tLoss Ss: 0.016500\n","\tEpoch:03 [001/005 (0480/0755)]\tLoss Ss: 0.037493\n","\tEpoch:03 [001/005 (0500/0755)]\tLoss Ss: 0.021055\n","\tEpoch:03 [001/005 (0520/0755)]\tLoss Ss: 0.019567\n","\tEpoch:03 [001/005 (0540/0755)]\tLoss Ss: 0.017443\n","\tEpoch:03 [001/005 (0560/0755)]\tLoss Ss: 0.030485\n","\tEpoch:03 [001/005 (0580/0755)]\tLoss Ss: 0.033635\n","\tEpoch:03 [001/005 (0600/0755)]\tLoss Ss: 0.025046\n","\tEpoch:03 [001/005 (0620/0755)]\tLoss Ss: 0.025518\n","\tEpoch:03 [001/005 (0640/0755)]\tLoss Ss: 0.029839\n","\tEpoch:03 [001/005 (0660/0755)]\tLoss Ss: 0.029292\n","\tEpoch:03 [001/005 (0680/0755)]\tLoss Ss: 0.019623\n","\tEpoch:03 [001/005 (0700/0755)]\tLoss Ss: 0.019905\n","\tEpoch:03 [001/005 (0720/0755)]\tLoss Ss: 0.028218\n","\tEpoch:03 [001/005 (0740/0755)]\tLoss Ss: 0.017126\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:03 [002/005 (0000/0755)]\tLoss Ss: 0.035132\n","\tEpoch:03 [002/005 (0020/0755)]\tLoss Ss: 0.017212\n","\tEpoch:03 [002/005 (0040/0755)]\tLoss Ss: 0.020283\n","\tEpoch:03 [002/005 (0060/0755)]\tLoss Ss: 0.030438\n","\tEpoch:03 [002/005 (0080/0755)]\tLoss Ss: 0.037774\n","\tEpoch:03 [002/005 (0100/0755)]\tLoss Ss: 0.020347\n","\tEpoch:03 [002/005 (0120/0755)]\tLoss Ss: 0.019495\n","\tEpoch:03 [002/005 (0140/0755)]\tLoss Ss: 0.039421\n","\tEpoch:03 [002/005 (0160/0755)]\tLoss Ss: 0.016754\n","\tEpoch:03 [002/005 (0180/0755)]\tLoss Ss: 0.028966\n","\tEpoch:03 [002/005 (0200/0755)]\tLoss Ss: 0.026793\n","\tEpoch:03 [002/005 (0220/0755)]\tLoss Ss: 0.019335\n","\tEpoch:03 [002/005 (0240/0755)]\tLoss Ss: 0.027871\n","\tEpoch:03 [002/005 (0260/0755)]\tLoss Ss: 0.019141\n","\tEpoch:03 [002/005 (0280/0755)]\tLoss Ss: 0.024869\n","\tEpoch:03 [002/005 (0300/0755)]\tLoss Ss: 0.019156\n","\tEpoch:03 [002/005 (0320/0755)]\tLoss Ss: 0.022103\n","\tEpoch:03 [002/005 (0340/0755)]\tLoss Ss: 0.019783\n","\tEpoch:03 [002/005 (0360/0755)]\tLoss Ss: 0.028343\n","\tEpoch:03 [002/005 (0380/0755)]\tLoss Ss: 0.017453\n","\tEpoch:03 [002/005 (0400/0755)]\tLoss Ss: 0.022202\n","\tEpoch:03 [002/005 (0420/0755)]\tLoss Ss: 0.021272\n","\tEpoch:03 [002/005 (0440/0755)]\tLoss Ss: 0.027508\n","\tEpoch:03 [002/005 (0460/0755)]\tLoss Ss: 0.019239\n","\tEpoch:03 [002/005 (0480/0755)]\tLoss Ss: 0.019677\n","\tEpoch:03 [002/005 (0500/0755)]\tLoss Ss: 0.030160\n","\tEpoch:03 [002/005 (0520/0755)]\tLoss Ss: 0.021041\n","\tEpoch:03 [002/005 (0540/0755)]\tLoss Ss: 0.023638\n","\tEpoch:03 [002/005 (0560/0755)]\tLoss Ss: 0.031050\n","\tEpoch:03 [002/005 (0580/0755)]\tLoss Ss: 0.017453\n","\tEpoch:03 [002/005 (0600/0755)]\tLoss Ss: 0.023780\n","\tEpoch:03 [002/005 (0620/0755)]\tLoss Ss: 0.024912\n","\tEpoch:03 [002/005 (0640/0755)]\tLoss Ss: 0.022584\n","\tEpoch:03 [002/005 (0660/0755)]\tLoss Ss: 0.027363\n","\tEpoch:03 [002/005 (0680/0755)]\tLoss Ss: 0.028059\n","\tEpoch:03 [002/005 (0700/0755)]\tLoss Ss: 0.014555\n","\tEpoch:03 [002/005 (0720/0755)]\tLoss Ss: 0.018241\n","\tEpoch:03 [002/005 (0740/0755)]\tLoss Ss: 0.018173\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tEpoch:03 [003/005 (0000/0614)]\tLoss Ss: 0.016543\n","\tEpoch:03 [003/005 (0020/0614)]\tLoss Ss: 0.017135\n","\tEpoch:03 [003/005 (0040/0614)]\tLoss Ss: 0.025348\n","\tEpoch:03 [003/005 (0060/0614)]\tLoss Ss: 0.019446\n","\tEpoch:03 [003/005 (0080/0614)]\tLoss Ss: 0.015665\n","\tEpoch:03 [003/005 (0100/0614)]\tLoss Ss: 0.008825\n","\tEpoch:03 [003/005 (0120/0614)]\tLoss Ss: 0.014604\n","\tEpoch:03 [003/005 (0140/0614)]\tLoss Ss: 0.016350\n","\tEpoch:03 [003/005 (0160/0614)]\tLoss Ss: 0.041100\n","\tEpoch:03 [003/005 (0180/0614)]\tLoss Ss: 0.014962\n","\tEpoch:03 [003/005 (0200/0614)]\tLoss Ss: 0.010184\n","\tEpoch:03 [003/005 (0220/0614)]\tLoss Ss: 0.018450\n","\tEpoch:03 [003/005 (0240/0614)]\tLoss Ss: 0.014114\n","\tEpoch:03 [003/005 (0260/0614)]\tLoss Ss: 0.008001\n","\tEpoch:03 [003/005 (0280/0614)]\tLoss Ss: 0.011787\n","\tEpoch:03 [003/005 (0300/0614)]\tLoss Ss: 0.012904\n","\tEpoch:03 [003/005 (0320/0614)]\tLoss Ss: 0.013407\n","\tEpoch:03 [003/005 (0340/0614)]\tLoss Ss: 0.011317\n","\tEpoch:03 [003/005 (0360/0614)]\tLoss Ss: 0.008822\n","\tEpoch:03 [003/005 (0380/0614)]\tLoss Ss: 0.009748\n","\tEpoch:03 [003/005 (0400/0614)]\tLoss Ss: 0.018927\n","\tEpoch:03 [003/005 (0420/0614)]\tLoss Ss: 0.008800\n","\tEpoch:03 [003/005 (0440/0614)]\tLoss Ss: 0.008915\n","\tEpoch:03 [003/005 (0460/0614)]\tLoss Ss: 0.009581\n","\tEpoch:03 [003/005 (0480/0614)]\tLoss Ss: 0.017536\n","\tEpoch:03 [003/005 (0500/0614)]\tLoss Ss: 0.007429\n","\tEpoch:03 [003/005 (0520/0614)]\tLoss Ss: 0.010627\n","\tEpoch:03 [003/005 (0540/0614)]\tLoss Ss: 0.010310\n","\tEpoch:03 [003/005 (0560/0614)]\tLoss Ss: 0.009213\n","\tEpoch:03 [003/005 (0580/0614)]\tLoss Ss: 0.014752\n","\tEpoch:03 [003/005 (0600/0614)]\tLoss Ss: 0.011984\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tEpoch:03 [004/005 (0000/0588)]\tLoss Ss: 0.019482\n","\tEpoch:03 [004/005 (0020/0588)]\tLoss Ss: 0.011214\n","\tEpoch:03 [004/005 (0040/0588)]\tLoss Ss: 0.008591\n","\tEpoch:03 [004/005 (0060/0588)]\tLoss Ss: 0.012886\n","\tEpoch:03 [004/005 (0080/0588)]\tLoss Ss: 0.008931\n","\tEpoch:03 [004/005 (0100/0588)]\tLoss Ss: 0.010605\n","\tEpoch:03 [004/005 (0120/0588)]\tLoss Ss: 0.011757\n","\tEpoch:03 [004/005 (0140/0588)]\tLoss Ss: 0.011257\n","\tEpoch:03 [004/005 (0160/0588)]\tLoss Ss: 0.011846\n","\tEpoch:03 [004/005 (0180/0588)]\tLoss Ss: 0.007243\n","\tEpoch:03 [004/005 (0200/0588)]\tLoss Ss: 0.008632\n","\tEpoch:03 [004/005 (0220/0588)]\tLoss Ss: 0.007128\n","\tEpoch:03 [004/005 (0240/0588)]\tLoss Ss: 0.012975\n","\tEpoch:03 [004/005 (0260/0588)]\tLoss Ss: 0.008675\n","\tEpoch:03 [004/005 (0280/0588)]\tLoss Ss: 0.008614\n","\tEpoch:03 [004/005 (0300/0588)]\tLoss Ss: 0.007895\n","\tEpoch:03 [004/005 (0320/0588)]\tLoss Ss: 0.006750\n","\tEpoch:03 [004/005 (0340/0588)]\tLoss Ss: 0.007943\n","\tEpoch:03 [004/005 (0360/0588)]\tLoss Ss: 0.006972\n","\tEpoch:03 [004/005 (0380/0588)]\tLoss Ss: 0.009815\n","\tEpoch:03 [004/005 (0400/0588)]\tLoss Ss: 0.009001\n","\tEpoch:03 [004/005 (0420/0588)]\tLoss Ss: 0.005846\n","\tEpoch:03 [004/005 (0440/0588)]\tLoss Ss: 0.005674\n","\tEpoch:03 [004/005 (0460/0588)]\tLoss Ss: 0.007756\n","\tEpoch:03 [004/005 (0480/0588)]\tLoss Ss: 0.008052\n","\tEpoch:03 [004/005 (0500/0588)]\tLoss Ss: 0.006519\n","\tEpoch:03 [004/005 (0520/0588)]\tLoss Ss: 0.007759\n","\tEpoch:03 [004/005 (0540/0588)]\tLoss Ss: 0.006400\n","\tEpoch:03 [004/005 (0560/0588)]\tLoss Ss: 0.007125\n","\tEpoch:03 [004/005 (0580/0588)]\tLoss Ss: 0.007702\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tEpoch:03 [005/005 (0000/0693)]\tLoss Ss: 0.041356\n","\tEpoch:03 [005/005 (0020/0693)]\tLoss Ss: 0.016394\n","\tEpoch:03 [005/005 (0040/0693)]\tLoss Ss: 0.017404\n","\tEpoch:03 [005/005 (0060/0693)]\tLoss Ss: 0.027420\n","\tEpoch:03 [005/005 (0080/0693)]\tLoss Ss: 0.026398\n","\tEpoch:03 [005/005 (0100/0693)]\tLoss Ss: 0.017977\n","\tEpoch:03 [005/005 (0120/0693)]\tLoss Ss: 0.035067\n","\tEpoch:03 [005/005 (0140/0693)]\tLoss Ss: 0.017961\n","\tEpoch:03 [005/005 (0160/0693)]\tLoss Ss: 0.016692\n","\tEpoch:03 [005/005 (0180/0693)]\tLoss Ss: 0.025997\n","\tEpoch:03 [005/005 (0200/0693)]\tLoss Ss: 0.024639\n","\tEpoch:03 [005/005 (0220/0693)]\tLoss Ss: 0.027302\n","\tEpoch:03 [005/005 (0240/0693)]\tLoss Ss: 0.022861\n","\tEpoch:03 [005/005 (0260/0693)]\tLoss Ss: 0.016475\n","\tEpoch:03 [005/005 (0280/0693)]\tLoss Ss: 0.019713\n","\tEpoch:03 [005/005 (0300/0693)]\tLoss Ss: 0.022178\n","\tEpoch:03 [005/005 (0320/0693)]\tLoss Ss: 0.022998\n","\tEpoch:03 [005/005 (0340/0693)]\tLoss Ss: 0.012469\n","\tEpoch:03 [005/005 (0360/0693)]\tLoss Ss: 0.018962\n","\tEpoch:03 [005/005 (0380/0693)]\tLoss Ss: 0.014122\n","\tEpoch:03 [005/005 (0400/0693)]\tLoss Ss: 0.032046\n","\tEpoch:03 [005/005 (0420/0693)]\tLoss Ss: 0.010785\n","\tEpoch:03 [005/005 (0440/0693)]\tLoss Ss: 0.018351\n","\tEpoch:03 [005/005 (0460/0693)]\tLoss Ss: 0.028956\n","\tEpoch:03 [005/005 (0480/0693)]\tLoss Ss: 0.019216\n","\tEpoch:03 [005/005 (0500/0693)]\tLoss Ss: 0.016355\n","\tEpoch:03 [005/005 (0520/0693)]\tLoss Ss: 0.030522\n","\tEpoch:03 [005/005 (0540/0693)]\tLoss Ss: 0.016247\n","\tEpoch:03 [005/005 (0560/0693)]\tLoss Ss: 0.024089\n","\tEpoch:03 [005/005 (0580/0693)]\tLoss Ss: 0.019846\n","\tEpoch:03 [005/005 (0600/0693)]\tLoss Ss: 0.020905\n","\tEpoch:03 [005/005 (0620/0693)]\tLoss Ss: 0.018846\n","\tEpoch:03 [005/005 (0640/0693)]\tLoss Ss: 0.018068\n","\tEpoch:03 [005/005 (0660/0693)]\tLoss Ss: 0.019556\n","\tEpoch:03 [005/005 (0680/0693)]\tLoss Ss: 0.013661\n","Now train the rotated image\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tRotated_Epoch:03 [000/005 (0000/0588)]\tLoss Ss: 0.416430\n","\tRotated_Epoch:03 [000/005 (0020/0588)]\tLoss Ss: 0.352994\n","\tRotated_Epoch:03 [000/005 (0040/0588)]\tLoss Ss: 0.405900\n","\tRotated_Epoch:03 [000/005 (0060/0588)]\tLoss Ss: 0.478435\n","\tRotated_Epoch:03 [000/005 (0080/0588)]\tLoss Ss: 0.276038\n","\tRotated_Epoch:03 [000/005 (0100/0588)]\tLoss Ss: 0.310414\n","\tRotated_Epoch:03 [000/005 (0120/0588)]\tLoss Ss: 0.179681\n","\tRotated_Epoch:03 [000/005 (0140/0588)]\tLoss Ss: 0.211857\n","\tRotated_Epoch:03 [000/005 (0160/0588)]\tLoss Ss: 0.144389\n","\tRotated_Epoch:03 [000/005 (0180/0588)]\tLoss Ss: 0.101493\n","\tRotated_Epoch:03 [000/005 (0200/0588)]\tLoss Ss: 0.183838\n","\tRotated_Epoch:03 [000/005 (0220/0588)]\tLoss Ss: 0.143192\n","\tRotated_Epoch:03 [000/005 (0240/0588)]\tLoss Ss: 0.104719\n","\tRotated_Epoch:03 [000/005 (0260/0588)]\tLoss Ss: 0.089774\n","\tRotated_Epoch:03 [000/005 (0280/0588)]\tLoss Ss: 0.103640\n","\tRotated_Epoch:03 [000/005 (0300/0588)]\tLoss Ss: 0.121903\n","\tRotated_Epoch:03 [000/005 (0320/0588)]\tLoss Ss: 0.129058\n","\tRotated_Epoch:03 [000/005 (0340/0588)]\tLoss Ss: 0.107573\n","\tRotated_Epoch:03 [000/005 (0360/0588)]\tLoss Ss: 0.079696\n","\tRotated_Epoch:03 [000/005 (0380/0588)]\tLoss Ss: 0.087001\n","\tRotated_Epoch:03 [000/005 (0400/0588)]\tLoss Ss: 0.084292\n","\tRotated_Epoch:03 [000/005 (0420/0588)]\tLoss Ss: 0.095804\n","\tRotated_Epoch:03 [000/005 (0440/0588)]\tLoss Ss: 0.124365\n","\tRotated_Epoch:03 [000/005 (0460/0588)]\tLoss Ss: 0.087344\n","\tRotated_Epoch:03 [000/005 (0480/0588)]\tLoss Ss: 0.117086\n","\tRotated_Epoch:03 [000/005 (0500/0588)]\tLoss Ss: 0.094481\n","\tRotated_Epoch:03 [000/005 (0520/0588)]\tLoss Ss: 0.074547\n","\tRotated_Epoch:03 [000/005 (0540/0588)]\tLoss Ss: 0.107059\n","\tRotated_Epoch:03 [000/005 (0560/0588)]\tLoss Ss: 0.075675\n","\tRotated_Epoch:03 [000/005 (0580/0588)]\tLoss Ss: 0.073374\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tRotated_Epoch:03 [001/005 (0000/0693)]\tLoss Ss: 0.283061\n","\tRotated_Epoch:03 [001/005 (0020/0693)]\tLoss Ss: 0.335048\n","\tRotated_Epoch:03 [001/005 (0040/0693)]\tLoss Ss: 0.253916\n","\tRotated_Epoch:03 [001/005 (0060/0693)]\tLoss Ss: 0.131259\n","\tRotated_Epoch:03 [001/005 (0080/0693)]\tLoss Ss: 0.106447\n","\tRotated_Epoch:03 [001/005 (0100/0693)]\tLoss Ss: 0.131813\n","\tRotated_Epoch:03 [001/005 (0120/0693)]\tLoss Ss: 0.084123\n","\tRotated_Epoch:03 [001/005 (0140/0693)]\tLoss Ss: 0.079425\n","\tRotated_Epoch:03 [001/005 (0160/0693)]\tLoss Ss: 0.063125\n","\tRotated_Epoch:03 [001/005 (0180/0693)]\tLoss Ss: 0.056588\n","\tRotated_Epoch:03 [001/005 (0200/0693)]\tLoss Ss: 0.059476\n","\tRotated_Epoch:03 [001/005 (0220/0693)]\tLoss Ss: 0.057577\n","\tRotated_Epoch:03 [001/005 (0240/0693)]\tLoss Ss: 0.057554\n","\tRotated_Epoch:03 [001/005 (0260/0693)]\tLoss Ss: 0.053051\n","\tRotated_Epoch:03 [001/005 (0280/0693)]\tLoss Ss: 0.055823\n","\tRotated_Epoch:03 [001/005 (0300/0693)]\tLoss Ss: 0.057498\n","\tRotated_Epoch:03 [001/005 (0320/0693)]\tLoss Ss: 0.047179\n","\tRotated_Epoch:03 [001/005 (0340/0693)]\tLoss Ss: 0.046770\n","\tRotated_Epoch:03 [001/005 (0360/0693)]\tLoss Ss: 0.039209\n","\tRotated_Epoch:03 [001/005 (0380/0693)]\tLoss Ss: 0.035907\n","\tRotated_Epoch:03 [001/005 (0400/0693)]\tLoss Ss: 0.038444\n","\tRotated_Epoch:03 [001/005 (0420/0693)]\tLoss Ss: 0.045704\n","\tRotated_Epoch:03 [001/005 (0440/0693)]\tLoss Ss: 0.036093\n","\tRotated_Epoch:03 [001/005 (0460/0693)]\tLoss Ss: 0.035680\n","\tRotated_Epoch:03 [001/005 (0480/0693)]\tLoss Ss: 0.037221\n","\tRotated_Epoch:03 [001/005 (0500/0693)]\tLoss Ss: 0.035638\n","\tRotated_Epoch:03 [001/005 (0520/0693)]\tLoss Ss: 0.034751\n","\tRotated_Epoch:03 [001/005 (0540/0693)]\tLoss Ss: 0.034261\n","\tRotated_Epoch:03 [001/005 (0560/0693)]\tLoss Ss: 0.031154\n","\tRotated_Epoch:03 [001/005 (0580/0693)]\tLoss Ss: 0.025466\n","\tRotated_Epoch:03 [001/005 (0600/0693)]\tLoss Ss: 0.026781\n","\tRotated_Epoch:03 [001/005 (0620/0693)]\tLoss Ss: 0.033041\n","\tRotated_Epoch:03 [001/005 (0640/0693)]\tLoss Ss: 0.024589\n","\tRotated_Epoch:03 [001/005 (0660/0693)]\tLoss Ss: 0.033342\n","\tRotated_Epoch:03 [001/005 (0680/0693)]\tLoss Ss: 0.027890\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tRotated_Epoch:03 [002/005 (0000/0693)]\tLoss Ss: 0.027912\n","\tRotated_Epoch:03 [002/005 (0020/0693)]\tLoss Ss: 0.031714\n","\tRotated_Epoch:03 [002/005 (0040/0693)]\tLoss Ss: 0.023954\n","\tRotated_Epoch:03 [002/005 (0060/0693)]\tLoss Ss: 0.023867\n","\tRotated_Epoch:03 [002/005 (0080/0693)]\tLoss Ss: 0.030727\n","\tRotated_Epoch:03 [002/005 (0100/0693)]\tLoss Ss: 0.031588\n","\tRotated_Epoch:03 [002/005 (0120/0693)]\tLoss Ss: 0.023804\n","\tRotated_Epoch:03 [002/005 (0140/0693)]\tLoss Ss: 0.019013\n","\tRotated_Epoch:03 [002/005 (0160/0693)]\tLoss Ss: 0.030542\n","\tRotated_Epoch:03 [002/005 (0180/0693)]\tLoss Ss: 0.021328\n","\tRotated_Epoch:03 [002/005 (0200/0693)]\tLoss Ss: 0.022363\n","\tRotated_Epoch:03 [002/005 (0220/0693)]\tLoss Ss: 0.028491\n","\tRotated_Epoch:03 [002/005 (0240/0693)]\tLoss Ss: 0.022291\n","\tRotated_Epoch:03 [002/005 (0260/0693)]\tLoss Ss: 0.021622\n","\tRotated_Epoch:03 [002/005 (0280/0693)]\tLoss Ss: 0.017243\n","\tRotated_Epoch:03 [002/005 (0300/0693)]\tLoss Ss: 0.025293\n","\tRotated_Epoch:03 [002/005 (0320/0693)]\tLoss Ss: 0.028501\n","\tRotated_Epoch:03 [002/005 (0340/0693)]\tLoss Ss: 0.019497\n","\tRotated_Epoch:03 [002/005 (0360/0693)]\tLoss Ss: 0.020264\n","\tRotated_Epoch:03 [002/005 (0380/0693)]\tLoss Ss: 0.015408\n","\tRotated_Epoch:03 [002/005 (0400/0693)]\tLoss Ss: 0.018439\n","\tRotated_Epoch:03 [002/005 (0420/0693)]\tLoss Ss: 0.025407\n","\tRotated_Epoch:03 [002/005 (0440/0693)]\tLoss Ss: 0.020589\n","\tRotated_Epoch:03 [002/005 (0460/0693)]\tLoss Ss: 0.017192\n","\tRotated_Epoch:03 [002/005 (0480/0693)]\tLoss Ss: 0.021198\n","\tRotated_Epoch:03 [002/005 (0500/0693)]\tLoss Ss: 0.023034\n","\tRotated_Epoch:03 [002/005 (0520/0693)]\tLoss Ss: 0.022534\n","\tRotated_Epoch:03 [002/005 (0540/0693)]\tLoss Ss: 0.020662\n","\tRotated_Epoch:03 [002/005 (0560/0693)]\tLoss Ss: 0.037814\n","\tRotated_Epoch:03 [002/005 (0580/0693)]\tLoss Ss: 0.020445\n","\tRotated_Epoch:03 [002/005 (0600/0693)]\tLoss Ss: 0.020386\n","\tRotated_Epoch:03 [002/005 (0620/0693)]\tLoss Ss: 0.023183\n","\tRotated_Epoch:03 [002/005 (0640/0693)]\tLoss Ss: 0.023818\n","\tRotated_Epoch:03 [002/005 (0660/0693)]\tLoss Ss: 0.018365\n","\tRotated_Epoch:03 [002/005 (0680/0693)]\tLoss Ss: 0.020932\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:03 [003/005 (0000/0755)]\tLoss Ss: 0.041263\n","\tRotated_Epoch:03 [003/005 (0020/0755)]\tLoss Ss: 0.029670\n","\tRotated_Epoch:03 [003/005 (0040/0755)]\tLoss Ss: 0.036084\n","\tRotated_Epoch:03 [003/005 (0060/0755)]\tLoss Ss: 0.043305\n","\tRotated_Epoch:03 [003/005 (0080/0755)]\tLoss Ss: 0.027520\n","\tRotated_Epoch:03 [003/005 (0100/0755)]\tLoss Ss: 0.033803\n","\tRotated_Epoch:03 [003/005 (0120/0755)]\tLoss Ss: 0.030288\n","\tRotated_Epoch:03 [003/005 (0140/0755)]\tLoss Ss: 0.036519\n","\tRotated_Epoch:03 [003/005 (0160/0755)]\tLoss Ss: 0.030276\n","\tRotated_Epoch:03 [003/005 (0180/0755)]\tLoss Ss: 0.042222\n","\tRotated_Epoch:03 [003/005 (0200/0755)]\tLoss Ss: 0.041839\n","\tRotated_Epoch:03 [003/005 (0220/0755)]\tLoss Ss: 0.025625\n","\tRotated_Epoch:03 [003/005 (0240/0755)]\tLoss Ss: 0.026738\n","\tRotated_Epoch:03 [003/005 (0260/0755)]\tLoss Ss: 0.040459\n","\tRotated_Epoch:03 [003/005 (0280/0755)]\tLoss Ss: 0.036877\n","\tRotated_Epoch:03 [003/005 (0300/0755)]\tLoss Ss: 0.022405\n","\tRotated_Epoch:03 [003/005 (0320/0755)]\tLoss Ss: 0.035079\n","\tRotated_Epoch:03 [003/005 (0340/0755)]\tLoss Ss: 0.029246\n","\tRotated_Epoch:03 [003/005 (0360/0755)]\tLoss Ss: 0.023656\n","\tRotated_Epoch:03 [003/005 (0380/0755)]\tLoss Ss: 0.029729\n","\tRotated_Epoch:03 [003/005 (0400/0755)]\tLoss Ss: 0.025254\n","\tRotated_Epoch:03 [003/005 (0420/0755)]\tLoss Ss: 0.032127\n","\tRotated_Epoch:03 [003/005 (0440/0755)]\tLoss Ss: 0.036687\n","\tRotated_Epoch:03 [003/005 (0460/0755)]\tLoss Ss: 0.024984\n","\tRotated_Epoch:03 [003/005 (0480/0755)]\tLoss Ss: 0.023948\n","\tRotated_Epoch:03 [003/005 (0500/0755)]\tLoss Ss: 0.029530\n","\tRotated_Epoch:03 [003/005 (0520/0755)]\tLoss Ss: 0.042154\n","\tRotated_Epoch:03 [003/005 (0540/0755)]\tLoss Ss: 0.019818\n","\tRotated_Epoch:03 [003/005 (0560/0755)]\tLoss Ss: 0.024184\n","\tRotated_Epoch:03 [003/005 (0580/0755)]\tLoss Ss: 0.029104\n","\tRotated_Epoch:03 [003/005 (0600/0755)]\tLoss Ss: 0.030735\n","\tRotated_Epoch:03 [003/005 (0620/0755)]\tLoss Ss: 0.022379\n","\tRotated_Epoch:03 [003/005 (0640/0755)]\tLoss Ss: 0.035941\n","\tRotated_Epoch:03 [003/005 (0660/0755)]\tLoss Ss: 0.023856\n","\tRotated_Epoch:03 [003/005 (0680/0755)]\tLoss Ss: 0.021687\n","\tRotated_Epoch:03 [003/005 (0700/0755)]\tLoss Ss: 0.026735\n","\tRotated_Epoch:03 [003/005 (0720/0755)]\tLoss Ss: 0.018565\n","\tRotated_Epoch:03 [003/005 (0740/0755)]\tLoss Ss: 0.016906\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:03 [004/005 (0000/0755)]\tLoss Ss: 0.470817\n","\tRotated_Epoch:03 [004/005 (0020/0755)]\tLoss Ss: 0.428030\n","\tRotated_Epoch:03 [004/005 (0040/0755)]\tLoss Ss: 0.299418\n","\tRotated_Epoch:03 [004/005 (0060/0755)]\tLoss Ss: 0.300869\n","\tRotated_Epoch:03 [004/005 (0080/0755)]\tLoss Ss: 0.241992\n","\tRotated_Epoch:03 [004/005 (0100/0755)]\tLoss Ss: 0.277263\n","\tRotated_Epoch:03 [004/005 (0120/0755)]\tLoss Ss: 0.145024\n","\tRotated_Epoch:03 [004/005 (0140/0755)]\tLoss Ss: 0.185306\n","\tRotated_Epoch:03 [004/005 (0160/0755)]\tLoss Ss: 0.255254\n","\tRotated_Epoch:03 [004/005 (0180/0755)]\tLoss Ss: 0.158156\n","\tRotated_Epoch:03 [004/005 (0200/0755)]\tLoss Ss: 0.097261\n","\tRotated_Epoch:03 [004/005 (0220/0755)]\tLoss Ss: 0.144031\n","\tRotated_Epoch:03 [004/005 (0240/0755)]\tLoss Ss: 0.191818\n","\tRotated_Epoch:03 [004/005 (0260/0755)]\tLoss Ss: 0.127057\n","\tRotated_Epoch:03 [004/005 (0280/0755)]\tLoss Ss: 0.183094\n","\tRotated_Epoch:03 [004/005 (0300/0755)]\tLoss Ss: 0.080495\n","\tRotated_Epoch:03 [004/005 (0320/0755)]\tLoss Ss: 0.146985\n","\tRotated_Epoch:03 [004/005 (0340/0755)]\tLoss Ss: 0.124434\n","\tRotated_Epoch:03 [004/005 (0360/0755)]\tLoss Ss: 0.105818\n","\tRotated_Epoch:03 [004/005 (0380/0755)]\tLoss Ss: 0.128226\n","\tRotated_Epoch:03 [004/005 (0400/0755)]\tLoss Ss: 0.164108\n","\tRotated_Epoch:03 [004/005 (0420/0755)]\tLoss Ss: 0.140974\n","\tRotated_Epoch:03 [004/005 (0440/0755)]\tLoss Ss: 0.096168\n","\tRotated_Epoch:03 [004/005 (0460/0755)]\tLoss Ss: 0.116372\n","\tRotated_Epoch:03 [004/005 (0480/0755)]\tLoss Ss: 0.223671\n","\tRotated_Epoch:03 [004/005 (0500/0755)]\tLoss Ss: 0.106211\n","\tRotated_Epoch:03 [004/005 (0520/0755)]\tLoss Ss: 0.104738\n","\tRotated_Epoch:03 [004/005 (0540/0755)]\tLoss Ss: 0.100230\n","\tRotated_Epoch:03 [004/005 (0560/0755)]\tLoss Ss: 0.103692\n","\tRotated_Epoch:03 [004/005 (0580/0755)]\tLoss Ss: 0.155919\n","\tRotated_Epoch:03 [004/005 (0600/0755)]\tLoss Ss: 0.170410\n","\tRotated_Epoch:03 [004/005 (0620/0755)]\tLoss Ss: 0.135812\n","\tRotated_Epoch:03 [004/005 (0640/0755)]\tLoss Ss: 0.110521\n","\tRotated_Epoch:03 [004/005 (0660/0755)]\tLoss Ss: 0.114395\n","\tRotated_Epoch:03 [004/005 (0680/0755)]\tLoss Ss: 0.093228\n","\tRotated_Epoch:03 [004/005 (0700/0755)]\tLoss Ss: 0.133210\n","\tRotated_Epoch:03 [004/005 (0720/0755)]\tLoss Ss: 0.102502\n","\tRotated_Epoch:03 [004/005 (0740/0755)]\tLoss Ss: 0.149583\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tRotated_Epoch:03 [005/005 (0000/0614)]\tLoss Ss: 0.080968\n","\tRotated_Epoch:03 [005/005 (0020/0614)]\tLoss Ss: 0.074685\n","\tRotated_Epoch:03 [005/005 (0040/0614)]\tLoss Ss: 0.072105\n","\tRotated_Epoch:03 [005/005 (0060/0614)]\tLoss Ss: 0.086791\n","\tRotated_Epoch:03 [005/005 (0080/0614)]\tLoss Ss: 0.051147\n","\tRotated_Epoch:03 [005/005 (0100/0614)]\tLoss Ss: 0.039205\n","\tRotated_Epoch:03 [005/005 (0120/0614)]\tLoss Ss: 0.044077\n","\tRotated_Epoch:03 [005/005 (0140/0614)]\tLoss Ss: 0.038978\n","\tRotated_Epoch:03 [005/005 (0160/0614)]\tLoss Ss: 0.035851\n","\tRotated_Epoch:03 [005/005 (0180/0614)]\tLoss Ss: 0.037508\n","\tRotated_Epoch:03 [005/005 (0200/0614)]\tLoss Ss: 0.037433\n","\tRotated_Epoch:03 [005/005 (0220/0614)]\tLoss Ss: 0.037568\n","\tRotated_Epoch:03 [005/005 (0240/0614)]\tLoss Ss: 0.034674\n","\tRotated_Epoch:03 [005/005 (0260/0614)]\tLoss Ss: 0.027916\n","\tRotated_Epoch:03 [005/005 (0280/0614)]\tLoss Ss: 0.027621\n","\tRotated_Epoch:03 [005/005 (0300/0614)]\tLoss Ss: 0.024172\n","\tRotated_Epoch:03 [005/005 (0320/0614)]\tLoss Ss: 0.027626\n","\tRotated_Epoch:03 [005/005 (0340/0614)]\tLoss Ss: 0.022323\n","\tRotated_Epoch:03 [005/005 (0360/0614)]\tLoss Ss: 0.023596\n","\tRotated_Epoch:03 [005/005 (0380/0614)]\tLoss Ss: 0.027534\n","\tRotated_Epoch:03 [005/005 (0400/0614)]\tLoss Ss: 0.026232\n","\tRotated_Epoch:03 [005/005 (0420/0614)]\tLoss Ss: 0.014065\n","\tRotated_Epoch:03 [005/005 (0440/0614)]\tLoss Ss: 0.019171\n","\tRotated_Epoch:03 [005/005 (0460/0614)]\tLoss Ss: 0.019272\n","\tRotated_Epoch:03 [005/005 (0480/0614)]\tLoss Ss: 0.016218\n","\tRotated_Epoch:03 [005/005 (0500/0614)]\tLoss Ss: 0.014754\n","\tRotated_Epoch:03 [005/005 (0520/0614)]\tLoss Ss: 0.019723\n","\tRotated_Epoch:03 [005/005 (0540/0614)]\tLoss Ss: 0.012727\n","\tRotated_Epoch:03 [005/005 (0560/0614)]\tLoss Ss: 0.017104\n","\tRotated_Epoch:03 [005/005 (0580/0614)]\tLoss Ss: 0.011438\n","\tRotated_Epoch:03 [005/005 (0600/0614)]\tLoss Ss: 0.017450\n","new_1_6_d.nii\n","new_1_6_m.nii\n","new_1_7_d.nii\n","new_1_7_m.nii\n","new_2_5_d.nii\n","new_2_5_m.nii\n","new_2_6_d.nii\n","new_2_6_m.nii\n","new_2_7_d.nii\n","new_2_7_m.nii\n","new_3_d (2).nii\n","new_3_m (2).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (3).nii\n","new_3_m (3).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (7).nii\n","new_3_m (7).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (8).nii\n","new_3_m (8).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (9).nii\n","new_3_m (9).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_4_10_d_163526.nii\n","new_4_10_m_163526.nii.gz\n","new_4_11_d_084336.nii\n","new_4_11_m_084336.nii.gz\n","new_4_7_d_184422.nii\n","new_4_7_m_184422.nii.gz\n","new_4_8_d_143412.nii\n","new_4_8_m_143412.nii.gz\n","new_4_9_d_155347.nii\n","new_4_9_m_155347.nii.gz\n","\tEpoch: 3; Dice: 0.9169 +/- 0.0204; Loss: 21.2689\n","Begin Epoch 4\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tEpoch:04 [000/005 (0000/0614)]\tLoss Ss: 0.014515\n","\tEpoch:04 [000/005 (0020/0614)]\tLoss Ss: 0.015477\n","\tEpoch:04 [000/005 (0040/0614)]\tLoss Ss: 0.016531\n","\tEpoch:04 [000/005 (0060/0614)]\tLoss Ss: 0.011446\n","\tEpoch:04 [000/005 (0080/0614)]\tLoss Ss: 0.016078\n","\tEpoch:04 [000/005 (0100/0614)]\tLoss Ss: 0.021710\n","\tEpoch:04 [000/005 (0120/0614)]\tLoss Ss: 0.012844\n","\tEpoch:04 [000/005 (0140/0614)]\tLoss Ss: 0.015404\n","\tEpoch:04 [000/005 (0160/0614)]\tLoss Ss: 0.015147\n","\tEpoch:04 [000/005 (0180/0614)]\tLoss Ss: 0.010934\n","\tEpoch:04 [000/005 (0200/0614)]\tLoss Ss: 0.013126\n","\tEpoch:04 [000/005 (0220/0614)]\tLoss Ss: 0.012998\n","\tEpoch:04 [000/005 (0240/0614)]\tLoss Ss: 0.012894\n","\tEpoch:04 [000/005 (0260/0614)]\tLoss Ss: 0.010631\n","\tEpoch:04 [000/005 (0280/0614)]\tLoss Ss: 0.012378\n","\tEpoch:04 [000/005 (0300/0614)]\tLoss Ss: 0.013583\n","\tEpoch:04 [000/005 (0320/0614)]\tLoss Ss: 0.011264\n","\tEpoch:04 [000/005 (0340/0614)]\tLoss Ss: 0.016849\n","\tEpoch:04 [000/005 (0360/0614)]\tLoss Ss: 0.006727\n","\tEpoch:04 [000/005 (0380/0614)]\tLoss Ss: 0.019818\n","\tEpoch:04 [000/005 (0400/0614)]\tLoss Ss: 0.016476\n","\tEpoch:04 [000/005 (0420/0614)]\tLoss Ss: 0.009181\n","\tEpoch:04 [000/005 (0440/0614)]\tLoss Ss: 0.012901\n","\tEpoch:04 [000/005 (0460/0614)]\tLoss Ss: 0.017025\n","\tEpoch:04 [000/005 (0480/0614)]\tLoss Ss: 0.011940\n","\tEpoch:04 [000/005 (0500/0614)]\tLoss Ss: 0.010999\n","\tEpoch:04 [000/005 (0520/0614)]\tLoss Ss: 0.012410\n","\tEpoch:04 [000/005 (0540/0614)]\tLoss Ss: 0.014554\n","\tEpoch:04 [000/005 (0560/0614)]\tLoss Ss: 0.010046\n","\tEpoch:04 [000/005 (0580/0614)]\tLoss Ss: 0.008362\n","\tEpoch:04 [000/005 (0600/0614)]\tLoss Ss: 0.009915\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tEpoch:04 [001/005 (0000/0693)]\tLoss Ss: 0.036798\n","\tEpoch:04 [001/005 (0020/0693)]\tLoss Ss: 0.040950\n","\tEpoch:04 [001/005 (0040/0693)]\tLoss Ss: 0.037427\n","\tEpoch:04 [001/005 (0060/0693)]\tLoss Ss: 0.033259\n","\tEpoch:04 [001/005 (0080/0693)]\tLoss Ss: 0.031088\n","\tEpoch:04 [001/005 (0100/0693)]\tLoss Ss: 0.024539\n","\tEpoch:04 [001/005 (0120/0693)]\tLoss Ss: 0.022552\n","\tEpoch:04 [001/005 (0140/0693)]\tLoss Ss: 0.031645\n","\tEpoch:04 [001/005 (0160/0693)]\tLoss Ss: 0.024646\n","\tEpoch:04 [001/005 (0180/0693)]\tLoss Ss: 0.026071\n","\tEpoch:04 [001/005 (0200/0693)]\tLoss Ss: 0.031260\n","\tEpoch:04 [001/005 (0220/0693)]\tLoss Ss: 0.023933\n","\tEpoch:04 [001/005 (0240/0693)]\tLoss Ss: 0.030701\n","\tEpoch:04 [001/005 (0260/0693)]\tLoss Ss: 0.017116\n","\tEpoch:04 [001/005 (0280/0693)]\tLoss Ss: 0.019636\n","\tEpoch:04 [001/005 (0300/0693)]\tLoss Ss: 0.025648\n","\tEpoch:04 [001/005 (0320/0693)]\tLoss Ss: 0.019504\n","\tEpoch:04 [001/005 (0340/0693)]\tLoss Ss: 0.022228\n","\tEpoch:04 [001/005 (0360/0693)]\tLoss Ss: 0.026097\n","\tEpoch:04 [001/005 (0380/0693)]\tLoss Ss: 0.015230\n","\tEpoch:04 [001/005 (0400/0693)]\tLoss Ss: 0.024247\n","\tEpoch:04 [001/005 (0420/0693)]\tLoss Ss: 0.021393\n","\tEpoch:04 [001/005 (0440/0693)]\tLoss Ss: 0.023577\n","\tEpoch:04 [001/005 (0460/0693)]\tLoss Ss: 0.023242\n","\tEpoch:04 [001/005 (0480/0693)]\tLoss Ss: 0.020712\n","\tEpoch:04 [001/005 (0500/0693)]\tLoss Ss: 0.017563\n","\tEpoch:04 [001/005 (0520/0693)]\tLoss Ss: 0.018624\n","\tEpoch:04 [001/005 (0540/0693)]\tLoss Ss: 0.055919\n","\tEpoch:04 [001/005 (0560/0693)]\tLoss Ss: 0.011102\n","\tEpoch:04 [001/005 (0580/0693)]\tLoss Ss: 0.012284\n","\tEpoch:04 [001/005 (0600/0693)]\tLoss Ss: 0.017048\n","\tEpoch:04 [001/005 (0620/0693)]\tLoss Ss: 0.018852\n","\tEpoch:04 [001/005 (0640/0693)]\tLoss Ss: 0.015040\n","\tEpoch:04 [001/005 (0660/0693)]\tLoss Ss: 0.023190\n","\tEpoch:04 [001/005 (0680/0693)]\tLoss Ss: 0.016243\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tEpoch:04 [002/005 (0000/0588)]\tLoss Ss: 0.022813\n","\tEpoch:04 [002/005 (0020/0588)]\tLoss Ss: 0.010964\n","\tEpoch:04 [002/005 (0040/0588)]\tLoss Ss: 0.014287\n","\tEpoch:04 [002/005 (0060/0588)]\tLoss Ss: 0.012225\n","\tEpoch:04 [002/005 (0080/0588)]\tLoss Ss: 0.012891\n","\tEpoch:04 [002/005 (0100/0588)]\tLoss Ss: 0.015494\n","\tEpoch:04 [002/005 (0120/0588)]\tLoss Ss: 0.009905\n","\tEpoch:04 [002/005 (0140/0588)]\tLoss Ss: 0.016824\n","\tEpoch:04 [002/005 (0160/0588)]\tLoss Ss: 0.018062\n","\tEpoch:04 [002/005 (0180/0588)]\tLoss Ss: 0.014046\n","\tEpoch:04 [002/005 (0200/0588)]\tLoss Ss: 0.009291\n","\tEpoch:04 [002/005 (0220/0588)]\tLoss Ss: 0.015285\n","\tEpoch:04 [002/005 (0240/0588)]\tLoss Ss: 0.010205\n","\tEpoch:04 [002/005 (0260/0588)]\tLoss Ss: 0.014152\n","\tEpoch:04 [002/005 (0280/0588)]\tLoss Ss: 0.012687\n","\tEpoch:04 [002/005 (0300/0588)]\tLoss Ss: 0.014060\n","\tEpoch:04 [002/005 (0320/0588)]\tLoss Ss: 0.010018\n","\tEpoch:04 [002/005 (0340/0588)]\tLoss Ss: 0.006183\n","\tEpoch:04 [002/005 (0360/0588)]\tLoss Ss: 0.007658\n","\tEpoch:04 [002/005 (0380/0588)]\tLoss Ss: 0.008145\n","\tEpoch:04 [002/005 (0400/0588)]\tLoss Ss: 0.007714\n","\tEpoch:04 [002/005 (0420/0588)]\tLoss Ss: 0.009915\n","\tEpoch:04 [002/005 (0440/0588)]\tLoss Ss: 0.008442\n","\tEpoch:04 [002/005 (0460/0588)]\tLoss Ss: 0.012570\n","\tEpoch:04 [002/005 (0480/0588)]\tLoss Ss: 0.008060\n","\tEpoch:04 [002/005 (0500/0588)]\tLoss Ss: 0.011120\n","\tEpoch:04 [002/005 (0520/0588)]\tLoss Ss: 0.010212\n","\tEpoch:04 [002/005 (0540/0588)]\tLoss Ss: 0.011076\n","\tEpoch:04 [002/005 (0560/0588)]\tLoss Ss: 0.009474\n","\tEpoch:04 [002/005 (0580/0588)]\tLoss Ss: 0.006636\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tEpoch:04 [003/005 (0000/0693)]\tLoss Ss: 0.019338\n","\tEpoch:04 [003/005 (0020/0693)]\tLoss Ss: 0.025192\n","\tEpoch:04 [003/005 (0040/0693)]\tLoss Ss: 0.019604\n","\tEpoch:04 [003/005 (0060/0693)]\tLoss Ss: 0.018677\n","\tEpoch:04 [003/005 (0080/0693)]\tLoss Ss: 0.020264\n","\tEpoch:04 [003/005 (0100/0693)]\tLoss Ss: 0.017987\n","\tEpoch:04 [003/005 (0120/0693)]\tLoss Ss: 0.031582\n","\tEpoch:04 [003/005 (0140/0693)]\tLoss Ss: 0.022542\n","\tEpoch:04 [003/005 (0160/0693)]\tLoss Ss: 0.022720\n","\tEpoch:04 [003/005 (0180/0693)]\tLoss Ss: 0.021929\n","\tEpoch:04 [003/005 (0200/0693)]\tLoss Ss: 0.021848\n","\tEpoch:04 [003/005 (0220/0693)]\tLoss Ss: 0.021497\n","\tEpoch:04 [003/005 (0240/0693)]\tLoss Ss: 0.016710\n","\tEpoch:04 [003/005 (0260/0693)]\tLoss Ss: 0.014515\n","\tEpoch:04 [003/005 (0280/0693)]\tLoss Ss: 0.022557\n","\tEpoch:04 [003/005 (0300/0693)]\tLoss Ss: 0.017705\n","\tEpoch:04 [003/005 (0320/0693)]\tLoss Ss: 0.021739\n","\tEpoch:04 [003/005 (0340/0693)]\tLoss Ss: 0.014855\n","\tEpoch:04 [003/005 (0360/0693)]\tLoss Ss: 0.016224\n","\tEpoch:04 [003/005 (0380/0693)]\tLoss Ss: 0.021267\n","\tEpoch:04 [003/005 (0400/0693)]\tLoss Ss: 0.013313\n","\tEpoch:04 [003/005 (0420/0693)]\tLoss Ss: 0.016924\n","\tEpoch:04 [003/005 (0440/0693)]\tLoss Ss: 0.015515\n","\tEpoch:04 [003/005 (0460/0693)]\tLoss Ss: 0.018423\n","\tEpoch:04 [003/005 (0480/0693)]\tLoss Ss: 0.013292\n","\tEpoch:04 [003/005 (0500/0693)]\tLoss Ss: 0.014695\n","\tEpoch:04 [003/005 (0520/0693)]\tLoss Ss: 0.020463\n","\tEpoch:04 [003/005 (0540/0693)]\tLoss Ss: 0.015233\n","\tEpoch:04 [003/005 (0560/0693)]\tLoss Ss: 0.017947\n","\tEpoch:04 [003/005 (0580/0693)]\tLoss Ss: 0.015209\n","\tEpoch:04 [003/005 (0600/0693)]\tLoss Ss: 0.023265\n","\tEpoch:04 [003/005 (0620/0693)]\tLoss Ss: 0.012512\n","\tEpoch:04 [003/005 (0640/0693)]\tLoss Ss: 0.016674\n","\tEpoch:04 [003/005 (0660/0693)]\tLoss Ss: 0.027226\n","\tEpoch:04 [003/005 (0680/0693)]\tLoss Ss: 0.031602\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:04 [004/005 (0000/0755)]\tLoss Ss: 0.028361\n","\tEpoch:04 [004/005 (0020/0755)]\tLoss Ss: 0.035782\n","\tEpoch:04 [004/005 (0040/0755)]\tLoss Ss: 0.031896\n","\tEpoch:04 [004/005 (0060/0755)]\tLoss Ss: 0.031038\n","\tEpoch:04 [004/005 (0080/0755)]\tLoss Ss: 0.036568\n","\tEpoch:04 [004/005 (0100/0755)]\tLoss Ss: 0.032530\n","\tEpoch:04 [004/005 (0120/0755)]\tLoss Ss: 0.031991\n","\tEpoch:04 [004/005 (0140/0755)]\tLoss Ss: 0.020674\n","\tEpoch:04 [004/005 (0160/0755)]\tLoss Ss: 0.031848\n","\tEpoch:04 [004/005 (0180/0755)]\tLoss Ss: 0.036535\n","\tEpoch:04 [004/005 (0200/0755)]\tLoss Ss: 0.025792\n","\tEpoch:04 [004/005 (0220/0755)]\tLoss Ss: 0.025744\n","\tEpoch:04 [004/005 (0240/0755)]\tLoss Ss: 0.031025\n","\tEpoch:04 [004/005 (0260/0755)]\tLoss Ss: 0.029202\n","\tEpoch:04 [004/005 (0280/0755)]\tLoss Ss: 0.016717\n","\tEpoch:04 [004/005 (0300/0755)]\tLoss Ss: 0.031840\n","\tEpoch:04 [004/005 (0320/0755)]\tLoss Ss: 0.032386\n","\tEpoch:04 [004/005 (0340/0755)]\tLoss Ss: 0.021269\n","\tEpoch:04 [004/005 (0360/0755)]\tLoss Ss: 0.020478\n","\tEpoch:04 [004/005 (0380/0755)]\tLoss Ss: 0.021464\n","\tEpoch:04 [004/005 (0400/0755)]\tLoss Ss: 0.028948\n","\tEpoch:04 [004/005 (0420/0755)]\tLoss Ss: 0.020192\n","\tEpoch:04 [004/005 (0440/0755)]\tLoss Ss: 0.024782\n","\tEpoch:04 [004/005 (0460/0755)]\tLoss Ss: 0.015958\n","\tEpoch:04 [004/005 (0480/0755)]\tLoss Ss: 0.020578\n","\tEpoch:04 [004/005 (0500/0755)]\tLoss Ss: 0.023934\n","\tEpoch:04 [004/005 (0520/0755)]\tLoss Ss: 0.019946\n","\tEpoch:04 [004/005 (0540/0755)]\tLoss Ss: 0.025180\n","\tEpoch:04 [004/005 (0560/0755)]\tLoss Ss: 0.017123\n","\tEpoch:04 [004/005 (0580/0755)]\tLoss Ss: 0.025003\n","\tEpoch:04 [004/005 (0600/0755)]\tLoss Ss: 0.030181\n","\tEpoch:04 [004/005 (0620/0755)]\tLoss Ss: 0.016590\n","\tEpoch:04 [004/005 (0640/0755)]\tLoss Ss: 0.026111\n","\tEpoch:04 [004/005 (0660/0755)]\tLoss Ss: 0.035693\n","\tEpoch:04 [004/005 (0680/0755)]\tLoss Ss: 0.016800\n","\tEpoch:04 [004/005 (0700/0755)]\tLoss Ss: 0.021544\n","\tEpoch:04 [004/005 (0720/0755)]\tLoss Ss: 0.021966\n","\tEpoch:04 [004/005 (0740/0755)]\tLoss Ss: 0.024458\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:04 [005/005 (0000/0755)]\tLoss Ss: 0.014861\n","\tEpoch:04 [005/005 (0020/0755)]\tLoss Ss: 0.025299\n","\tEpoch:04 [005/005 (0040/0755)]\tLoss Ss: 0.025684\n","\tEpoch:04 [005/005 (0060/0755)]\tLoss Ss: 0.027428\n","\tEpoch:04 [005/005 (0080/0755)]\tLoss Ss: 0.035676\n","\tEpoch:04 [005/005 (0100/0755)]\tLoss Ss: 0.043382\n","\tEpoch:04 [005/005 (0120/0755)]\tLoss Ss: 0.034052\n","\tEpoch:04 [005/005 (0140/0755)]\tLoss Ss: 0.022839\n","\tEpoch:04 [005/005 (0160/0755)]\tLoss Ss: 0.018264\n","\tEpoch:04 [005/005 (0180/0755)]\tLoss Ss: 0.019916\n","\tEpoch:04 [005/005 (0200/0755)]\tLoss Ss: 0.024696\n","\tEpoch:04 [005/005 (0220/0755)]\tLoss Ss: 0.019115\n","\tEpoch:04 [005/005 (0240/0755)]\tLoss Ss: 0.025566\n","\tEpoch:04 [005/005 (0260/0755)]\tLoss Ss: 0.021525\n","\tEpoch:04 [005/005 (0280/0755)]\tLoss Ss: 0.016214\n","\tEpoch:04 [005/005 (0300/0755)]\tLoss Ss: 0.019366\n","\tEpoch:04 [005/005 (0320/0755)]\tLoss Ss: 0.024953\n","\tEpoch:04 [005/005 (0340/0755)]\tLoss Ss: 0.026610\n","\tEpoch:04 [005/005 (0360/0755)]\tLoss Ss: 0.018714\n","\tEpoch:04 [005/005 (0380/0755)]\tLoss Ss: 0.024308\n","\tEpoch:04 [005/005 (0400/0755)]\tLoss Ss: 0.025720\n","\tEpoch:04 [005/005 (0420/0755)]\tLoss Ss: 0.030233\n","\tEpoch:04 [005/005 (0440/0755)]\tLoss Ss: 0.033154\n","\tEpoch:04 [005/005 (0460/0755)]\tLoss Ss: 0.024263\n","\tEpoch:04 [005/005 (0480/0755)]\tLoss Ss: 0.024071\n","\tEpoch:04 [005/005 (0500/0755)]\tLoss Ss: 0.022780\n","\tEpoch:04 [005/005 (0520/0755)]\tLoss Ss: 0.015009\n","\tEpoch:04 [005/005 (0540/0755)]\tLoss Ss: 0.019372\n","\tEpoch:04 [005/005 (0560/0755)]\tLoss Ss: 0.016794\n","\tEpoch:04 [005/005 (0580/0755)]\tLoss Ss: 0.013831\n","\tEpoch:04 [005/005 (0600/0755)]\tLoss Ss: 0.023405\n","\tEpoch:04 [005/005 (0620/0755)]\tLoss Ss: 0.018104\n","\tEpoch:04 [005/005 (0640/0755)]\tLoss Ss: 0.024021\n","\tEpoch:04 [005/005 (0660/0755)]\tLoss Ss: 0.026074\n","\tEpoch:04 [005/005 (0680/0755)]\tLoss Ss: 0.023859\n","\tEpoch:04 [005/005 (0700/0755)]\tLoss Ss: 0.025525\n","\tEpoch:04 [005/005 (0720/0755)]\tLoss Ss: 0.024327\n","\tEpoch:04 [005/005 (0740/0755)]\tLoss Ss: 0.026931\n","Now train the rotated image\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tRotated_Epoch:04 [000/005 (0000/0614)]\tLoss Ss: 0.028035\n","\tRotated_Epoch:04 [000/005 (0020/0614)]\tLoss Ss: 0.020373\n","\tRotated_Epoch:04 [000/005 (0040/0614)]\tLoss Ss: 0.009468\n","\tRotated_Epoch:04 [000/005 (0060/0614)]\tLoss Ss: 0.010429\n","\tRotated_Epoch:04 [000/005 (0080/0614)]\tLoss Ss: 0.015408\n","\tRotated_Epoch:04 [000/005 (0100/0614)]\tLoss Ss: 0.025465\n","\tRotated_Epoch:04 [000/005 (0120/0614)]\tLoss Ss: 0.018621\n","\tRotated_Epoch:04 [000/005 (0140/0614)]\tLoss Ss: 0.023629\n","\tRotated_Epoch:04 [000/005 (0160/0614)]\tLoss Ss: 0.014361\n","\tRotated_Epoch:04 [000/005 (0180/0614)]\tLoss Ss: 0.013149\n","\tRotated_Epoch:04 [000/005 (0200/0614)]\tLoss Ss: 0.010259\n","\tRotated_Epoch:04 [000/005 (0220/0614)]\tLoss Ss: 0.016094\n","\tRotated_Epoch:04 [000/005 (0240/0614)]\tLoss Ss: 0.008948\n","\tRotated_Epoch:04 [000/005 (0260/0614)]\tLoss Ss: 0.014108\n","\tRotated_Epoch:04 [000/005 (0280/0614)]\tLoss Ss: 0.012179\n","\tRotated_Epoch:04 [000/005 (0300/0614)]\tLoss Ss: 0.015482\n","\tRotated_Epoch:04 [000/005 (0320/0614)]\tLoss Ss: 0.008110\n","\tRotated_Epoch:04 [000/005 (0340/0614)]\tLoss Ss: 0.010709\n","\tRotated_Epoch:04 [000/005 (0360/0614)]\tLoss Ss: 0.008182\n","\tRotated_Epoch:04 [000/005 (0380/0614)]\tLoss Ss: 0.010873\n","\tRotated_Epoch:04 [000/005 (0400/0614)]\tLoss Ss: 0.012361\n","\tRotated_Epoch:04 [000/005 (0420/0614)]\tLoss Ss: 0.009003\n","\tRotated_Epoch:04 [000/005 (0440/0614)]\tLoss Ss: 0.008592\n","\tRotated_Epoch:04 [000/005 (0460/0614)]\tLoss Ss: 0.005131\n","\tRotated_Epoch:04 [000/005 (0480/0614)]\tLoss Ss: 0.010048\n","\tRotated_Epoch:04 [000/005 (0500/0614)]\tLoss Ss: 0.009207\n","\tRotated_Epoch:04 [000/005 (0520/0614)]\tLoss Ss: 0.015648\n","\tRotated_Epoch:04 [000/005 (0540/0614)]\tLoss Ss: 0.013861\n","\tRotated_Epoch:04 [000/005 (0560/0614)]\tLoss Ss: 0.012031\n","\tRotated_Epoch:04 [000/005 (0580/0614)]\tLoss Ss: 0.008461\n","\tRotated_Epoch:04 [000/005 (0600/0614)]\tLoss Ss: 0.008831\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tRotated_Epoch:04 [001/005 (0000/0693)]\tLoss Ss: 0.027605\n","\tRotated_Epoch:04 [001/005 (0020/0693)]\tLoss Ss: 0.022489\n","\tRotated_Epoch:04 [001/005 (0040/0693)]\tLoss Ss: 0.017813\n","\tRotated_Epoch:04 [001/005 (0060/0693)]\tLoss Ss: 0.033143\n","\tRotated_Epoch:04 [001/005 (0080/0693)]\tLoss Ss: 0.025775\n","\tRotated_Epoch:04 [001/005 (0100/0693)]\tLoss Ss: 0.021340\n","\tRotated_Epoch:04 [001/005 (0120/0693)]\tLoss Ss: 0.018883\n","\tRotated_Epoch:04 [001/005 (0140/0693)]\tLoss Ss: 0.027496\n","\tRotated_Epoch:04 [001/005 (0160/0693)]\tLoss Ss: 0.029803\n","\tRotated_Epoch:04 [001/005 (0180/0693)]\tLoss Ss: 0.018650\n","\tRotated_Epoch:04 [001/005 (0200/0693)]\tLoss Ss: 0.018735\n","\tRotated_Epoch:04 [001/005 (0220/0693)]\tLoss Ss: 0.016738\n","\tRotated_Epoch:04 [001/005 (0240/0693)]\tLoss Ss: 0.021986\n","\tRotated_Epoch:04 [001/005 (0260/0693)]\tLoss Ss: 0.023991\n","\tRotated_Epoch:04 [001/005 (0280/0693)]\tLoss Ss: 0.014837\n","\tRotated_Epoch:04 [001/005 (0300/0693)]\tLoss Ss: 0.020535\n","\tRotated_Epoch:04 [001/005 (0320/0693)]\tLoss Ss: 0.032840\n","\tRotated_Epoch:04 [001/005 (0340/0693)]\tLoss Ss: 0.020195\n","\tRotated_Epoch:04 [001/005 (0360/0693)]\tLoss Ss: 0.014269\n","\tRotated_Epoch:04 [001/005 (0380/0693)]\tLoss Ss: 0.027710\n","\tRotated_Epoch:04 [001/005 (0400/0693)]\tLoss Ss: 0.022754\n","\tRotated_Epoch:04 [001/005 (0420/0693)]\tLoss Ss: 0.015489\n","\tRotated_Epoch:04 [001/005 (0440/0693)]\tLoss Ss: 0.021849\n","\tRotated_Epoch:04 [001/005 (0460/0693)]\tLoss Ss: 0.020086\n","\tRotated_Epoch:04 [001/005 (0480/0693)]\tLoss Ss: 0.023529\n","\tRotated_Epoch:04 [001/005 (0500/0693)]\tLoss Ss: 0.029584\n","\tRotated_Epoch:04 [001/005 (0520/0693)]\tLoss Ss: 0.035529\n","\tRotated_Epoch:04 [001/005 (0540/0693)]\tLoss Ss: 0.022557\n","\tRotated_Epoch:04 [001/005 (0560/0693)]\tLoss Ss: 0.028313\n","\tRotated_Epoch:04 [001/005 (0580/0693)]\tLoss Ss: 0.020175\n","\tRotated_Epoch:04 [001/005 (0600/0693)]\tLoss Ss: 0.016129\n","\tRotated_Epoch:04 [001/005 (0620/0693)]\tLoss Ss: 0.019608\n","\tRotated_Epoch:04 [001/005 (0640/0693)]\tLoss Ss: 0.016015\n","\tRotated_Epoch:04 [001/005 (0660/0693)]\tLoss Ss: 0.024810\n","\tRotated_Epoch:04 [001/005 (0680/0693)]\tLoss Ss: 0.019713\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tRotated_Epoch:04 [002/005 (0000/0693)]\tLoss Ss: 0.020701\n","\tRotated_Epoch:04 [002/005 (0020/0693)]\tLoss Ss: 0.033587\n","\tRotated_Epoch:04 [002/005 (0040/0693)]\tLoss Ss: 0.024976\n","\tRotated_Epoch:04 [002/005 (0060/0693)]\tLoss Ss: 0.017075\n","\tRotated_Epoch:04 [002/005 (0080/0693)]\tLoss Ss: 0.020670\n","\tRotated_Epoch:04 [002/005 (0100/0693)]\tLoss Ss: 0.017041\n","\tRotated_Epoch:04 [002/005 (0120/0693)]\tLoss Ss: 0.011973\n","\tRotated_Epoch:04 [002/005 (0140/0693)]\tLoss Ss: 0.020995\n","\tRotated_Epoch:04 [002/005 (0160/0693)]\tLoss Ss: 0.026413\n","\tRotated_Epoch:04 [002/005 (0180/0693)]\tLoss Ss: 0.015177\n","\tRotated_Epoch:04 [002/005 (0200/0693)]\tLoss Ss: 0.019081\n","\tRotated_Epoch:04 [002/005 (0220/0693)]\tLoss Ss: 0.028886\n","\tRotated_Epoch:04 [002/005 (0240/0693)]\tLoss Ss: 0.025797\n","\tRotated_Epoch:04 [002/005 (0260/0693)]\tLoss Ss: 0.020539\n","\tRotated_Epoch:04 [002/005 (0280/0693)]\tLoss Ss: 0.018326\n","\tRotated_Epoch:04 [002/005 (0300/0693)]\tLoss Ss: 0.020860\n","\tRotated_Epoch:04 [002/005 (0320/0693)]\tLoss Ss: 0.017800\n","\tRotated_Epoch:04 [002/005 (0340/0693)]\tLoss Ss: 0.022043\n","\tRotated_Epoch:04 [002/005 (0360/0693)]\tLoss Ss: 0.015638\n","\tRotated_Epoch:04 [002/005 (0380/0693)]\tLoss Ss: 0.020114\n","\tRotated_Epoch:04 [002/005 (0400/0693)]\tLoss Ss: 0.018109\n","\tRotated_Epoch:04 [002/005 (0420/0693)]\tLoss Ss: 0.017581\n","\tRotated_Epoch:04 [002/005 (0440/0693)]\tLoss Ss: 0.019869\n","\tRotated_Epoch:04 [002/005 (0460/0693)]\tLoss Ss: 0.019064\n","\tRotated_Epoch:04 [002/005 (0480/0693)]\tLoss Ss: 0.025919\n","\tRotated_Epoch:04 [002/005 (0500/0693)]\tLoss Ss: 0.018313\n","\tRotated_Epoch:04 [002/005 (0520/0693)]\tLoss Ss: 0.022600\n","\tRotated_Epoch:04 [002/005 (0540/0693)]\tLoss Ss: 0.022112\n","\tRotated_Epoch:04 [002/005 (0560/0693)]\tLoss Ss: 0.018387\n","\tRotated_Epoch:04 [002/005 (0580/0693)]\tLoss Ss: 0.016170\n","\tRotated_Epoch:04 [002/005 (0600/0693)]\tLoss Ss: 0.023202\n","\tRotated_Epoch:04 [002/005 (0620/0693)]\tLoss Ss: 0.017247\n","\tRotated_Epoch:04 [002/005 (0640/0693)]\tLoss Ss: 0.023656\n","\tRotated_Epoch:04 [002/005 (0660/0693)]\tLoss Ss: 0.020291\n","\tRotated_Epoch:04 [002/005 (0680/0693)]\tLoss Ss: 0.017735\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:04 [003/005 (0000/0755)]\tLoss Ss: 0.041471\n","\tRotated_Epoch:04 [003/005 (0020/0755)]\tLoss Ss: 0.031121\n","\tRotated_Epoch:04 [003/005 (0040/0755)]\tLoss Ss: 0.029450\n","\tRotated_Epoch:04 [003/005 (0060/0755)]\tLoss Ss: 0.038068\n","\tRotated_Epoch:04 [003/005 (0080/0755)]\tLoss Ss: 0.026992\n","\tRotated_Epoch:04 [003/005 (0100/0755)]\tLoss Ss: 0.019565\n","\tRotated_Epoch:04 [003/005 (0120/0755)]\tLoss Ss: 0.027421\n","\tRotated_Epoch:04 [003/005 (0140/0755)]\tLoss Ss: 0.026129\n","\tRotated_Epoch:04 [003/005 (0160/0755)]\tLoss Ss: 0.029978\n","\tRotated_Epoch:04 [003/005 (0180/0755)]\tLoss Ss: 0.022700\n","\tRotated_Epoch:04 [003/005 (0200/0755)]\tLoss Ss: 0.023162\n","\tRotated_Epoch:04 [003/005 (0220/0755)]\tLoss Ss: 0.026805\n","\tRotated_Epoch:04 [003/005 (0240/0755)]\tLoss Ss: 0.022199\n","\tRotated_Epoch:04 [003/005 (0260/0755)]\tLoss Ss: 0.024861\n","\tRotated_Epoch:04 [003/005 (0280/0755)]\tLoss Ss: 0.017924\n","\tRotated_Epoch:04 [003/005 (0300/0755)]\tLoss Ss: 0.033211\n","\tRotated_Epoch:04 [003/005 (0320/0755)]\tLoss Ss: 0.026268\n","\tRotated_Epoch:04 [003/005 (0340/0755)]\tLoss Ss: 0.023508\n","\tRotated_Epoch:04 [003/005 (0360/0755)]\tLoss Ss: 0.018554\n","\tRotated_Epoch:04 [003/005 (0380/0755)]\tLoss Ss: 0.017398\n","\tRotated_Epoch:04 [003/005 (0400/0755)]\tLoss Ss: 0.030414\n","\tRotated_Epoch:04 [003/005 (0420/0755)]\tLoss Ss: 0.043974\n","\tRotated_Epoch:04 [003/005 (0440/0755)]\tLoss Ss: 0.015735\n","\tRotated_Epoch:04 [003/005 (0460/0755)]\tLoss Ss: 0.020117\n","\tRotated_Epoch:04 [003/005 (0480/0755)]\tLoss Ss: 0.026848\n","\tRotated_Epoch:04 [003/005 (0500/0755)]\tLoss Ss: 0.024067\n","\tRotated_Epoch:04 [003/005 (0520/0755)]\tLoss Ss: 0.016484\n","\tRotated_Epoch:04 [003/005 (0540/0755)]\tLoss Ss: 0.021891\n","\tRotated_Epoch:04 [003/005 (0560/0755)]\tLoss Ss: 0.031630\n","\tRotated_Epoch:04 [003/005 (0580/0755)]\tLoss Ss: 0.020475\n","\tRotated_Epoch:04 [003/005 (0600/0755)]\tLoss Ss: 0.021435\n","\tRotated_Epoch:04 [003/005 (0620/0755)]\tLoss Ss: 0.011712\n","\tRotated_Epoch:04 [003/005 (0640/0755)]\tLoss Ss: 0.019363\n","\tRotated_Epoch:04 [003/005 (0660/0755)]\tLoss Ss: 0.020390\n","\tRotated_Epoch:04 [003/005 (0680/0755)]\tLoss Ss: 0.017929\n","\tRotated_Epoch:04 [003/005 (0700/0755)]\tLoss Ss: 0.018090\n","\tRotated_Epoch:04 [003/005 (0720/0755)]\tLoss Ss: 0.014747\n","\tRotated_Epoch:04 [003/005 (0740/0755)]\tLoss Ss: 0.026698\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tRotated_Epoch:04 [004/005 (0000/0588)]\tLoss Ss: 0.474632\n","\tRotated_Epoch:04 [004/005 (0020/0588)]\tLoss Ss: 0.249431\n","\tRotated_Epoch:04 [004/005 (0040/0588)]\tLoss Ss: 0.389081\n","\tRotated_Epoch:04 [004/005 (0060/0588)]\tLoss Ss: 0.216930\n","\tRotated_Epoch:04 [004/005 (0080/0588)]\tLoss Ss: 0.259452\n","\tRotated_Epoch:04 [004/005 (0100/0588)]\tLoss Ss: 0.235839\n","\tRotated_Epoch:04 [004/005 (0120/0588)]\tLoss Ss: 0.133376\n","\tRotated_Epoch:04 [004/005 (0140/0588)]\tLoss Ss: 0.147008\n","\tRotated_Epoch:04 [004/005 (0160/0588)]\tLoss Ss: 0.153745\n","\tRotated_Epoch:04 [004/005 (0180/0588)]\tLoss Ss: 0.098041\n","\tRotated_Epoch:04 [004/005 (0200/0588)]\tLoss Ss: 0.100267\n","\tRotated_Epoch:04 [004/005 (0220/0588)]\tLoss Ss: 0.117613\n","\tRotated_Epoch:04 [004/005 (0240/0588)]\tLoss Ss: 0.099974\n","\tRotated_Epoch:04 [004/005 (0260/0588)]\tLoss Ss: 0.062849\n","\tRotated_Epoch:04 [004/005 (0280/0588)]\tLoss Ss: 0.088079\n","\tRotated_Epoch:04 [004/005 (0300/0588)]\tLoss Ss: 0.091657\n","\tRotated_Epoch:04 [004/005 (0320/0588)]\tLoss Ss: 0.083432\n","\tRotated_Epoch:04 [004/005 (0340/0588)]\tLoss Ss: 0.071007\n","\tRotated_Epoch:04 [004/005 (0360/0588)]\tLoss Ss: 0.087761\n","\tRotated_Epoch:04 [004/005 (0380/0588)]\tLoss Ss: 0.078972\n","\tRotated_Epoch:04 [004/005 (0400/0588)]\tLoss Ss: 0.081709\n","\tRotated_Epoch:04 [004/005 (0420/0588)]\tLoss Ss: 0.127135\n","\tRotated_Epoch:04 [004/005 (0440/0588)]\tLoss Ss: 0.095147\n","\tRotated_Epoch:04 [004/005 (0460/0588)]\tLoss Ss: 0.076101\n","\tRotated_Epoch:04 [004/005 (0480/0588)]\tLoss Ss: 0.080850\n","\tRotated_Epoch:04 [004/005 (0500/0588)]\tLoss Ss: 0.101503\n","\tRotated_Epoch:04 [004/005 (0520/0588)]\tLoss Ss: 0.076865\n","\tRotated_Epoch:04 [004/005 (0540/0588)]\tLoss Ss: 0.085211\n","\tRotated_Epoch:04 [004/005 (0560/0588)]\tLoss Ss: 0.070578\n","\tRotated_Epoch:04 [004/005 (0580/0588)]\tLoss Ss: 0.075884\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:04 [005/005 (0000/0755)]\tLoss Ss: 0.170820\n","\tRotated_Epoch:04 [005/005 (0020/0755)]\tLoss Ss: 0.199689\n","\tRotated_Epoch:04 [005/005 (0040/0755)]\tLoss Ss: 0.175235\n","\tRotated_Epoch:04 [005/005 (0060/0755)]\tLoss Ss: 0.363539\n","\tRotated_Epoch:04 [005/005 (0080/0755)]\tLoss Ss: 0.350047\n","\tRotated_Epoch:04 [005/005 (0100/0755)]\tLoss Ss: 0.203021\n","\tRotated_Epoch:04 [005/005 (0120/0755)]\tLoss Ss: 0.118707\n","\tRotated_Epoch:04 [005/005 (0140/0755)]\tLoss Ss: 0.170393\n","\tRotated_Epoch:04 [005/005 (0160/0755)]\tLoss Ss: 0.159857\n","\tRotated_Epoch:04 [005/005 (0180/0755)]\tLoss Ss: 0.141617\n","\tRotated_Epoch:04 [005/005 (0200/0755)]\tLoss Ss: 0.180670\n","\tRotated_Epoch:04 [005/005 (0220/0755)]\tLoss Ss: 0.174946\n","\tRotated_Epoch:04 [005/005 (0240/0755)]\tLoss Ss: 0.169758\n","\tRotated_Epoch:04 [005/005 (0260/0755)]\tLoss Ss: 0.181550\n","\tRotated_Epoch:04 [005/005 (0280/0755)]\tLoss Ss: 0.147643\n","\tRotated_Epoch:04 [005/005 (0300/0755)]\tLoss Ss: 0.134155\n","\tRotated_Epoch:04 [005/005 (0320/0755)]\tLoss Ss: 0.131495\n","\tRotated_Epoch:04 [005/005 (0340/0755)]\tLoss Ss: 0.133758\n","\tRotated_Epoch:04 [005/005 (0360/0755)]\tLoss Ss: 0.140496\n","\tRotated_Epoch:04 [005/005 (0380/0755)]\tLoss Ss: 0.241632\n","\tRotated_Epoch:04 [005/005 (0400/0755)]\tLoss Ss: 0.187004\n","\tRotated_Epoch:04 [005/005 (0420/0755)]\tLoss Ss: 0.203399\n","\tRotated_Epoch:04 [005/005 (0440/0755)]\tLoss Ss: 0.155376\n","\tRotated_Epoch:04 [005/005 (0460/0755)]\tLoss Ss: 0.132335\n","\tRotated_Epoch:04 [005/005 (0480/0755)]\tLoss Ss: 0.117878\n","\tRotated_Epoch:04 [005/005 (0500/0755)]\tLoss Ss: 0.121211\n","\tRotated_Epoch:04 [005/005 (0520/0755)]\tLoss Ss: 0.112769\n","\tRotated_Epoch:04 [005/005 (0540/0755)]\tLoss Ss: 0.111190\n","\tRotated_Epoch:04 [005/005 (0560/0755)]\tLoss Ss: 0.181915\n","\tRotated_Epoch:04 [005/005 (0580/0755)]\tLoss Ss: 0.190635\n","\tRotated_Epoch:04 [005/005 (0600/0755)]\tLoss Ss: 0.132811\n","\tRotated_Epoch:04 [005/005 (0620/0755)]\tLoss Ss: 0.136533\n","\tRotated_Epoch:04 [005/005 (0640/0755)]\tLoss Ss: 0.113718\n","\tRotated_Epoch:04 [005/005 (0660/0755)]\tLoss Ss: 0.108706\n","\tRotated_Epoch:04 [005/005 (0680/0755)]\tLoss Ss: 0.064050\n","\tRotated_Epoch:04 [005/005 (0700/0755)]\tLoss Ss: 0.125609\n","\tRotated_Epoch:04 [005/005 (0720/0755)]\tLoss Ss: 0.107881\n","\tRotated_Epoch:04 [005/005 (0740/0755)]\tLoss Ss: 0.122859\n","new_1_6_d.nii\n","new_1_6_m.nii\n","new_1_7_d.nii\n","new_1_7_m.nii\n","new_2_5_d.nii\n","new_2_5_m.nii\n","new_2_6_d.nii\n","new_2_6_m.nii\n","new_2_7_d.nii\n","new_2_7_m.nii\n","new_3_d (2).nii\n","new_3_m (2).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (3).nii\n","new_3_m (3).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (7).nii\n","new_3_m (7).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (8).nii\n","new_3_m (8).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (9).nii\n","new_3_m (9).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_4_10_d_163526.nii\n","new_4_10_m_163526.nii.gz\n","new_4_11_d_084336.nii\n","new_4_11_m_084336.nii.gz\n","new_4_7_d_184422.nii\n","new_4_7_m_184422.nii.gz\n","new_4_8_d_143412.nii\n","new_4_8_m_143412.nii.gz\n","new_4_9_d_155347.nii\n","new_4_9_m_155347.nii.gz\n","\tEpoch: 4; Dice: 0.4730 +/- 0.0722; Loss: 17.2689\n","Begin Epoch 5\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tEpoch:05 [000/005 (0000/0693)]\tLoss Ss: 0.157359\n","\tEpoch:05 [000/005 (0020/0693)]\tLoss Ss: 0.169021\n","\tEpoch:05 [000/005 (0040/0693)]\tLoss Ss: 0.143456\n","\tEpoch:05 [000/005 (0060/0693)]\tLoss Ss: 0.122840\n","\tEpoch:05 [000/005 (0080/0693)]\tLoss Ss: 0.092219\n","\tEpoch:05 [000/005 (0100/0693)]\tLoss Ss: 0.067006\n","\tEpoch:05 [000/005 (0120/0693)]\tLoss Ss: 0.055539\n","\tEpoch:05 [000/005 (0140/0693)]\tLoss Ss: 0.045531\n","\tEpoch:05 [000/005 (0160/0693)]\tLoss Ss: 0.031141\n","\tEpoch:05 [000/005 (0180/0693)]\tLoss Ss: 0.038618\n","\tEpoch:05 [000/005 (0200/0693)]\tLoss Ss: 0.041756\n","\tEpoch:05 [000/005 (0220/0693)]\tLoss Ss: 0.043112\n","\tEpoch:05 [000/005 (0240/0693)]\tLoss Ss: 0.027786\n","\tEpoch:05 [000/005 (0260/0693)]\tLoss Ss: 0.034095\n","\tEpoch:05 [000/005 (0280/0693)]\tLoss Ss: 0.034271\n","\tEpoch:05 [000/005 (0300/0693)]\tLoss Ss: 0.032400\n","\tEpoch:05 [000/005 (0320/0693)]\tLoss Ss: 0.030212\n","\tEpoch:05 [000/005 (0340/0693)]\tLoss Ss: 0.027142\n","\tEpoch:05 [000/005 (0360/0693)]\tLoss Ss: 0.035977\n","\tEpoch:05 [000/005 (0380/0693)]\tLoss Ss: 0.027757\n","\tEpoch:05 [000/005 (0400/0693)]\tLoss Ss: 0.038963\n","\tEpoch:05 [000/005 (0420/0693)]\tLoss Ss: 0.025401\n","\tEpoch:05 [000/005 (0440/0693)]\tLoss Ss: 0.023566\n","\tEpoch:05 [000/005 (0460/0693)]\tLoss Ss: 0.032377\n","\tEpoch:05 [000/005 (0480/0693)]\tLoss Ss: 0.033289\n","\tEpoch:05 [000/005 (0500/0693)]\tLoss Ss: 0.024405\n","\tEpoch:05 [000/005 (0520/0693)]\tLoss Ss: 0.030393\n","\tEpoch:05 [000/005 (0540/0693)]\tLoss Ss: 0.024268\n","\tEpoch:05 [000/005 (0560/0693)]\tLoss Ss: 0.030482\n","\tEpoch:05 [000/005 (0580/0693)]\tLoss Ss: 0.026247\n","\tEpoch:05 [000/005 (0600/0693)]\tLoss Ss: 0.025480\n","\tEpoch:05 [000/005 (0620/0693)]\tLoss Ss: 0.035182\n","\tEpoch:05 [000/005 (0640/0693)]\tLoss Ss: 0.026711\n","\tEpoch:05 [000/005 (0660/0693)]\tLoss Ss: 0.019982\n","\tEpoch:05 [000/005 (0680/0693)]\tLoss Ss: 0.025687\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tEpoch:05 [001/005 (0000/0614)]\tLoss Ss: 0.095845\n","\tEpoch:05 [001/005 (0020/0614)]\tLoss Ss: 0.130592\n","\tEpoch:05 [001/005 (0040/0614)]\tLoss Ss: 0.083478\n","\tEpoch:05 [001/005 (0060/0614)]\tLoss Ss: 0.060670\n","\tEpoch:05 [001/005 (0080/0614)]\tLoss Ss: 0.040252\n","\tEpoch:05 [001/005 (0100/0614)]\tLoss Ss: 0.042279\n","\tEpoch:05 [001/005 (0120/0614)]\tLoss Ss: 0.038733\n","\tEpoch:05 [001/005 (0140/0614)]\tLoss Ss: 0.031644\n","\tEpoch:05 [001/005 (0160/0614)]\tLoss Ss: 0.030081\n","\tEpoch:05 [001/005 (0180/0614)]\tLoss Ss: 0.026855\n","\tEpoch:05 [001/005 (0200/0614)]\tLoss Ss: 0.023233\n","\tEpoch:05 [001/005 (0220/0614)]\tLoss Ss: 0.029119\n","\tEpoch:05 [001/005 (0240/0614)]\tLoss Ss: 0.020584\n","\tEpoch:05 [001/005 (0260/0614)]\tLoss Ss: 0.017063\n","\tEpoch:05 [001/005 (0280/0614)]\tLoss Ss: 0.028752\n","\tEpoch:05 [001/005 (0300/0614)]\tLoss Ss: 0.020193\n","\tEpoch:05 [001/005 (0320/0614)]\tLoss Ss: 0.020415\n","\tEpoch:05 [001/005 (0340/0614)]\tLoss Ss: 0.024226\n","\tEpoch:05 [001/005 (0360/0614)]\tLoss Ss: 0.017654\n","\tEpoch:05 [001/005 (0380/0614)]\tLoss Ss: 0.012414\n","\tEpoch:05 [001/005 (0400/0614)]\tLoss Ss: 0.039720\n","\tEpoch:05 [001/005 (0420/0614)]\tLoss Ss: 0.013621\n","\tEpoch:05 [001/005 (0440/0614)]\tLoss Ss: 0.013012\n","\tEpoch:05 [001/005 (0460/0614)]\tLoss Ss: 0.034094\n","\tEpoch:05 [001/005 (0480/0614)]\tLoss Ss: 0.011985\n","\tEpoch:05 [001/005 (0500/0614)]\tLoss Ss: 0.017719\n","\tEpoch:05 [001/005 (0520/0614)]\tLoss Ss: 0.013847\n","\tEpoch:05 [001/005 (0540/0614)]\tLoss Ss: 0.017973\n","\tEpoch:05 [001/005 (0560/0614)]\tLoss Ss: 0.019340\n","\tEpoch:05 [001/005 (0580/0614)]\tLoss Ss: 0.012181\n","\tEpoch:05 [001/005 (0600/0614)]\tLoss Ss: 0.013696\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tEpoch:05 [002/005 (0000/0693)]\tLoss Ss: 0.051448\n","\tEpoch:05 [002/005 (0020/0693)]\tLoss Ss: 0.019590\n","\tEpoch:05 [002/005 (0040/0693)]\tLoss Ss: 0.016843\n","\tEpoch:05 [002/005 (0060/0693)]\tLoss Ss: 0.032152\n","\tEpoch:05 [002/005 (0080/0693)]\tLoss Ss: 0.017315\n","\tEpoch:05 [002/005 (0100/0693)]\tLoss Ss: 0.023368\n","\tEpoch:05 [002/005 (0120/0693)]\tLoss Ss: 0.019084\n","\tEpoch:05 [002/005 (0140/0693)]\tLoss Ss: 0.023563\n","\tEpoch:05 [002/005 (0160/0693)]\tLoss Ss: 0.026748\n","\tEpoch:05 [002/005 (0180/0693)]\tLoss Ss: 0.024438\n","\tEpoch:05 [002/005 (0200/0693)]\tLoss Ss: 0.037393\n","\tEpoch:05 [002/005 (0220/0693)]\tLoss Ss: 0.019322\n","\tEpoch:05 [002/005 (0240/0693)]\tLoss Ss: 0.025839\n","\tEpoch:05 [002/005 (0260/0693)]\tLoss Ss: 0.018234\n","\tEpoch:05 [002/005 (0280/0693)]\tLoss Ss: 0.017433\n","\tEpoch:05 [002/005 (0300/0693)]\tLoss Ss: 0.017227\n","\tEpoch:05 [002/005 (0320/0693)]\tLoss Ss: 0.017061\n","\tEpoch:05 [002/005 (0340/0693)]\tLoss Ss: 0.024045\n","\tEpoch:05 [002/005 (0360/0693)]\tLoss Ss: 0.013490\n","\tEpoch:05 [002/005 (0380/0693)]\tLoss Ss: 0.029472\n","\tEpoch:05 [002/005 (0400/0693)]\tLoss Ss: 0.023844\n","\tEpoch:05 [002/005 (0420/0693)]\tLoss Ss: 0.022529\n","\tEpoch:05 [002/005 (0440/0693)]\tLoss Ss: 0.017964\n","\tEpoch:05 [002/005 (0460/0693)]\tLoss Ss: 0.018371\n","\tEpoch:05 [002/005 (0480/0693)]\tLoss Ss: 0.022245\n","\tEpoch:05 [002/005 (0500/0693)]\tLoss Ss: 0.016932\n","\tEpoch:05 [002/005 (0520/0693)]\tLoss Ss: 0.021279\n","\tEpoch:05 [002/005 (0540/0693)]\tLoss Ss: 0.017549\n","\tEpoch:05 [002/005 (0560/0693)]\tLoss Ss: 0.013648\n","\tEpoch:05 [002/005 (0580/0693)]\tLoss Ss: 0.019569\n","\tEpoch:05 [002/005 (0600/0693)]\tLoss Ss: 0.013559\n","\tEpoch:05 [002/005 (0620/0693)]\tLoss Ss: 0.017357\n","\tEpoch:05 [002/005 (0640/0693)]\tLoss Ss: 0.016063\n","\tEpoch:05 [002/005 (0660/0693)]\tLoss Ss: 0.018958\n","\tEpoch:05 [002/005 (0680/0693)]\tLoss Ss: 0.014303\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:05 [003/005 (0000/0755)]\tLoss Ss: 0.042818\n","\tEpoch:05 [003/005 (0020/0755)]\tLoss Ss: 0.045319\n","\tEpoch:05 [003/005 (0040/0755)]\tLoss Ss: 0.028057\n","\tEpoch:05 [003/005 (0060/0755)]\tLoss Ss: 0.035579\n","\tEpoch:05 [003/005 (0080/0755)]\tLoss Ss: 0.039392\n","\tEpoch:05 [003/005 (0100/0755)]\tLoss Ss: 0.039766\n","\tEpoch:05 [003/005 (0120/0755)]\tLoss Ss: 0.025340\n","\tEpoch:05 [003/005 (0140/0755)]\tLoss Ss: 0.022254\n","\tEpoch:05 [003/005 (0160/0755)]\tLoss Ss: 0.027667\n","\tEpoch:05 [003/005 (0180/0755)]\tLoss Ss: 0.021122\n","\tEpoch:05 [003/005 (0200/0755)]\tLoss Ss: 0.027865\n","\tEpoch:05 [003/005 (0220/0755)]\tLoss Ss: 0.018840\n","\tEpoch:05 [003/005 (0240/0755)]\tLoss Ss: 0.032331\n","\tEpoch:05 [003/005 (0260/0755)]\tLoss Ss: 0.031901\n","\tEpoch:05 [003/005 (0280/0755)]\tLoss Ss: 0.024160\n","\tEpoch:05 [003/005 (0300/0755)]\tLoss Ss: 0.030265\n","\tEpoch:05 [003/005 (0320/0755)]\tLoss Ss: 0.024941\n","\tEpoch:05 [003/005 (0340/0755)]\tLoss Ss: 0.029364\n","\tEpoch:05 [003/005 (0360/0755)]\tLoss Ss: 0.011205\n","\tEpoch:05 [003/005 (0380/0755)]\tLoss Ss: 0.032178\n","\tEpoch:05 [003/005 (0400/0755)]\tLoss Ss: 0.029019\n","\tEpoch:05 [003/005 (0420/0755)]\tLoss Ss: 0.021709\n","\tEpoch:05 [003/005 (0440/0755)]\tLoss Ss: 0.040032\n","\tEpoch:05 [003/005 (0460/0755)]\tLoss Ss: 0.022834\n","\tEpoch:05 [003/005 (0480/0755)]\tLoss Ss: 0.031065\n","\tEpoch:05 [003/005 (0500/0755)]\tLoss Ss: 0.016698\n","\tEpoch:05 [003/005 (0520/0755)]\tLoss Ss: 0.020812\n","\tEpoch:05 [003/005 (0540/0755)]\tLoss Ss: 0.031185\n","\tEpoch:05 [003/005 (0560/0755)]\tLoss Ss: 0.018609\n","\tEpoch:05 [003/005 (0580/0755)]\tLoss Ss: 0.026748\n","\tEpoch:05 [003/005 (0600/0755)]\tLoss Ss: 0.023229\n","\tEpoch:05 [003/005 (0620/0755)]\tLoss Ss: 0.020684\n","\tEpoch:05 [003/005 (0640/0755)]\tLoss Ss: 0.028698\n","\tEpoch:05 [003/005 (0660/0755)]\tLoss Ss: 0.033301\n","\tEpoch:05 [003/005 (0680/0755)]\tLoss Ss: 0.020785\n","\tEpoch:05 [003/005 (0700/0755)]\tLoss Ss: 0.019647\n","\tEpoch:05 [003/005 (0720/0755)]\tLoss Ss: 0.020251\n","\tEpoch:05 [003/005 (0740/0755)]\tLoss Ss: 0.019650\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:05 [004/005 (0000/0755)]\tLoss Ss: 0.020785\n","\tEpoch:05 [004/005 (0020/0755)]\tLoss Ss: 0.021176\n","\tEpoch:05 [004/005 (0040/0755)]\tLoss Ss: 0.021478\n","\tEpoch:05 [004/005 (0060/0755)]\tLoss Ss: 0.020025\n","\tEpoch:05 [004/005 (0080/0755)]\tLoss Ss: 0.021360\n","\tEpoch:05 [004/005 (0100/0755)]\tLoss Ss: 0.018861\n","\tEpoch:05 [004/005 (0120/0755)]\tLoss Ss: 0.024264\n","\tEpoch:05 [004/005 (0140/0755)]\tLoss Ss: 0.019489\n","\tEpoch:05 [004/005 (0160/0755)]\tLoss Ss: 0.015224\n","\tEpoch:05 [004/005 (0180/0755)]\tLoss Ss: 0.023890\n","\tEpoch:05 [004/005 (0200/0755)]\tLoss Ss: 0.025810\n","\tEpoch:05 [004/005 (0220/0755)]\tLoss Ss: 0.017895\n","\tEpoch:05 [004/005 (0240/0755)]\tLoss Ss: 0.016851\n","\tEpoch:05 [004/005 (0260/0755)]\tLoss Ss: 0.027395\n","\tEpoch:05 [004/005 (0280/0755)]\tLoss Ss: 0.012059\n","\tEpoch:05 [004/005 (0300/0755)]\tLoss Ss: 0.030657\n","\tEpoch:05 [004/005 (0320/0755)]\tLoss Ss: 0.019286\n","\tEpoch:05 [004/005 (0340/0755)]\tLoss Ss: 0.022407\n","\tEpoch:05 [004/005 (0360/0755)]\tLoss Ss: 0.015476\n","\tEpoch:05 [004/005 (0380/0755)]\tLoss Ss: 0.016042\n","\tEpoch:05 [004/005 (0400/0755)]\tLoss Ss: 0.015432\n","\tEpoch:05 [004/005 (0420/0755)]\tLoss Ss: 0.013457\n","\tEpoch:05 [004/005 (0440/0755)]\tLoss Ss: 0.016561\n","\tEpoch:05 [004/005 (0460/0755)]\tLoss Ss: 0.023048\n","\tEpoch:05 [004/005 (0480/0755)]\tLoss Ss: 0.019899\n","\tEpoch:05 [004/005 (0500/0755)]\tLoss Ss: 0.025368\n","\tEpoch:05 [004/005 (0520/0755)]\tLoss Ss: 0.020991\n","\tEpoch:05 [004/005 (0540/0755)]\tLoss Ss: 0.020372\n","\tEpoch:05 [004/005 (0560/0755)]\tLoss Ss: 0.014766\n","\tEpoch:05 [004/005 (0580/0755)]\tLoss Ss: 0.039533\n","\tEpoch:05 [004/005 (0600/0755)]\tLoss Ss: 0.027182\n","\tEpoch:05 [004/005 (0620/0755)]\tLoss Ss: 0.019921\n","\tEpoch:05 [004/005 (0640/0755)]\tLoss Ss: 0.016863\n","\tEpoch:05 [004/005 (0660/0755)]\tLoss Ss: 0.017469\n","\tEpoch:05 [004/005 (0680/0755)]\tLoss Ss: 0.017511\n","\tEpoch:05 [004/005 (0700/0755)]\tLoss Ss: 0.028146\n","\tEpoch:05 [004/005 (0720/0755)]\tLoss Ss: 0.021587\n","\tEpoch:05 [004/005 (0740/0755)]\tLoss Ss: 0.017256\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tEpoch:05 [005/005 (0000/0588)]\tLoss Ss: 0.017602\n","\tEpoch:05 [005/005 (0020/0588)]\tLoss Ss: 0.020829\n","\tEpoch:05 [005/005 (0040/0588)]\tLoss Ss: 0.018133\n","\tEpoch:05 [005/005 (0060/0588)]\tLoss Ss: 0.014329\n","\tEpoch:05 [005/005 (0080/0588)]\tLoss Ss: 0.009587\n","\tEpoch:05 [005/005 (0100/0588)]\tLoss Ss: 0.017851\n","\tEpoch:05 [005/005 (0120/0588)]\tLoss Ss: 0.011446\n","\tEpoch:05 [005/005 (0140/0588)]\tLoss Ss: 0.017433\n","\tEpoch:05 [005/005 (0160/0588)]\tLoss Ss: 0.007776\n","\tEpoch:05 [005/005 (0180/0588)]\tLoss Ss: 0.014784\n","\tEpoch:05 [005/005 (0200/0588)]\tLoss Ss: 0.008235\n","\tEpoch:05 [005/005 (0220/0588)]\tLoss Ss: 0.010849\n","\tEpoch:05 [005/005 (0240/0588)]\tLoss Ss: 0.013058\n","\tEpoch:05 [005/005 (0260/0588)]\tLoss Ss: 0.007963\n","\tEpoch:05 [005/005 (0280/0588)]\tLoss Ss: 0.010153\n","\tEpoch:05 [005/005 (0300/0588)]\tLoss Ss: 0.015354\n","\tEpoch:05 [005/005 (0320/0588)]\tLoss Ss: 0.011306\n","\tEpoch:05 [005/005 (0340/0588)]\tLoss Ss: 0.008643\n","\tEpoch:05 [005/005 (0360/0588)]\tLoss Ss: 0.007744\n","\tEpoch:05 [005/005 (0380/0588)]\tLoss Ss: 0.010507\n","\tEpoch:05 [005/005 (0400/0588)]\tLoss Ss: 0.009632\n","\tEpoch:05 [005/005 (0420/0588)]\tLoss Ss: 0.006838\n","\tEpoch:05 [005/005 (0440/0588)]\tLoss Ss: 0.007924\n","\tEpoch:05 [005/005 (0460/0588)]\tLoss Ss: 0.009862\n","\tEpoch:05 [005/005 (0480/0588)]\tLoss Ss: 0.006727\n","\tEpoch:05 [005/005 (0500/0588)]\tLoss Ss: 0.009558\n","\tEpoch:05 [005/005 (0520/0588)]\tLoss Ss: 0.008623\n","\tEpoch:05 [005/005 (0540/0588)]\tLoss Ss: 0.010198\n","\tEpoch:05 [005/005 (0560/0588)]\tLoss Ss: 0.010359\n","\tEpoch:05 [005/005 (0580/0588)]\tLoss Ss: 0.007131\n","Now train the rotated image\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tRotated_Epoch:05 [000/005 (0000/0693)]\tLoss Ss: 0.024976\n","\tRotated_Epoch:05 [000/005 (0020/0693)]\tLoss Ss: 0.026479\n","\tRotated_Epoch:05 [000/005 (0040/0693)]\tLoss Ss: 0.028284\n","\tRotated_Epoch:05 [000/005 (0060/0693)]\tLoss Ss: 0.023961\n","\tRotated_Epoch:05 [000/005 (0080/0693)]\tLoss Ss: 0.021278\n","\tRotated_Epoch:05 [000/005 (0100/0693)]\tLoss Ss: 0.019422\n","\tRotated_Epoch:05 [000/005 (0120/0693)]\tLoss Ss: 0.025108\n","\tRotated_Epoch:05 [000/005 (0140/0693)]\tLoss Ss: 0.016775\n","\tRotated_Epoch:05 [000/005 (0160/0693)]\tLoss Ss: 0.024936\n","\tRotated_Epoch:05 [000/005 (0180/0693)]\tLoss Ss: 0.032703\n","\tRotated_Epoch:05 [000/005 (0200/0693)]\tLoss Ss: 0.019165\n","\tRotated_Epoch:05 [000/005 (0220/0693)]\tLoss Ss: 0.022330\n","\tRotated_Epoch:05 [000/005 (0240/0693)]\tLoss Ss: 0.020175\n","\tRotated_Epoch:05 [000/005 (0260/0693)]\tLoss Ss: 0.023140\n","\tRotated_Epoch:05 [000/005 (0280/0693)]\tLoss Ss: 0.024346\n","\tRotated_Epoch:05 [000/005 (0300/0693)]\tLoss Ss: 0.018977\n","\tRotated_Epoch:05 [000/005 (0320/0693)]\tLoss Ss: 0.031938\n","\tRotated_Epoch:05 [000/005 (0340/0693)]\tLoss Ss: 0.025357\n","\tRotated_Epoch:05 [000/005 (0360/0693)]\tLoss Ss: 0.035461\n","\tRotated_Epoch:05 [000/005 (0380/0693)]\tLoss Ss: 0.030658\n","\tRotated_Epoch:05 [000/005 (0400/0693)]\tLoss Ss: 0.023948\n","\tRotated_Epoch:05 [000/005 (0420/0693)]\tLoss Ss: 0.022654\n","\tRotated_Epoch:05 [000/005 (0440/0693)]\tLoss Ss: 0.017368\n","\tRotated_Epoch:05 [000/005 (0460/0693)]\tLoss Ss: 0.018045\n","\tRotated_Epoch:05 [000/005 (0480/0693)]\tLoss Ss: 0.019092\n","\tRotated_Epoch:05 [000/005 (0500/0693)]\tLoss Ss: 0.023677\n","\tRotated_Epoch:05 [000/005 (0520/0693)]\tLoss Ss: 0.016297\n","\tRotated_Epoch:05 [000/005 (0540/0693)]\tLoss Ss: 0.033104\n","\tRotated_Epoch:05 [000/005 (0560/0693)]\tLoss Ss: 0.017972\n","\tRotated_Epoch:05 [000/005 (0580/0693)]\tLoss Ss: 0.025344\n","\tRotated_Epoch:05 [000/005 (0600/0693)]\tLoss Ss: 0.016165\n","\tRotated_Epoch:05 [000/005 (0620/0693)]\tLoss Ss: 0.018738\n","\tRotated_Epoch:05 [000/005 (0640/0693)]\tLoss Ss: 0.015360\n","\tRotated_Epoch:05 [000/005 (0660/0693)]\tLoss Ss: 0.016356\n","\tRotated_Epoch:05 [000/005 (0680/0693)]\tLoss Ss: 0.029533\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tRotated_Epoch:05 [001/005 (0000/0588)]\tLoss Ss: 0.317761\n","\tRotated_Epoch:05 [001/005 (0020/0588)]\tLoss Ss: 0.192168\n","\tRotated_Epoch:05 [001/005 (0040/0588)]\tLoss Ss: 0.154248\n","\tRotated_Epoch:05 [001/005 (0060/0588)]\tLoss Ss: 0.215274\n","\tRotated_Epoch:05 [001/005 (0080/0588)]\tLoss Ss: 0.190989\n","\tRotated_Epoch:05 [001/005 (0100/0588)]\tLoss Ss: 0.233334\n","\tRotated_Epoch:05 [001/005 (0120/0588)]\tLoss Ss: 0.190205\n","\tRotated_Epoch:05 [001/005 (0140/0588)]\tLoss Ss: 0.105196\n","\tRotated_Epoch:05 [001/005 (0160/0588)]\tLoss Ss: 0.167664\n","\tRotated_Epoch:05 [001/005 (0180/0588)]\tLoss Ss: 0.151476\n","\tRotated_Epoch:05 [001/005 (0200/0588)]\tLoss Ss: 0.136040\n","\tRotated_Epoch:05 [001/005 (0220/0588)]\tLoss Ss: 0.104694\n","\tRotated_Epoch:05 [001/005 (0240/0588)]\tLoss Ss: 0.065427\n","\tRotated_Epoch:05 [001/005 (0260/0588)]\tLoss Ss: 0.079047\n","\tRotated_Epoch:05 [001/005 (0280/0588)]\tLoss Ss: 0.073025\n","\tRotated_Epoch:05 [001/005 (0300/0588)]\tLoss Ss: 0.097494\n","\tRotated_Epoch:05 [001/005 (0320/0588)]\tLoss Ss: 0.115797\n","\tRotated_Epoch:05 [001/005 (0340/0588)]\tLoss Ss: 0.134727\n","\tRotated_Epoch:05 [001/005 (0360/0588)]\tLoss Ss: 0.092816\n","\tRotated_Epoch:05 [001/005 (0380/0588)]\tLoss Ss: 0.107030\n","\tRotated_Epoch:05 [001/005 (0400/0588)]\tLoss Ss: 0.079126\n","\tRotated_Epoch:05 [001/005 (0420/0588)]\tLoss Ss: 0.095339\n","\tRotated_Epoch:05 [001/005 (0440/0588)]\tLoss Ss: 0.115227\n","\tRotated_Epoch:05 [001/005 (0460/0588)]\tLoss Ss: 0.082058\n","\tRotated_Epoch:05 [001/005 (0480/0588)]\tLoss Ss: 0.109006\n","\tRotated_Epoch:05 [001/005 (0500/0588)]\tLoss Ss: 0.081355\n","\tRotated_Epoch:05 [001/005 (0520/0588)]\tLoss Ss: 0.142465\n","\tRotated_Epoch:05 [001/005 (0540/0588)]\tLoss Ss: 0.139202\n","\tRotated_Epoch:05 [001/005 (0560/0588)]\tLoss Ss: 0.088026\n","\tRotated_Epoch:05 [001/005 (0580/0588)]\tLoss Ss: 0.117004\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tRotated_Epoch:05 [002/005 (0000/0614)]\tLoss Ss: 0.144679\n","\tRotated_Epoch:05 [002/005 (0020/0614)]\tLoss Ss: 0.119481\n","\tRotated_Epoch:05 [002/005 (0040/0614)]\tLoss Ss: 0.128787\n","\tRotated_Epoch:05 [002/005 (0060/0614)]\tLoss Ss: 0.075331\n","\tRotated_Epoch:05 [002/005 (0080/0614)]\tLoss Ss: 0.089913\n","\tRotated_Epoch:05 [002/005 (0100/0614)]\tLoss Ss: 0.112102\n","\tRotated_Epoch:05 [002/005 (0120/0614)]\tLoss Ss: 0.066583\n","\tRotated_Epoch:05 [002/005 (0140/0614)]\tLoss Ss: 0.073274\n","\tRotated_Epoch:05 [002/005 (0160/0614)]\tLoss Ss: 0.058283\n","\tRotated_Epoch:05 [002/005 (0180/0614)]\tLoss Ss: 0.051245\n","\tRotated_Epoch:05 [002/005 (0200/0614)]\tLoss Ss: 0.046487\n","\tRotated_Epoch:05 [002/005 (0220/0614)]\tLoss Ss: 0.059302\n","\tRotated_Epoch:05 [002/005 (0240/0614)]\tLoss Ss: 0.061293\n","\tRotated_Epoch:05 [002/005 (0260/0614)]\tLoss Ss: 0.052164\n","\tRotated_Epoch:05 [002/005 (0280/0614)]\tLoss Ss: 0.054563\n","\tRotated_Epoch:05 [002/005 (0300/0614)]\tLoss Ss: 0.046221\n","\tRotated_Epoch:05 [002/005 (0320/0614)]\tLoss Ss: 0.042772\n","\tRotated_Epoch:05 [002/005 (0340/0614)]\tLoss Ss: 0.035366\n","\tRotated_Epoch:05 [002/005 (0360/0614)]\tLoss Ss: 0.042843\n","\tRotated_Epoch:05 [002/005 (0380/0614)]\tLoss Ss: 0.047800\n","\tRotated_Epoch:05 [002/005 (0400/0614)]\tLoss Ss: 0.036624\n","\tRotated_Epoch:05 [002/005 (0420/0614)]\tLoss Ss: 0.032342\n","\tRotated_Epoch:05 [002/005 (0440/0614)]\tLoss Ss: 0.035192\n","\tRotated_Epoch:05 [002/005 (0460/0614)]\tLoss Ss: 0.031556\n","\tRotated_Epoch:05 [002/005 (0480/0614)]\tLoss Ss: 0.046027\n","\tRotated_Epoch:05 [002/005 (0500/0614)]\tLoss Ss: 0.039042\n","\tRotated_Epoch:05 [002/005 (0520/0614)]\tLoss Ss: 0.034447\n","\tRotated_Epoch:05 [002/005 (0540/0614)]\tLoss Ss: 0.031746\n","\tRotated_Epoch:05 [002/005 (0560/0614)]\tLoss Ss: 0.031205\n","\tRotated_Epoch:05 [002/005 (0580/0614)]\tLoss Ss: 0.037522\n","\tRotated_Epoch:05 [002/005 (0600/0614)]\tLoss Ss: 0.030429\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:05 [003/005 (0000/0755)]\tLoss Ss: 0.149201\n","\tRotated_Epoch:05 [003/005 (0020/0755)]\tLoss Ss: 0.205812\n","\tRotated_Epoch:05 [003/005 (0040/0755)]\tLoss Ss: 0.217270\n","\tRotated_Epoch:05 [003/005 (0060/0755)]\tLoss Ss: 0.240375\n","\tRotated_Epoch:05 [003/005 (0080/0755)]\tLoss Ss: 0.289464\n","\tRotated_Epoch:05 [003/005 (0100/0755)]\tLoss Ss: 0.211054\n","\tRotated_Epoch:05 [003/005 (0120/0755)]\tLoss Ss: 0.139424\n","\tRotated_Epoch:05 [003/005 (0140/0755)]\tLoss Ss: 0.240284\n","\tRotated_Epoch:05 [003/005 (0160/0755)]\tLoss Ss: 0.149968\n","\tRotated_Epoch:05 [003/005 (0180/0755)]\tLoss Ss: 0.072231\n","\tRotated_Epoch:05 [003/005 (0200/0755)]\tLoss Ss: 0.139913\n","\tRotated_Epoch:05 [003/005 (0220/0755)]\tLoss Ss: 0.152299\n","\tRotated_Epoch:05 [003/005 (0240/0755)]\tLoss Ss: 0.136007\n","\tRotated_Epoch:05 [003/005 (0260/0755)]\tLoss Ss: 0.116300\n","\tRotated_Epoch:05 [003/005 (0280/0755)]\tLoss Ss: 0.179960\n","\tRotated_Epoch:05 [003/005 (0300/0755)]\tLoss Ss: 0.185455\n","\tRotated_Epoch:05 [003/005 (0320/0755)]\tLoss Ss: 0.151956\n","\tRotated_Epoch:05 [003/005 (0340/0755)]\tLoss Ss: 0.108677\n","\tRotated_Epoch:05 [003/005 (0360/0755)]\tLoss Ss: 0.117673\n","\tRotated_Epoch:05 [003/005 (0380/0755)]\tLoss Ss: 0.076838\n","\tRotated_Epoch:05 [003/005 (0400/0755)]\tLoss Ss: 0.106737\n","\tRotated_Epoch:05 [003/005 (0420/0755)]\tLoss Ss: 0.107551\n","\tRotated_Epoch:05 [003/005 (0440/0755)]\tLoss Ss: 0.109414\n","\tRotated_Epoch:05 [003/005 (0460/0755)]\tLoss Ss: 0.101600\n","\tRotated_Epoch:05 [003/005 (0480/0755)]\tLoss Ss: 0.117763\n","\tRotated_Epoch:05 [003/005 (0500/0755)]\tLoss Ss: 0.099171\n","\tRotated_Epoch:05 [003/005 (0520/0755)]\tLoss Ss: 0.112003\n","\tRotated_Epoch:05 [003/005 (0540/0755)]\tLoss Ss: 0.091544\n","\tRotated_Epoch:05 [003/005 (0560/0755)]\tLoss Ss: 0.088856\n","\tRotated_Epoch:05 [003/005 (0580/0755)]\tLoss Ss: 0.085192\n","\tRotated_Epoch:05 [003/005 (0600/0755)]\tLoss Ss: 0.098105\n","\tRotated_Epoch:05 [003/005 (0620/0755)]\tLoss Ss: 0.122230\n","\tRotated_Epoch:05 [003/005 (0640/0755)]\tLoss Ss: 0.095127\n","\tRotated_Epoch:05 [003/005 (0660/0755)]\tLoss Ss: 0.104106\n","\tRotated_Epoch:05 [003/005 (0680/0755)]\tLoss Ss: 0.078949\n","\tRotated_Epoch:05 [003/005 (0700/0755)]\tLoss Ss: 0.127583\n","\tRotated_Epoch:05 [003/005 (0720/0755)]\tLoss Ss: 0.077230\n","\tRotated_Epoch:05 [003/005 (0740/0755)]\tLoss Ss: 0.075585\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tRotated_Epoch:05 [004/005 (0000/0693)]\tLoss Ss: 0.080872\n","\tRotated_Epoch:05 [004/005 (0020/0693)]\tLoss Ss: 0.095796\n","\tRotated_Epoch:05 [004/005 (0040/0693)]\tLoss Ss: 0.072394\n","\tRotated_Epoch:05 [004/005 (0060/0693)]\tLoss Ss: 0.074780\n","\tRotated_Epoch:05 [004/005 (0080/0693)]\tLoss Ss: 0.074115\n","\tRotated_Epoch:05 [004/005 (0100/0693)]\tLoss Ss: 0.072432\n","\tRotated_Epoch:05 [004/005 (0120/0693)]\tLoss Ss: 0.055740\n","\tRotated_Epoch:05 [004/005 (0140/0693)]\tLoss Ss: 0.055907\n","\tRotated_Epoch:05 [004/005 (0160/0693)]\tLoss Ss: 0.058512\n","\tRotated_Epoch:05 [004/005 (0180/0693)]\tLoss Ss: 0.043841\n","\tRotated_Epoch:05 [004/005 (0200/0693)]\tLoss Ss: 0.045886\n","\tRotated_Epoch:05 [004/005 (0220/0693)]\tLoss Ss: 0.050869\n","\tRotated_Epoch:05 [004/005 (0240/0693)]\tLoss Ss: 0.041258\n","\tRotated_Epoch:05 [004/005 (0260/0693)]\tLoss Ss: 0.049502\n","\tRotated_Epoch:05 [004/005 (0280/0693)]\tLoss Ss: 0.034048\n","\tRotated_Epoch:05 [004/005 (0300/0693)]\tLoss Ss: 0.043094\n","\tRotated_Epoch:05 [004/005 (0320/0693)]\tLoss Ss: 0.045488\n","\tRotated_Epoch:05 [004/005 (0340/0693)]\tLoss Ss: 0.028185\n","\tRotated_Epoch:05 [004/005 (0360/0693)]\tLoss Ss: 0.036805\n","\tRotated_Epoch:05 [004/005 (0380/0693)]\tLoss Ss: 0.039494\n","\tRotated_Epoch:05 [004/005 (0400/0693)]\tLoss Ss: 0.032581\n","\tRotated_Epoch:05 [004/005 (0420/0693)]\tLoss Ss: 0.036399\n","\tRotated_Epoch:05 [004/005 (0440/0693)]\tLoss Ss: 0.027054\n","\tRotated_Epoch:05 [004/005 (0460/0693)]\tLoss Ss: 0.030174\n","\tRotated_Epoch:05 [004/005 (0480/0693)]\tLoss Ss: 0.031559\n","\tRotated_Epoch:05 [004/005 (0500/0693)]\tLoss Ss: 0.032832\n","\tRotated_Epoch:05 [004/005 (0520/0693)]\tLoss Ss: 0.026947\n","\tRotated_Epoch:05 [004/005 (0540/0693)]\tLoss Ss: 0.036087\n","\tRotated_Epoch:05 [004/005 (0560/0693)]\tLoss Ss: 0.027135\n","\tRotated_Epoch:05 [004/005 (0580/0693)]\tLoss Ss: 0.023493\n","\tRotated_Epoch:05 [004/005 (0600/0693)]\tLoss Ss: 0.030675\n","\tRotated_Epoch:05 [004/005 (0620/0693)]\tLoss Ss: 0.028555\n","\tRotated_Epoch:05 [004/005 (0640/0693)]\tLoss Ss: 0.029619\n","\tRotated_Epoch:05 [004/005 (0660/0693)]\tLoss Ss: 0.017962\n","\tRotated_Epoch:05 [004/005 (0680/0693)]\tLoss Ss: 0.029000\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:05 [005/005 (0000/0755)]\tLoss Ss: 0.064164\n","\tRotated_Epoch:05 [005/005 (0020/0755)]\tLoss Ss: 0.042784\n","\tRotated_Epoch:05 [005/005 (0040/0755)]\tLoss Ss: 0.070667\n","\tRotated_Epoch:05 [005/005 (0060/0755)]\tLoss Ss: 0.061060\n","\tRotated_Epoch:05 [005/005 (0080/0755)]\tLoss Ss: 0.058860\n","\tRotated_Epoch:05 [005/005 (0100/0755)]\tLoss Ss: 0.052849\n","\tRotated_Epoch:05 [005/005 (0120/0755)]\tLoss Ss: 0.037790\n","\tRotated_Epoch:05 [005/005 (0140/0755)]\tLoss Ss: 0.052909\n","\tRotated_Epoch:05 [005/005 (0160/0755)]\tLoss Ss: 0.044633\n","\tRotated_Epoch:05 [005/005 (0180/0755)]\tLoss Ss: 0.032264\n","\tRotated_Epoch:05 [005/005 (0200/0755)]\tLoss Ss: 0.041410\n","\tRotated_Epoch:05 [005/005 (0220/0755)]\tLoss Ss: 0.033274\n","\tRotated_Epoch:05 [005/005 (0240/0755)]\tLoss Ss: 0.031614\n","\tRotated_Epoch:05 [005/005 (0260/0755)]\tLoss Ss: 0.047539\n","\tRotated_Epoch:05 [005/005 (0280/0755)]\tLoss Ss: 0.031936\n","\tRotated_Epoch:05 [005/005 (0300/0755)]\tLoss Ss: 0.037994\n","\tRotated_Epoch:05 [005/005 (0320/0755)]\tLoss Ss: 0.039165\n","\tRotated_Epoch:05 [005/005 (0340/0755)]\tLoss Ss: 0.065327\n","\tRotated_Epoch:05 [005/005 (0360/0755)]\tLoss Ss: 0.026557\n","\tRotated_Epoch:05 [005/005 (0380/0755)]\tLoss Ss: 0.030498\n","\tRotated_Epoch:05 [005/005 (0400/0755)]\tLoss Ss: 0.037828\n","\tRotated_Epoch:05 [005/005 (0420/0755)]\tLoss Ss: 0.023372\n","\tRotated_Epoch:05 [005/005 (0440/0755)]\tLoss Ss: 0.029897\n","\tRotated_Epoch:05 [005/005 (0460/0755)]\tLoss Ss: 0.037078\n","\tRotated_Epoch:05 [005/005 (0480/0755)]\tLoss Ss: 0.027894\n","\tRotated_Epoch:05 [005/005 (0500/0755)]\tLoss Ss: 0.026339\n","\tRotated_Epoch:05 [005/005 (0520/0755)]\tLoss Ss: 0.025814\n","\tRotated_Epoch:05 [005/005 (0540/0755)]\tLoss Ss: 0.031927\n","\tRotated_Epoch:05 [005/005 (0560/0755)]\tLoss Ss: 0.024890\n","\tRotated_Epoch:05 [005/005 (0580/0755)]\tLoss Ss: 0.023953\n","\tRotated_Epoch:05 [005/005 (0600/0755)]\tLoss Ss: 0.023127\n","\tRotated_Epoch:05 [005/005 (0620/0755)]\tLoss Ss: 0.013995\n","\tRotated_Epoch:05 [005/005 (0640/0755)]\tLoss Ss: 0.027989\n","\tRotated_Epoch:05 [005/005 (0660/0755)]\tLoss Ss: 0.020715\n","\tRotated_Epoch:05 [005/005 (0680/0755)]\tLoss Ss: 0.019709\n","\tRotated_Epoch:05 [005/005 (0700/0755)]\tLoss Ss: 0.018274\n","\tRotated_Epoch:05 [005/005 (0720/0755)]\tLoss Ss: 0.023729\n","\tRotated_Epoch:05 [005/005 (0740/0755)]\tLoss Ss: 0.015327\n","new_1_6_d.nii\n","new_1_6_m.nii\n","new_1_7_d.nii\n","new_1_7_m.nii\n","new_2_5_d.nii\n","new_2_5_m.nii\n","new_2_6_d.nii\n","new_2_6_m.nii\n","new_2_7_d.nii\n","new_2_7_m.nii\n","new_3_d (2).nii\n","new_3_m (2).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (3).nii\n","new_3_m (3).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (7).nii\n","new_3_m (7).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (8).nii\n","new_3_m (8).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (9).nii\n","new_3_m (9).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_4_10_d_163526.nii\n","new_4_10_m_163526.nii.gz\n","new_4_11_d_084336.nii\n","new_4_11_m_084336.nii.gz\n","new_4_7_d_184422.nii\n","new_4_7_m_184422.nii.gz\n","new_4_8_d_143412.nii\n","new_4_8_m_143412.nii.gz\n","new_4_9_d_155347.nii\n","new_4_9_m_155347.nii.gz\n","\tEpoch: 5; Dice: 0.9294 +/- 0.0216; Loss: 20.1408\n","Begin Epoch 6\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tEpoch:06 [000/005 (0000/0614)]\tLoss Ss: 0.026977\n","\tEpoch:06 [000/005 (0020/0614)]\tLoss Ss: 0.034819\n","\tEpoch:06 [000/005 (0040/0614)]\tLoss Ss: 0.033805\n","\tEpoch:06 [000/005 (0060/0614)]\tLoss Ss: 0.026830\n","\tEpoch:06 [000/005 (0080/0614)]\tLoss Ss: 0.028876\n","\tEpoch:06 [000/005 (0100/0614)]\tLoss Ss: 0.054799\n","\tEpoch:06 [000/005 (0120/0614)]\tLoss Ss: 0.026569\n","\tEpoch:06 [000/005 (0140/0614)]\tLoss Ss: 0.022961\n","\tEpoch:06 [000/005 (0160/0614)]\tLoss Ss: 0.024368\n","\tEpoch:06 [000/005 (0180/0614)]\tLoss Ss: 0.016651\n","\tEpoch:06 [000/005 (0200/0614)]\tLoss Ss: 0.017692\n","\tEpoch:06 [000/005 (0220/0614)]\tLoss Ss: 0.022240\n","\tEpoch:06 [000/005 (0240/0614)]\tLoss Ss: 0.019239\n","\tEpoch:06 [000/005 (0260/0614)]\tLoss Ss: 0.019498\n","\tEpoch:06 [000/005 (0280/0614)]\tLoss Ss: 0.019652\n","\tEpoch:06 [000/005 (0300/0614)]\tLoss Ss: 0.018152\n","\tEpoch:06 [000/005 (0320/0614)]\tLoss Ss: 0.015256\n","\tEpoch:06 [000/005 (0340/0614)]\tLoss Ss: 0.024334\n","\tEpoch:06 [000/005 (0360/0614)]\tLoss Ss: 0.016155\n","\tEpoch:06 [000/005 (0380/0614)]\tLoss Ss: 0.019209\n","\tEpoch:06 [000/005 (0400/0614)]\tLoss Ss: 0.023255\n","\tEpoch:06 [000/005 (0420/0614)]\tLoss Ss: 0.016638\n","\tEpoch:06 [000/005 (0440/0614)]\tLoss Ss: 0.015488\n","\tEpoch:06 [000/005 (0460/0614)]\tLoss Ss: 0.011846\n","\tEpoch:06 [000/005 (0480/0614)]\tLoss Ss: 0.013080\n","\tEpoch:06 [000/005 (0500/0614)]\tLoss Ss: 0.016035\n","\tEpoch:06 [000/005 (0520/0614)]\tLoss Ss: 0.011673\n","\tEpoch:06 [000/005 (0540/0614)]\tLoss Ss: 0.025010\n","\tEpoch:06 [000/005 (0560/0614)]\tLoss Ss: 0.014549\n","\tEpoch:06 [000/005 (0580/0614)]\tLoss Ss: 0.011521\n","\tEpoch:06 [000/005 (0600/0614)]\tLoss Ss: 0.011506\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tEpoch:06 [001/005 (0000/0693)]\tLoss Ss: 0.024361\n","\tEpoch:06 [001/005 (0020/0693)]\tLoss Ss: 0.017959\n","\tEpoch:06 [001/005 (0040/0693)]\tLoss Ss: 0.034198\n","\tEpoch:06 [001/005 (0060/0693)]\tLoss Ss: 0.020062\n","\tEpoch:06 [001/005 (0080/0693)]\tLoss Ss: 0.014965\n","\tEpoch:06 [001/005 (0100/0693)]\tLoss Ss: 0.030640\n","\tEpoch:06 [001/005 (0120/0693)]\tLoss Ss: 0.027168\n","\tEpoch:06 [001/005 (0140/0693)]\tLoss Ss: 0.033977\n","\tEpoch:06 [001/005 (0160/0693)]\tLoss Ss: 0.019502\n","\tEpoch:06 [001/005 (0180/0693)]\tLoss Ss: 0.023050\n","\tEpoch:06 [001/005 (0200/0693)]\tLoss Ss: 0.023691\n","\tEpoch:06 [001/005 (0220/0693)]\tLoss Ss: 0.016211\n","\tEpoch:06 [001/005 (0240/0693)]\tLoss Ss: 0.021365\n","\tEpoch:06 [001/005 (0260/0693)]\tLoss Ss: 0.019409\n","\tEpoch:06 [001/005 (0280/0693)]\tLoss Ss: 0.022486\n","\tEpoch:06 [001/005 (0300/0693)]\tLoss Ss: 0.028678\n","\tEpoch:06 [001/005 (0320/0693)]\tLoss Ss: 0.022760\n","\tEpoch:06 [001/005 (0340/0693)]\tLoss Ss: 0.015447\n","\tEpoch:06 [001/005 (0360/0693)]\tLoss Ss: 0.022463\n","\tEpoch:06 [001/005 (0380/0693)]\tLoss Ss: 0.016434\n","\tEpoch:06 [001/005 (0400/0693)]\tLoss Ss: 0.021935\n","\tEpoch:06 [001/005 (0420/0693)]\tLoss Ss: 0.025962\n","\tEpoch:06 [001/005 (0440/0693)]\tLoss Ss: 0.023183\n","\tEpoch:06 [001/005 (0460/0693)]\tLoss Ss: 0.017011\n","\tEpoch:06 [001/005 (0480/0693)]\tLoss Ss: 0.017872\n","\tEpoch:06 [001/005 (0500/0693)]\tLoss Ss: 0.019798\n","\tEpoch:06 [001/005 (0520/0693)]\tLoss Ss: 0.016262\n","\tEpoch:06 [001/005 (0540/0693)]\tLoss Ss: 0.016162\n","\tEpoch:06 [001/005 (0560/0693)]\tLoss Ss: 0.020725\n","\tEpoch:06 [001/005 (0580/0693)]\tLoss Ss: 0.011165\n","\tEpoch:06 [001/005 (0600/0693)]\tLoss Ss: 0.019096\n","\tEpoch:06 [001/005 (0620/0693)]\tLoss Ss: 0.026512\n","\tEpoch:06 [001/005 (0640/0693)]\tLoss Ss: 0.024188\n","\tEpoch:06 [001/005 (0660/0693)]\tLoss Ss: 0.016487\n","\tEpoch:06 [001/005 (0680/0693)]\tLoss Ss: 0.020796\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:06 [002/005 (0000/0755)]\tLoss Ss: 0.023796\n","\tEpoch:06 [002/005 (0020/0755)]\tLoss Ss: 0.025389\n","\tEpoch:06 [002/005 (0040/0755)]\tLoss Ss: 0.022531\n","\tEpoch:06 [002/005 (0060/0755)]\tLoss Ss: 0.030003\n","\tEpoch:06 [002/005 (0080/0755)]\tLoss Ss: 0.021933\n","\tEpoch:06 [002/005 (0100/0755)]\tLoss Ss: 0.027459\n","\tEpoch:06 [002/005 (0120/0755)]\tLoss Ss: 0.026615\n","\tEpoch:06 [002/005 (0140/0755)]\tLoss Ss: 0.020147\n","\tEpoch:06 [002/005 (0160/0755)]\tLoss Ss: 0.022846\n","\tEpoch:06 [002/005 (0180/0755)]\tLoss Ss: 0.027755\n","\tEpoch:06 [002/005 (0200/0755)]\tLoss Ss: 0.018105\n","\tEpoch:06 [002/005 (0220/0755)]\tLoss Ss: 0.019431\n","\tEpoch:06 [002/005 (0240/0755)]\tLoss Ss: 0.022162\n","\tEpoch:06 [002/005 (0260/0755)]\tLoss Ss: 0.018634\n","\tEpoch:06 [002/005 (0280/0755)]\tLoss Ss: 0.018236\n","\tEpoch:06 [002/005 (0300/0755)]\tLoss Ss: 0.028412\n","\tEpoch:06 [002/005 (0320/0755)]\tLoss Ss: 0.024658\n","\tEpoch:06 [002/005 (0340/0755)]\tLoss Ss: 0.020008\n","\tEpoch:06 [002/005 (0360/0755)]\tLoss Ss: 0.024092\n","\tEpoch:06 [002/005 (0380/0755)]\tLoss Ss: 0.016901\n","\tEpoch:06 [002/005 (0400/0755)]\tLoss Ss: 0.019235\n","\tEpoch:06 [002/005 (0420/0755)]\tLoss Ss: 0.017665\n","\tEpoch:06 [002/005 (0440/0755)]\tLoss Ss: 0.016543\n","\tEpoch:06 [002/005 (0460/0755)]\tLoss Ss: 0.019477\n","\tEpoch:06 [002/005 (0480/0755)]\tLoss Ss: 0.013708\n","\tEpoch:06 [002/005 (0500/0755)]\tLoss Ss: 0.018151\n","\tEpoch:06 [002/005 (0520/0755)]\tLoss Ss: 0.024733\n","\tEpoch:06 [002/005 (0540/0755)]\tLoss Ss: 0.013709\n","\tEpoch:06 [002/005 (0560/0755)]\tLoss Ss: 0.017292\n","\tEpoch:06 [002/005 (0580/0755)]\tLoss Ss: 0.016618\n","\tEpoch:06 [002/005 (0600/0755)]\tLoss Ss: 0.013434\n","\tEpoch:06 [002/005 (0620/0755)]\tLoss Ss: 0.014940\n","\tEpoch:06 [002/005 (0640/0755)]\tLoss Ss: 0.018781\n","\tEpoch:06 [002/005 (0660/0755)]\tLoss Ss: 0.015883\n","\tEpoch:06 [002/005 (0680/0755)]\tLoss Ss: 0.015197\n","\tEpoch:06 [002/005 (0700/0755)]\tLoss Ss: 0.017176\n","\tEpoch:06 [002/005 (0720/0755)]\tLoss Ss: 0.021616\n","\tEpoch:06 [002/005 (0740/0755)]\tLoss Ss: 0.020841\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tEpoch:06 [003/005 (0000/0693)]\tLoss Ss: 0.016926\n","\tEpoch:06 [003/005 (0020/0693)]\tLoss Ss: 0.033972\n","\tEpoch:06 [003/005 (0040/0693)]\tLoss Ss: 0.018778\n","\tEpoch:06 [003/005 (0060/0693)]\tLoss Ss: 0.020895\n","\tEpoch:06 [003/005 (0080/0693)]\tLoss Ss: 0.018782\n","\tEpoch:06 [003/005 (0100/0693)]\tLoss Ss: 0.019997\n","\tEpoch:06 [003/005 (0120/0693)]\tLoss Ss: 0.020517\n","\tEpoch:06 [003/005 (0140/0693)]\tLoss Ss: 0.022283\n","\tEpoch:06 [003/005 (0160/0693)]\tLoss Ss: 0.023922\n","\tEpoch:06 [003/005 (0180/0693)]\tLoss Ss: 0.018042\n","\tEpoch:06 [003/005 (0200/0693)]\tLoss Ss: 0.014600\n","\tEpoch:06 [003/005 (0220/0693)]\tLoss Ss: 0.014053\n","\tEpoch:06 [003/005 (0240/0693)]\tLoss Ss: 0.014387\n","\tEpoch:06 [003/005 (0260/0693)]\tLoss Ss: 0.022958\n","\tEpoch:06 [003/005 (0280/0693)]\tLoss Ss: 0.012216\n","\tEpoch:06 [003/005 (0300/0693)]\tLoss Ss: 0.014472\n","\tEpoch:06 [003/005 (0320/0693)]\tLoss Ss: 0.017711\n","\tEpoch:06 [003/005 (0340/0693)]\tLoss Ss: 0.018110\n","\tEpoch:06 [003/005 (0360/0693)]\tLoss Ss: 0.019854\n","\tEpoch:06 [003/005 (0380/0693)]\tLoss Ss: 0.013821\n","\tEpoch:06 [003/005 (0400/0693)]\tLoss Ss: 0.018010\n","\tEpoch:06 [003/005 (0420/0693)]\tLoss Ss: 0.012798\n","\tEpoch:06 [003/005 (0440/0693)]\tLoss Ss: 0.019582\n","\tEpoch:06 [003/005 (0460/0693)]\tLoss Ss: 0.020185\n","\tEpoch:06 [003/005 (0480/0693)]\tLoss Ss: 0.014383\n","\tEpoch:06 [003/005 (0500/0693)]\tLoss Ss: 0.022851\n","\tEpoch:06 [003/005 (0520/0693)]\tLoss Ss: 0.014841\n","\tEpoch:06 [003/005 (0540/0693)]\tLoss Ss: 0.014081\n","\tEpoch:06 [003/005 (0560/0693)]\tLoss Ss: 0.012822\n","\tEpoch:06 [003/005 (0580/0693)]\tLoss Ss: 0.030059\n","\tEpoch:06 [003/005 (0600/0693)]\tLoss Ss: 0.018119\n","\tEpoch:06 [003/005 (0620/0693)]\tLoss Ss: 0.021761\n","\tEpoch:06 [003/005 (0640/0693)]\tLoss Ss: 0.013317\n","\tEpoch:06 [003/005 (0660/0693)]\tLoss Ss: 0.015490\n","\tEpoch:06 [003/005 (0680/0693)]\tLoss Ss: 0.018846\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tEpoch:06 [004/005 (0000/0588)]\tLoss Ss: 0.014251\n","\tEpoch:06 [004/005 (0020/0588)]\tLoss Ss: 0.011546\n","\tEpoch:06 [004/005 (0040/0588)]\tLoss Ss: 0.023711\n","\tEpoch:06 [004/005 (0060/0588)]\tLoss Ss: 0.010077\n","\tEpoch:06 [004/005 (0080/0588)]\tLoss Ss: 0.009425\n","\tEpoch:06 [004/005 (0100/0588)]\tLoss Ss: 0.013162\n","\tEpoch:06 [004/005 (0120/0588)]\tLoss Ss: 0.012191\n","\tEpoch:06 [004/005 (0140/0588)]\tLoss Ss: 0.014926\n","\tEpoch:06 [004/005 (0160/0588)]\tLoss Ss: 0.015397\n","\tEpoch:06 [004/005 (0180/0588)]\tLoss Ss: 0.010047\n","\tEpoch:06 [004/005 (0200/0588)]\tLoss Ss: 0.009993\n","\tEpoch:06 [004/005 (0220/0588)]\tLoss Ss: 0.011353\n","\tEpoch:06 [004/005 (0240/0588)]\tLoss Ss: 0.011132\n","\tEpoch:06 [004/005 (0260/0588)]\tLoss Ss: 0.013698\n","\tEpoch:06 [004/005 (0280/0588)]\tLoss Ss: 0.014556\n","\tEpoch:06 [004/005 (0300/0588)]\tLoss Ss: 0.009257\n","\tEpoch:06 [004/005 (0320/0588)]\tLoss Ss: 0.010469\n","\tEpoch:06 [004/005 (0340/0588)]\tLoss Ss: 0.011070\n","\tEpoch:06 [004/005 (0360/0588)]\tLoss Ss: 0.008906\n","\tEpoch:06 [004/005 (0380/0588)]\tLoss Ss: 0.008195\n","\tEpoch:06 [004/005 (0400/0588)]\tLoss Ss: 0.006721\n","\tEpoch:06 [004/005 (0420/0588)]\tLoss Ss: 0.013762\n","\tEpoch:06 [004/005 (0440/0588)]\tLoss Ss: 0.009530\n","\tEpoch:06 [004/005 (0460/0588)]\tLoss Ss: 0.006345\n","\tEpoch:06 [004/005 (0480/0588)]\tLoss Ss: 0.007118\n","\tEpoch:06 [004/005 (0500/0588)]\tLoss Ss: 0.010546\n","\tEpoch:06 [004/005 (0520/0588)]\tLoss Ss: 0.012990\n","\tEpoch:06 [004/005 (0540/0588)]\tLoss Ss: 0.009729\n","\tEpoch:06 [004/005 (0560/0588)]\tLoss Ss: 0.006560\n","\tEpoch:06 [004/005 (0580/0588)]\tLoss Ss: 0.009171\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:06 [005/005 (0000/0755)]\tLoss Ss: 0.034646\n","\tEpoch:06 [005/005 (0020/0755)]\tLoss Ss: 0.040586\n","\tEpoch:06 [005/005 (0040/0755)]\tLoss Ss: 0.033925\n","\tEpoch:06 [005/005 (0060/0755)]\tLoss Ss: 0.046482\n","\tEpoch:06 [005/005 (0080/0755)]\tLoss Ss: 0.018965\n","\tEpoch:06 [005/005 (0100/0755)]\tLoss Ss: 0.030636\n","\tEpoch:06 [005/005 (0120/0755)]\tLoss Ss: 0.015859\n","\tEpoch:06 [005/005 (0140/0755)]\tLoss Ss: 0.024731\n","\tEpoch:06 [005/005 (0160/0755)]\tLoss Ss: 0.031809\n","\tEpoch:06 [005/005 (0180/0755)]\tLoss Ss: 0.025562\n","\tEpoch:06 [005/005 (0200/0755)]\tLoss Ss: 0.017687\n","\tEpoch:06 [005/005 (0220/0755)]\tLoss Ss: 0.023121\n","\tEpoch:06 [005/005 (0240/0755)]\tLoss Ss: 0.021864\n","\tEpoch:06 [005/005 (0260/0755)]\tLoss Ss: 0.020702\n","\tEpoch:06 [005/005 (0280/0755)]\tLoss Ss: 0.024923\n","\tEpoch:06 [005/005 (0300/0755)]\tLoss Ss: 0.048059\n","\tEpoch:06 [005/005 (0320/0755)]\tLoss Ss: 0.018175\n","\tEpoch:06 [005/005 (0340/0755)]\tLoss Ss: 0.021848\n","\tEpoch:06 [005/005 (0360/0755)]\tLoss Ss: 0.023853\n","\tEpoch:06 [005/005 (0380/0755)]\tLoss Ss: 0.026887\n","\tEpoch:06 [005/005 (0400/0755)]\tLoss Ss: 0.028182\n","\tEpoch:06 [005/005 (0420/0755)]\tLoss Ss: 0.016487\n","\tEpoch:06 [005/005 (0440/0755)]\tLoss Ss: 0.011427\n","\tEpoch:06 [005/005 (0460/0755)]\tLoss Ss: 0.019395\n","\tEpoch:06 [005/005 (0480/0755)]\tLoss Ss: 0.033482\n","\tEpoch:06 [005/005 (0500/0755)]\tLoss Ss: 0.020759\n","\tEpoch:06 [005/005 (0520/0755)]\tLoss Ss: 0.020295\n","\tEpoch:06 [005/005 (0540/0755)]\tLoss Ss: 0.021534\n","\tEpoch:06 [005/005 (0560/0755)]\tLoss Ss: 0.023217\n","\tEpoch:06 [005/005 (0580/0755)]\tLoss Ss: 0.018995\n","\tEpoch:06 [005/005 (0600/0755)]\tLoss Ss: 0.021060\n","\tEpoch:06 [005/005 (0620/0755)]\tLoss Ss: 0.024221\n","\tEpoch:06 [005/005 (0640/0755)]\tLoss Ss: 0.016755\n","\tEpoch:06 [005/005 (0660/0755)]\tLoss Ss: 0.019530\n","\tEpoch:06 [005/005 (0680/0755)]\tLoss Ss: 0.020899\n","\tEpoch:06 [005/005 (0700/0755)]\tLoss Ss: 0.017294\n","\tEpoch:06 [005/005 (0720/0755)]\tLoss Ss: 0.026290\n","\tEpoch:06 [005/005 (0740/0755)]\tLoss Ss: 0.025077\n","Now train the rotated image\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tRotated_Epoch:06 [000/005 (0000/0693)]\tLoss Ss: 0.027846\n","\tRotated_Epoch:06 [000/005 (0020/0693)]\tLoss Ss: 0.034564\n","\tRotated_Epoch:06 [000/005 (0040/0693)]\tLoss Ss: 0.038705\n","\tRotated_Epoch:06 [000/005 (0060/0693)]\tLoss Ss: 0.059024\n","\tRotated_Epoch:06 [000/005 (0080/0693)]\tLoss Ss: 0.021919\n","\tRotated_Epoch:06 [000/005 (0100/0693)]\tLoss Ss: 0.028132\n","\tRotated_Epoch:06 [000/005 (0120/0693)]\tLoss Ss: 0.019200\n","\tRotated_Epoch:06 [000/005 (0140/0693)]\tLoss Ss: 0.027015\n","\tRotated_Epoch:06 [000/005 (0160/0693)]\tLoss Ss: 0.028380\n","\tRotated_Epoch:06 [000/005 (0180/0693)]\tLoss Ss: 0.022776\n","\tRotated_Epoch:06 [000/005 (0200/0693)]\tLoss Ss: 0.023352\n","\tRotated_Epoch:06 [000/005 (0220/0693)]\tLoss Ss: 0.023541\n","\tRotated_Epoch:06 [000/005 (0240/0693)]\tLoss Ss: 0.018611\n","\tRotated_Epoch:06 [000/005 (0260/0693)]\tLoss Ss: 0.017535\n","\tRotated_Epoch:06 [000/005 (0280/0693)]\tLoss Ss: 0.021625\n","\tRotated_Epoch:06 [000/005 (0300/0693)]\tLoss Ss: 0.027546\n","\tRotated_Epoch:06 [000/005 (0320/0693)]\tLoss Ss: 0.020268\n","\tRotated_Epoch:06 [000/005 (0340/0693)]\tLoss Ss: 0.024498\n","\tRotated_Epoch:06 [000/005 (0360/0693)]\tLoss Ss: 0.022528\n","\tRotated_Epoch:06 [000/005 (0380/0693)]\tLoss Ss: 0.017354\n","\tRotated_Epoch:06 [000/005 (0400/0693)]\tLoss Ss: 0.026763\n","\tRotated_Epoch:06 [000/005 (0420/0693)]\tLoss Ss: 0.021500\n","\tRotated_Epoch:06 [000/005 (0440/0693)]\tLoss Ss: 0.030298\n","\tRotated_Epoch:06 [000/005 (0460/0693)]\tLoss Ss: 0.022246\n","\tRotated_Epoch:06 [000/005 (0480/0693)]\tLoss Ss: 0.019659\n","\tRotated_Epoch:06 [000/005 (0500/0693)]\tLoss Ss: 0.016196\n","\tRotated_Epoch:06 [000/005 (0520/0693)]\tLoss Ss: 0.013361\n","\tRotated_Epoch:06 [000/005 (0540/0693)]\tLoss Ss: 0.019070\n","\tRotated_Epoch:06 [000/005 (0560/0693)]\tLoss Ss: 0.015858\n","\tRotated_Epoch:06 [000/005 (0580/0693)]\tLoss Ss: 0.020074\n","\tRotated_Epoch:06 [000/005 (0600/0693)]\tLoss Ss: 0.017517\n","\tRotated_Epoch:06 [000/005 (0620/0693)]\tLoss Ss: 0.021029\n","\tRotated_Epoch:06 [000/005 (0640/0693)]\tLoss Ss: 0.016662\n","\tRotated_Epoch:06 [000/005 (0660/0693)]\tLoss Ss: 0.018183\n","\tRotated_Epoch:06 [000/005 (0680/0693)]\tLoss Ss: 0.009229\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tRotated_Epoch:06 [001/005 (0000/0614)]\tLoss Ss: 0.011234\n","\tRotated_Epoch:06 [001/005 (0020/0614)]\tLoss Ss: 0.009128\n","\tRotated_Epoch:06 [001/005 (0040/0614)]\tLoss Ss: 0.017566\n","\tRotated_Epoch:06 [001/005 (0060/0614)]\tLoss Ss: 0.013662\n","\tRotated_Epoch:06 [001/005 (0080/0614)]\tLoss Ss: 0.013812\n","\tRotated_Epoch:06 [001/005 (0100/0614)]\tLoss Ss: 0.027294\n","\tRotated_Epoch:06 [001/005 (0120/0614)]\tLoss Ss: 0.011279\n","\tRotated_Epoch:06 [001/005 (0140/0614)]\tLoss Ss: 0.016880\n","\tRotated_Epoch:06 [001/005 (0160/0614)]\tLoss Ss: 0.016796\n","\tRotated_Epoch:06 [001/005 (0180/0614)]\tLoss Ss: 0.012529\n","\tRotated_Epoch:06 [001/005 (0200/0614)]\tLoss Ss: 0.009898\n","\tRotated_Epoch:06 [001/005 (0220/0614)]\tLoss Ss: 0.011298\n","\tRotated_Epoch:06 [001/005 (0240/0614)]\tLoss Ss: 0.011155\n","\tRotated_Epoch:06 [001/005 (0260/0614)]\tLoss Ss: 0.012031\n","\tRotated_Epoch:06 [001/005 (0280/0614)]\tLoss Ss: 0.009059\n","\tRotated_Epoch:06 [001/005 (0300/0614)]\tLoss Ss: 0.010744\n","\tRotated_Epoch:06 [001/005 (0320/0614)]\tLoss Ss: 0.009140\n","\tRotated_Epoch:06 [001/005 (0340/0614)]\tLoss Ss: 0.013893\n","\tRotated_Epoch:06 [001/005 (0360/0614)]\tLoss Ss: 0.011295\n","\tRotated_Epoch:06 [001/005 (0380/0614)]\tLoss Ss: 0.008670\n","\tRotated_Epoch:06 [001/005 (0400/0614)]\tLoss Ss: 0.010037\n","\tRotated_Epoch:06 [001/005 (0420/0614)]\tLoss Ss: 0.010847\n","\tRotated_Epoch:06 [001/005 (0440/0614)]\tLoss Ss: 0.011294\n","\tRotated_Epoch:06 [001/005 (0460/0614)]\tLoss Ss: 0.008166\n","\tRotated_Epoch:06 [001/005 (0480/0614)]\tLoss Ss: 0.007514\n","\tRotated_Epoch:06 [001/005 (0500/0614)]\tLoss Ss: 0.009280\n","\tRotated_Epoch:06 [001/005 (0520/0614)]\tLoss Ss: 0.010841\n","\tRotated_Epoch:06 [001/005 (0540/0614)]\tLoss Ss: 0.012946\n","\tRotated_Epoch:06 [001/005 (0560/0614)]\tLoss Ss: 0.016550\n","\tRotated_Epoch:06 [001/005 (0580/0614)]\tLoss Ss: 0.009121\n","\tRotated_Epoch:06 [001/005 (0600/0614)]\tLoss Ss: 0.009712\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tRotated_Epoch:06 [002/005 (0000/0588)]\tLoss Ss: 0.423518\n","\tRotated_Epoch:06 [002/005 (0020/0588)]\tLoss Ss: 0.217963\n","\tRotated_Epoch:06 [002/005 (0040/0588)]\tLoss Ss: 0.203750\n","\tRotated_Epoch:06 [002/005 (0060/0588)]\tLoss Ss: 0.235530\n","\tRotated_Epoch:06 [002/005 (0080/0588)]\tLoss Ss: 0.215064\n","\tRotated_Epoch:06 [002/005 (0100/0588)]\tLoss Ss: 0.160370\n","\tRotated_Epoch:06 [002/005 (0120/0588)]\tLoss Ss: 0.184107\n","\tRotated_Epoch:06 [002/005 (0140/0588)]\tLoss Ss: 0.158661\n","\tRotated_Epoch:06 [002/005 (0160/0588)]\tLoss Ss: 0.086222\n","\tRotated_Epoch:06 [002/005 (0180/0588)]\tLoss Ss: 0.072808\n","\tRotated_Epoch:06 [002/005 (0200/0588)]\tLoss Ss: 0.136843\n","\tRotated_Epoch:06 [002/005 (0220/0588)]\tLoss Ss: 0.102193\n","\tRotated_Epoch:06 [002/005 (0240/0588)]\tLoss Ss: 0.093648\n","\tRotated_Epoch:06 [002/005 (0260/0588)]\tLoss Ss: 0.083165\n","\tRotated_Epoch:06 [002/005 (0280/0588)]\tLoss Ss: 0.075873\n","\tRotated_Epoch:06 [002/005 (0300/0588)]\tLoss Ss: 0.068486\n","\tRotated_Epoch:06 [002/005 (0320/0588)]\tLoss Ss: 0.083897\n","\tRotated_Epoch:06 [002/005 (0340/0588)]\tLoss Ss: 0.090619\n","\tRotated_Epoch:06 [002/005 (0360/0588)]\tLoss Ss: 0.082392\n","\tRotated_Epoch:06 [002/005 (0380/0588)]\tLoss Ss: 0.076667\n","\tRotated_Epoch:06 [002/005 (0400/0588)]\tLoss Ss: 0.117012\n","\tRotated_Epoch:06 [002/005 (0420/0588)]\tLoss Ss: 0.117309\n","\tRotated_Epoch:06 [002/005 (0440/0588)]\tLoss Ss: 0.095549\n","\tRotated_Epoch:06 [002/005 (0460/0588)]\tLoss Ss: 0.119149\n","\tRotated_Epoch:06 [002/005 (0480/0588)]\tLoss Ss: 0.114971\n","\tRotated_Epoch:06 [002/005 (0500/0588)]\tLoss Ss: 0.105393\n","\tRotated_Epoch:06 [002/005 (0520/0588)]\tLoss Ss: 0.090788\n","\tRotated_Epoch:06 [002/005 (0540/0588)]\tLoss Ss: 0.098469\n","\tRotated_Epoch:06 [002/005 (0560/0588)]\tLoss Ss: 0.088864\n","\tRotated_Epoch:06 [002/005 (0580/0588)]\tLoss Ss: 0.080929\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:06 [003/005 (0000/0755)]\tLoss Ss: 0.159917\n","\tRotated_Epoch:06 [003/005 (0020/0755)]\tLoss Ss: 0.109523\n","\tRotated_Epoch:06 [003/005 (0040/0755)]\tLoss Ss: 0.079254\n","\tRotated_Epoch:06 [003/005 (0060/0755)]\tLoss Ss: 0.033079\n","\tRotated_Epoch:06 [003/005 (0080/0755)]\tLoss Ss: 0.039125\n","\tRotated_Epoch:06 [003/005 (0100/0755)]\tLoss Ss: 0.051033\n","\tRotated_Epoch:06 [003/005 (0120/0755)]\tLoss Ss: 0.027975\n","\tRotated_Epoch:06 [003/005 (0140/0755)]\tLoss Ss: 0.047438\n","\tRotated_Epoch:06 [003/005 (0160/0755)]\tLoss Ss: 0.025530\n","\tRotated_Epoch:06 [003/005 (0180/0755)]\tLoss Ss: 0.036221\n","\tRotated_Epoch:06 [003/005 (0200/0755)]\tLoss Ss: 0.046023\n","\tRotated_Epoch:06 [003/005 (0220/0755)]\tLoss Ss: 0.064435\n","\tRotated_Epoch:06 [003/005 (0240/0755)]\tLoss Ss: 0.044761\n","\tRotated_Epoch:06 [003/005 (0260/0755)]\tLoss Ss: 0.037096\n","\tRotated_Epoch:06 [003/005 (0280/0755)]\tLoss Ss: 0.037044\n","\tRotated_Epoch:06 [003/005 (0300/0755)]\tLoss Ss: 0.035878\n","\tRotated_Epoch:06 [003/005 (0320/0755)]\tLoss Ss: 0.023593\n","\tRotated_Epoch:06 [003/005 (0340/0755)]\tLoss Ss: 0.038741\n","\tRotated_Epoch:06 [003/005 (0360/0755)]\tLoss Ss: 0.037386\n","\tRotated_Epoch:06 [003/005 (0380/0755)]\tLoss Ss: 0.027789\n","\tRotated_Epoch:06 [003/005 (0400/0755)]\tLoss Ss: 0.040571\n","\tRotated_Epoch:06 [003/005 (0420/0755)]\tLoss Ss: 0.029151\n","\tRotated_Epoch:06 [003/005 (0440/0755)]\tLoss Ss: 0.017474\n","\tRotated_Epoch:06 [003/005 (0460/0755)]\tLoss Ss: 0.034901\n","\tRotated_Epoch:06 [003/005 (0480/0755)]\tLoss Ss: 0.029357\n","\tRotated_Epoch:06 [003/005 (0500/0755)]\tLoss Ss: 0.034723\n","\tRotated_Epoch:06 [003/005 (0520/0755)]\tLoss Ss: 0.026335\n","\tRotated_Epoch:06 [003/005 (0540/0755)]\tLoss Ss: 0.021672\n","\tRotated_Epoch:06 [003/005 (0560/0755)]\tLoss Ss: 0.027981\n","\tRotated_Epoch:06 [003/005 (0580/0755)]\tLoss Ss: 0.014914\n","\tRotated_Epoch:06 [003/005 (0600/0755)]\tLoss Ss: 0.025618\n","\tRotated_Epoch:06 [003/005 (0620/0755)]\tLoss Ss: 0.026391\n","\tRotated_Epoch:06 [003/005 (0640/0755)]\tLoss Ss: 0.022982\n","\tRotated_Epoch:06 [003/005 (0660/0755)]\tLoss Ss: 0.024242\n","\tRotated_Epoch:06 [003/005 (0680/0755)]\tLoss Ss: 0.029587\n","\tRotated_Epoch:06 [003/005 (0700/0755)]\tLoss Ss: 0.030530\n","\tRotated_Epoch:06 [003/005 (0720/0755)]\tLoss Ss: 0.032205\n","\tRotated_Epoch:06 [003/005 (0740/0755)]\tLoss Ss: 0.038008\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tRotated_Epoch:06 [004/005 (0000/0693)]\tLoss Ss: 0.034829\n","\tRotated_Epoch:06 [004/005 (0020/0693)]\tLoss Ss: 0.036308\n","\tRotated_Epoch:06 [004/005 (0040/0693)]\tLoss Ss: 0.030471\n","\tRotated_Epoch:06 [004/005 (0060/0693)]\tLoss Ss: 0.021952\n","\tRotated_Epoch:06 [004/005 (0080/0693)]\tLoss Ss: 0.030002\n","\tRotated_Epoch:06 [004/005 (0100/0693)]\tLoss Ss: 0.026543\n","\tRotated_Epoch:06 [004/005 (0120/0693)]\tLoss Ss: 0.021869\n","\tRotated_Epoch:06 [004/005 (0140/0693)]\tLoss Ss: 0.023451\n","\tRotated_Epoch:06 [004/005 (0160/0693)]\tLoss Ss: 0.024969\n","\tRotated_Epoch:06 [004/005 (0180/0693)]\tLoss Ss: 0.020692\n","\tRotated_Epoch:06 [004/005 (0200/0693)]\tLoss Ss: 0.025896\n","\tRotated_Epoch:06 [004/005 (0220/0693)]\tLoss Ss: 0.024812\n","\tRotated_Epoch:06 [004/005 (0240/0693)]\tLoss Ss: 0.021036\n","\tRotated_Epoch:06 [004/005 (0260/0693)]\tLoss Ss: 0.017776\n","\tRotated_Epoch:06 [004/005 (0280/0693)]\tLoss Ss: 0.027329\n","\tRotated_Epoch:06 [004/005 (0300/0693)]\tLoss Ss: 0.017338\n","\tRotated_Epoch:06 [004/005 (0320/0693)]\tLoss Ss: 0.019800\n","\tRotated_Epoch:06 [004/005 (0340/0693)]\tLoss Ss: 0.030163\n","\tRotated_Epoch:06 [004/005 (0360/0693)]\tLoss Ss: 0.020717\n","\tRotated_Epoch:06 [004/005 (0380/0693)]\tLoss Ss: 0.021324\n","\tRotated_Epoch:06 [004/005 (0400/0693)]\tLoss Ss: 0.017091\n","\tRotated_Epoch:06 [004/005 (0420/0693)]\tLoss Ss: 0.022619\n","\tRotated_Epoch:06 [004/005 (0440/0693)]\tLoss Ss: 0.019233\n","\tRotated_Epoch:06 [004/005 (0460/0693)]\tLoss Ss: 0.028192\n","\tRotated_Epoch:06 [004/005 (0480/0693)]\tLoss Ss: 0.018024\n","\tRotated_Epoch:06 [004/005 (0500/0693)]\tLoss Ss: 0.019109\n","\tRotated_Epoch:06 [004/005 (0520/0693)]\tLoss Ss: 0.015536\n","\tRotated_Epoch:06 [004/005 (0540/0693)]\tLoss Ss: 0.024536\n","\tRotated_Epoch:06 [004/005 (0560/0693)]\tLoss Ss: 0.019595\n","\tRotated_Epoch:06 [004/005 (0580/0693)]\tLoss Ss: 0.013233\n","\tRotated_Epoch:06 [004/005 (0600/0693)]\tLoss Ss: 0.015355\n","\tRotated_Epoch:06 [004/005 (0620/0693)]\tLoss Ss: 0.028702\n","\tRotated_Epoch:06 [004/005 (0640/0693)]\tLoss Ss: 0.021112\n","\tRotated_Epoch:06 [004/005 (0660/0693)]\tLoss Ss: 0.024554\n","\tRotated_Epoch:06 [004/005 (0680/0693)]\tLoss Ss: 0.013444\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:06 [005/005 (0000/0755)]\tLoss Ss: 0.273367\n","\tRotated_Epoch:06 [005/005 (0020/0755)]\tLoss Ss: 0.278156\n","\tRotated_Epoch:06 [005/005 (0040/0755)]\tLoss Ss: 0.261450\n","\tRotated_Epoch:06 [005/005 (0060/0755)]\tLoss Ss: 0.544333\n","\tRotated_Epoch:06 [005/005 (0080/0755)]\tLoss Ss: 0.282794\n","\tRotated_Epoch:06 [005/005 (0100/0755)]\tLoss Ss: 0.185378\n","\tRotated_Epoch:06 [005/005 (0120/0755)]\tLoss Ss: 0.324324\n","\tRotated_Epoch:06 [005/005 (0140/0755)]\tLoss Ss: 0.234652\n","\tRotated_Epoch:06 [005/005 (0160/0755)]\tLoss Ss: 0.217634\n","\tRotated_Epoch:06 [005/005 (0180/0755)]\tLoss Ss: 0.100245\n","\tRotated_Epoch:06 [005/005 (0200/0755)]\tLoss Ss: 0.139537\n","\tRotated_Epoch:06 [005/005 (0220/0755)]\tLoss Ss: 0.134773\n","\tRotated_Epoch:06 [005/005 (0240/0755)]\tLoss Ss: 0.138396\n","\tRotated_Epoch:06 [005/005 (0260/0755)]\tLoss Ss: 0.196954\n","\tRotated_Epoch:06 [005/005 (0280/0755)]\tLoss Ss: 0.175969\n","\tRotated_Epoch:06 [005/005 (0300/0755)]\tLoss Ss: 0.129425\n","\tRotated_Epoch:06 [005/005 (0320/0755)]\tLoss Ss: 0.148733\n","\tRotated_Epoch:06 [005/005 (0340/0755)]\tLoss Ss: 0.122484\n","\tRotated_Epoch:06 [005/005 (0360/0755)]\tLoss Ss: 0.193486\n","\tRotated_Epoch:06 [005/005 (0380/0755)]\tLoss Ss: 0.157231\n","\tRotated_Epoch:06 [005/005 (0400/0755)]\tLoss Ss: 0.106259\n","\tRotated_Epoch:06 [005/005 (0420/0755)]\tLoss Ss: 0.162478\n","\tRotated_Epoch:06 [005/005 (0440/0755)]\tLoss Ss: 0.130997\n","\tRotated_Epoch:06 [005/005 (0460/0755)]\tLoss Ss: 0.142637\n","\tRotated_Epoch:06 [005/005 (0480/0755)]\tLoss Ss: 0.108705\n","\tRotated_Epoch:06 [005/005 (0500/0755)]\tLoss Ss: 0.129736\n","\tRotated_Epoch:06 [005/005 (0520/0755)]\tLoss Ss: 0.134647\n","\tRotated_Epoch:06 [005/005 (0540/0755)]\tLoss Ss: 0.161875\n","\tRotated_Epoch:06 [005/005 (0560/0755)]\tLoss Ss: 0.142463\n","\tRotated_Epoch:06 [005/005 (0580/0755)]\tLoss Ss: 0.154487\n","\tRotated_Epoch:06 [005/005 (0600/0755)]\tLoss Ss: 0.141478\n","\tRotated_Epoch:06 [005/005 (0620/0755)]\tLoss Ss: 0.144050\n","\tRotated_Epoch:06 [005/005 (0640/0755)]\tLoss Ss: 0.112356\n","\tRotated_Epoch:06 [005/005 (0660/0755)]\tLoss Ss: 0.122644\n","\tRotated_Epoch:06 [005/005 (0680/0755)]\tLoss Ss: 0.080010\n","\tRotated_Epoch:06 [005/005 (0700/0755)]\tLoss Ss: 0.149459\n","\tRotated_Epoch:06 [005/005 (0720/0755)]\tLoss Ss: 0.138155\n","\tRotated_Epoch:06 [005/005 (0740/0755)]\tLoss Ss: 0.136356\n","new_1_6_d.nii\n","new_1_6_m.nii\n","new_1_7_d.nii\n","new_1_7_m.nii\n","new_2_5_d.nii\n","new_2_5_m.nii\n","new_2_6_d.nii\n","new_2_6_m.nii\n","new_2_7_d.nii\n","new_2_7_m.nii\n","new_3_d (2).nii\n","new_3_m (2).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (3).nii\n","new_3_m (3).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (7).nii\n","new_3_m (7).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (8).nii\n","new_3_m (8).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (9).nii\n","new_3_m (9).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_4_10_d_163526.nii\n","new_4_10_m_163526.nii.gz\n","new_4_11_d_084336.nii\n","new_4_11_m_084336.nii.gz\n","new_4_7_d_184422.nii\n","new_4_7_m_184422.nii.gz\n","new_4_8_d_143412.nii\n","new_4_8_m_143412.nii.gz\n","new_4_9_d_155347.nii\n","new_4_9_m_155347.nii.gz\n","\tEpoch: 6; Dice: 0.8664 +/- 0.0287; Loss: 18.1094\n","Begin Epoch 7\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tEpoch:07 [000/005 (0000/0614)]\tLoss Ss: 0.063155\n","\tEpoch:07 [000/005 (0020/0614)]\tLoss Ss: 0.049400\n","\tEpoch:07 [000/005 (0040/0614)]\tLoss Ss: 0.046387\n","\tEpoch:07 [000/005 (0060/0614)]\tLoss Ss: 0.053233\n","\tEpoch:07 [000/005 (0080/0614)]\tLoss Ss: 0.044417\n","\tEpoch:07 [000/005 (0100/0614)]\tLoss Ss: 0.044602\n","\tEpoch:07 [000/005 (0120/0614)]\tLoss Ss: 0.031690\n","\tEpoch:07 [000/005 (0140/0614)]\tLoss Ss: 0.029014\n","\tEpoch:07 [000/005 (0160/0614)]\tLoss Ss: 0.027003\n","\tEpoch:07 [000/005 (0180/0614)]\tLoss Ss: 0.028269\n","\tEpoch:07 [000/005 (0200/0614)]\tLoss Ss: 0.023478\n","\tEpoch:07 [000/005 (0220/0614)]\tLoss Ss: 0.027251\n","\tEpoch:07 [000/005 (0240/0614)]\tLoss Ss: 0.020946\n","\tEpoch:07 [000/005 (0260/0614)]\tLoss Ss: 0.025691\n","\tEpoch:07 [000/005 (0280/0614)]\tLoss Ss: 0.023019\n","\tEpoch:07 [000/005 (0300/0614)]\tLoss Ss: 0.020233\n","\tEpoch:07 [000/005 (0320/0614)]\tLoss Ss: 0.019804\n","\tEpoch:07 [000/005 (0340/0614)]\tLoss Ss: 0.015591\n","\tEpoch:07 [000/005 (0360/0614)]\tLoss Ss: 0.020490\n","\tEpoch:07 [000/005 (0380/0614)]\tLoss Ss: 0.022422\n","\tEpoch:07 [000/005 (0400/0614)]\tLoss Ss: 0.011369\n","\tEpoch:07 [000/005 (0420/0614)]\tLoss Ss: 0.011197\n","\tEpoch:07 [000/005 (0440/0614)]\tLoss Ss: 0.016126\n","\tEpoch:07 [000/005 (0460/0614)]\tLoss Ss: 0.016541\n","\tEpoch:07 [000/005 (0480/0614)]\tLoss Ss: 0.014981\n","\tEpoch:07 [000/005 (0500/0614)]\tLoss Ss: 0.012523\n","\tEpoch:07 [000/005 (0520/0614)]\tLoss Ss: 0.014608\n","\tEpoch:07 [000/005 (0540/0614)]\tLoss Ss: 0.015130\n","\tEpoch:07 [000/005 (0560/0614)]\tLoss Ss: 0.013970\n","\tEpoch:07 [000/005 (0580/0614)]\tLoss Ss: 0.007890\n","\tEpoch:07 [000/005 (0600/0614)]\tLoss Ss: 0.010426\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tEpoch:07 [001/005 (0000/0693)]\tLoss Ss: 0.026057\n","\tEpoch:07 [001/005 (0020/0693)]\tLoss Ss: 0.025863\n","\tEpoch:07 [001/005 (0040/0693)]\tLoss Ss: 0.022535\n","\tEpoch:07 [001/005 (0060/0693)]\tLoss Ss: 0.027532\n","\tEpoch:07 [001/005 (0080/0693)]\tLoss Ss: 0.022137\n","\tEpoch:07 [001/005 (0100/0693)]\tLoss Ss: 0.036066\n","\tEpoch:07 [001/005 (0120/0693)]\tLoss Ss: 0.017880\n","\tEpoch:07 [001/005 (0140/0693)]\tLoss Ss: 0.024050\n","\tEpoch:07 [001/005 (0160/0693)]\tLoss Ss: 0.028141\n","\tEpoch:07 [001/005 (0180/0693)]\tLoss Ss: 0.030145\n","\tEpoch:07 [001/005 (0200/0693)]\tLoss Ss: 0.029504\n","\tEpoch:07 [001/005 (0220/0693)]\tLoss Ss: 0.029610\n","\tEpoch:07 [001/005 (0240/0693)]\tLoss Ss: 0.023020\n","\tEpoch:07 [001/005 (0260/0693)]\tLoss Ss: 0.028812\n","\tEpoch:07 [001/005 (0280/0693)]\tLoss Ss: 0.027475\n","\tEpoch:07 [001/005 (0300/0693)]\tLoss Ss: 0.017136\n","\tEpoch:07 [001/005 (0320/0693)]\tLoss Ss: 0.020570\n","\tEpoch:07 [001/005 (0340/0693)]\tLoss Ss: 0.027453\n","\tEpoch:07 [001/005 (0360/0693)]\tLoss Ss: 0.030415\n","\tEpoch:07 [001/005 (0380/0693)]\tLoss Ss: 0.014372\n","\tEpoch:07 [001/005 (0400/0693)]\tLoss Ss: 0.021072\n","\tEpoch:07 [001/005 (0420/0693)]\tLoss Ss: 0.023398\n","\tEpoch:07 [001/005 (0440/0693)]\tLoss Ss: 0.020022\n","\tEpoch:07 [001/005 (0460/0693)]\tLoss Ss: 0.016940\n","\tEpoch:07 [001/005 (0480/0693)]\tLoss Ss: 0.018414\n","\tEpoch:07 [001/005 (0500/0693)]\tLoss Ss: 0.011845\n","\tEpoch:07 [001/005 (0520/0693)]\tLoss Ss: 0.019090\n","\tEpoch:07 [001/005 (0540/0693)]\tLoss Ss: 0.019264\n","\tEpoch:07 [001/005 (0560/0693)]\tLoss Ss: 0.016772\n","\tEpoch:07 [001/005 (0580/0693)]\tLoss Ss: 0.014509\n","\tEpoch:07 [001/005 (0600/0693)]\tLoss Ss: 0.014598\n","\tEpoch:07 [001/005 (0620/0693)]\tLoss Ss: 0.020370\n","\tEpoch:07 [001/005 (0640/0693)]\tLoss Ss: 0.021895\n","\tEpoch:07 [001/005 (0660/0693)]\tLoss Ss: 0.014843\n","\tEpoch:07 [001/005 (0680/0693)]\tLoss Ss: 0.020530\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tEpoch:07 [002/005 (0000/0588)]\tLoss Ss: 0.016431\n","\tEpoch:07 [002/005 (0020/0588)]\tLoss Ss: 0.010943\n","\tEpoch:07 [002/005 (0040/0588)]\tLoss Ss: 0.011801\n","\tEpoch:07 [002/005 (0060/0588)]\tLoss Ss: 0.020543\n","\tEpoch:07 [002/005 (0080/0588)]\tLoss Ss: 0.015154\n","\tEpoch:07 [002/005 (0100/0588)]\tLoss Ss: 0.013885\n","\tEpoch:07 [002/005 (0120/0588)]\tLoss Ss: 0.013574\n","\tEpoch:07 [002/005 (0140/0588)]\tLoss Ss: 0.011931\n","\tEpoch:07 [002/005 (0160/0588)]\tLoss Ss: 0.013095\n","\tEpoch:07 [002/005 (0180/0588)]\tLoss Ss: 0.009544\n","\tEpoch:07 [002/005 (0200/0588)]\tLoss Ss: 0.009010\n","\tEpoch:07 [002/005 (0220/0588)]\tLoss Ss: 0.012937\n","\tEpoch:07 [002/005 (0240/0588)]\tLoss Ss: 0.010900\n","\tEpoch:07 [002/005 (0260/0588)]\tLoss Ss: 0.011018\n","\tEpoch:07 [002/005 (0280/0588)]\tLoss Ss: 0.011012\n","\tEpoch:07 [002/005 (0300/0588)]\tLoss Ss: 0.008829\n","\tEpoch:07 [002/005 (0320/0588)]\tLoss Ss: 0.009635\n","\tEpoch:07 [002/005 (0340/0588)]\tLoss Ss: 0.009146\n","\tEpoch:07 [002/005 (0360/0588)]\tLoss Ss: 0.008393\n","\tEpoch:07 [002/005 (0380/0588)]\tLoss Ss: 0.012057\n","\tEpoch:07 [002/005 (0400/0588)]\tLoss Ss: 0.006793\n","\tEpoch:07 [002/005 (0420/0588)]\tLoss Ss: 0.007197\n","\tEpoch:07 [002/005 (0440/0588)]\tLoss Ss: 0.008976\n","\tEpoch:07 [002/005 (0460/0588)]\tLoss Ss: 0.008250\n","\tEpoch:07 [002/005 (0480/0588)]\tLoss Ss: 0.010309\n","\tEpoch:07 [002/005 (0500/0588)]\tLoss Ss: 0.007851\n","\tEpoch:07 [002/005 (0520/0588)]\tLoss Ss: 0.008075\n","\tEpoch:07 [002/005 (0540/0588)]\tLoss Ss: 0.008443\n","\tEpoch:07 [002/005 (0560/0588)]\tLoss Ss: 0.006926\n","\tEpoch:07 [002/005 (0580/0588)]\tLoss Ss: 0.005714\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tEpoch:07 [003/005 (0000/0693)]\tLoss Ss: 0.022082\n","\tEpoch:07 [003/005 (0020/0693)]\tLoss Ss: 0.017730\n","\tEpoch:07 [003/005 (0040/0693)]\tLoss Ss: 0.018308\n","\tEpoch:07 [003/005 (0060/0693)]\tLoss Ss: 0.017871\n","\tEpoch:07 [003/005 (0080/0693)]\tLoss Ss: 0.016062\n","\tEpoch:07 [003/005 (0100/0693)]\tLoss Ss: 0.022574\n","\tEpoch:07 [003/005 (0120/0693)]\tLoss Ss: 0.017471\n","\tEpoch:07 [003/005 (0140/0693)]\tLoss Ss: 0.019086\n","\tEpoch:07 [003/005 (0160/0693)]\tLoss Ss: 0.019907\n","\tEpoch:07 [003/005 (0180/0693)]\tLoss Ss: 0.016787\n","\tEpoch:07 [003/005 (0200/0693)]\tLoss Ss: 0.018027\n","\tEpoch:07 [003/005 (0220/0693)]\tLoss Ss: 0.019646\n","\tEpoch:07 [003/005 (0240/0693)]\tLoss Ss: 0.015093\n","\tEpoch:07 [003/005 (0260/0693)]\tLoss Ss: 0.012041\n","\tEpoch:07 [003/005 (0280/0693)]\tLoss Ss: 0.013460\n","\tEpoch:07 [003/005 (0300/0693)]\tLoss Ss: 0.017404\n","\tEpoch:07 [003/005 (0320/0693)]\tLoss Ss: 0.026458\n","\tEpoch:07 [003/005 (0340/0693)]\tLoss Ss: 0.017764\n","\tEpoch:07 [003/005 (0360/0693)]\tLoss Ss: 0.020567\n","\tEpoch:07 [003/005 (0380/0693)]\tLoss Ss: 0.018310\n","\tEpoch:07 [003/005 (0400/0693)]\tLoss Ss: 0.025689\n","\tEpoch:07 [003/005 (0420/0693)]\tLoss Ss: 0.017621\n","\tEpoch:07 [003/005 (0440/0693)]\tLoss Ss: 0.014855\n","\tEpoch:07 [003/005 (0460/0693)]\tLoss Ss: 0.010702\n","\tEpoch:07 [003/005 (0480/0693)]\tLoss Ss: 0.017747\n","\tEpoch:07 [003/005 (0500/0693)]\tLoss Ss: 0.015160\n","\tEpoch:07 [003/005 (0520/0693)]\tLoss Ss: 0.019778\n","\tEpoch:07 [003/005 (0540/0693)]\tLoss Ss: 0.015811\n","\tEpoch:07 [003/005 (0560/0693)]\tLoss Ss: 0.013468\n","\tEpoch:07 [003/005 (0580/0693)]\tLoss Ss: 0.019219\n","\tEpoch:07 [003/005 (0600/0693)]\tLoss Ss: 0.015444\n","\tEpoch:07 [003/005 (0620/0693)]\tLoss Ss: 0.015598\n","\tEpoch:07 [003/005 (0640/0693)]\tLoss Ss: 0.013094\n","\tEpoch:07 [003/005 (0660/0693)]\tLoss Ss: 0.014682\n","\tEpoch:07 [003/005 (0680/0693)]\tLoss Ss: 0.017572\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:07 [004/005 (0000/0755)]\tLoss Ss: 0.034612\n","\tEpoch:07 [004/005 (0020/0755)]\tLoss Ss: 0.045249\n","\tEpoch:07 [004/005 (0040/0755)]\tLoss Ss: 0.038376\n","\tEpoch:07 [004/005 (0060/0755)]\tLoss Ss: 0.030869\n","\tEpoch:07 [004/005 (0080/0755)]\tLoss Ss: 0.037243\n","\tEpoch:07 [004/005 (0100/0755)]\tLoss Ss: 0.032391\n","\tEpoch:07 [004/005 (0120/0755)]\tLoss Ss: 0.028454\n","\tEpoch:07 [004/005 (0140/0755)]\tLoss Ss: 0.028963\n","\tEpoch:07 [004/005 (0160/0755)]\tLoss Ss: 0.038266\n","\tEpoch:07 [004/005 (0180/0755)]\tLoss Ss: 0.023520\n","\tEpoch:07 [004/005 (0200/0755)]\tLoss Ss: 0.018623\n","\tEpoch:07 [004/005 (0220/0755)]\tLoss Ss: 0.022875\n","\tEpoch:07 [004/005 (0240/0755)]\tLoss Ss: 0.021058\n","\tEpoch:07 [004/005 (0260/0755)]\tLoss Ss: 0.028957\n","\tEpoch:07 [004/005 (0280/0755)]\tLoss Ss: 0.027176\n","\tEpoch:07 [004/005 (0300/0755)]\tLoss Ss: 0.033201\n","\tEpoch:07 [004/005 (0320/0755)]\tLoss Ss: 0.025022\n","\tEpoch:07 [004/005 (0340/0755)]\tLoss Ss: 0.021430\n","\tEpoch:07 [004/005 (0360/0755)]\tLoss Ss: 0.019797\n","\tEpoch:07 [004/005 (0380/0755)]\tLoss Ss: 0.026929\n","\tEpoch:07 [004/005 (0400/0755)]\tLoss Ss: 0.018404\n","\tEpoch:07 [004/005 (0420/0755)]\tLoss Ss: 0.022500\n","\tEpoch:07 [004/005 (0440/0755)]\tLoss Ss: 0.032678\n","\tEpoch:07 [004/005 (0460/0755)]\tLoss Ss: 0.014367\n","\tEpoch:07 [004/005 (0480/0755)]\tLoss Ss: 0.025638\n","\tEpoch:07 [004/005 (0500/0755)]\tLoss Ss: 0.025462\n","\tEpoch:07 [004/005 (0520/0755)]\tLoss Ss: 0.017898\n","\tEpoch:07 [004/005 (0540/0755)]\tLoss Ss: 0.062571\n","\tEpoch:07 [004/005 (0560/0755)]\tLoss Ss: 0.021855\n","\tEpoch:07 [004/005 (0580/0755)]\tLoss Ss: 0.022403\n","\tEpoch:07 [004/005 (0600/0755)]\tLoss Ss: 0.017097\n","\tEpoch:07 [004/005 (0620/0755)]\tLoss Ss: 0.020839\n","\tEpoch:07 [004/005 (0640/0755)]\tLoss Ss: 0.023235\n","\tEpoch:07 [004/005 (0660/0755)]\tLoss Ss: 0.024528\n","\tEpoch:07 [004/005 (0680/0755)]\tLoss Ss: 0.015492\n","\tEpoch:07 [004/005 (0700/0755)]\tLoss Ss: 0.024989\n","\tEpoch:07 [004/005 (0720/0755)]\tLoss Ss: 0.026553\n","\tEpoch:07 [004/005 (0740/0755)]\tLoss Ss: 0.019266\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:07 [005/005 (0000/0755)]\tLoss Ss: 0.027883\n","\tEpoch:07 [005/005 (0020/0755)]\tLoss Ss: 0.024823\n","\tEpoch:07 [005/005 (0040/0755)]\tLoss Ss: 0.024459\n","\tEpoch:07 [005/005 (0060/0755)]\tLoss Ss: 0.018696\n","\tEpoch:07 [005/005 (0080/0755)]\tLoss Ss: 0.016408\n","\tEpoch:07 [005/005 (0100/0755)]\tLoss Ss: 0.027859\n","\tEpoch:07 [005/005 (0120/0755)]\tLoss Ss: 0.024139\n","\tEpoch:07 [005/005 (0140/0755)]\tLoss Ss: 0.015530\n","\tEpoch:07 [005/005 (0160/0755)]\tLoss Ss: 0.023593\n","\tEpoch:07 [005/005 (0180/0755)]\tLoss Ss: 0.024018\n","\tEpoch:07 [005/005 (0200/0755)]\tLoss Ss: 0.021897\n","\tEpoch:07 [005/005 (0220/0755)]\tLoss Ss: 0.018021\n","\tEpoch:07 [005/005 (0240/0755)]\tLoss Ss: 0.017874\n","\tEpoch:07 [005/005 (0260/0755)]\tLoss Ss: 0.012120\n","\tEpoch:07 [005/005 (0280/0755)]\tLoss Ss: 0.028301\n","\tEpoch:07 [005/005 (0300/0755)]\tLoss Ss: 0.019714\n","\tEpoch:07 [005/005 (0320/0755)]\tLoss Ss: 0.018396\n","\tEpoch:07 [005/005 (0340/0755)]\tLoss Ss: 0.016501\n","\tEpoch:07 [005/005 (0360/0755)]\tLoss Ss: 0.019745\n","\tEpoch:07 [005/005 (0380/0755)]\tLoss Ss: 0.022373\n","\tEpoch:07 [005/005 (0400/0755)]\tLoss Ss: 0.015480\n","\tEpoch:07 [005/005 (0420/0755)]\tLoss Ss: 0.020119\n","\tEpoch:07 [005/005 (0440/0755)]\tLoss Ss: 0.021170\n","\tEpoch:07 [005/005 (0460/0755)]\tLoss Ss: 0.019383\n","\tEpoch:07 [005/005 (0480/0755)]\tLoss Ss: 0.013286\n","\tEpoch:07 [005/005 (0500/0755)]\tLoss Ss: 0.011613\n","\tEpoch:07 [005/005 (0520/0755)]\tLoss Ss: 0.015599\n","\tEpoch:07 [005/005 (0540/0755)]\tLoss Ss: 0.011930\n","\tEpoch:07 [005/005 (0560/0755)]\tLoss Ss: 0.014311\n","\tEpoch:07 [005/005 (0580/0755)]\tLoss Ss: 0.016360\n","\tEpoch:07 [005/005 (0600/0755)]\tLoss Ss: 0.022282\n","\tEpoch:07 [005/005 (0620/0755)]\tLoss Ss: 0.013082\n","\tEpoch:07 [005/005 (0640/0755)]\tLoss Ss: 0.021404\n","\tEpoch:07 [005/005 (0660/0755)]\tLoss Ss: 0.019910\n","\tEpoch:07 [005/005 (0680/0755)]\tLoss Ss: 0.013962\n","\tEpoch:07 [005/005 (0700/0755)]\tLoss Ss: 0.022376\n","\tEpoch:07 [005/005 (0720/0755)]\tLoss Ss: 0.016815\n","\tEpoch:07 [005/005 (0740/0755)]\tLoss Ss: 0.015778\n","Now train the rotated image\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tRotated_Epoch:07 [000/005 (0000/0693)]\tLoss Ss: 0.022709\n","\tRotated_Epoch:07 [000/005 (0020/0693)]\tLoss Ss: 0.020156\n","\tRotated_Epoch:07 [000/005 (0040/0693)]\tLoss Ss: 0.035422\n","\tRotated_Epoch:07 [000/005 (0060/0693)]\tLoss Ss: 0.031343\n","\tRotated_Epoch:07 [000/005 (0080/0693)]\tLoss Ss: 0.022108\n","\tRotated_Epoch:07 [000/005 (0100/0693)]\tLoss Ss: 0.024564\n","\tRotated_Epoch:07 [000/005 (0120/0693)]\tLoss Ss: 0.013319\n","\tRotated_Epoch:07 [000/005 (0140/0693)]\tLoss Ss: 0.015152\n","\tRotated_Epoch:07 [000/005 (0160/0693)]\tLoss Ss: 0.020609\n","\tRotated_Epoch:07 [000/005 (0180/0693)]\tLoss Ss: 0.019495\n","\tRotated_Epoch:07 [000/005 (0200/0693)]\tLoss Ss: 0.016868\n","\tRotated_Epoch:07 [000/005 (0220/0693)]\tLoss Ss: 0.019544\n","\tRotated_Epoch:07 [000/005 (0240/0693)]\tLoss Ss: 0.018783\n","\tRotated_Epoch:07 [000/005 (0260/0693)]\tLoss Ss: 0.022407\n","\tRotated_Epoch:07 [000/005 (0280/0693)]\tLoss Ss: 0.013528\n","\tRotated_Epoch:07 [000/005 (0300/0693)]\tLoss Ss: 0.014691\n","\tRotated_Epoch:07 [000/005 (0320/0693)]\tLoss Ss: 0.019778\n","\tRotated_Epoch:07 [000/005 (0340/0693)]\tLoss Ss: 0.019657\n","\tRotated_Epoch:07 [000/005 (0360/0693)]\tLoss Ss: 0.022766\n","\tRotated_Epoch:07 [000/005 (0380/0693)]\tLoss Ss: 0.012679\n","\tRotated_Epoch:07 [000/005 (0400/0693)]\tLoss Ss: 0.019272\n","\tRotated_Epoch:07 [000/005 (0420/0693)]\tLoss Ss: 0.012998\n","\tRotated_Epoch:07 [000/005 (0440/0693)]\tLoss Ss: 0.021355\n","\tRotated_Epoch:07 [000/005 (0460/0693)]\tLoss Ss: 0.018794\n","\tRotated_Epoch:07 [000/005 (0480/0693)]\tLoss Ss: 0.013596\n","\tRotated_Epoch:07 [000/005 (0500/0693)]\tLoss Ss: 0.018649\n","\tRotated_Epoch:07 [000/005 (0520/0693)]\tLoss Ss: 0.014754\n","\tRotated_Epoch:07 [000/005 (0540/0693)]\tLoss Ss: 0.019072\n","\tRotated_Epoch:07 [000/005 (0560/0693)]\tLoss Ss: 0.019572\n","\tRotated_Epoch:07 [000/005 (0580/0693)]\tLoss Ss: 0.012564\n","\tRotated_Epoch:07 [000/005 (0600/0693)]\tLoss Ss: 0.019717\n","\tRotated_Epoch:07 [000/005 (0620/0693)]\tLoss Ss: 0.021701\n","\tRotated_Epoch:07 [000/005 (0640/0693)]\tLoss Ss: 0.021603\n","\tRotated_Epoch:07 [000/005 (0660/0693)]\tLoss Ss: 0.021951\n","\tRotated_Epoch:07 [000/005 (0680/0693)]\tLoss Ss: 0.016100\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:07 [001/005 (0000/0755)]\tLoss Ss: 0.023672\n","\tRotated_Epoch:07 [001/005 (0020/0755)]\tLoss Ss: 0.021424\n","\tRotated_Epoch:07 [001/005 (0040/0755)]\tLoss Ss: 0.020229\n","\tRotated_Epoch:07 [001/005 (0060/0755)]\tLoss Ss: 0.024118\n","\tRotated_Epoch:07 [001/005 (0080/0755)]\tLoss Ss: 0.018664\n","\tRotated_Epoch:07 [001/005 (0100/0755)]\tLoss Ss: 0.016577\n","\tRotated_Epoch:07 [001/005 (0120/0755)]\tLoss Ss: 0.020242\n","\tRotated_Epoch:07 [001/005 (0140/0755)]\tLoss Ss: 0.018389\n","\tRotated_Epoch:07 [001/005 (0160/0755)]\tLoss Ss: 0.017539\n","\tRotated_Epoch:07 [001/005 (0180/0755)]\tLoss Ss: 0.030023\n","\tRotated_Epoch:07 [001/005 (0200/0755)]\tLoss Ss: 0.023312\n","\tRotated_Epoch:07 [001/005 (0220/0755)]\tLoss Ss: 0.022677\n","\tRotated_Epoch:07 [001/005 (0240/0755)]\tLoss Ss: 0.012925\n","\tRotated_Epoch:07 [001/005 (0260/0755)]\tLoss Ss: 0.026136\n","\tRotated_Epoch:07 [001/005 (0280/0755)]\tLoss Ss: 0.026118\n","\tRotated_Epoch:07 [001/005 (0300/0755)]\tLoss Ss: 0.017798\n","\tRotated_Epoch:07 [001/005 (0320/0755)]\tLoss Ss: 0.024890\n","\tRotated_Epoch:07 [001/005 (0340/0755)]\tLoss Ss: 0.031964\n","\tRotated_Epoch:07 [001/005 (0360/0755)]\tLoss Ss: 0.016944\n","\tRotated_Epoch:07 [001/005 (0380/0755)]\tLoss Ss: 0.011676\n","\tRotated_Epoch:07 [001/005 (0400/0755)]\tLoss Ss: 0.017404\n","\tRotated_Epoch:07 [001/005 (0420/0755)]\tLoss Ss: 0.016716\n","\tRotated_Epoch:07 [001/005 (0440/0755)]\tLoss Ss: 0.018535\n","\tRotated_Epoch:07 [001/005 (0460/0755)]\tLoss Ss: 0.012293\n","\tRotated_Epoch:07 [001/005 (0480/0755)]\tLoss Ss: 0.012113\n","\tRotated_Epoch:07 [001/005 (0500/0755)]\tLoss Ss: 0.023474\n","\tRotated_Epoch:07 [001/005 (0520/0755)]\tLoss Ss: 0.021390\n","\tRotated_Epoch:07 [001/005 (0540/0755)]\tLoss Ss: 0.019029\n","\tRotated_Epoch:07 [001/005 (0560/0755)]\tLoss Ss: 0.019183\n","\tRotated_Epoch:07 [001/005 (0580/0755)]\tLoss Ss: 0.016293\n","\tRotated_Epoch:07 [001/005 (0600/0755)]\tLoss Ss: 0.014000\n","\tRotated_Epoch:07 [001/005 (0620/0755)]\tLoss Ss: 0.023504\n","\tRotated_Epoch:07 [001/005 (0640/0755)]\tLoss Ss: 0.017027\n","\tRotated_Epoch:07 [001/005 (0660/0755)]\tLoss Ss: 0.025234\n","\tRotated_Epoch:07 [001/005 (0680/0755)]\tLoss Ss: 0.019156\n","\tRotated_Epoch:07 [001/005 (0700/0755)]\tLoss Ss: 0.023248\n","\tRotated_Epoch:07 [001/005 (0720/0755)]\tLoss Ss: 0.020460\n","\tRotated_Epoch:07 [001/005 (0740/0755)]\tLoss Ss: 0.032651\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tRotated_Epoch:07 [002/005 (0000/0588)]\tLoss Ss: 0.170376\n","\tRotated_Epoch:07 [002/005 (0020/0588)]\tLoss Ss: 0.151005\n","\tRotated_Epoch:07 [002/005 (0040/0588)]\tLoss Ss: 0.152749\n","\tRotated_Epoch:07 [002/005 (0060/0588)]\tLoss Ss: 0.129526\n","\tRotated_Epoch:07 [002/005 (0080/0588)]\tLoss Ss: 0.124202\n","\tRotated_Epoch:07 [002/005 (0100/0588)]\tLoss Ss: 0.163924\n","\tRotated_Epoch:07 [002/005 (0120/0588)]\tLoss Ss: 0.144756\n","\tRotated_Epoch:07 [002/005 (0140/0588)]\tLoss Ss: 0.088096\n","\tRotated_Epoch:07 [002/005 (0160/0588)]\tLoss Ss: 0.096973\n","\tRotated_Epoch:07 [002/005 (0180/0588)]\tLoss Ss: 0.093449\n","\tRotated_Epoch:07 [002/005 (0200/0588)]\tLoss Ss: 0.072754\n","\tRotated_Epoch:07 [002/005 (0220/0588)]\tLoss Ss: 0.109879\n","\tRotated_Epoch:07 [002/005 (0240/0588)]\tLoss Ss: 0.102933\n","\tRotated_Epoch:07 [002/005 (0260/0588)]\tLoss Ss: 0.101170\n","\tRotated_Epoch:07 [002/005 (0280/0588)]\tLoss Ss: 0.071962\n","\tRotated_Epoch:07 [002/005 (0300/0588)]\tLoss Ss: 0.083612\n","\tRotated_Epoch:07 [002/005 (0320/0588)]\tLoss Ss: 0.075827\n","\tRotated_Epoch:07 [002/005 (0340/0588)]\tLoss Ss: 0.096848\n","\tRotated_Epoch:07 [002/005 (0360/0588)]\tLoss Ss: 0.096424\n","\tRotated_Epoch:07 [002/005 (0380/0588)]\tLoss Ss: 0.111141\n","\tRotated_Epoch:07 [002/005 (0400/0588)]\tLoss Ss: 0.090111\n","\tRotated_Epoch:07 [002/005 (0420/0588)]\tLoss Ss: 0.100729\n","\tRotated_Epoch:07 [002/005 (0440/0588)]\tLoss Ss: 0.089712\n","\tRotated_Epoch:07 [002/005 (0460/0588)]\tLoss Ss: 0.082716\n","\tRotated_Epoch:07 [002/005 (0480/0588)]\tLoss Ss: 0.078003\n","\tRotated_Epoch:07 [002/005 (0500/0588)]\tLoss Ss: 0.085520\n","\tRotated_Epoch:07 [002/005 (0520/0588)]\tLoss Ss: 0.101692\n","\tRotated_Epoch:07 [002/005 (0540/0588)]\tLoss Ss: 0.073960\n","\tRotated_Epoch:07 [002/005 (0560/0588)]\tLoss Ss: 0.091508\n","\tRotated_Epoch:07 [002/005 (0580/0588)]\tLoss Ss: 0.070427\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:07 [003/005 (0000/0755)]\tLoss Ss: 0.171974\n","\tRotated_Epoch:07 [003/005 (0020/0755)]\tLoss Ss: 0.195811\n","\tRotated_Epoch:07 [003/005 (0040/0755)]\tLoss Ss: 0.162949\n","\tRotated_Epoch:07 [003/005 (0060/0755)]\tLoss Ss: 0.203790\n","\tRotated_Epoch:07 [003/005 (0080/0755)]\tLoss Ss: 0.156614\n","\tRotated_Epoch:07 [003/005 (0100/0755)]\tLoss Ss: 0.147344\n","\tRotated_Epoch:07 [003/005 (0120/0755)]\tLoss Ss: 0.163038\n","\tRotated_Epoch:07 [003/005 (0140/0755)]\tLoss Ss: 0.148854\n","\tRotated_Epoch:07 [003/005 (0160/0755)]\tLoss Ss: 0.120344\n","\tRotated_Epoch:07 [003/005 (0180/0755)]\tLoss Ss: 0.103346\n","\tRotated_Epoch:07 [003/005 (0200/0755)]\tLoss Ss: 0.133555\n","\tRotated_Epoch:07 [003/005 (0220/0755)]\tLoss Ss: 0.119901\n","\tRotated_Epoch:07 [003/005 (0240/0755)]\tLoss Ss: 0.146871\n","\tRotated_Epoch:07 [003/005 (0260/0755)]\tLoss Ss: 0.112236\n","\tRotated_Epoch:07 [003/005 (0280/0755)]\tLoss Ss: 0.104947\n","\tRotated_Epoch:07 [003/005 (0300/0755)]\tLoss Ss: 0.078433\n","\tRotated_Epoch:07 [003/005 (0320/0755)]\tLoss Ss: 0.130172\n","\tRotated_Epoch:07 [003/005 (0340/0755)]\tLoss Ss: 0.099695\n","\tRotated_Epoch:07 [003/005 (0360/0755)]\tLoss Ss: 0.148989\n","\tRotated_Epoch:07 [003/005 (0380/0755)]\tLoss Ss: 0.099836\n","\tRotated_Epoch:07 [003/005 (0400/0755)]\tLoss Ss: 0.096484\n","\tRotated_Epoch:07 [003/005 (0420/0755)]\tLoss Ss: 0.083533\n","\tRotated_Epoch:07 [003/005 (0440/0755)]\tLoss Ss: 0.101119\n","\tRotated_Epoch:07 [003/005 (0460/0755)]\tLoss Ss: 0.106204\n","\tRotated_Epoch:07 [003/005 (0480/0755)]\tLoss Ss: 0.076608\n","\tRotated_Epoch:07 [003/005 (0500/0755)]\tLoss Ss: 0.101984\n","\tRotated_Epoch:07 [003/005 (0520/0755)]\tLoss Ss: 0.077076\n","\tRotated_Epoch:07 [003/005 (0540/0755)]\tLoss Ss: 0.102261\n","\tRotated_Epoch:07 [003/005 (0560/0755)]\tLoss Ss: 0.116867\n","\tRotated_Epoch:07 [003/005 (0580/0755)]\tLoss Ss: 0.094152\n","\tRotated_Epoch:07 [003/005 (0600/0755)]\tLoss Ss: 0.081404\n","\tRotated_Epoch:07 [003/005 (0620/0755)]\tLoss Ss: 0.084323\n","\tRotated_Epoch:07 [003/005 (0640/0755)]\tLoss Ss: 0.076898\n","\tRotated_Epoch:07 [003/005 (0660/0755)]\tLoss Ss: 0.079346\n","\tRotated_Epoch:07 [003/005 (0680/0755)]\tLoss Ss: 0.080244\n","\tRotated_Epoch:07 [003/005 (0700/0755)]\tLoss Ss: 0.090017\n","\tRotated_Epoch:07 [003/005 (0720/0755)]\tLoss Ss: 0.055488\n","\tRotated_Epoch:07 [003/005 (0740/0755)]\tLoss Ss: 0.079300\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tRotated_Epoch:07 [004/005 (0000/0614)]\tLoss Ss: 0.121102\n","\tRotated_Epoch:07 [004/005 (0020/0614)]\tLoss Ss: 0.100863\n","\tRotated_Epoch:07 [004/005 (0040/0614)]\tLoss Ss: 0.116236\n","\tRotated_Epoch:07 [004/005 (0060/0614)]\tLoss Ss: 0.066283\n","\tRotated_Epoch:07 [004/005 (0080/0614)]\tLoss Ss: 0.044325\n","\tRotated_Epoch:07 [004/005 (0100/0614)]\tLoss Ss: 0.029322\n","\tRotated_Epoch:07 [004/005 (0120/0614)]\tLoss Ss: 0.027203\n","\tRotated_Epoch:07 [004/005 (0140/0614)]\tLoss Ss: 0.022568\n","\tRotated_Epoch:07 [004/005 (0160/0614)]\tLoss Ss: 0.021378\n","\tRotated_Epoch:07 [004/005 (0180/0614)]\tLoss Ss: 0.019498\n","\tRotated_Epoch:07 [004/005 (0200/0614)]\tLoss Ss: 0.029554\n","\tRotated_Epoch:07 [004/005 (0220/0614)]\tLoss Ss: 0.033171\n","\tRotated_Epoch:07 [004/005 (0240/0614)]\tLoss Ss: 0.023431\n","\tRotated_Epoch:07 [004/005 (0260/0614)]\tLoss Ss: 0.027965\n","\tRotated_Epoch:07 [004/005 (0280/0614)]\tLoss Ss: 0.020004\n","\tRotated_Epoch:07 [004/005 (0300/0614)]\tLoss Ss: 0.021039\n","\tRotated_Epoch:07 [004/005 (0320/0614)]\tLoss Ss: 0.014583\n","\tRotated_Epoch:07 [004/005 (0340/0614)]\tLoss Ss: 0.025831\n","\tRotated_Epoch:07 [004/005 (0360/0614)]\tLoss Ss: 0.020349\n","\tRotated_Epoch:07 [004/005 (0380/0614)]\tLoss Ss: 0.024384\n","\tRotated_Epoch:07 [004/005 (0400/0614)]\tLoss Ss: 0.019035\n","\tRotated_Epoch:07 [004/005 (0420/0614)]\tLoss Ss: 0.026433\n","\tRotated_Epoch:07 [004/005 (0440/0614)]\tLoss Ss: 0.018767\n","\tRotated_Epoch:07 [004/005 (0460/0614)]\tLoss Ss: 0.013497\n","\tRotated_Epoch:07 [004/005 (0480/0614)]\tLoss Ss: 0.014657\n","\tRotated_Epoch:07 [004/005 (0500/0614)]\tLoss Ss: 0.012100\n","\tRotated_Epoch:07 [004/005 (0520/0614)]\tLoss Ss: 0.019514\n","\tRotated_Epoch:07 [004/005 (0540/0614)]\tLoss Ss: 0.011192\n","\tRotated_Epoch:07 [004/005 (0560/0614)]\tLoss Ss: 0.015518\n","\tRotated_Epoch:07 [004/005 (0580/0614)]\tLoss Ss: 0.023565\n","\tRotated_Epoch:07 [004/005 (0600/0614)]\tLoss Ss: 0.011785\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tRotated_Epoch:07 [005/005 (0000/0693)]\tLoss Ss: 0.066245\n","\tRotated_Epoch:07 [005/005 (0020/0693)]\tLoss Ss: 0.106990\n","\tRotated_Epoch:07 [005/005 (0040/0693)]\tLoss Ss: 0.044537\n","\tRotated_Epoch:07 [005/005 (0060/0693)]\tLoss Ss: 0.052835\n","\tRotated_Epoch:07 [005/005 (0080/0693)]\tLoss Ss: 0.047871\n","\tRotated_Epoch:07 [005/005 (0100/0693)]\tLoss Ss: 0.055504\n","\tRotated_Epoch:07 [005/005 (0120/0693)]\tLoss Ss: 0.044050\n","\tRotated_Epoch:07 [005/005 (0140/0693)]\tLoss Ss: 0.063398\n","\tRotated_Epoch:07 [005/005 (0160/0693)]\tLoss Ss: 0.029929\n","\tRotated_Epoch:07 [005/005 (0180/0693)]\tLoss Ss: 0.047305\n","\tRotated_Epoch:07 [005/005 (0200/0693)]\tLoss Ss: 0.050799\n","\tRotated_Epoch:07 [005/005 (0220/0693)]\tLoss Ss: 0.031405\n","\tRotated_Epoch:07 [005/005 (0240/0693)]\tLoss Ss: 0.027861\n","\tRotated_Epoch:07 [005/005 (0260/0693)]\tLoss Ss: 0.031251\n","\tRotated_Epoch:07 [005/005 (0280/0693)]\tLoss Ss: 0.030089\n","\tRotated_Epoch:07 [005/005 (0300/0693)]\tLoss Ss: 0.054339\n","\tRotated_Epoch:07 [005/005 (0320/0693)]\tLoss Ss: 0.033316\n","\tRotated_Epoch:07 [005/005 (0340/0693)]\tLoss Ss: 0.031422\n","\tRotated_Epoch:07 [005/005 (0360/0693)]\tLoss Ss: 0.044060\n","\tRotated_Epoch:07 [005/005 (0380/0693)]\tLoss Ss: 0.027096\n","\tRotated_Epoch:07 [005/005 (0400/0693)]\tLoss Ss: 0.037261\n","\tRotated_Epoch:07 [005/005 (0420/0693)]\tLoss Ss: 0.021185\n","\tRotated_Epoch:07 [005/005 (0440/0693)]\tLoss Ss: 0.027590\n","\tRotated_Epoch:07 [005/005 (0460/0693)]\tLoss Ss: 0.028928\n","\tRotated_Epoch:07 [005/005 (0480/0693)]\tLoss Ss: 0.023766\n","\tRotated_Epoch:07 [005/005 (0500/0693)]\tLoss Ss: 0.024492\n","\tRotated_Epoch:07 [005/005 (0520/0693)]\tLoss Ss: 0.021619\n","\tRotated_Epoch:07 [005/005 (0540/0693)]\tLoss Ss: 0.027558\n","\tRotated_Epoch:07 [005/005 (0560/0693)]\tLoss Ss: 0.022181\n","\tRotated_Epoch:07 [005/005 (0580/0693)]\tLoss Ss: 0.031941\n","\tRotated_Epoch:07 [005/005 (0600/0693)]\tLoss Ss: 0.023858\n","\tRotated_Epoch:07 [005/005 (0620/0693)]\tLoss Ss: 0.017858\n","\tRotated_Epoch:07 [005/005 (0640/0693)]\tLoss Ss: 0.025451\n","\tRotated_Epoch:07 [005/005 (0660/0693)]\tLoss Ss: 0.024228\n","\tRotated_Epoch:07 [005/005 (0680/0693)]\tLoss Ss: 0.019887\n","new_1_6_d.nii\n","new_1_6_m.nii\n","new_1_7_d.nii\n","new_1_7_m.nii\n","new_2_5_d.nii\n","new_2_5_m.nii\n","new_2_6_d.nii\n","new_2_6_m.nii\n","new_2_7_d.nii\n","new_2_7_m.nii\n","new_3_d (2).nii\n","new_3_m (2).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (3).nii\n","new_3_m (3).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (7).nii\n","new_3_m (7).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (8).nii\n","new_3_m (8).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (9).nii\n","new_3_m (9).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_4_10_d_163526.nii\n","new_4_10_m_163526.nii.gz\n","new_4_11_d_084336.nii\n","new_4_11_m_084336.nii.gz\n","new_4_7_d_184422.nii\n","new_4_7_m_184422.nii.gz\n","new_4_8_d_143412.nii\n","new_4_8_m_143412.nii.gz\n","new_4_9_d_155347.nii\n","new_4_9_m_155347.nii.gz\n","\tEpoch: 7; Dice: 0.9500 +/- 0.0150; Loss: 15.4222\n","Begin Epoch 8\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tEpoch:08 [000/005 (0000/0588)]\tLoss Ss: 0.020802\n","\tEpoch:08 [000/005 (0020/0588)]\tLoss Ss: 0.025723\n","\tEpoch:08 [000/005 (0040/0588)]\tLoss Ss: 0.017383\n","\tEpoch:08 [000/005 (0060/0588)]\tLoss Ss: 0.021099\n","\tEpoch:08 [000/005 (0080/0588)]\tLoss Ss: 0.020347\n","\tEpoch:08 [000/005 (0100/0588)]\tLoss Ss: 0.013173\n","\tEpoch:08 [000/005 (0120/0588)]\tLoss Ss: 0.019282\n","\tEpoch:08 [000/005 (0140/0588)]\tLoss Ss: 0.018340\n","\tEpoch:08 [000/005 (0160/0588)]\tLoss Ss: 0.012623\n","\tEpoch:08 [000/005 (0180/0588)]\tLoss Ss: 0.010359\n","\tEpoch:08 [000/005 (0200/0588)]\tLoss Ss: 0.015937\n","\tEpoch:08 [000/005 (0220/0588)]\tLoss Ss: 0.010553\n","\tEpoch:08 [000/005 (0240/0588)]\tLoss Ss: 0.011648\n","\tEpoch:08 [000/005 (0260/0588)]\tLoss Ss: 0.007018\n","\tEpoch:08 [000/005 (0280/0588)]\tLoss Ss: 0.009927\n","\tEpoch:08 [000/005 (0300/0588)]\tLoss Ss: 0.012645\n","\tEpoch:08 [000/005 (0320/0588)]\tLoss Ss: 0.013382\n","\tEpoch:08 [000/005 (0340/0588)]\tLoss Ss: 0.013412\n","\tEpoch:08 [000/005 (0360/0588)]\tLoss Ss: 0.009697\n","\tEpoch:08 [000/005 (0380/0588)]\tLoss Ss: 0.015181\n","\tEpoch:08 [000/005 (0400/0588)]\tLoss Ss: 0.016871\n","\tEpoch:08 [000/005 (0420/0588)]\tLoss Ss: 0.016451\n","\tEpoch:08 [000/005 (0440/0588)]\tLoss Ss: 0.009007\n","\tEpoch:08 [000/005 (0460/0588)]\tLoss Ss: 0.015597\n","\tEpoch:08 [000/005 (0480/0588)]\tLoss Ss: 0.013560\n","\tEpoch:08 [000/005 (0500/0588)]\tLoss Ss: 0.011958\n","\tEpoch:08 [000/005 (0520/0588)]\tLoss Ss: 0.008478\n","\tEpoch:08 [000/005 (0540/0588)]\tLoss Ss: 0.007310\n","\tEpoch:08 [000/005 (0560/0588)]\tLoss Ss: 0.009510\n","\tEpoch:08 [000/005 (0580/0588)]\tLoss Ss: 0.008463\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tEpoch:08 [001/005 (0000/0614)]\tLoss Ss: 0.010769\n","\tEpoch:08 [001/005 (0020/0614)]\tLoss Ss: 0.013788\n","\tEpoch:08 [001/005 (0040/0614)]\tLoss Ss: 0.012084\n","\tEpoch:08 [001/005 (0060/0614)]\tLoss Ss: 0.016460\n","\tEpoch:08 [001/005 (0080/0614)]\tLoss Ss: 0.008980\n","\tEpoch:08 [001/005 (0100/0614)]\tLoss Ss: 0.012158\n","\tEpoch:08 [001/005 (0120/0614)]\tLoss Ss: 0.015460\n","\tEpoch:08 [001/005 (0140/0614)]\tLoss Ss: 0.007650\n","\tEpoch:08 [001/005 (0160/0614)]\tLoss Ss: 0.011733\n","\tEpoch:08 [001/005 (0180/0614)]\tLoss Ss: 0.008959\n","\tEpoch:08 [001/005 (0200/0614)]\tLoss Ss: 0.008745\n","\tEpoch:08 [001/005 (0220/0614)]\tLoss Ss: 0.010838\n","\tEpoch:08 [001/005 (0240/0614)]\tLoss Ss: 0.010441\n","\tEpoch:08 [001/005 (0260/0614)]\tLoss Ss: 0.009569\n","\tEpoch:08 [001/005 (0280/0614)]\tLoss Ss: 0.013195\n","\tEpoch:08 [001/005 (0300/0614)]\tLoss Ss: 0.014477\n","\tEpoch:08 [001/005 (0320/0614)]\tLoss Ss: 0.008206\n","\tEpoch:08 [001/005 (0340/0614)]\tLoss Ss: 0.011160\n","\tEpoch:08 [001/005 (0360/0614)]\tLoss Ss: 0.010336\n","\tEpoch:08 [001/005 (0380/0614)]\tLoss Ss: 0.010661\n","\tEpoch:08 [001/005 (0400/0614)]\tLoss Ss: 0.012430\n","\tEpoch:08 [001/005 (0420/0614)]\tLoss Ss: 0.012425\n","\tEpoch:08 [001/005 (0440/0614)]\tLoss Ss: 0.009222\n","\tEpoch:08 [001/005 (0460/0614)]\tLoss Ss: 0.006261\n","\tEpoch:08 [001/005 (0480/0614)]\tLoss Ss: 0.012670\n","\tEpoch:08 [001/005 (0500/0614)]\tLoss Ss: 0.009514\n","\tEpoch:08 [001/005 (0520/0614)]\tLoss Ss: 0.008379\n","\tEpoch:08 [001/005 (0540/0614)]\tLoss Ss: 0.014357\n","\tEpoch:08 [001/005 (0560/0614)]\tLoss Ss: 0.006229\n","\tEpoch:08 [001/005 (0580/0614)]\tLoss Ss: 0.009181\n","\tEpoch:08 [001/005 (0600/0614)]\tLoss Ss: 0.007627\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:08 [002/005 (0000/0755)]\tLoss Ss: 0.049086\n","\tEpoch:08 [002/005 (0020/0755)]\tLoss Ss: 0.044408\n","\tEpoch:08 [002/005 (0040/0755)]\tLoss Ss: 0.034342\n","\tEpoch:08 [002/005 (0060/0755)]\tLoss Ss: 0.037718\n","\tEpoch:08 [002/005 (0080/0755)]\tLoss Ss: 0.041604\n","\tEpoch:08 [002/005 (0100/0755)]\tLoss Ss: 0.028396\n","\tEpoch:08 [002/005 (0120/0755)]\tLoss Ss: 0.025606\n","\tEpoch:08 [002/005 (0140/0755)]\tLoss Ss: 0.036052\n","\tEpoch:08 [002/005 (0160/0755)]\tLoss Ss: 0.049060\n","\tEpoch:08 [002/005 (0180/0755)]\tLoss Ss: 0.023281\n","\tEpoch:08 [002/005 (0200/0755)]\tLoss Ss: 0.053319\n","\tEpoch:08 [002/005 (0220/0755)]\tLoss Ss: 0.035941\n","\tEpoch:08 [002/005 (0240/0755)]\tLoss Ss: 0.027721\n","\tEpoch:08 [002/005 (0260/0755)]\tLoss Ss: 0.028993\n","\tEpoch:08 [002/005 (0280/0755)]\tLoss Ss: 0.018576\n","\tEpoch:08 [002/005 (0300/0755)]\tLoss Ss: 0.022015\n","\tEpoch:08 [002/005 (0320/0755)]\tLoss Ss: 0.021415\n","\tEpoch:08 [002/005 (0340/0755)]\tLoss Ss: 0.039213\n","\tEpoch:08 [002/005 (0360/0755)]\tLoss Ss: 0.018393\n","\tEpoch:08 [002/005 (0380/0755)]\tLoss Ss: 0.032608\n","\tEpoch:08 [002/005 (0400/0755)]\tLoss Ss: 0.020121\n","\tEpoch:08 [002/005 (0420/0755)]\tLoss Ss: 0.021421\n","\tEpoch:08 [002/005 (0440/0755)]\tLoss Ss: 0.021755\n","\tEpoch:08 [002/005 (0460/0755)]\tLoss Ss: 0.023147\n","\tEpoch:08 [002/005 (0480/0755)]\tLoss Ss: 0.018016\n","\tEpoch:08 [002/005 (0500/0755)]\tLoss Ss: 0.026176\n","\tEpoch:08 [002/005 (0520/0755)]\tLoss Ss: 0.022248\n","\tEpoch:08 [002/005 (0540/0755)]\tLoss Ss: 0.020640\n","\tEpoch:08 [002/005 (0560/0755)]\tLoss Ss: 0.029042\n","\tEpoch:08 [002/005 (0580/0755)]\tLoss Ss: 0.019042\n","\tEpoch:08 [002/005 (0600/0755)]\tLoss Ss: 0.016826\n","\tEpoch:08 [002/005 (0620/0755)]\tLoss Ss: 0.019949\n","\tEpoch:08 [002/005 (0640/0755)]\tLoss Ss: 0.024816\n","\tEpoch:08 [002/005 (0660/0755)]\tLoss Ss: 0.022572\n","\tEpoch:08 [002/005 (0680/0755)]\tLoss Ss: 0.020592\n","\tEpoch:08 [002/005 (0700/0755)]\tLoss Ss: 0.020026\n","\tEpoch:08 [002/005 (0720/0755)]\tLoss Ss: 0.018534\n","\tEpoch:08 [002/005 (0740/0755)]\tLoss Ss: 0.022942\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tEpoch:08 [003/005 (0000/0693)]\tLoss Ss: 0.019031\n","\tEpoch:08 [003/005 (0020/0693)]\tLoss Ss: 0.027233\n","\tEpoch:08 [003/005 (0040/0693)]\tLoss Ss: 0.019814\n","\tEpoch:08 [003/005 (0060/0693)]\tLoss Ss: 0.025087\n","\tEpoch:08 [003/005 (0080/0693)]\tLoss Ss: 0.020153\n","\tEpoch:08 [003/005 (0100/0693)]\tLoss Ss: 0.025389\n","\tEpoch:08 [003/005 (0120/0693)]\tLoss Ss: 0.018150\n","\tEpoch:08 [003/005 (0140/0693)]\tLoss Ss: 0.015838\n","\tEpoch:08 [003/005 (0160/0693)]\tLoss Ss: 0.018944\n","\tEpoch:08 [003/005 (0180/0693)]\tLoss Ss: 0.022878\n","\tEpoch:08 [003/005 (0200/0693)]\tLoss Ss: 0.017556\n","\tEpoch:08 [003/005 (0220/0693)]\tLoss Ss: 0.018775\n","\tEpoch:08 [003/005 (0240/0693)]\tLoss Ss: 0.015103\n","\tEpoch:08 [003/005 (0260/0693)]\tLoss Ss: 0.015012\n","\tEpoch:08 [003/005 (0280/0693)]\tLoss Ss: 0.020285\n","\tEpoch:08 [003/005 (0300/0693)]\tLoss Ss: 0.019237\n","\tEpoch:08 [003/005 (0320/0693)]\tLoss Ss: 0.017450\n","\tEpoch:08 [003/005 (0340/0693)]\tLoss Ss: 0.012153\n","\tEpoch:08 [003/005 (0360/0693)]\tLoss Ss: 0.017500\n","\tEpoch:08 [003/005 (0380/0693)]\tLoss Ss: 0.017670\n","\tEpoch:08 [003/005 (0400/0693)]\tLoss Ss: 0.017885\n","\tEpoch:08 [003/005 (0420/0693)]\tLoss Ss: 0.023293\n","\tEpoch:08 [003/005 (0440/0693)]\tLoss Ss: 0.018312\n","\tEpoch:08 [003/005 (0460/0693)]\tLoss Ss: 0.020461\n","\tEpoch:08 [003/005 (0480/0693)]\tLoss Ss: 0.017790\n","\tEpoch:08 [003/005 (0500/0693)]\tLoss Ss: 0.016289\n","\tEpoch:08 [003/005 (0520/0693)]\tLoss Ss: 0.017671\n","\tEpoch:08 [003/005 (0540/0693)]\tLoss Ss: 0.017965\n","\tEpoch:08 [003/005 (0560/0693)]\tLoss Ss: 0.014618\n","\tEpoch:08 [003/005 (0580/0693)]\tLoss Ss: 0.025400\n","\tEpoch:08 [003/005 (0600/0693)]\tLoss Ss: 0.019193\n","\tEpoch:08 [003/005 (0620/0693)]\tLoss Ss: 0.016714\n","\tEpoch:08 [003/005 (0640/0693)]\tLoss Ss: 0.020087\n","\tEpoch:08 [003/005 (0660/0693)]\tLoss Ss: 0.018200\n","\tEpoch:08 [003/005 (0680/0693)]\tLoss Ss: 0.009882\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tEpoch:08 [004/005 (0000/0693)]\tLoss Ss: 0.023852\n","\tEpoch:08 [004/005 (0020/0693)]\tLoss Ss: 0.021404\n","\tEpoch:08 [004/005 (0040/0693)]\tLoss Ss: 0.022801\n","\tEpoch:08 [004/005 (0060/0693)]\tLoss Ss: 0.014480\n","\tEpoch:08 [004/005 (0080/0693)]\tLoss Ss: 0.019122\n","\tEpoch:08 [004/005 (0100/0693)]\tLoss Ss: 0.012742\n","\tEpoch:08 [004/005 (0120/0693)]\tLoss Ss: 0.016700\n","\tEpoch:08 [004/005 (0140/0693)]\tLoss Ss: 0.016685\n","\tEpoch:08 [004/005 (0160/0693)]\tLoss Ss: 0.022756\n","\tEpoch:08 [004/005 (0180/0693)]\tLoss Ss: 0.027883\n","\tEpoch:08 [004/005 (0200/0693)]\tLoss Ss: 0.015600\n","\tEpoch:08 [004/005 (0220/0693)]\tLoss Ss: 0.017398\n","\tEpoch:08 [004/005 (0240/0693)]\tLoss Ss: 0.020580\n","\tEpoch:08 [004/005 (0260/0693)]\tLoss Ss: 0.021754\n","\tEpoch:08 [004/005 (0280/0693)]\tLoss Ss: 0.013081\n","\tEpoch:08 [004/005 (0300/0693)]\tLoss Ss: 0.018035\n","\tEpoch:08 [004/005 (0320/0693)]\tLoss Ss: 0.013886\n","\tEpoch:08 [004/005 (0340/0693)]\tLoss Ss: 0.012609\n","\tEpoch:08 [004/005 (0360/0693)]\tLoss Ss: 0.015488\n","\tEpoch:08 [004/005 (0380/0693)]\tLoss Ss: 0.014174\n","\tEpoch:08 [004/005 (0400/0693)]\tLoss Ss: 0.016979\n","\tEpoch:08 [004/005 (0420/0693)]\tLoss Ss: 0.023443\n","\tEpoch:08 [004/005 (0440/0693)]\tLoss Ss: 0.020885\n","\tEpoch:08 [004/005 (0460/0693)]\tLoss Ss: 0.022800\n","\tEpoch:08 [004/005 (0480/0693)]\tLoss Ss: 0.012120\n","\tEpoch:08 [004/005 (0500/0693)]\tLoss Ss: 0.008051\n","\tEpoch:08 [004/005 (0520/0693)]\tLoss Ss: 0.013398\n","\tEpoch:08 [004/005 (0540/0693)]\tLoss Ss: 0.011987\n","\tEpoch:08 [004/005 (0560/0693)]\tLoss Ss: 0.015300\n","\tEpoch:08 [004/005 (0580/0693)]\tLoss Ss: 0.009794\n","\tEpoch:08 [004/005 (0600/0693)]\tLoss Ss: 0.016923\n","\tEpoch:08 [004/005 (0620/0693)]\tLoss Ss: 0.020276\n","\tEpoch:08 [004/005 (0640/0693)]\tLoss Ss: 0.012124\n","\tEpoch:08 [004/005 (0660/0693)]\tLoss Ss: 0.010990\n","\tEpoch:08 [004/005 (0680/0693)]\tLoss Ss: 0.013121\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:08 [005/005 (0000/0755)]\tLoss Ss: 0.018183\n","\tEpoch:08 [005/005 (0020/0755)]\tLoss Ss: 0.015830\n","\tEpoch:08 [005/005 (0040/0755)]\tLoss Ss: 0.016719\n","\tEpoch:08 [005/005 (0060/0755)]\tLoss Ss: 0.022573\n","\tEpoch:08 [005/005 (0080/0755)]\tLoss Ss: 0.016078\n","\tEpoch:08 [005/005 (0100/0755)]\tLoss Ss: 0.021370\n","\tEpoch:08 [005/005 (0120/0755)]\tLoss Ss: 0.018171\n","\tEpoch:08 [005/005 (0140/0755)]\tLoss Ss: 0.027487\n","\tEpoch:08 [005/005 (0160/0755)]\tLoss Ss: 0.017512\n","\tEpoch:08 [005/005 (0180/0755)]\tLoss Ss: 0.023402\n","\tEpoch:08 [005/005 (0200/0755)]\tLoss Ss: 0.015958\n","\tEpoch:08 [005/005 (0220/0755)]\tLoss Ss: 0.014187\n","\tEpoch:08 [005/005 (0240/0755)]\tLoss Ss: 0.015222\n","\tEpoch:08 [005/005 (0260/0755)]\tLoss Ss: 0.022057\n","\tEpoch:08 [005/005 (0280/0755)]\tLoss Ss: 0.015289\n","\tEpoch:08 [005/005 (0300/0755)]\tLoss Ss: 0.016328\n","\tEpoch:08 [005/005 (0320/0755)]\tLoss Ss: 0.019223\n","\tEpoch:08 [005/005 (0340/0755)]\tLoss Ss: 0.016073\n","\tEpoch:08 [005/005 (0360/0755)]\tLoss Ss: 0.036977\n","\tEpoch:08 [005/005 (0380/0755)]\tLoss Ss: 0.016872\n","\tEpoch:08 [005/005 (0400/0755)]\tLoss Ss: 0.015593\n","\tEpoch:08 [005/005 (0420/0755)]\tLoss Ss: 0.013034\n","\tEpoch:08 [005/005 (0440/0755)]\tLoss Ss: 0.016020\n","\tEpoch:08 [005/005 (0460/0755)]\tLoss Ss: 0.017367\n","\tEpoch:08 [005/005 (0480/0755)]\tLoss Ss: 0.008967\n","\tEpoch:08 [005/005 (0500/0755)]\tLoss Ss: 0.017208\n","\tEpoch:08 [005/005 (0520/0755)]\tLoss Ss: 0.012059\n","\tEpoch:08 [005/005 (0540/0755)]\tLoss Ss: 0.013125\n","\tEpoch:08 [005/005 (0560/0755)]\tLoss Ss: 0.019510\n","\tEpoch:08 [005/005 (0580/0755)]\tLoss Ss: 0.017099\n","\tEpoch:08 [005/005 (0600/0755)]\tLoss Ss: 0.018274\n","\tEpoch:08 [005/005 (0620/0755)]\tLoss Ss: 0.016885\n","\tEpoch:08 [005/005 (0640/0755)]\tLoss Ss: 0.015783\n","\tEpoch:08 [005/005 (0660/0755)]\tLoss Ss: 0.015401\n","\tEpoch:08 [005/005 (0680/0755)]\tLoss Ss: 0.016979\n","\tEpoch:08 [005/005 (0700/0755)]\tLoss Ss: 0.014135\n","\tEpoch:08 [005/005 (0720/0755)]\tLoss Ss: 0.018008\n","\tEpoch:08 [005/005 (0740/0755)]\tLoss Ss: 0.011115\n","Now train the rotated image\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tRotated_Epoch:08 [000/005 (0000/0588)]\tLoss Ss: 0.397268\n","\tRotated_Epoch:08 [000/005 (0020/0588)]\tLoss Ss: 0.283941\n","\tRotated_Epoch:08 [000/005 (0040/0588)]\tLoss Ss: 0.355146\n","\tRotated_Epoch:08 [000/005 (0060/0588)]\tLoss Ss: 0.146665\n","\tRotated_Epoch:08 [000/005 (0080/0588)]\tLoss Ss: 0.219341\n","\tRotated_Epoch:08 [000/005 (0100/0588)]\tLoss Ss: 0.166034\n","\tRotated_Epoch:08 [000/005 (0120/0588)]\tLoss Ss: 0.130227\n","\tRotated_Epoch:08 [000/005 (0140/0588)]\tLoss Ss: 0.113370\n","\tRotated_Epoch:08 [000/005 (0160/0588)]\tLoss Ss: 0.137583\n","\tRotated_Epoch:08 [000/005 (0180/0588)]\tLoss Ss: 0.139951\n","\tRotated_Epoch:08 [000/005 (0200/0588)]\tLoss Ss: 0.098521\n","\tRotated_Epoch:08 [000/005 (0220/0588)]\tLoss Ss: 0.081463\n","\tRotated_Epoch:08 [000/005 (0240/0588)]\tLoss Ss: 0.115843\n","\tRotated_Epoch:08 [000/005 (0260/0588)]\tLoss Ss: 0.100605\n","\tRotated_Epoch:08 [000/005 (0280/0588)]\tLoss Ss: 0.105628\n","\tRotated_Epoch:08 [000/005 (0300/0588)]\tLoss Ss: 0.120567\n","\tRotated_Epoch:08 [000/005 (0320/0588)]\tLoss Ss: 0.086490\n","\tRotated_Epoch:08 [000/005 (0340/0588)]\tLoss Ss: 0.066186\n","\tRotated_Epoch:08 [000/005 (0360/0588)]\tLoss Ss: 0.094179\n","\tRotated_Epoch:08 [000/005 (0380/0588)]\tLoss Ss: 0.107546\n","\tRotated_Epoch:08 [000/005 (0400/0588)]\tLoss Ss: 0.079029\n","\tRotated_Epoch:08 [000/005 (0420/0588)]\tLoss Ss: 0.123670\n","\tRotated_Epoch:08 [000/005 (0440/0588)]\tLoss Ss: 0.101427\n","\tRotated_Epoch:08 [000/005 (0460/0588)]\tLoss Ss: 0.095707\n","\tRotated_Epoch:08 [000/005 (0480/0588)]\tLoss Ss: 0.107169\n","\tRotated_Epoch:08 [000/005 (0500/0588)]\tLoss Ss: 0.060469\n","\tRotated_Epoch:08 [000/005 (0520/0588)]\tLoss Ss: 0.117327\n","\tRotated_Epoch:08 [000/005 (0540/0588)]\tLoss Ss: 0.087275\n","\tRotated_Epoch:08 [000/005 (0560/0588)]\tLoss Ss: 0.061866\n","\tRotated_Epoch:08 [000/005 (0580/0588)]\tLoss Ss: 0.145584\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:08 [001/005 (0000/0755)]\tLoss Ss: 0.122902\n","\tRotated_Epoch:08 [001/005 (0020/0755)]\tLoss Ss: 0.136462\n","\tRotated_Epoch:08 [001/005 (0040/0755)]\tLoss Ss: 0.125934\n","\tRotated_Epoch:08 [001/005 (0060/0755)]\tLoss Ss: 0.063293\n","\tRotated_Epoch:08 [001/005 (0080/0755)]\tLoss Ss: 0.077577\n","\tRotated_Epoch:08 [001/005 (0100/0755)]\tLoss Ss: 0.035718\n","\tRotated_Epoch:08 [001/005 (0120/0755)]\tLoss Ss: 0.042977\n","\tRotated_Epoch:08 [001/005 (0140/0755)]\tLoss Ss: 0.046746\n","\tRotated_Epoch:08 [001/005 (0160/0755)]\tLoss Ss: 0.045443\n","\tRotated_Epoch:08 [001/005 (0180/0755)]\tLoss Ss: 0.037180\n","\tRotated_Epoch:08 [001/005 (0200/0755)]\tLoss Ss: 0.046250\n","\tRotated_Epoch:08 [001/005 (0220/0755)]\tLoss Ss: 0.027207\n","\tRotated_Epoch:08 [001/005 (0240/0755)]\tLoss Ss: 0.035390\n","\tRotated_Epoch:08 [001/005 (0260/0755)]\tLoss Ss: 0.031295\n","\tRotated_Epoch:08 [001/005 (0280/0755)]\tLoss Ss: 0.037123\n","\tRotated_Epoch:08 [001/005 (0300/0755)]\tLoss Ss: 0.031559\n","\tRotated_Epoch:08 [001/005 (0320/0755)]\tLoss Ss: 0.029116\n","\tRotated_Epoch:08 [001/005 (0340/0755)]\tLoss Ss: 0.034730\n","\tRotated_Epoch:08 [001/005 (0360/0755)]\tLoss Ss: 0.038640\n","\tRotated_Epoch:08 [001/005 (0380/0755)]\tLoss Ss: 0.033210\n","\tRotated_Epoch:08 [001/005 (0400/0755)]\tLoss Ss: 0.037094\n","\tRotated_Epoch:08 [001/005 (0420/0755)]\tLoss Ss: 0.039974\n","\tRotated_Epoch:08 [001/005 (0440/0755)]\tLoss Ss: 0.027453\n","\tRotated_Epoch:08 [001/005 (0460/0755)]\tLoss Ss: 0.029173\n","\tRotated_Epoch:08 [001/005 (0480/0755)]\tLoss Ss: 0.024750\n","\tRotated_Epoch:08 [001/005 (0500/0755)]\tLoss Ss: 0.033631\n","\tRotated_Epoch:08 [001/005 (0520/0755)]\tLoss Ss: 0.016181\n","\tRotated_Epoch:08 [001/005 (0540/0755)]\tLoss Ss: 0.025666\n","\tRotated_Epoch:08 [001/005 (0560/0755)]\tLoss Ss: 0.019841\n","\tRotated_Epoch:08 [001/005 (0580/0755)]\tLoss Ss: 0.021468\n","\tRotated_Epoch:08 [001/005 (0600/0755)]\tLoss Ss: 0.021895\n","\tRotated_Epoch:08 [001/005 (0620/0755)]\tLoss Ss: 0.018557\n","\tRotated_Epoch:08 [001/005 (0640/0755)]\tLoss Ss: 0.030320\n","\tRotated_Epoch:08 [001/005 (0660/0755)]\tLoss Ss: 0.025241\n","\tRotated_Epoch:08 [001/005 (0680/0755)]\tLoss Ss: 0.027245\n","\tRotated_Epoch:08 [001/005 (0700/0755)]\tLoss Ss: 0.025837\n","\tRotated_Epoch:08 [001/005 (0720/0755)]\tLoss Ss: 0.022319\n","\tRotated_Epoch:08 [001/005 (0740/0755)]\tLoss Ss: 0.017368\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:08 [002/005 (0000/0755)]\tLoss Ss: 0.336028\n","\tRotated_Epoch:08 [002/005 (0020/0755)]\tLoss Ss: 0.424276\n","\tRotated_Epoch:08 [002/005 (0040/0755)]\tLoss Ss: 0.211867\n","\tRotated_Epoch:08 [002/005 (0060/0755)]\tLoss Ss: 0.249637\n","\tRotated_Epoch:08 [002/005 (0080/0755)]\tLoss Ss: 0.182850\n","\tRotated_Epoch:08 [002/005 (0100/0755)]\tLoss Ss: 0.273979\n","\tRotated_Epoch:08 [002/005 (0120/0755)]\tLoss Ss: 0.078049\n","\tRotated_Epoch:08 [002/005 (0140/0755)]\tLoss Ss: 0.203473\n","\tRotated_Epoch:08 [002/005 (0160/0755)]\tLoss Ss: 0.279132\n","\tRotated_Epoch:08 [002/005 (0180/0755)]\tLoss Ss: 0.176029\n","\tRotated_Epoch:08 [002/005 (0200/0755)]\tLoss Ss: 0.183100\n","\tRotated_Epoch:08 [002/005 (0220/0755)]\tLoss Ss: 0.107175\n","\tRotated_Epoch:08 [002/005 (0240/0755)]\tLoss Ss: 0.154781\n","\tRotated_Epoch:08 [002/005 (0260/0755)]\tLoss Ss: 0.119379\n","\tRotated_Epoch:08 [002/005 (0280/0755)]\tLoss Ss: 0.191914\n","\tRotated_Epoch:08 [002/005 (0300/0755)]\tLoss Ss: 0.187437\n","\tRotated_Epoch:08 [002/005 (0320/0755)]\tLoss Ss: 0.191128\n","\tRotated_Epoch:08 [002/005 (0340/0755)]\tLoss Ss: 0.104526\n","\tRotated_Epoch:08 [002/005 (0360/0755)]\tLoss Ss: 0.171042\n","\tRotated_Epoch:08 [002/005 (0380/0755)]\tLoss Ss: 0.131126\n","\tRotated_Epoch:08 [002/005 (0400/0755)]\tLoss Ss: 0.115686\n","\tRotated_Epoch:08 [002/005 (0420/0755)]\tLoss Ss: 0.097124\n","\tRotated_Epoch:08 [002/005 (0440/0755)]\tLoss Ss: 0.097300\n","\tRotated_Epoch:08 [002/005 (0460/0755)]\tLoss Ss: 0.109793\n","\tRotated_Epoch:08 [002/005 (0480/0755)]\tLoss Ss: 0.151618\n","\tRotated_Epoch:08 [002/005 (0500/0755)]\tLoss Ss: 0.150831\n","\tRotated_Epoch:08 [002/005 (0520/0755)]\tLoss Ss: 0.160975\n","\tRotated_Epoch:08 [002/005 (0540/0755)]\tLoss Ss: 0.093925\n","\tRotated_Epoch:08 [002/005 (0560/0755)]\tLoss Ss: 0.144208\n","\tRotated_Epoch:08 [002/005 (0580/0755)]\tLoss Ss: 0.124529\n","\tRotated_Epoch:08 [002/005 (0600/0755)]\tLoss Ss: 0.097615\n","\tRotated_Epoch:08 [002/005 (0620/0755)]\tLoss Ss: 0.165073\n","\tRotated_Epoch:08 [002/005 (0640/0755)]\tLoss Ss: 0.119316\n","\tRotated_Epoch:08 [002/005 (0660/0755)]\tLoss Ss: 0.097881\n","\tRotated_Epoch:08 [002/005 (0680/0755)]\tLoss Ss: 0.102312\n","\tRotated_Epoch:08 [002/005 (0700/0755)]\tLoss Ss: 0.103355\n","\tRotated_Epoch:08 [002/005 (0720/0755)]\tLoss Ss: 0.084663\n","\tRotated_Epoch:08 [002/005 (0740/0755)]\tLoss Ss: 0.098753\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tRotated_Epoch:08 [003/005 (0000/0693)]\tLoss Ss: 0.059455\n","\tRotated_Epoch:08 [003/005 (0020/0693)]\tLoss Ss: 0.058869\n","\tRotated_Epoch:08 [003/005 (0040/0693)]\tLoss Ss: 0.046834\n","\tRotated_Epoch:08 [003/005 (0060/0693)]\tLoss Ss: 0.066139\n","\tRotated_Epoch:08 [003/005 (0080/0693)]\tLoss Ss: 0.045796\n","\tRotated_Epoch:08 [003/005 (0100/0693)]\tLoss Ss: 0.049939\n","\tRotated_Epoch:08 [003/005 (0120/0693)]\tLoss Ss: 0.044292\n","\tRotated_Epoch:08 [003/005 (0140/0693)]\tLoss Ss: 0.045509\n","\tRotated_Epoch:08 [003/005 (0160/0693)]\tLoss Ss: 0.039390\n","\tRotated_Epoch:08 [003/005 (0180/0693)]\tLoss Ss: 0.057747\n","\tRotated_Epoch:08 [003/005 (0200/0693)]\tLoss Ss: 0.028952\n","\tRotated_Epoch:08 [003/005 (0220/0693)]\tLoss Ss: 0.036350\n","\tRotated_Epoch:08 [003/005 (0240/0693)]\tLoss Ss: 0.033752\n","\tRotated_Epoch:08 [003/005 (0260/0693)]\tLoss Ss: 0.025301\n","\tRotated_Epoch:08 [003/005 (0280/0693)]\tLoss Ss: 0.035956\n","\tRotated_Epoch:08 [003/005 (0300/0693)]\tLoss Ss: 0.025034\n","\tRotated_Epoch:08 [003/005 (0320/0693)]\tLoss Ss: 0.023081\n","\tRotated_Epoch:08 [003/005 (0340/0693)]\tLoss Ss: 0.018820\n","\tRotated_Epoch:08 [003/005 (0360/0693)]\tLoss Ss: 0.022339\n","\tRotated_Epoch:08 [003/005 (0380/0693)]\tLoss Ss: 0.019372\n","\tRotated_Epoch:08 [003/005 (0400/0693)]\tLoss Ss: 0.029683\n","\tRotated_Epoch:08 [003/005 (0420/0693)]\tLoss Ss: 0.023332\n","\tRotated_Epoch:08 [003/005 (0440/0693)]\tLoss Ss: 0.023851\n","\tRotated_Epoch:08 [003/005 (0460/0693)]\tLoss Ss: 0.025901\n","\tRotated_Epoch:08 [003/005 (0480/0693)]\tLoss Ss: 0.028564\n","\tRotated_Epoch:08 [003/005 (0500/0693)]\tLoss Ss: 0.024774\n","\tRotated_Epoch:08 [003/005 (0520/0693)]\tLoss Ss: 0.016819\n","\tRotated_Epoch:08 [003/005 (0540/0693)]\tLoss Ss: 0.019367\n","\tRotated_Epoch:08 [003/005 (0560/0693)]\tLoss Ss: 0.024582\n","\tRotated_Epoch:08 [003/005 (0580/0693)]\tLoss Ss: 0.019419\n","\tRotated_Epoch:08 [003/005 (0600/0693)]\tLoss Ss: 0.015601\n","\tRotated_Epoch:08 [003/005 (0620/0693)]\tLoss Ss: 0.020419\n","\tRotated_Epoch:08 [003/005 (0640/0693)]\tLoss Ss: 0.022293\n","\tRotated_Epoch:08 [003/005 (0660/0693)]\tLoss Ss: 0.020855\n","\tRotated_Epoch:08 [003/005 (0680/0693)]\tLoss Ss: 0.017046\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tRotated_Epoch:08 [004/005 (0000/0693)]\tLoss Ss: 0.025096\n","\tRotated_Epoch:08 [004/005 (0020/0693)]\tLoss Ss: 0.015636\n","\tRotated_Epoch:08 [004/005 (0040/0693)]\tLoss Ss: 0.019908\n","\tRotated_Epoch:08 [004/005 (0060/0693)]\tLoss Ss: 0.018172\n","\tRotated_Epoch:08 [004/005 (0080/0693)]\tLoss Ss: 0.020492\n","\tRotated_Epoch:08 [004/005 (0100/0693)]\tLoss Ss: 0.017690\n","\tRotated_Epoch:08 [004/005 (0120/0693)]\tLoss Ss: 0.023370\n","\tRotated_Epoch:08 [004/005 (0140/0693)]\tLoss Ss: 0.015544\n","\tRotated_Epoch:08 [004/005 (0160/0693)]\tLoss Ss: 0.012811\n","\tRotated_Epoch:08 [004/005 (0180/0693)]\tLoss Ss: 0.016655\n","\tRotated_Epoch:08 [004/005 (0200/0693)]\tLoss Ss: 0.018460\n","\tRotated_Epoch:08 [004/005 (0220/0693)]\tLoss Ss: 0.025963\n","\tRotated_Epoch:08 [004/005 (0240/0693)]\tLoss Ss: 0.016866\n","\tRotated_Epoch:08 [004/005 (0260/0693)]\tLoss Ss: 0.013400\n","\tRotated_Epoch:08 [004/005 (0280/0693)]\tLoss Ss: 0.028182\n","\tRotated_Epoch:08 [004/005 (0300/0693)]\tLoss Ss: 0.017458\n","\tRotated_Epoch:08 [004/005 (0320/0693)]\tLoss Ss: 0.031335\n","\tRotated_Epoch:08 [004/005 (0340/0693)]\tLoss Ss: 0.019323\n","\tRotated_Epoch:08 [004/005 (0360/0693)]\tLoss Ss: 0.016367\n","\tRotated_Epoch:08 [004/005 (0380/0693)]\tLoss Ss: 0.018754\n","\tRotated_Epoch:08 [004/005 (0400/0693)]\tLoss Ss: 0.016788\n","\tRotated_Epoch:08 [004/005 (0420/0693)]\tLoss Ss: 0.023879\n","\tRotated_Epoch:08 [004/005 (0440/0693)]\tLoss Ss: 0.015824\n","\tRotated_Epoch:08 [004/005 (0460/0693)]\tLoss Ss: 0.014533\n","\tRotated_Epoch:08 [004/005 (0480/0693)]\tLoss Ss: 0.021071\n","\tRotated_Epoch:08 [004/005 (0500/0693)]\tLoss Ss: 0.010479\n","\tRotated_Epoch:08 [004/005 (0520/0693)]\tLoss Ss: 0.021079\n","\tRotated_Epoch:08 [004/005 (0540/0693)]\tLoss Ss: 0.033027\n","\tRotated_Epoch:08 [004/005 (0560/0693)]\tLoss Ss: 0.026170\n","\tRotated_Epoch:08 [004/005 (0580/0693)]\tLoss Ss: 0.018786\n","\tRotated_Epoch:08 [004/005 (0600/0693)]\tLoss Ss: 0.023771\n","\tRotated_Epoch:08 [004/005 (0620/0693)]\tLoss Ss: 0.015809\n","\tRotated_Epoch:08 [004/005 (0640/0693)]\tLoss Ss: 0.019590\n","\tRotated_Epoch:08 [004/005 (0660/0693)]\tLoss Ss: 0.017414\n","\tRotated_Epoch:08 [004/005 (0680/0693)]\tLoss Ss: 0.012998\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tRotated_Epoch:08 [005/005 (0000/0614)]\tLoss Ss: 0.155159\n","\tRotated_Epoch:08 [005/005 (0020/0614)]\tLoss Ss: 0.059363\n","\tRotated_Epoch:08 [005/005 (0040/0614)]\tLoss Ss: 0.077976\n","\tRotated_Epoch:08 [005/005 (0060/0614)]\tLoss Ss: 0.051298\n","\tRotated_Epoch:08 [005/005 (0080/0614)]\tLoss Ss: 0.030970\n","\tRotated_Epoch:08 [005/005 (0100/0614)]\tLoss Ss: 0.019902\n","\tRotated_Epoch:08 [005/005 (0120/0614)]\tLoss Ss: 0.019310\n","\tRotated_Epoch:08 [005/005 (0140/0614)]\tLoss Ss: 0.012939\n","\tRotated_Epoch:08 [005/005 (0160/0614)]\tLoss Ss: 0.016127\n","\tRotated_Epoch:08 [005/005 (0180/0614)]\tLoss Ss: 0.015996\n","\tRotated_Epoch:08 [005/005 (0200/0614)]\tLoss Ss: 0.012141\n","\tRotated_Epoch:08 [005/005 (0220/0614)]\tLoss Ss: 0.023094\n","\tRotated_Epoch:08 [005/005 (0240/0614)]\tLoss Ss: 0.017270\n","\tRotated_Epoch:08 [005/005 (0260/0614)]\tLoss Ss: 0.013967\n","\tRotated_Epoch:08 [005/005 (0280/0614)]\tLoss Ss: 0.010783\n","\tRotated_Epoch:08 [005/005 (0300/0614)]\tLoss Ss: 0.009735\n","\tRotated_Epoch:08 [005/005 (0320/0614)]\tLoss Ss: 0.012654\n","\tRotated_Epoch:08 [005/005 (0340/0614)]\tLoss Ss: 0.011503\n","\tRotated_Epoch:08 [005/005 (0360/0614)]\tLoss Ss: 0.011772\n","\tRotated_Epoch:08 [005/005 (0380/0614)]\tLoss Ss: 0.015664\n","\tRotated_Epoch:08 [005/005 (0400/0614)]\tLoss Ss: 0.010690\n","\tRotated_Epoch:08 [005/005 (0420/0614)]\tLoss Ss: 0.012596\n","\tRotated_Epoch:08 [005/005 (0440/0614)]\tLoss Ss: 0.017554\n","\tRotated_Epoch:08 [005/005 (0460/0614)]\tLoss Ss: 0.013070\n","\tRotated_Epoch:08 [005/005 (0480/0614)]\tLoss Ss: 0.006812\n","\tRotated_Epoch:08 [005/005 (0500/0614)]\tLoss Ss: 0.015600\n","\tRotated_Epoch:08 [005/005 (0520/0614)]\tLoss Ss: 0.012755\n","\tRotated_Epoch:08 [005/005 (0540/0614)]\tLoss Ss: 0.017480\n","\tRotated_Epoch:08 [005/005 (0560/0614)]\tLoss Ss: 0.016368\n","\tRotated_Epoch:08 [005/005 (0580/0614)]\tLoss Ss: 0.011080\n","\tRotated_Epoch:08 [005/005 (0600/0614)]\tLoss Ss: 0.010828\n","new_1_6_d.nii\n","new_1_6_m.nii\n","new_1_7_d.nii\n","new_1_7_m.nii\n","new_2_5_d.nii\n","new_2_5_m.nii\n","new_2_6_d.nii\n","new_2_6_m.nii\n","new_2_7_d.nii\n","new_2_7_m.nii\n","new_3_d (2).nii\n","new_3_m (2).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (3).nii\n","new_3_m (3).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (7).nii\n","new_3_m (7).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (8).nii\n","new_3_m (8).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (9).nii\n","new_3_m (9).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_4_10_d_163526.nii\n","new_4_10_m_163526.nii.gz\n","new_4_11_d_084336.nii\n","new_4_11_m_084336.nii.gz\n","new_4_7_d_184422.nii\n","new_4_7_m_184422.nii.gz\n","new_4_8_d_143412.nii\n","new_4_8_m_143412.nii.gz\n","new_4_9_d_155347.nii\n","new_4_9_m_155347.nii.gz\n","\tEpoch: 8; Dice: 0.9095 +/- 0.0301; Loss: 17.9150\n","Begin Epoch 9\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tEpoch:09 [000/005 (0000/0693)]\tLoss Ss: 0.017170\n","\tEpoch:09 [000/005 (0020/0693)]\tLoss Ss: 0.023041\n","\tEpoch:09 [000/005 (0040/0693)]\tLoss Ss: 0.024689\n","\tEpoch:09 [000/005 (0060/0693)]\tLoss Ss: 0.028197\n","\tEpoch:09 [000/005 (0080/0693)]\tLoss Ss: 0.022948\n","\tEpoch:09 [000/005 (0100/0693)]\tLoss Ss: 0.023116\n","\tEpoch:09 [000/005 (0120/0693)]\tLoss Ss: 0.020682\n","\tEpoch:09 [000/005 (0140/0693)]\tLoss Ss: 0.015921\n","\tEpoch:09 [000/005 (0160/0693)]\tLoss Ss: 0.015698\n","\tEpoch:09 [000/005 (0180/0693)]\tLoss Ss: 0.013072\n","\tEpoch:09 [000/005 (0200/0693)]\tLoss Ss: 0.010600\n","\tEpoch:09 [000/005 (0220/0693)]\tLoss Ss: 0.021486\n","\tEpoch:09 [000/005 (0240/0693)]\tLoss Ss: 0.018253\n","\tEpoch:09 [000/005 (0260/0693)]\tLoss Ss: 0.018261\n","\tEpoch:09 [000/005 (0280/0693)]\tLoss Ss: 0.026723\n","\tEpoch:09 [000/005 (0300/0693)]\tLoss Ss: 0.022986\n","\tEpoch:09 [000/005 (0320/0693)]\tLoss Ss: 0.011976\n","\tEpoch:09 [000/005 (0340/0693)]\tLoss Ss: 0.024850\n","\tEpoch:09 [000/005 (0360/0693)]\tLoss Ss: 0.015577\n","\tEpoch:09 [000/005 (0380/0693)]\tLoss Ss: 0.024733\n","\tEpoch:09 [000/005 (0400/0693)]\tLoss Ss: 0.031742\n","\tEpoch:09 [000/005 (0420/0693)]\tLoss Ss: 0.016839\n","\tEpoch:09 [000/005 (0440/0693)]\tLoss Ss: 0.017618\n","\tEpoch:09 [000/005 (0460/0693)]\tLoss Ss: 0.017150\n","\tEpoch:09 [000/005 (0480/0693)]\tLoss Ss: 0.017598\n","\tEpoch:09 [000/005 (0500/0693)]\tLoss Ss: 0.016375\n","\tEpoch:09 [000/005 (0520/0693)]\tLoss Ss: 0.019730\n","\tEpoch:09 [000/005 (0540/0693)]\tLoss Ss: 0.021903\n","\tEpoch:09 [000/005 (0560/0693)]\tLoss Ss: 0.011884\n","\tEpoch:09 [000/005 (0580/0693)]\tLoss Ss: 0.016234\n","\tEpoch:09 [000/005 (0600/0693)]\tLoss Ss: 0.017827\n","\tEpoch:09 [000/005 (0620/0693)]\tLoss Ss: 0.024313\n","\tEpoch:09 [000/005 (0640/0693)]\tLoss Ss: 0.016864\n","\tEpoch:09 [000/005 (0660/0693)]\tLoss Ss: 0.027632\n","\tEpoch:09 [000/005 (0680/0693)]\tLoss Ss: 0.012613\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tEpoch:09 [001/005 (0000/0693)]\tLoss Ss: 0.022420\n","\tEpoch:09 [001/005 (0020/0693)]\tLoss Ss: 0.016382\n","\tEpoch:09 [001/005 (0040/0693)]\tLoss Ss: 0.019148\n","\tEpoch:09 [001/005 (0060/0693)]\tLoss Ss: 0.019097\n","\tEpoch:09 [001/005 (0080/0693)]\tLoss Ss: 0.013317\n","\tEpoch:09 [001/005 (0100/0693)]\tLoss Ss: 0.015989\n","\tEpoch:09 [001/005 (0120/0693)]\tLoss Ss: 0.014386\n","\tEpoch:09 [001/005 (0140/0693)]\tLoss Ss: 0.025672\n","\tEpoch:09 [001/005 (0160/0693)]\tLoss Ss: 0.019944\n","\tEpoch:09 [001/005 (0180/0693)]\tLoss Ss: 0.016761\n","\tEpoch:09 [001/005 (0200/0693)]\tLoss Ss: 0.014766\n","\tEpoch:09 [001/005 (0220/0693)]\tLoss Ss: 0.020042\n","\tEpoch:09 [001/005 (0240/0693)]\tLoss Ss: 0.012306\n","\tEpoch:09 [001/005 (0260/0693)]\tLoss Ss: 0.017515\n","\tEpoch:09 [001/005 (0280/0693)]\tLoss Ss: 0.014446\n","\tEpoch:09 [001/005 (0300/0693)]\tLoss Ss: 0.016566\n","\tEpoch:09 [001/005 (0320/0693)]\tLoss Ss: 0.009734\n","\tEpoch:09 [001/005 (0340/0693)]\tLoss Ss: 0.018250\n","\tEpoch:09 [001/005 (0360/0693)]\tLoss Ss: 0.016828\n","\tEpoch:09 [001/005 (0380/0693)]\tLoss Ss: 0.019293\n","\tEpoch:09 [001/005 (0400/0693)]\tLoss Ss: 0.016899\n","\tEpoch:09 [001/005 (0420/0693)]\tLoss Ss: 0.014859\n","\tEpoch:09 [001/005 (0440/0693)]\tLoss Ss: 0.020764\n","\tEpoch:09 [001/005 (0460/0693)]\tLoss Ss: 0.014368\n","\tEpoch:09 [001/005 (0480/0693)]\tLoss Ss: 0.011119\n","\tEpoch:09 [001/005 (0500/0693)]\tLoss Ss: 0.020806\n","\tEpoch:09 [001/005 (0520/0693)]\tLoss Ss: 0.011059\n","\tEpoch:09 [001/005 (0540/0693)]\tLoss Ss: 0.023944\n","\tEpoch:09 [001/005 (0560/0693)]\tLoss Ss: 0.017076\n","\tEpoch:09 [001/005 (0580/0693)]\tLoss Ss: 0.011581\n","\tEpoch:09 [001/005 (0600/0693)]\tLoss Ss: 0.012538\n","\tEpoch:09 [001/005 (0620/0693)]\tLoss Ss: 0.014055\n","\tEpoch:09 [001/005 (0640/0693)]\tLoss Ss: 0.015802\n","\tEpoch:09 [001/005 (0660/0693)]\tLoss Ss: 0.019312\n","\tEpoch:09 [001/005 (0680/0693)]\tLoss Ss: 0.017658\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:09 [002/005 (0000/0755)]\tLoss Ss: 0.061140\n","\tEpoch:09 [002/005 (0020/0755)]\tLoss Ss: 0.046275\n","\tEpoch:09 [002/005 (0040/0755)]\tLoss Ss: 0.048009\n","\tEpoch:09 [002/005 (0060/0755)]\tLoss Ss: 0.052673\n","\tEpoch:09 [002/005 (0080/0755)]\tLoss Ss: 0.036838\n","\tEpoch:09 [002/005 (0100/0755)]\tLoss Ss: 0.029333\n","\tEpoch:09 [002/005 (0120/0755)]\tLoss Ss: 0.031156\n","\tEpoch:09 [002/005 (0140/0755)]\tLoss Ss: 0.028269\n","\tEpoch:09 [002/005 (0160/0755)]\tLoss Ss: 0.026298\n","\tEpoch:09 [002/005 (0180/0755)]\tLoss Ss: 0.033468\n","\tEpoch:09 [002/005 (0200/0755)]\tLoss Ss: 0.028056\n","\tEpoch:09 [002/005 (0220/0755)]\tLoss Ss: 0.025070\n","\tEpoch:09 [002/005 (0240/0755)]\tLoss Ss: 0.025904\n","\tEpoch:09 [002/005 (0260/0755)]\tLoss Ss: 0.015273\n","\tEpoch:09 [002/005 (0280/0755)]\tLoss Ss: 0.034657\n","\tEpoch:09 [002/005 (0300/0755)]\tLoss Ss: 0.016390\n","\tEpoch:09 [002/005 (0320/0755)]\tLoss Ss: 0.024692\n","\tEpoch:09 [002/005 (0340/0755)]\tLoss Ss: 0.025210\n","\tEpoch:09 [002/005 (0360/0755)]\tLoss Ss: 0.034323\n","\tEpoch:09 [002/005 (0380/0755)]\tLoss Ss: 0.016034\n","\tEpoch:09 [002/005 (0400/0755)]\tLoss Ss: 0.017795\n","\tEpoch:09 [002/005 (0420/0755)]\tLoss Ss: 0.022322\n","\tEpoch:09 [002/005 (0440/0755)]\tLoss Ss: 0.028927\n","\tEpoch:09 [002/005 (0460/0755)]\tLoss Ss: 0.026575\n","\tEpoch:09 [002/005 (0480/0755)]\tLoss Ss: 0.025819\n","\tEpoch:09 [002/005 (0500/0755)]\tLoss Ss: 0.019795\n","\tEpoch:09 [002/005 (0520/0755)]\tLoss Ss: 0.025852\n","\tEpoch:09 [002/005 (0540/0755)]\tLoss Ss: 0.018769\n","\tEpoch:09 [002/005 (0560/0755)]\tLoss Ss: 0.021461\n","\tEpoch:09 [002/005 (0580/0755)]\tLoss Ss: 0.022652\n","\tEpoch:09 [002/005 (0600/0755)]\tLoss Ss: 0.016330\n","\tEpoch:09 [002/005 (0620/0755)]\tLoss Ss: 0.022309\n","\tEpoch:09 [002/005 (0640/0755)]\tLoss Ss: 0.019164\n","\tEpoch:09 [002/005 (0660/0755)]\tLoss Ss: 0.022515\n","\tEpoch:09 [002/005 (0680/0755)]\tLoss Ss: 0.019429\n","\tEpoch:09 [002/005 (0700/0755)]\tLoss Ss: 0.027839\n","\tEpoch:09 [002/005 (0720/0755)]\tLoss Ss: 0.019425\n","\tEpoch:09 [002/005 (0740/0755)]\tLoss Ss: 0.032063\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:09 [003/005 (0000/0755)]\tLoss Ss: 0.021591\n","\tEpoch:09 [003/005 (0020/0755)]\tLoss Ss: 0.019249\n","\tEpoch:09 [003/005 (0040/0755)]\tLoss Ss: 0.021085\n","\tEpoch:09 [003/005 (0060/0755)]\tLoss Ss: 0.011391\n","\tEpoch:09 [003/005 (0080/0755)]\tLoss Ss: 0.019826\n","\tEpoch:09 [003/005 (0100/0755)]\tLoss Ss: 0.020409\n","\tEpoch:09 [003/005 (0120/0755)]\tLoss Ss: 0.026650\n","\tEpoch:09 [003/005 (0140/0755)]\tLoss Ss: 0.019424\n","\tEpoch:09 [003/005 (0160/0755)]\tLoss Ss: 0.025639\n","\tEpoch:09 [003/005 (0180/0755)]\tLoss Ss: 0.021848\n","\tEpoch:09 [003/005 (0200/0755)]\tLoss Ss: 0.016787\n","\tEpoch:09 [003/005 (0220/0755)]\tLoss Ss: 0.017878\n","\tEpoch:09 [003/005 (0240/0755)]\tLoss Ss: 0.014836\n","\tEpoch:09 [003/005 (0260/0755)]\tLoss Ss: 0.021387\n","\tEpoch:09 [003/005 (0280/0755)]\tLoss Ss: 0.023668\n","\tEpoch:09 [003/005 (0300/0755)]\tLoss Ss: 0.024805\n","\tEpoch:09 [003/005 (0320/0755)]\tLoss Ss: 0.017590\n","\tEpoch:09 [003/005 (0340/0755)]\tLoss Ss: 0.019690\n","\tEpoch:09 [003/005 (0360/0755)]\tLoss Ss: 0.015314\n","\tEpoch:09 [003/005 (0380/0755)]\tLoss Ss: 0.016528\n","\tEpoch:09 [003/005 (0400/0755)]\tLoss Ss: 0.016510\n","\tEpoch:09 [003/005 (0420/0755)]\tLoss Ss: 0.015484\n","\tEpoch:09 [003/005 (0440/0755)]\tLoss Ss: 0.018438\n","\tEpoch:09 [003/005 (0460/0755)]\tLoss Ss: 0.015015\n","\tEpoch:09 [003/005 (0480/0755)]\tLoss Ss: 0.013840\n","\tEpoch:09 [003/005 (0500/0755)]\tLoss Ss: 0.011898\n","\tEpoch:09 [003/005 (0520/0755)]\tLoss Ss: 0.014168\n","\tEpoch:09 [003/005 (0540/0755)]\tLoss Ss: 0.013992\n","\tEpoch:09 [003/005 (0560/0755)]\tLoss Ss: 0.020203\n","\tEpoch:09 [003/005 (0580/0755)]\tLoss Ss: 0.015118\n","\tEpoch:09 [003/005 (0600/0755)]\tLoss Ss: 0.016560\n","\tEpoch:09 [003/005 (0620/0755)]\tLoss Ss: 0.013554\n","\tEpoch:09 [003/005 (0640/0755)]\tLoss Ss: 0.018384\n","\tEpoch:09 [003/005 (0660/0755)]\tLoss Ss: 0.011165\n","\tEpoch:09 [003/005 (0680/0755)]\tLoss Ss: 0.015882\n","\tEpoch:09 [003/005 (0700/0755)]\tLoss Ss: 0.017720\n","\tEpoch:09 [003/005 (0720/0755)]\tLoss Ss: 0.019591\n","\tEpoch:09 [003/005 (0740/0755)]\tLoss Ss: 0.018558\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tEpoch:09 [004/005 (0000/0588)]\tLoss Ss: 0.020017\n","\tEpoch:09 [004/005 (0020/0588)]\tLoss Ss: 0.013845\n","\tEpoch:09 [004/005 (0040/0588)]\tLoss Ss: 0.010544\n","\tEpoch:09 [004/005 (0060/0588)]\tLoss Ss: 0.016646\n","\tEpoch:09 [004/005 (0080/0588)]\tLoss Ss: 0.013651\n","\tEpoch:09 [004/005 (0100/0588)]\tLoss Ss: 0.005640\n","\tEpoch:09 [004/005 (0120/0588)]\tLoss Ss: 0.007046\n","\tEpoch:09 [004/005 (0140/0588)]\tLoss Ss: 0.006564\n","\tEpoch:09 [004/005 (0160/0588)]\tLoss Ss: 0.008022\n","\tEpoch:09 [004/005 (0180/0588)]\tLoss Ss: 0.007945\n","\tEpoch:09 [004/005 (0200/0588)]\tLoss Ss: 0.008359\n","\tEpoch:09 [004/005 (0220/0588)]\tLoss Ss: 0.007638\n","\tEpoch:09 [004/005 (0240/0588)]\tLoss Ss: 0.007053\n","\tEpoch:09 [004/005 (0260/0588)]\tLoss Ss: 0.007131\n","\tEpoch:09 [004/005 (0280/0588)]\tLoss Ss: 0.010018\n","\tEpoch:09 [004/005 (0300/0588)]\tLoss Ss: 0.007589\n","\tEpoch:09 [004/005 (0320/0588)]\tLoss Ss: 0.009933\n","\tEpoch:09 [004/005 (0340/0588)]\tLoss Ss: 0.007717\n","\tEpoch:09 [004/005 (0360/0588)]\tLoss Ss: 0.004735\n","\tEpoch:09 [004/005 (0380/0588)]\tLoss Ss: 0.006840\n","\tEpoch:09 [004/005 (0400/0588)]\tLoss Ss: 0.005608\n","\tEpoch:09 [004/005 (0420/0588)]\tLoss Ss: 0.010973\n","\tEpoch:09 [004/005 (0440/0588)]\tLoss Ss: 0.005256\n","\tEpoch:09 [004/005 (0460/0588)]\tLoss Ss: 0.007202\n","\tEpoch:09 [004/005 (0480/0588)]\tLoss Ss: 0.008013\n","\tEpoch:09 [004/005 (0500/0588)]\tLoss Ss: 0.006695\n","\tEpoch:09 [004/005 (0520/0588)]\tLoss Ss: 0.006718\n","\tEpoch:09 [004/005 (0540/0588)]\tLoss Ss: 0.004573\n","\tEpoch:09 [004/005 (0560/0588)]\tLoss Ss: 0.008165\n","\tEpoch:09 [004/005 (0580/0588)]\tLoss Ss: 0.005715\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tEpoch:09 [005/005 (0000/0614)]\tLoss Ss: 0.010746\n","\tEpoch:09 [005/005 (0020/0614)]\tLoss Ss: 0.010987\n","\tEpoch:09 [005/005 (0040/0614)]\tLoss Ss: 0.011685\n","\tEpoch:09 [005/005 (0060/0614)]\tLoss Ss: 0.012257\n","\tEpoch:09 [005/005 (0080/0614)]\tLoss Ss: 0.007645\n","\tEpoch:09 [005/005 (0100/0614)]\tLoss Ss: 0.010147\n","\tEpoch:09 [005/005 (0120/0614)]\tLoss Ss: 0.008094\n","\tEpoch:09 [005/005 (0140/0614)]\tLoss Ss: 0.008535\n","\tEpoch:09 [005/005 (0160/0614)]\tLoss Ss: 0.006382\n","\tEpoch:09 [005/005 (0180/0614)]\tLoss Ss: 0.009865\n","\tEpoch:09 [005/005 (0200/0614)]\tLoss Ss: 0.006370\n","\tEpoch:09 [005/005 (0220/0614)]\tLoss Ss: 0.005894\n","\tEpoch:09 [005/005 (0240/0614)]\tLoss Ss: 0.006309\n","\tEpoch:09 [005/005 (0260/0614)]\tLoss Ss: 0.007292\n","\tEpoch:09 [005/005 (0280/0614)]\tLoss Ss: 0.015277\n","\tEpoch:09 [005/005 (0300/0614)]\tLoss Ss: 0.009055\n","\tEpoch:09 [005/005 (0320/0614)]\tLoss Ss: 0.005875\n","\tEpoch:09 [005/005 (0340/0614)]\tLoss Ss: 0.004552\n","\tEpoch:09 [005/005 (0360/0614)]\tLoss Ss: 0.004449\n","\tEpoch:09 [005/005 (0380/0614)]\tLoss Ss: 0.007084\n","\tEpoch:09 [005/005 (0400/0614)]\tLoss Ss: 0.007589\n","\tEpoch:09 [005/005 (0420/0614)]\tLoss Ss: 0.009972\n","\tEpoch:09 [005/005 (0440/0614)]\tLoss Ss: 0.008527\n","\tEpoch:09 [005/005 (0460/0614)]\tLoss Ss: 0.008349\n","\tEpoch:09 [005/005 (0480/0614)]\tLoss Ss: 0.006590\n","\tEpoch:09 [005/005 (0500/0614)]\tLoss Ss: 0.008327\n","\tEpoch:09 [005/005 (0520/0614)]\tLoss Ss: 0.003288\n","\tEpoch:09 [005/005 (0540/0614)]\tLoss Ss: 0.008010\n","\tEpoch:09 [005/005 (0560/0614)]\tLoss Ss: 0.007135\n","\tEpoch:09 [005/005 (0580/0614)]\tLoss Ss: 0.007077\n","\tEpoch:09 [005/005 (0600/0614)]\tLoss Ss: 0.008798\n","Now train the rotated image\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tRotated_Epoch:09 [000/005 (0000/0614)]\tLoss Ss: 0.004954\n","\tRotated_Epoch:09 [000/005 (0020/0614)]\tLoss Ss: 0.012711\n","\tRotated_Epoch:09 [000/005 (0040/0614)]\tLoss Ss: 0.011619\n","\tRotated_Epoch:09 [000/005 (0060/0614)]\tLoss Ss: 0.013229\n","\tRotated_Epoch:09 [000/005 (0080/0614)]\tLoss Ss: 0.009648\n","\tRotated_Epoch:09 [000/005 (0100/0614)]\tLoss Ss: 0.007786\n","\tRotated_Epoch:09 [000/005 (0120/0614)]\tLoss Ss: 0.015840\n","\tRotated_Epoch:09 [000/005 (0140/0614)]\tLoss Ss: 0.011644\n","\tRotated_Epoch:09 [000/005 (0160/0614)]\tLoss Ss: 0.006479\n","\tRotated_Epoch:09 [000/005 (0180/0614)]\tLoss Ss: 0.010277\n","\tRotated_Epoch:09 [000/005 (0200/0614)]\tLoss Ss: 0.011591\n","\tRotated_Epoch:09 [000/005 (0220/0614)]\tLoss Ss: 0.007277\n","\tRotated_Epoch:09 [000/005 (0240/0614)]\tLoss Ss: 0.006955\n","\tRotated_Epoch:09 [000/005 (0260/0614)]\tLoss Ss: 0.008201\n","\tRotated_Epoch:09 [000/005 (0280/0614)]\tLoss Ss: 0.005617\n","\tRotated_Epoch:09 [000/005 (0300/0614)]\tLoss Ss: 0.007425\n","\tRotated_Epoch:09 [000/005 (0320/0614)]\tLoss Ss: 0.011176\n","\tRotated_Epoch:09 [000/005 (0340/0614)]\tLoss Ss: 0.005268\n","\tRotated_Epoch:09 [000/005 (0360/0614)]\tLoss Ss: 0.009479\n","\tRotated_Epoch:09 [000/005 (0380/0614)]\tLoss Ss: 0.007614\n","\tRotated_Epoch:09 [000/005 (0400/0614)]\tLoss Ss: 0.007081\n","\tRotated_Epoch:09 [000/005 (0420/0614)]\tLoss Ss: 0.007990\n","\tRotated_Epoch:09 [000/005 (0440/0614)]\tLoss Ss: 0.008416\n","\tRotated_Epoch:09 [000/005 (0460/0614)]\tLoss Ss: 0.007159\n","\tRotated_Epoch:09 [000/005 (0480/0614)]\tLoss Ss: 0.007014\n","\tRotated_Epoch:09 [000/005 (0500/0614)]\tLoss Ss: 0.009973\n","\tRotated_Epoch:09 [000/005 (0520/0614)]\tLoss Ss: 0.007348\n","\tRotated_Epoch:09 [000/005 (0540/0614)]\tLoss Ss: 0.008632\n","\tRotated_Epoch:09 [000/005 (0560/0614)]\tLoss Ss: 0.005966\n","\tRotated_Epoch:09 [000/005 (0580/0614)]\tLoss Ss: 0.007243\n","\tRotated_Epoch:09 [000/005 (0600/0614)]\tLoss Ss: 0.007909\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tRotated_Epoch:09 [001/005 (0000/0588)]\tLoss Ss: 0.446021\n","\tRotated_Epoch:09 [001/005 (0020/0588)]\tLoss Ss: 0.742068\n","\tRotated_Epoch:09 [001/005 (0040/0588)]\tLoss Ss: 0.279690\n","\tRotated_Epoch:09 [001/005 (0060/0588)]\tLoss Ss: 0.244594\n","\tRotated_Epoch:09 [001/005 (0080/0588)]\tLoss Ss: 0.221606\n","\tRotated_Epoch:09 [001/005 (0100/0588)]\tLoss Ss: 0.141561\n","\tRotated_Epoch:09 [001/005 (0120/0588)]\tLoss Ss: 0.173966\n","\tRotated_Epoch:09 [001/005 (0140/0588)]\tLoss Ss: 0.116772\n","\tRotated_Epoch:09 [001/005 (0160/0588)]\tLoss Ss: 0.113038\n","\tRotated_Epoch:09 [001/005 (0180/0588)]\tLoss Ss: 0.088042\n","\tRotated_Epoch:09 [001/005 (0200/0588)]\tLoss Ss: 0.062621\n","\tRotated_Epoch:09 [001/005 (0220/0588)]\tLoss Ss: 0.131389\n","\tRotated_Epoch:09 [001/005 (0240/0588)]\tLoss Ss: 0.103957\n","\tRotated_Epoch:09 [001/005 (0260/0588)]\tLoss Ss: 0.105864\n","\tRotated_Epoch:09 [001/005 (0280/0588)]\tLoss Ss: 0.145280\n","\tRotated_Epoch:09 [001/005 (0300/0588)]\tLoss Ss: 0.088813\n","\tRotated_Epoch:09 [001/005 (0320/0588)]\tLoss Ss: 0.063911\n","\tRotated_Epoch:09 [001/005 (0340/0588)]\tLoss Ss: 0.106284\n","\tRotated_Epoch:09 [001/005 (0360/0588)]\tLoss Ss: 0.113418\n","\tRotated_Epoch:09 [001/005 (0380/0588)]\tLoss Ss: 0.085501\n","\tRotated_Epoch:09 [001/005 (0400/0588)]\tLoss Ss: 0.071558\n","\tRotated_Epoch:09 [001/005 (0420/0588)]\tLoss Ss: 0.097687\n","\tRotated_Epoch:09 [001/005 (0440/0588)]\tLoss Ss: 0.073317\n","\tRotated_Epoch:09 [001/005 (0460/0588)]\tLoss Ss: 0.074647\n","\tRotated_Epoch:09 [001/005 (0480/0588)]\tLoss Ss: 0.069132\n","\tRotated_Epoch:09 [001/005 (0500/0588)]\tLoss Ss: 0.093100\n","\tRotated_Epoch:09 [001/005 (0520/0588)]\tLoss Ss: 0.070522\n","\tRotated_Epoch:09 [001/005 (0540/0588)]\tLoss Ss: 0.064758\n","\tRotated_Epoch:09 [001/005 (0560/0588)]\tLoss Ss: 0.084869\n","\tRotated_Epoch:09 [001/005 (0580/0588)]\tLoss Ss: 0.101648\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:09 [002/005 (0000/0755)]\tLoss Ss: 0.184295\n","\tRotated_Epoch:09 [002/005 (0020/0755)]\tLoss Ss: 0.141153\n","\tRotated_Epoch:09 [002/005 (0040/0755)]\tLoss Ss: 0.114882\n","\tRotated_Epoch:09 [002/005 (0060/0755)]\tLoss Ss: 0.130551\n","\tRotated_Epoch:09 [002/005 (0080/0755)]\tLoss Ss: 0.109885\n","\tRotated_Epoch:09 [002/005 (0100/0755)]\tLoss Ss: 0.123426\n","\tRotated_Epoch:09 [002/005 (0120/0755)]\tLoss Ss: 0.089830\n","\tRotated_Epoch:09 [002/005 (0140/0755)]\tLoss Ss: 0.162601\n","\tRotated_Epoch:09 [002/005 (0160/0755)]\tLoss Ss: 0.086125\n","\tRotated_Epoch:09 [002/005 (0180/0755)]\tLoss Ss: 0.085877\n","\tRotated_Epoch:09 [002/005 (0200/0755)]\tLoss Ss: 0.102607\n","\tRotated_Epoch:09 [002/005 (0220/0755)]\tLoss Ss: 0.104295\n","\tRotated_Epoch:09 [002/005 (0240/0755)]\tLoss Ss: 0.087447\n","\tRotated_Epoch:09 [002/005 (0260/0755)]\tLoss Ss: 0.087814\n","\tRotated_Epoch:09 [002/005 (0280/0755)]\tLoss Ss: 0.060133\n","\tRotated_Epoch:09 [002/005 (0300/0755)]\tLoss Ss: 0.080609\n","\tRotated_Epoch:09 [002/005 (0320/0755)]\tLoss Ss: 0.075735\n","\tRotated_Epoch:09 [002/005 (0340/0755)]\tLoss Ss: 0.080300\n","\tRotated_Epoch:09 [002/005 (0360/0755)]\tLoss Ss: 0.077646\n","\tRotated_Epoch:09 [002/005 (0380/0755)]\tLoss Ss: 0.074019\n","\tRotated_Epoch:09 [002/005 (0400/0755)]\tLoss Ss: 0.090299\n","\tRotated_Epoch:09 [002/005 (0420/0755)]\tLoss Ss: 0.058076\n","\tRotated_Epoch:09 [002/005 (0440/0755)]\tLoss Ss: 0.071731\n","\tRotated_Epoch:09 [002/005 (0460/0755)]\tLoss Ss: 0.057431\n","\tRotated_Epoch:09 [002/005 (0480/0755)]\tLoss Ss: 0.068810\n","\tRotated_Epoch:09 [002/005 (0500/0755)]\tLoss Ss: 0.084180\n","\tRotated_Epoch:09 [002/005 (0520/0755)]\tLoss Ss: 0.066454\n","\tRotated_Epoch:09 [002/005 (0540/0755)]\tLoss Ss: 0.058319\n","\tRotated_Epoch:09 [002/005 (0560/0755)]\tLoss Ss: 0.082239\n","\tRotated_Epoch:09 [002/005 (0580/0755)]\tLoss Ss: 0.047557\n","\tRotated_Epoch:09 [002/005 (0600/0755)]\tLoss Ss: 0.054389\n","\tRotated_Epoch:09 [002/005 (0620/0755)]\tLoss Ss: 0.075662\n","\tRotated_Epoch:09 [002/005 (0640/0755)]\tLoss Ss: 0.059666\n","\tRotated_Epoch:09 [002/005 (0660/0755)]\tLoss Ss: 0.077268\n","\tRotated_Epoch:09 [002/005 (0680/0755)]\tLoss Ss: 0.062925\n","\tRotated_Epoch:09 [002/005 (0700/0755)]\tLoss Ss: 0.083989\n","\tRotated_Epoch:09 [002/005 (0720/0755)]\tLoss Ss: 0.064672\n","\tRotated_Epoch:09 [002/005 (0740/0755)]\tLoss Ss: 0.062890\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tRotated_Epoch:09 [003/005 (0000/0693)]\tLoss Ss: 0.272315\n","\tRotated_Epoch:09 [003/005 (0020/0693)]\tLoss Ss: 0.203968\n","\tRotated_Epoch:09 [003/005 (0040/0693)]\tLoss Ss: 0.119334\n","\tRotated_Epoch:09 [003/005 (0060/0693)]\tLoss Ss: 0.042565\n","\tRotated_Epoch:09 [003/005 (0080/0693)]\tLoss Ss: 0.035059\n","\tRotated_Epoch:09 [003/005 (0100/0693)]\tLoss Ss: 0.045420\n","\tRotated_Epoch:09 [003/005 (0120/0693)]\tLoss Ss: 0.045050\n","\tRotated_Epoch:09 [003/005 (0140/0693)]\tLoss Ss: 0.041713\n","\tRotated_Epoch:09 [003/005 (0160/0693)]\tLoss Ss: 0.036142\n","\tRotated_Epoch:09 [003/005 (0180/0693)]\tLoss Ss: 0.042347\n","\tRotated_Epoch:09 [003/005 (0200/0693)]\tLoss Ss: 0.039149\n","\tRotated_Epoch:09 [003/005 (0220/0693)]\tLoss Ss: 0.050894\n","\tRotated_Epoch:09 [003/005 (0240/0693)]\tLoss Ss: 0.036401\n","\tRotated_Epoch:09 [003/005 (0260/0693)]\tLoss Ss: 0.032612\n","\tRotated_Epoch:09 [003/005 (0280/0693)]\tLoss Ss: 0.032968\n","\tRotated_Epoch:09 [003/005 (0300/0693)]\tLoss Ss: 0.037137\n","\tRotated_Epoch:09 [003/005 (0320/0693)]\tLoss Ss: 0.034905\n","\tRotated_Epoch:09 [003/005 (0340/0693)]\tLoss Ss: 0.039387\n","\tRotated_Epoch:09 [003/005 (0360/0693)]\tLoss Ss: 0.033619\n","\tRotated_Epoch:09 [003/005 (0380/0693)]\tLoss Ss: 0.027571\n","\tRotated_Epoch:09 [003/005 (0400/0693)]\tLoss Ss: 0.029304\n","\tRotated_Epoch:09 [003/005 (0420/0693)]\tLoss Ss: 0.026126\n","\tRotated_Epoch:09 [003/005 (0440/0693)]\tLoss Ss: 0.031371\n","\tRotated_Epoch:09 [003/005 (0460/0693)]\tLoss Ss: 0.027947\n","\tRotated_Epoch:09 [003/005 (0480/0693)]\tLoss Ss: 0.030421\n","\tRotated_Epoch:09 [003/005 (0500/0693)]\tLoss Ss: 0.023358\n","\tRotated_Epoch:09 [003/005 (0520/0693)]\tLoss Ss: 0.029762\n","\tRotated_Epoch:09 [003/005 (0540/0693)]\tLoss Ss: 0.023160\n","\tRotated_Epoch:09 [003/005 (0560/0693)]\tLoss Ss: 0.025985\n","\tRotated_Epoch:09 [003/005 (0580/0693)]\tLoss Ss: 0.026359\n","\tRotated_Epoch:09 [003/005 (0600/0693)]\tLoss Ss: 0.023592\n","\tRotated_Epoch:09 [003/005 (0620/0693)]\tLoss Ss: 0.033307\n","\tRotated_Epoch:09 [003/005 (0640/0693)]\tLoss Ss: 0.025190\n","\tRotated_Epoch:09 [003/005 (0660/0693)]\tLoss Ss: 0.016577\n","\tRotated_Epoch:09 [003/005 (0680/0693)]\tLoss Ss: 0.019124\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:09 [004/005 (0000/0755)]\tLoss Ss: 0.053391\n","\tRotated_Epoch:09 [004/005 (0020/0755)]\tLoss Ss: 0.047107\n","\tRotated_Epoch:09 [004/005 (0040/0755)]\tLoss Ss: 0.042041\n","\tRotated_Epoch:09 [004/005 (0060/0755)]\tLoss Ss: 0.063839\n","\tRotated_Epoch:09 [004/005 (0080/0755)]\tLoss Ss: 0.047299\n","\tRotated_Epoch:09 [004/005 (0100/0755)]\tLoss Ss: 0.028345\n","\tRotated_Epoch:09 [004/005 (0120/0755)]\tLoss Ss: 0.038270\n","\tRotated_Epoch:09 [004/005 (0140/0755)]\tLoss Ss: 0.044297\n","\tRotated_Epoch:09 [004/005 (0160/0755)]\tLoss Ss: 0.057081\n","\tRotated_Epoch:09 [004/005 (0180/0755)]\tLoss Ss: 0.031019\n","\tRotated_Epoch:09 [004/005 (0200/0755)]\tLoss Ss: 0.039381\n","\tRotated_Epoch:09 [004/005 (0220/0755)]\tLoss Ss: 0.022098\n","\tRotated_Epoch:09 [004/005 (0240/0755)]\tLoss Ss: 0.040220\n","\tRotated_Epoch:09 [004/005 (0260/0755)]\tLoss Ss: 0.027296\n","\tRotated_Epoch:09 [004/005 (0280/0755)]\tLoss Ss: 0.031565\n","\tRotated_Epoch:09 [004/005 (0300/0755)]\tLoss Ss: 0.026041\n","\tRotated_Epoch:09 [004/005 (0320/0755)]\tLoss Ss: 0.023660\n","\tRotated_Epoch:09 [004/005 (0340/0755)]\tLoss Ss: 0.023385\n","\tRotated_Epoch:09 [004/005 (0360/0755)]\tLoss Ss: 0.033052\n","\tRotated_Epoch:09 [004/005 (0380/0755)]\tLoss Ss: 0.022169\n","\tRotated_Epoch:09 [004/005 (0400/0755)]\tLoss Ss: 0.033633\n","\tRotated_Epoch:09 [004/005 (0420/0755)]\tLoss Ss: 0.034254\n","\tRotated_Epoch:09 [004/005 (0440/0755)]\tLoss Ss: 0.033683\n","\tRotated_Epoch:09 [004/005 (0460/0755)]\tLoss Ss: 0.021504\n","\tRotated_Epoch:09 [004/005 (0480/0755)]\tLoss Ss: 0.020336\n","\tRotated_Epoch:09 [004/005 (0500/0755)]\tLoss Ss: 0.023219\n","\tRotated_Epoch:09 [004/005 (0520/0755)]\tLoss Ss: 0.020186\n","\tRotated_Epoch:09 [004/005 (0540/0755)]\tLoss Ss: 0.024675\n","\tRotated_Epoch:09 [004/005 (0560/0755)]\tLoss Ss: 0.025168\n","\tRotated_Epoch:09 [004/005 (0580/0755)]\tLoss Ss: 0.016024\n","\tRotated_Epoch:09 [004/005 (0600/0755)]\tLoss Ss: 0.023321\n","\tRotated_Epoch:09 [004/005 (0620/0755)]\tLoss Ss: 0.020806\n","\tRotated_Epoch:09 [004/005 (0640/0755)]\tLoss Ss: 0.025198\n","\tRotated_Epoch:09 [004/005 (0660/0755)]\tLoss Ss: 0.026669\n","\tRotated_Epoch:09 [004/005 (0680/0755)]\tLoss Ss: 0.017228\n","\tRotated_Epoch:09 [004/005 (0700/0755)]\tLoss Ss: 0.022811\n","\tRotated_Epoch:09 [004/005 (0720/0755)]\tLoss Ss: 0.016248\n","\tRotated_Epoch:09 [004/005 (0740/0755)]\tLoss Ss: 0.025429\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tRotated_Epoch:09 [005/005 (0000/0693)]\tLoss Ss: 0.015842\n","\tRotated_Epoch:09 [005/005 (0020/0693)]\tLoss Ss: 0.019455\n","\tRotated_Epoch:09 [005/005 (0040/0693)]\tLoss Ss: 0.015347\n","\tRotated_Epoch:09 [005/005 (0060/0693)]\tLoss Ss: 0.030658\n","\tRotated_Epoch:09 [005/005 (0080/0693)]\tLoss Ss: 0.026952\n","\tRotated_Epoch:09 [005/005 (0100/0693)]\tLoss Ss: 0.019035\n","\tRotated_Epoch:09 [005/005 (0120/0693)]\tLoss Ss: 0.018892\n","\tRotated_Epoch:09 [005/005 (0140/0693)]\tLoss Ss: 0.023810\n","\tRotated_Epoch:09 [005/005 (0160/0693)]\tLoss Ss: 0.015349\n","\tRotated_Epoch:09 [005/005 (0180/0693)]\tLoss Ss: 0.021751\n","\tRotated_Epoch:09 [005/005 (0200/0693)]\tLoss Ss: 0.020817\n","\tRotated_Epoch:09 [005/005 (0220/0693)]\tLoss Ss: 0.021736\n","\tRotated_Epoch:09 [005/005 (0240/0693)]\tLoss Ss: 0.024384\n","\tRotated_Epoch:09 [005/005 (0260/0693)]\tLoss Ss: 0.021037\n","\tRotated_Epoch:09 [005/005 (0280/0693)]\tLoss Ss: 0.019044\n","\tRotated_Epoch:09 [005/005 (0300/0693)]\tLoss Ss: 0.025817\n","\tRotated_Epoch:09 [005/005 (0320/0693)]\tLoss Ss: 0.019264\n","\tRotated_Epoch:09 [005/005 (0340/0693)]\tLoss Ss: 0.019559\n","\tRotated_Epoch:09 [005/005 (0360/0693)]\tLoss Ss: 0.021307\n","\tRotated_Epoch:09 [005/005 (0380/0693)]\tLoss Ss: 0.043582\n","\tRotated_Epoch:09 [005/005 (0400/0693)]\tLoss Ss: 0.014273\n","\tRotated_Epoch:09 [005/005 (0420/0693)]\tLoss Ss: 0.014914\n","\tRotated_Epoch:09 [005/005 (0440/0693)]\tLoss Ss: 0.017773\n","\tRotated_Epoch:09 [005/005 (0460/0693)]\tLoss Ss: 0.022321\n","\tRotated_Epoch:09 [005/005 (0480/0693)]\tLoss Ss: 0.020524\n","\tRotated_Epoch:09 [005/005 (0500/0693)]\tLoss Ss: 0.014860\n","\tRotated_Epoch:09 [005/005 (0520/0693)]\tLoss Ss: 0.021697\n","\tRotated_Epoch:09 [005/005 (0540/0693)]\tLoss Ss: 0.022415\n","\tRotated_Epoch:09 [005/005 (0560/0693)]\tLoss Ss: 0.016193\n","\tRotated_Epoch:09 [005/005 (0580/0693)]\tLoss Ss: 0.019128\n","\tRotated_Epoch:09 [005/005 (0600/0693)]\tLoss Ss: 0.019115\n","\tRotated_Epoch:09 [005/005 (0620/0693)]\tLoss Ss: 0.018998\n","\tRotated_Epoch:09 [005/005 (0640/0693)]\tLoss Ss: 0.018221\n","\tRotated_Epoch:09 [005/005 (0660/0693)]\tLoss Ss: 0.017210\n","\tRotated_Epoch:09 [005/005 (0680/0693)]\tLoss Ss: 0.023169\n","new_1_6_d.nii\n","new_1_6_m.nii\n","new_1_7_d.nii\n","new_1_7_m.nii\n","new_2_5_d.nii\n","new_2_5_m.nii\n","new_2_6_d.nii\n","new_2_6_m.nii\n","new_2_7_d.nii\n","new_2_7_m.nii\n","new_3_d (2).nii\n","new_3_m (2).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (3).nii\n","new_3_m (3).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (7).nii\n","new_3_m (7).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (8).nii\n","new_3_m (8).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (9).nii\n","new_3_m (9).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_4_10_d_163526.nii\n","new_4_10_m_163526.nii.gz\n","new_4_11_d_084336.nii\n","new_4_11_m_084336.nii.gz\n","new_4_7_d_184422.nii\n","new_4_7_m_184422.nii.gz\n","new_4_8_d_143412.nii\n","new_4_8_m_143412.nii.gz\n","new_4_9_d_155347.nii\n","new_4_9_m_155347.nii.gz\n","\tEpoch: 9; Dice: 0.9398 +/- 0.0160; Loss: 14.9763\n","Begin Epoch 10\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tEpoch:10 [000/005 (0000/0588)]\tLoss Ss: 0.025793\n","\tEpoch:10 [000/005 (0020/0588)]\tLoss Ss: 0.036635\n","\tEpoch:10 [000/005 (0040/0588)]\tLoss Ss: 0.023051\n","\tEpoch:10 [000/005 (0060/0588)]\tLoss Ss: 0.022478\n","\tEpoch:10 [000/005 (0080/0588)]\tLoss Ss: 0.019909\n","\tEpoch:10 [000/005 (0100/0588)]\tLoss Ss: 0.022395\n","\tEpoch:10 [000/005 (0120/0588)]\tLoss Ss: 0.017223\n","\tEpoch:10 [000/005 (0140/0588)]\tLoss Ss: 0.015025\n","\tEpoch:10 [000/005 (0160/0588)]\tLoss Ss: 0.013543\n","\tEpoch:10 [000/005 (0180/0588)]\tLoss Ss: 0.021712\n","\tEpoch:10 [000/005 (0200/0588)]\tLoss Ss: 0.015684\n","\tEpoch:10 [000/005 (0220/0588)]\tLoss Ss: 0.016142\n","\tEpoch:10 [000/005 (0240/0588)]\tLoss Ss: 0.011539\n","\tEpoch:10 [000/005 (0260/0588)]\tLoss Ss: 0.010640\n","\tEpoch:10 [000/005 (0280/0588)]\tLoss Ss: 0.011801\n","\tEpoch:10 [000/005 (0300/0588)]\tLoss Ss: 0.012860\n","\tEpoch:10 [000/005 (0320/0588)]\tLoss Ss: 0.009592\n","\tEpoch:10 [000/005 (0340/0588)]\tLoss Ss: 0.013270\n","\tEpoch:10 [000/005 (0360/0588)]\tLoss Ss: 0.011910\n","\tEpoch:10 [000/005 (0380/0588)]\tLoss Ss: 0.011951\n","\tEpoch:10 [000/005 (0400/0588)]\tLoss Ss: 0.012459\n","\tEpoch:10 [000/005 (0420/0588)]\tLoss Ss: 0.009059\n","\tEpoch:10 [000/005 (0440/0588)]\tLoss Ss: 0.008181\n","\tEpoch:10 [000/005 (0460/0588)]\tLoss Ss: 0.009568\n","\tEpoch:10 [000/005 (0480/0588)]\tLoss Ss: 0.008599\n","\tEpoch:10 [000/005 (0500/0588)]\tLoss Ss: 0.010296\n","\tEpoch:10 [000/005 (0520/0588)]\tLoss Ss: 0.010333\n","\tEpoch:10 [000/005 (0540/0588)]\tLoss Ss: 0.008618\n","\tEpoch:10 [000/005 (0560/0588)]\tLoss Ss: 0.007860\n","\tEpoch:10 [000/005 (0580/0588)]\tLoss Ss: 0.009922\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tEpoch:10 [001/005 (0000/0693)]\tLoss Ss: 0.018727\n","\tEpoch:10 [001/005 (0020/0693)]\tLoss Ss: 0.020545\n","\tEpoch:10 [001/005 (0040/0693)]\tLoss Ss: 0.020168\n","\tEpoch:10 [001/005 (0060/0693)]\tLoss Ss: 0.023619\n","\tEpoch:10 [001/005 (0080/0693)]\tLoss Ss: 0.022406\n","\tEpoch:10 [001/005 (0100/0693)]\tLoss Ss: 0.020704\n","\tEpoch:10 [001/005 (0120/0693)]\tLoss Ss: 0.019244\n","\tEpoch:10 [001/005 (0140/0693)]\tLoss Ss: 0.016346\n","\tEpoch:10 [001/005 (0160/0693)]\tLoss Ss: 0.016129\n","\tEpoch:10 [001/005 (0180/0693)]\tLoss Ss: 0.018447\n","\tEpoch:10 [001/005 (0200/0693)]\tLoss Ss: 0.015087\n","\tEpoch:10 [001/005 (0220/0693)]\tLoss Ss: 0.017725\n","\tEpoch:10 [001/005 (0240/0693)]\tLoss Ss: 0.011547\n","\tEpoch:10 [001/005 (0260/0693)]\tLoss Ss: 0.012921\n","\tEpoch:10 [001/005 (0280/0693)]\tLoss Ss: 0.015107\n","\tEpoch:10 [001/005 (0300/0693)]\tLoss Ss: 0.021960\n","\tEpoch:10 [001/005 (0320/0693)]\tLoss Ss: 0.022926\n","\tEpoch:10 [001/005 (0340/0693)]\tLoss Ss: 0.014567\n","\tEpoch:10 [001/005 (0360/0693)]\tLoss Ss: 0.015857\n","\tEpoch:10 [001/005 (0380/0693)]\tLoss Ss: 0.023202\n","\tEpoch:10 [001/005 (0400/0693)]\tLoss Ss: 0.017191\n","\tEpoch:10 [001/005 (0420/0693)]\tLoss Ss: 0.022048\n","\tEpoch:10 [001/005 (0440/0693)]\tLoss Ss: 0.018341\n","\tEpoch:10 [001/005 (0460/0693)]\tLoss Ss: 0.017433\n","\tEpoch:10 [001/005 (0480/0693)]\tLoss Ss: 0.014013\n","\tEpoch:10 [001/005 (0500/0693)]\tLoss Ss: 0.011792\n","\tEpoch:10 [001/005 (0520/0693)]\tLoss Ss: 0.014117\n","\tEpoch:10 [001/005 (0540/0693)]\tLoss Ss: 0.012662\n","\tEpoch:10 [001/005 (0560/0693)]\tLoss Ss: 0.016155\n","\tEpoch:10 [001/005 (0580/0693)]\tLoss Ss: 0.014482\n","\tEpoch:10 [001/005 (0600/0693)]\tLoss Ss: 0.016141\n","\tEpoch:10 [001/005 (0620/0693)]\tLoss Ss: 0.013527\n","\tEpoch:10 [001/005 (0640/0693)]\tLoss Ss: 0.013913\n","\tEpoch:10 [001/005 (0660/0693)]\tLoss Ss: 0.014301\n","\tEpoch:10 [001/005 (0680/0693)]\tLoss Ss: 0.020880\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tEpoch:10 [002/005 (0000/0614)]\tLoss Ss: 0.026382\n","\tEpoch:10 [002/005 (0020/0614)]\tLoss Ss: 0.047598\n","\tEpoch:10 [002/005 (0040/0614)]\tLoss Ss: 0.009983\n","\tEpoch:10 [002/005 (0060/0614)]\tLoss Ss: 0.014427\n","\tEpoch:10 [002/005 (0080/0614)]\tLoss Ss: 0.011785\n","\tEpoch:10 [002/005 (0100/0614)]\tLoss Ss: 0.009761\n","\tEpoch:10 [002/005 (0120/0614)]\tLoss Ss: 0.012274\n","\tEpoch:10 [002/005 (0140/0614)]\tLoss Ss: 0.013127\n","\tEpoch:10 [002/005 (0160/0614)]\tLoss Ss: 0.009846\n","\tEpoch:10 [002/005 (0180/0614)]\tLoss Ss: 0.011558\n","\tEpoch:10 [002/005 (0200/0614)]\tLoss Ss: 0.010931\n","\tEpoch:10 [002/005 (0220/0614)]\tLoss Ss: 0.009035\n","\tEpoch:10 [002/005 (0240/0614)]\tLoss Ss: 0.017896\n","\tEpoch:10 [002/005 (0260/0614)]\tLoss Ss: 0.012459\n","\tEpoch:10 [002/005 (0280/0614)]\tLoss Ss: 0.012231\n","\tEpoch:10 [002/005 (0300/0614)]\tLoss Ss: 0.010910\n","\tEpoch:10 [002/005 (0320/0614)]\tLoss Ss: 0.009920\n","\tEpoch:10 [002/005 (0340/0614)]\tLoss Ss: 0.008236\n","\tEpoch:10 [002/005 (0360/0614)]\tLoss Ss: 0.012131\n","\tEpoch:10 [002/005 (0380/0614)]\tLoss Ss: 0.011962\n","\tEpoch:10 [002/005 (0400/0614)]\tLoss Ss: 0.006851\n","\tEpoch:10 [002/005 (0420/0614)]\tLoss Ss: 0.013147\n","\tEpoch:10 [002/005 (0440/0614)]\tLoss Ss: 0.010358\n","\tEpoch:10 [002/005 (0460/0614)]\tLoss Ss: 0.009077\n","\tEpoch:10 [002/005 (0480/0614)]\tLoss Ss: 0.008408\n","\tEpoch:10 [002/005 (0500/0614)]\tLoss Ss: 0.009628\n","\tEpoch:10 [002/005 (0520/0614)]\tLoss Ss: 0.009283\n","\tEpoch:10 [002/005 (0540/0614)]\tLoss Ss: 0.007182\n","\tEpoch:10 [002/005 (0560/0614)]\tLoss Ss: 0.011115\n","\tEpoch:10 [002/005 (0580/0614)]\tLoss Ss: 0.013061\n","\tEpoch:10 [002/005 (0600/0614)]\tLoss Ss: 0.008473\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:10 [003/005 (0000/0755)]\tLoss Ss: 0.023958\n","\tEpoch:10 [003/005 (0020/0755)]\tLoss Ss: 0.015862\n","\tEpoch:10 [003/005 (0040/0755)]\tLoss Ss: 0.019809\n","\tEpoch:10 [003/005 (0060/0755)]\tLoss Ss: 0.031416\n","\tEpoch:10 [003/005 (0080/0755)]\tLoss Ss: 0.009259\n","\tEpoch:10 [003/005 (0100/0755)]\tLoss Ss: 0.015354\n","\tEpoch:10 [003/005 (0120/0755)]\tLoss Ss: 0.027802\n","\tEpoch:10 [003/005 (0140/0755)]\tLoss Ss: 0.020272\n","\tEpoch:10 [003/005 (0160/0755)]\tLoss Ss: 0.021289\n","\tEpoch:10 [003/005 (0180/0755)]\tLoss Ss: 0.019662\n","\tEpoch:10 [003/005 (0200/0755)]\tLoss Ss: 0.022113\n","\tEpoch:10 [003/005 (0220/0755)]\tLoss Ss: 0.017216\n","\tEpoch:10 [003/005 (0240/0755)]\tLoss Ss: 0.019741\n","\tEpoch:10 [003/005 (0260/0755)]\tLoss Ss: 0.015614\n","\tEpoch:10 [003/005 (0280/0755)]\tLoss Ss: 0.024206\n","\tEpoch:10 [003/005 (0300/0755)]\tLoss Ss: 0.017139\n","\tEpoch:10 [003/005 (0320/0755)]\tLoss Ss: 0.020704\n","\tEpoch:10 [003/005 (0340/0755)]\tLoss Ss: 0.013437\n","\tEpoch:10 [003/005 (0360/0755)]\tLoss Ss: 0.022314\n","\tEpoch:10 [003/005 (0380/0755)]\tLoss Ss: 0.013066\n","\tEpoch:10 [003/005 (0400/0755)]\tLoss Ss: 0.016518\n","\tEpoch:10 [003/005 (0420/0755)]\tLoss Ss: 0.014831\n","\tEpoch:10 [003/005 (0440/0755)]\tLoss Ss: 0.024535\n","\tEpoch:10 [003/005 (0460/0755)]\tLoss Ss: 0.013297\n","\tEpoch:10 [003/005 (0480/0755)]\tLoss Ss: 0.019246\n","\tEpoch:10 [003/005 (0500/0755)]\tLoss Ss: 0.021289\n","\tEpoch:10 [003/005 (0520/0755)]\tLoss Ss: 0.020196\n","\tEpoch:10 [003/005 (0540/0755)]\tLoss Ss: 0.020042\n","\tEpoch:10 [003/005 (0560/0755)]\tLoss Ss: 0.020685\n","\tEpoch:10 [003/005 (0580/0755)]\tLoss Ss: 0.017634\n","\tEpoch:10 [003/005 (0600/0755)]\tLoss Ss: 0.015352\n","\tEpoch:10 [003/005 (0620/0755)]\tLoss Ss: 0.015211\n","\tEpoch:10 [003/005 (0640/0755)]\tLoss Ss: 0.016945\n","\tEpoch:10 [003/005 (0660/0755)]\tLoss Ss: 0.016332\n","\tEpoch:10 [003/005 (0680/0755)]\tLoss Ss: 0.014773\n","\tEpoch:10 [003/005 (0700/0755)]\tLoss Ss: 0.017341\n","\tEpoch:10 [003/005 (0720/0755)]\tLoss Ss: 0.018736\n","\tEpoch:10 [003/005 (0740/0755)]\tLoss Ss: 0.018616\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:10 [004/005 (0000/0755)]\tLoss Ss: 0.023155\n","\tEpoch:10 [004/005 (0020/0755)]\tLoss Ss: 0.032571\n","\tEpoch:10 [004/005 (0040/0755)]\tLoss Ss: 0.022026\n","\tEpoch:10 [004/005 (0060/0755)]\tLoss Ss: 0.027758\n","\tEpoch:10 [004/005 (0080/0755)]\tLoss Ss: 0.033467\n","\tEpoch:10 [004/005 (0100/0755)]\tLoss Ss: 0.019319\n","\tEpoch:10 [004/005 (0120/0755)]\tLoss Ss: 0.029856\n","\tEpoch:10 [004/005 (0140/0755)]\tLoss Ss: 0.019561\n","\tEpoch:10 [004/005 (0160/0755)]\tLoss Ss: 0.015126\n","\tEpoch:10 [004/005 (0180/0755)]\tLoss Ss: 0.020130\n","\tEpoch:10 [004/005 (0200/0755)]\tLoss Ss: 0.020240\n","\tEpoch:10 [004/005 (0220/0755)]\tLoss Ss: 0.016360\n","\tEpoch:10 [004/005 (0240/0755)]\tLoss Ss: 0.029629\n","\tEpoch:10 [004/005 (0260/0755)]\tLoss Ss: 0.029123\n","\tEpoch:10 [004/005 (0280/0755)]\tLoss Ss: 0.018253\n","\tEpoch:10 [004/005 (0300/0755)]\tLoss Ss: 0.020511\n","\tEpoch:10 [004/005 (0320/0755)]\tLoss Ss: 0.016644\n","\tEpoch:10 [004/005 (0340/0755)]\tLoss Ss: 0.014330\n","\tEpoch:10 [004/005 (0360/0755)]\tLoss Ss: 0.011304\n","\tEpoch:10 [004/005 (0380/0755)]\tLoss Ss: 0.020187\n","\tEpoch:10 [004/005 (0400/0755)]\tLoss Ss: 0.015034\n","\tEpoch:10 [004/005 (0420/0755)]\tLoss Ss: 0.024142\n","\tEpoch:10 [004/005 (0440/0755)]\tLoss Ss: 0.012883\n","\tEpoch:10 [004/005 (0460/0755)]\tLoss Ss: 0.022785\n","\tEpoch:10 [004/005 (0480/0755)]\tLoss Ss: 0.018222\n","\tEpoch:10 [004/005 (0500/0755)]\tLoss Ss: 0.019970\n","\tEpoch:10 [004/005 (0520/0755)]\tLoss Ss: 0.021028\n","\tEpoch:10 [004/005 (0540/0755)]\tLoss Ss: 0.022366\n","\tEpoch:10 [004/005 (0560/0755)]\tLoss Ss: 0.016770\n","\tEpoch:10 [004/005 (0580/0755)]\tLoss Ss: 0.014162\n","\tEpoch:10 [004/005 (0600/0755)]\tLoss Ss: 0.018751\n","\tEpoch:10 [004/005 (0620/0755)]\tLoss Ss: 0.018282\n","\tEpoch:10 [004/005 (0640/0755)]\tLoss Ss: 0.022768\n","\tEpoch:10 [004/005 (0660/0755)]\tLoss Ss: 0.018326\n","\tEpoch:10 [004/005 (0680/0755)]\tLoss Ss: 0.022305\n","\tEpoch:10 [004/005 (0700/0755)]\tLoss Ss: 0.013576\n","\tEpoch:10 [004/005 (0720/0755)]\tLoss Ss: 0.017515\n","\tEpoch:10 [004/005 (0740/0755)]\tLoss Ss: 0.019559\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tEpoch:10 [005/005 (0000/0693)]\tLoss Ss: 0.023598\n","\tEpoch:10 [005/005 (0020/0693)]\tLoss Ss: 0.040994\n","\tEpoch:10 [005/005 (0040/0693)]\tLoss Ss: 0.026312\n","\tEpoch:10 [005/005 (0060/0693)]\tLoss Ss: 0.033515\n","\tEpoch:10 [005/005 (0080/0693)]\tLoss Ss: 0.021218\n","\tEpoch:10 [005/005 (0100/0693)]\tLoss Ss: 0.019189\n","\tEpoch:10 [005/005 (0120/0693)]\tLoss Ss: 0.016300\n","\tEpoch:10 [005/005 (0140/0693)]\tLoss Ss: 0.020552\n","\tEpoch:10 [005/005 (0160/0693)]\tLoss Ss: 0.015406\n","\tEpoch:10 [005/005 (0180/0693)]\tLoss Ss: 0.026155\n","\tEpoch:10 [005/005 (0200/0693)]\tLoss Ss: 0.015074\n","\tEpoch:10 [005/005 (0220/0693)]\tLoss Ss: 0.035146\n","\tEpoch:10 [005/005 (0240/0693)]\tLoss Ss: 0.013557\n","\tEpoch:10 [005/005 (0260/0693)]\tLoss Ss: 0.016247\n","\tEpoch:10 [005/005 (0280/0693)]\tLoss Ss: 0.016586\n","\tEpoch:10 [005/005 (0300/0693)]\tLoss Ss: 0.024731\n","\tEpoch:10 [005/005 (0320/0693)]\tLoss Ss: 0.020438\n","\tEpoch:10 [005/005 (0340/0693)]\tLoss Ss: 0.012283\n","\tEpoch:10 [005/005 (0360/0693)]\tLoss Ss: 0.019433\n","\tEpoch:10 [005/005 (0380/0693)]\tLoss Ss: 0.014037\n","\tEpoch:10 [005/005 (0400/0693)]\tLoss Ss: 0.019049\n","\tEpoch:10 [005/005 (0420/0693)]\tLoss Ss: 0.014367\n","\tEpoch:10 [005/005 (0440/0693)]\tLoss Ss: 0.013468\n","\tEpoch:10 [005/005 (0460/0693)]\tLoss Ss: 0.017184\n","\tEpoch:10 [005/005 (0480/0693)]\tLoss Ss: 0.019515\n","\tEpoch:10 [005/005 (0500/0693)]\tLoss Ss: 0.017457\n","\tEpoch:10 [005/005 (0520/0693)]\tLoss Ss: 0.015460\n","\tEpoch:10 [005/005 (0540/0693)]\tLoss Ss: 0.017983\n","\tEpoch:10 [005/005 (0560/0693)]\tLoss Ss: 0.019492\n","\tEpoch:10 [005/005 (0580/0693)]\tLoss Ss: 0.011941\n","\tEpoch:10 [005/005 (0600/0693)]\tLoss Ss: 0.020670\n","\tEpoch:10 [005/005 (0620/0693)]\tLoss Ss: 0.011198\n","\tEpoch:10 [005/005 (0640/0693)]\tLoss Ss: 0.011232\n","\tEpoch:10 [005/005 (0660/0693)]\tLoss Ss: 0.015756\n","\tEpoch:10 [005/005 (0680/0693)]\tLoss Ss: 0.012793\n","Now train the rotated image\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:10 [000/005 (0000/0755)]\tLoss Ss: 0.417845\n","\tRotated_Epoch:10 [000/005 (0020/0755)]\tLoss Ss: 0.260892\n","\tRotated_Epoch:10 [000/005 (0040/0755)]\tLoss Ss: 0.378929\n","\tRotated_Epoch:10 [000/005 (0060/0755)]\tLoss Ss: 0.433753\n","\tRotated_Epoch:10 [000/005 (0080/0755)]\tLoss Ss: 0.306163\n","\tRotated_Epoch:10 [000/005 (0100/0755)]\tLoss Ss: 0.266421\n","\tRotated_Epoch:10 [000/005 (0120/0755)]\tLoss Ss: 0.264689\n","\tRotated_Epoch:10 [000/005 (0140/0755)]\tLoss Ss: 0.352486\n","\tRotated_Epoch:10 [000/005 (0160/0755)]\tLoss Ss: 0.292854\n","\tRotated_Epoch:10 [000/005 (0180/0755)]\tLoss Ss: 0.197790\n","\tRotated_Epoch:10 [000/005 (0200/0755)]\tLoss Ss: 0.108801\n","\tRotated_Epoch:10 [000/005 (0220/0755)]\tLoss Ss: 0.243152\n","\tRotated_Epoch:10 [000/005 (0240/0755)]\tLoss Ss: 0.200917\n","\tRotated_Epoch:10 [000/005 (0260/0755)]\tLoss Ss: 0.146322\n","\tRotated_Epoch:10 [000/005 (0280/0755)]\tLoss Ss: 0.137371\n","\tRotated_Epoch:10 [000/005 (0300/0755)]\tLoss Ss: 0.096511\n","\tRotated_Epoch:10 [000/005 (0320/0755)]\tLoss Ss: 0.144863\n","\tRotated_Epoch:10 [000/005 (0340/0755)]\tLoss Ss: 0.138057\n","\tRotated_Epoch:10 [000/005 (0360/0755)]\tLoss Ss: 0.153505\n","\tRotated_Epoch:10 [000/005 (0380/0755)]\tLoss Ss: 0.107156\n","\tRotated_Epoch:10 [000/005 (0400/0755)]\tLoss Ss: 0.127347\n","\tRotated_Epoch:10 [000/005 (0420/0755)]\tLoss Ss: 0.137618\n","\tRotated_Epoch:10 [000/005 (0440/0755)]\tLoss Ss: 0.102731\n","\tRotated_Epoch:10 [000/005 (0460/0755)]\tLoss Ss: 0.068285\n","\tRotated_Epoch:10 [000/005 (0480/0755)]\tLoss Ss: 0.104382\n","\tRotated_Epoch:10 [000/005 (0500/0755)]\tLoss Ss: 0.162957\n","\tRotated_Epoch:10 [000/005 (0520/0755)]\tLoss Ss: 0.074878\n","\tRotated_Epoch:10 [000/005 (0540/0755)]\tLoss Ss: 0.109249\n","\tRotated_Epoch:10 [000/005 (0560/0755)]\tLoss Ss: 0.132596\n","\tRotated_Epoch:10 [000/005 (0580/0755)]\tLoss Ss: 0.122359\n","\tRotated_Epoch:10 [000/005 (0600/0755)]\tLoss Ss: 0.105037\n","\tRotated_Epoch:10 [000/005 (0620/0755)]\tLoss Ss: 0.096062\n","\tRotated_Epoch:10 [000/005 (0640/0755)]\tLoss Ss: 0.076648\n","\tRotated_Epoch:10 [000/005 (0660/0755)]\tLoss Ss: 0.086346\n","\tRotated_Epoch:10 [000/005 (0680/0755)]\tLoss Ss: 0.079178\n","\tRotated_Epoch:10 [000/005 (0700/0755)]\tLoss Ss: 0.074368\n","\tRotated_Epoch:10 [000/005 (0720/0755)]\tLoss Ss: 0.117757\n","\tRotated_Epoch:10 [000/005 (0740/0755)]\tLoss Ss: 0.073333\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tRotated_Epoch:10 [001/005 (0000/0588)]\tLoss Ss: 0.149523\n","\tRotated_Epoch:10 [001/005 (0020/0588)]\tLoss Ss: 0.124348\n","\tRotated_Epoch:10 [001/005 (0040/0588)]\tLoss Ss: 0.135560\n","\tRotated_Epoch:10 [001/005 (0060/0588)]\tLoss Ss: 0.099345\n","\tRotated_Epoch:10 [001/005 (0080/0588)]\tLoss Ss: 0.087662\n","\tRotated_Epoch:10 [001/005 (0100/0588)]\tLoss Ss: 0.096199\n","\tRotated_Epoch:10 [001/005 (0120/0588)]\tLoss Ss: 0.068300\n","\tRotated_Epoch:10 [001/005 (0140/0588)]\tLoss Ss: 0.074334\n","\tRotated_Epoch:10 [001/005 (0160/0588)]\tLoss Ss: 0.087172\n","\tRotated_Epoch:10 [001/005 (0180/0588)]\tLoss Ss: 0.126416\n","\tRotated_Epoch:10 [001/005 (0200/0588)]\tLoss Ss: 0.119493\n","\tRotated_Epoch:10 [001/005 (0220/0588)]\tLoss Ss: 0.081603\n","\tRotated_Epoch:10 [001/005 (0240/0588)]\tLoss Ss: 0.068029\n","\tRotated_Epoch:10 [001/005 (0260/0588)]\tLoss Ss: 0.086571\n","\tRotated_Epoch:10 [001/005 (0280/0588)]\tLoss Ss: 0.081533\n","\tRotated_Epoch:10 [001/005 (0300/0588)]\tLoss Ss: 0.065785\n","\tRotated_Epoch:10 [001/005 (0320/0588)]\tLoss Ss: 0.074762\n","\tRotated_Epoch:10 [001/005 (0340/0588)]\tLoss Ss: 0.080804\n","\tRotated_Epoch:10 [001/005 (0360/0588)]\tLoss Ss: 0.064677\n","\tRotated_Epoch:10 [001/005 (0380/0588)]\tLoss Ss: 0.055459\n","\tRotated_Epoch:10 [001/005 (0400/0588)]\tLoss Ss: 0.089728\n","\tRotated_Epoch:10 [001/005 (0420/0588)]\tLoss Ss: 0.071089\n","\tRotated_Epoch:10 [001/005 (0440/0588)]\tLoss Ss: 0.083551\n","\tRotated_Epoch:10 [001/005 (0460/0588)]\tLoss Ss: 0.055951\n","\tRotated_Epoch:10 [001/005 (0480/0588)]\tLoss Ss: 0.073406\n","\tRotated_Epoch:10 [001/005 (0500/0588)]\tLoss Ss: 0.061655\n","\tRotated_Epoch:10 [001/005 (0520/0588)]\tLoss Ss: 0.134366\n","\tRotated_Epoch:10 [001/005 (0540/0588)]\tLoss Ss: 0.072261\n","\tRotated_Epoch:10 [001/005 (0560/0588)]\tLoss Ss: 0.064379\n","\tRotated_Epoch:10 [001/005 (0580/0588)]\tLoss Ss: 0.060577\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tRotated_Epoch:10 [002/005 (0000/0614)]\tLoss Ss: 0.148358\n","\tRotated_Epoch:10 [002/005 (0020/0614)]\tLoss Ss: 0.160241\n","\tRotated_Epoch:10 [002/005 (0040/0614)]\tLoss Ss: 0.126024\n","\tRotated_Epoch:10 [002/005 (0060/0614)]\tLoss Ss: 0.088351\n","\tRotated_Epoch:10 [002/005 (0080/0614)]\tLoss Ss: 0.054907\n","\tRotated_Epoch:10 [002/005 (0100/0614)]\tLoss Ss: 0.043002\n","\tRotated_Epoch:10 [002/005 (0120/0614)]\tLoss Ss: 0.049670\n","\tRotated_Epoch:10 [002/005 (0140/0614)]\tLoss Ss: 0.032933\n","\tRotated_Epoch:10 [002/005 (0160/0614)]\tLoss Ss: 0.034235\n","\tRotated_Epoch:10 [002/005 (0180/0614)]\tLoss Ss: 0.036254\n","\tRotated_Epoch:10 [002/005 (0200/0614)]\tLoss Ss: 0.037514\n","\tRotated_Epoch:10 [002/005 (0220/0614)]\tLoss Ss: 0.033862\n","\tRotated_Epoch:10 [002/005 (0240/0614)]\tLoss Ss: 0.033143\n","\tRotated_Epoch:10 [002/005 (0260/0614)]\tLoss Ss: 0.037225\n","\tRotated_Epoch:10 [002/005 (0280/0614)]\tLoss Ss: 0.027868\n","\tRotated_Epoch:10 [002/005 (0300/0614)]\tLoss Ss: 0.032731\n","\tRotated_Epoch:10 [002/005 (0320/0614)]\tLoss Ss: 0.028828\n","\tRotated_Epoch:10 [002/005 (0340/0614)]\tLoss Ss: 0.026629\n","\tRotated_Epoch:10 [002/005 (0360/0614)]\tLoss Ss: 0.019160\n","\tRotated_Epoch:10 [002/005 (0380/0614)]\tLoss Ss: 0.025622\n","\tRotated_Epoch:10 [002/005 (0400/0614)]\tLoss Ss: 0.025840\n","\tRotated_Epoch:10 [002/005 (0420/0614)]\tLoss Ss: 0.026178\n","\tRotated_Epoch:10 [002/005 (0440/0614)]\tLoss Ss: 0.020539\n","\tRotated_Epoch:10 [002/005 (0460/0614)]\tLoss Ss: 0.022112\n","\tRotated_Epoch:10 [002/005 (0480/0614)]\tLoss Ss: 0.015276\n","\tRotated_Epoch:10 [002/005 (0500/0614)]\tLoss Ss: 0.016757\n","\tRotated_Epoch:10 [002/005 (0520/0614)]\tLoss Ss: 0.016177\n","\tRotated_Epoch:10 [002/005 (0540/0614)]\tLoss Ss: 0.016823\n","\tRotated_Epoch:10 [002/005 (0560/0614)]\tLoss Ss: 0.016704\n","\tRotated_Epoch:10 [002/005 (0580/0614)]\tLoss Ss: 0.018716\n","\tRotated_Epoch:10 [002/005 (0600/0614)]\tLoss Ss: 0.014299\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:10 [003/005 (0000/0755)]\tLoss Ss: 0.082587\n","\tRotated_Epoch:10 [003/005 (0020/0755)]\tLoss Ss: 0.067982\n","\tRotated_Epoch:10 [003/005 (0040/0755)]\tLoss Ss: 0.056971\n","\tRotated_Epoch:10 [003/005 (0060/0755)]\tLoss Ss: 0.046336\n","\tRotated_Epoch:10 [003/005 (0080/0755)]\tLoss Ss: 0.034125\n","\tRotated_Epoch:10 [003/005 (0100/0755)]\tLoss Ss: 0.081345\n","\tRotated_Epoch:10 [003/005 (0120/0755)]\tLoss Ss: 0.032901\n","\tRotated_Epoch:10 [003/005 (0140/0755)]\tLoss Ss: 0.039051\n","\tRotated_Epoch:10 [003/005 (0160/0755)]\tLoss Ss: 0.045212\n","\tRotated_Epoch:10 [003/005 (0180/0755)]\tLoss Ss: 0.027302\n","\tRotated_Epoch:10 [003/005 (0200/0755)]\tLoss Ss: 0.033729\n","\tRotated_Epoch:10 [003/005 (0220/0755)]\tLoss Ss: 0.044502\n","\tRotated_Epoch:10 [003/005 (0240/0755)]\tLoss Ss: 0.029092\n","\tRotated_Epoch:10 [003/005 (0260/0755)]\tLoss Ss: 0.039755\n","\tRotated_Epoch:10 [003/005 (0280/0755)]\tLoss Ss: 0.030393\n","\tRotated_Epoch:10 [003/005 (0300/0755)]\tLoss Ss: 0.041327\n","\tRotated_Epoch:10 [003/005 (0320/0755)]\tLoss Ss: 0.031223\n","\tRotated_Epoch:10 [003/005 (0340/0755)]\tLoss Ss: 0.038740\n","\tRotated_Epoch:10 [003/005 (0360/0755)]\tLoss Ss: 0.031493\n","\tRotated_Epoch:10 [003/005 (0380/0755)]\tLoss Ss: 0.030468\n","\tRotated_Epoch:10 [003/005 (0400/0755)]\tLoss Ss: 0.036359\n","\tRotated_Epoch:10 [003/005 (0420/0755)]\tLoss Ss: 0.032142\n","\tRotated_Epoch:10 [003/005 (0440/0755)]\tLoss Ss: 0.029848\n","\tRotated_Epoch:10 [003/005 (0460/0755)]\tLoss Ss: 0.031210\n","\tRotated_Epoch:10 [003/005 (0480/0755)]\tLoss Ss: 0.033899\n","\tRotated_Epoch:10 [003/005 (0500/0755)]\tLoss Ss: 0.033725\n","\tRotated_Epoch:10 [003/005 (0520/0755)]\tLoss Ss: 0.038484\n","\tRotated_Epoch:10 [003/005 (0540/0755)]\tLoss Ss: 0.024767\n","\tRotated_Epoch:10 [003/005 (0560/0755)]\tLoss Ss: 0.041313\n","\tRotated_Epoch:10 [003/005 (0580/0755)]\tLoss Ss: 0.035723\n","\tRotated_Epoch:10 [003/005 (0600/0755)]\tLoss Ss: 0.036113\n","\tRotated_Epoch:10 [003/005 (0620/0755)]\tLoss Ss: 0.031363\n","\tRotated_Epoch:10 [003/005 (0640/0755)]\tLoss Ss: 0.026404\n","\tRotated_Epoch:10 [003/005 (0660/0755)]\tLoss Ss: 0.034801\n","\tRotated_Epoch:10 [003/005 (0680/0755)]\tLoss Ss: 0.033160\n","\tRotated_Epoch:10 [003/005 (0700/0755)]\tLoss Ss: 0.036526\n","\tRotated_Epoch:10 [003/005 (0720/0755)]\tLoss Ss: 0.039919\n","\tRotated_Epoch:10 [003/005 (0740/0755)]\tLoss Ss: 0.027431\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tRotated_Epoch:10 [004/005 (0000/0693)]\tLoss Ss: 0.027484\n","\tRotated_Epoch:10 [004/005 (0020/0693)]\tLoss Ss: 0.029424\n","\tRotated_Epoch:10 [004/005 (0040/0693)]\tLoss Ss: 0.020508\n","\tRotated_Epoch:10 [004/005 (0060/0693)]\tLoss Ss: 0.024192\n","\tRotated_Epoch:10 [004/005 (0080/0693)]\tLoss Ss: 0.018759\n","\tRotated_Epoch:10 [004/005 (0100/0693)]\tLoss Ss: 0.021755\n","\tRotated_Epoch:10 [004/005 (0120/0693)]\tLoss Ss: 0.025070\n","\tRotated_Epoch:10 [004/005 (0140/0693)]\tLoss Ss: 0.023037\n","\tRotated_Epoch:10 [004/005 (0160/0693)]\tLoss Ss: 0.018299\n","\tRotated_Epoch:10 [004/005 (0180/0693)]\tLoss Ss: 0.015842\n","\tRotated_Epoch:10 [004/005 (0200/0693)]\tLoss Ss: 0.018191\n","\tRotated_Epoch:10 [004/005 (0220/0693)]\tLoss Ss: 0.022798\n","\tRotated_Epoch:10 [004/005 (0240/0693)]\tLoss Ss: 0.015974\n","\tRotated_Epoch:10 [004/005 (0260/0693)]\tLoss Ss: 0.023046\n","\tRotated_Epoch:10 [004/005 (0280/0693)]\tLoss Ss: 0.031383\n","\tRotated_Epoch:10 [004/005 (0300/0693)]\tLoss Ss: 0.016783\n","\tRotated_Epoch:10 [004/005 (0320/0693)]\tLoss Ss: 0.024969\n","\tRotated_Epoch:10 [004/005 (0340/0693)]\tLoss Ss: 0.023785\n","\tRotated_Epoch:10 [004/005 (0360/0693)]\tLoss Ss: 0.017946\n","\tRotated_Epoch:10 [004/005 (0380/0693)]\tLoss Ss: 0.017091\n","\tRotated_Epoch:10 [004/005 (0400/0693)]\tLoss Ss: 0.024207\n","\tRotated_Epoch:10 [004/005 (0420/0693)]\tLoss Ss: 0.021109\n","\tRotated_Epoch:10 [004/005 (0440/0693)]\tLoss Ss: 0.019523\n","\tRotated_Epoch:10 [004/005 (0460/0693)]\tLoss Ss: 0.014432\n","\tRotated_Epoch:10 [004/005 (0480/0693)]\tLoss Ss: 0.028717\n","\tRotated_Epoch:10 [004/005 (0500/0693)]\tLoss Ss: 0.016296\n","\tRotated_Epoch:10 [004/005 (0520/0693)]\tLoss Ss: 0.015279\n","\tRotated_Epoch:10 [004/005 (0540/0693)]\tLoss Ss: 0.023342\n","\tRotated_Epoch:10 [004/005 (0560/0693)]\tLoss Ss: 0.028767\n","\tRotated_Epoch:10 [004/005 (0580/0693)]\tLoss Ss: 0.016869\n","\tRotated_Epoch:10 [004/005 (0600/0693)]\tLoss Ss: 0.013319\n","\tRotated_Epoch:10 [004/005 (0620/0693)]\tLoss Ss: 0.023697\n","\tRotated_Epoch:10 [004/005 (0640/0693)]\tLoss Ss: 0.015212\n","\tRotated_Epoch:10 [004/005 (0660/0693)]\tLoss Ss: 0.018246\n","\tRotated_Epoch:10 [004/005 (0680/0693)]\tLoss Ss: 0.014928\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tRotated_Epoch:10 [005/005 (0000/0693)]\tLoss Ss: 0.025097\n","\tRotated_Epoch:10 [005/005 (0020/0693)]\tLoss Ss: 0.021568\n","\tRotated_Epoch:10 [005/005 (0040/0693)]\tLoss Ss: 0.020906\n","\tRotated_Epoch:10 [005/005 (0060/0693)]\tLoss Ss: 0.013648\n","\tRotated_Epoch:10 [005/005 (0080/0693)]\tLoss Ss: 0.026048\n","\tRotated_Epoch:10 [005/005 (0100/0693)]\tLoss Ss: 0.019388\n","\tRotated_Epoch:10 [005/005 (0120/0693)]\tLoss Ss: 0.024442\n","\tRotated_Epoch:10 [005/005 (0140/0693)]\tLoss Ss: 0.019178\n","\tRotated_Epoch:10 [005/005 (0160/0693)]\tLoss Ss: 0.018788\n","\tRotated_Epoch:10 [005/005 (0180/0693)]\tLoss Ss: 0.019259\n","\tRotated_Epoch:10 [005/005 (0200/0693)]\tLoss Ss: 0.022227\n","\tRotated_Epoch:10 [005/005 (0220/0693)]\tLoss Ss: 0.021501\n","\tRotated_Epoch:10 [005/005 (0240/0693)]\tLoss Ss: 0.017424\n","\tRotated_Epoch:10 [005/005 (0260/0693)]\tLoss Ss: 0.023808\n","\tRotated_Epoch:10 [005/005 (0280/0693)]\tLoss Ss: 0.017147\n","\tRotated_Epoch:10 [005/005 (0300/0693)]\tLoss Ss: 0.020158\n","\tRotated_Epoch:10 [005/005 (0320/0693)]\tLoss Ss: 0.020625\n","\tRotated_Epoch:10 [005/005 (0340/0693)]\tLoss Ss: 0.015945\n","\tRotated_Epoch:10 [005/005 (0360/0693)]\tLoss Ss: 0.017308\n","\tRotated_Epoch:10 [005/005 (0380/0693)]\tLoss Ss: 0.016472\n","\tRotated_Epoch:10 [005/005 (0400/0693)]\tLoss Ss: 0.016878\n","\tRotated_Epoch:10 [005/005 (0420/0693)]\tLoss Ss: 0.019158\n","\tRotated_Epoch:10 [005/005 (0440/0693)]\tLoss Ss: 0.015812\n","\tRotated_Epoch:10 [005/005 (0460/0693)]\tLoss Ss: 0.020828\n","\tRotated_Epoch:10 [005/005 (0480/0693)]\tLoss Ss: 0.016834\n","\tRotated_Epoch:10 [005/005 (0500/0693)]\tLoss Ss: 0.014070\n","\tRotated_Epoch:10 [005/005 (0520/0693)]\tLoss Ss: 0.017027\n","\tRotated_Epoch:10 [005/005 (0540/0693)]\tLoss Ss: 0.017628\n","\tRotated_Epoch:10 [005/005 (0560/0693)]\tLoss Ss: 0.018631\n","\tRotated_Epoch:10 [005/005 (0580/0693)]\tLoss Ss: 0.015359\n","\tRotated_Epoch:10 [005/005 (0600/0693)]\tLoss Ss: 0.018786\n","\tRotated_Epoch:10 [005/005 (0620/0693)]\tLoss Ss: 0.016122\n","\tRotated_Epoch:10 [005/005 (0640/0693)]\tLoss Ss: 0.018645\n","\tRotated_Epoch:10 [005/005 (0660/0693)]\tLoss Ss: 0.017860\n","\tRotated_Epoch:10 [005/005 (0680/0693)]\tLoss Ss: 0.018615\n","new_1_6_d.nii\n","new_1_6_m.nii\n","new_1_7_d.nii\n","new_1_7_m.nii\n","new_2_5_d.nii\n","new_2_5_m.nii\n","new_2_6_d.nii\n","new_2_6_m.nii\n","new_2_7_d.nii\n","new_2_7_m.nii\n","new_3_d (2).nii\n","new_3_m (2).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (3).nii\n","new_3_m (3).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (7).nii\n","new_3_m (7).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (8).nii\n","new_3_m (8).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (9).nii\n","new_3_m (9).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_4_10_d_163526.nii\n","new_4_10_m_163526.nii.gz\n","new_4_11_d_084336.nii\n","new_4_11_m_084336.nii.gz\n","new_4_7_d_184422.nii\n","new_4_7_m_184422.nii.gz\n","new_4_8_d_143412.nii\n","new_4_8_m_143412.nii.gz\n","new_4_9_d_155347.nii\n","new_4_9_m_155347.nii.gz\n","\tEpoch: 10; Dice: 0.9496 +/- 0.0290; Loss: 16.8308\n","Begin Epoch 11\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tEpoch:11 [000/005 (0000/0588)]\tLoss Ss: 0.011586\n","\tEpoch:11 [000/005 (0020/0588)]\tLoss Ss: 0.012209\n","\tEpoch:11 [000/005 (0040/0588)]\tLoss Ss: 0.016969\n","\tEpoch:11 [000/005 (0060/0588)]\tLoss Ss: 0.013970\n","\tEpoch:11 [000/005 (0080/0588)]\tLoss Ss: 0.011640\n","\tEpoch:11 [000/005 (0100/0588)]\tLoss Ss: 0.013531\n","\tEpoch:11 [000/005 (0120/0588)]\tLoss Ss: 0.009935\n","\tEpoch:11 [000/005 (0140/0588)]\tLoss Ss: 0.009619\n","\tEpoch:11 [000/005 (0160/0588)]\tLoss Ss: 0.007540\n","\tEpoch:11 [000/005 (0180/0588)]\tLoss Ss: 0.008639\n","\tEpoch:11 [000/005 (0200/0588)]\tLoss Ss: 0.010827\n","\tEpoch:11 [000/005 (0220/0588)]\tLoss Ss: 0.009151\n","\tEpoch:11 [000/005 (0240/0588)]\tLoss Ss: 0.008684\n","\tEpoch:11 [000/005 (0260/0588)]\tLoss Ss: 0.008544\n","\tEpoch:11 [000/005 (0280/0588)]\tLoss Ss: 0.014471\n","\tEpoch:11 [000/005 (0300/0588)]\tLoss Ss: 0.005881\n","\tEpoch:11 [000/005 (0320/0588)]\tLoss Ss: 0.007071\n","\tEpoch:11 [000/005 (0340/0588)]\tLoss Ss: 0.007536\n","\tEpoch:11 [000/005 (0360/0588)]\tLoss Ss: 0.006392\n","\tEpoch:11 [000/005 (0380/0588)]\tLoss Ss: 0.007404\n","\tEpoch:11 [000/005 (0400/0588)]\tLoss Ss: 0.006558\n","\tEpoch:11 [000/005 (0420/0588)]\tLoss Ss: 0.006600\n","\tEpoch:11 [000/005 (0440/0588)]\tLoss Ss: 0.006362\n","\tEpoch:11 [000/005 (0460/0588)]\tLoss Ss: 0.007505\n","\tEpoch:11 [000/005 (0480/0588)]\tLoss Ss: 0.005288\n","\tEpoch:11 [000/005 (0500/0588)]\tLoss Ss: 0.012345\n","\tEpoch:11 [000/005 (0520/0588)]\tLoss Ss: 0.007691\n","\tEpoch:11 [000/005 (0540/0588)]\tLoss Ss: 0.009274\n","\tEpoch:11 [000/005 (0560/0588)]\tLoss Ss: 0.008782\n","\tEpoch:11 [000/005 (0580/0588)]\tLoss Ss: 0.005625\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tEpoch:11 [001/005 (0000/0614)]\tLoss Ss: 0.012513\n","\tEpoch:11 [001/005 (0020/0614)]\tLoss Ss: 0.010105\n","\tEpoch:11 [001/005 (0040/0614)]\tLoss Ss: 0.010273\n","\tEpoch:11 [001/005 (0060/0614)]\tLoss Ss: 0.013605\n","\tEpoch:11 [001/005 (0080/0614)]\tLoss Ss: 0.019575\n","\tEpoch:11 [001/005 (0100/0614)]\tLoss Ss: 0.010236\n","\tEpoch:11 [001/005 (0120/0614)]\tLoss Ss: 0.010251\n","\tEpoch:11 [001/005 (0140/0614)]\tLoss Ss: 0.010736\n","\tEpoch:11 [001/005 (0160/0614)]\tLoss Ss: 0.011736\n","\tEpoch:11 [001/005 (0180/0614)]\tLoss Ss: 0.012504\n","\tEpoch:11 [001/005 (0200/0614)]\tLoss Ss: 0.011201\n","\tEpoch:11 [001/005 (0220/0614)]\tLoss Ss: 0.009897\n","\tEpoch:11 [001/005 (0240/0614)]\tLoss Ss: 0.010398\n","\tEpoch:11 [001/005 (0260/0614)]\tLoss Ss: 0.010567\n","\tEpoch:11 [001/005 (0280/0614)]\tLoss Ss: 0.006298\n","\tEpoch:11 [001/005 (0300/0614)]\tLoss Ss: 0.013220\n","\tEpoch:11 [001/005 (0320/0614)]\tLoss Ss: 0.013193\n","\tEpoch:11 [001/005 (0340/0614)]\tLoss Ss: 0.015608\n","\tEpoch:11 [001/005 (0360/0614)]\tLoss Ss: 0.006685\n","\tEpoch:11 [001/005 (0380/0614)]\tLoss Ss: 0.009142\n","\tEpoch:11 [001/005 (0400/0614)]\tLoss Ss: 0.008933\n","\tEpoch:11 [001/005 (0420/0614)]\tLoss Ss: 0.008503\n","\tEpoch:11 [001/005 (0440/0614)]\tLoss Ss: 0.006575\n","\tEpoch:11 [001/005 (0460/0614)]\tLoss Ss: 0.010328\n","\tEpoch:11 [001/005 (0480/0614)]\tLoss Ss: 0.008653\n","\tEpoch:11 [001/005 (0500/0614)]\tLoss Ss: 0.007155\n","\tEpoch:11 [001/005 (0520/0614)]\tLoss Ss: 0.009127\n","\tEpoch:11 [001/005 (0540/0614)]\tLoss Ss: 0.007618\n","\tEpoch:11 [001/005 (0560/0614)]\tLoss Ss: 0.007035\n","\tEpoch:11 [001/005 (0580/0614)]\tLoss Ss: 0.005712\n","\tEpoch:11 [001/005 (0600/0614)]\tLoss Ss: 0.006137\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:11 [002/005 (0000/0755)]\tLoss Ss: 0.022287\n","\tEpoch:11 [002/005 (0020/0755)]\tLoss Ss: 0.023906\n","\tEpoch:11 [002/005 (0040/0755)]\tLoss Ss: 0.022829\n","\tEpoch:11 [002/005 (0060/0755)]\tLoss Ss: 0.026985\n","\tEpoch:11 [002/005 (0080/0755)]\tLoss Ss: 0.023407\n","\tEpoch:11 [002/005 (0100/0755)]\tLoss Ss: 0.019137\n","\tEpoch:11 [002/005 (0120/0755)]\tLoss Ss: 0.025825\n","\tEpoch:11 [002/005 (0140/0755)]\tLoss Ss: 0.015043\n","\tEpoch:11 [002/005 (0160/0755)]\tLoss Ss: 0.022428\n","\tEpoch:11 [002/005 (0180/0755)]\tLoss Ss: 0.013976\n","\tEpoch:11 [002/005 (0200/0755)]\tLoss Ss: 0.010758\n","\tEpoch:11 [002/005 (0220/0755)]\tLoss Ss: 0.017739\n","\tEpoch:11 [002/005 (0240/0755)]\tLoss Ss: 0.032347\n","\tEpoch:11 [002/005 (0260/0755)]\tLoss Ss: 0.018065\n","\tEpoch:11 [002/005 (0280/0755)]\tLoss Ss: 0.022416\n","\tEpoch:11 [002/005 (0300/0755)]\tLoss Ss: 0.024123\n","\tEpoch:11 [002/005 (0320/0755)]\tLoss Ss: 0.014906\n","\tEpoch:11 [002/005 (0340/0755)]\tLoss Ss: 0.016946\n","\tEpoch:11 [002/005 (0360/0755)]\tLoss Ss: 0.023283\n","\tEpoch:11 [002/005 (0380/0755)]\tLoss Ss: 0.013487\n","\tEpoch:11 [002/005 (0400/0755)]\tLoss Ss: 0.017287\n","\tEpoch:11 [002/005 (0420/0755)]\tLoss Ss: 0.018089\n","\tEpoch:11 [002/005 (0440/0755)]\tLoss Ss: 0.016512\n","\tEpoch:11 [002/005 (0460/0755)]\tLoss Ss: 0.014339\n","\tEpoch:11 [002/005 (0480/0755)]\tLoss Ss: 0.013515\n","\tEpoch:11 [002/005 (0500/0755)]\tLoss Ss: 0.022400\n","\tEpoch:11 [002/005 (0520/0755)]\tLoss Ss: 0.019702\n","\tEpoch:11 [002/005 (0540/0755)]\tLoss Ss: 0.011355\n","\tEpoch:11 [002/005 (0560/0755)]\tLoss Ss: 0.009916\n","\tEpoch:11 [002/005 (0580/0755)]\tLoss Ss: 0.014307\n","\tEpoch:11 [002/005 (0600/0755)]\tLoss Ss: 0.012664\n","\tEpoch:11 [002/005 (0620/0755)]\tLoss Ss: 0.020327\n","\tEpoch:11 [002/005 (0640/0755)]\tLoss Ss: 0.014970\n","\tEpoch:11 [002/005 (0660/0755)]\tLoss Ss: 0.013071\n","\tEpoch:11 [002/005 (0680/0755)]\tLoss Ss: 0.018993\n","\tEpoch:11 [002/005 (0700/0755)]\tLoss Ss: 0.013810\n","\tEpoch:11 [002/005 (0720/0755)]\tLoss Ss: 0.013553\n","\tEpoch:11 [002/005 (0740/0755)]\tLoss Ss: 0.012363\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:11 [003/005 (0000/0755)]\tLoss Ss: 0.030626\n","\tEpoch:11 [003/005 (0020/0755)]\tLoss Ss: 0.024597\n","\tEpoch:11 [003/005 (0040/0755)]\tLoss Ss: 0.029916\n","\tEpoch:11 [003/005 (0060/0755)]\tLoss Ss: 0.022248\n","\tEpoch:11 [003/005 (0080/0755)]\tLoss Ss: 0.018427\n","\tEpoch:11 [003/005 (0100/0755)]\tLoss Ss: 0.017622\n","\tEpoch:11 [003/005 (0120/0755)]\tLoss Ss: 0.018433\n","\tEpoch:11 [003/005 (0140/0755)]\tLoss Ss: 0.021246\n","\tEpoch:11 [003/005 (0160/0755)]\tLoss Ss: 0.018925\n","\tEpoch:11 [003/005 (0180/0755)]\tLoss Ss: 0.024923\n","\tEpoch:11 [003/005 (0200/0755)]\tLoss Ss: 0.022852\n","\tEpoch:11 [003/005 (0220/0755)]\tLoss Ss: 0.026569\n","\tEpoch:11 [003/005 (0240/0755)]\tLoss Ss: 0.016518\n","\tEpoch:11 [003/005 (0260/0755)]\tLoss Ss: 0.022259\n","\tEpoch:11 [003/005 (0280/0755)]\tLoss Ss: 0.014709\n","\tEpoch:11 [003/005 (0300/0755)]\tLoss Ss: 0.024195\n","\tEpoch:11 [003/005 (0320/0755)]\tLoss Ss: 0.016459\n","\tEpoch:11 [003/005 (0340/0755)]\tLoss Ss: 0.012028\n","\tEpoch:11 [003/005 (0360/0755)]\tLoss Ss: 0.023710\n","\tEpoch:11 [003/005 (0380/0755)]\tLoss Ss: 0.017246\n","\tEpoch:11 [003/005 (0400/0755)]\tLoss Ss: 0.014754\n","\tEpoch:11 [003/005 (0420/0755)]\tLoss Ss: 0.022084\n","\tEpoch:11 [003/005 (0440/0755)]\tLoss Ss: 0.020538\n","\tEpoch:11 [003/005 (0460/0755)]\tLoss Ss: 0.023322\n","\tEpoch:11 [003/005 (0480/0755)]\tLoss Ss: 0.013875\n","\tEpoch:11 [003/005 (0500/0755)]\tLoss Ss: 0.008730\n","\tEpoch:11 [003/005 (0520/0755)]\tLoss Ss: 0.014036\n","\tEpoch:11 [003/005 (0540/0755)]\tLoss Ss: 0.013296\n","\tEpoch:11 [003/005 (0560/0755)]\tLoss Ss: 0.018943\n","\tEpoch:11 [003/005 (0580/0755)]\tLoss Ss: 0.013048\n","\tEpoch:11 [003/005 (0600/0755)]\tLoss Ss: 0.023463\n","\tEpoch:11 [003/005 (0620/0755)]\tLoss Ss: 0.019444\n","\tEpoch:11 [003/005 (0640/0755)]\tLoss Ss: 0.021202\n","\tEpoch:11 [003/005 (0660/0755)]\tLoss Ss: 0.014147\n","\tEpoch:11 [003/005 (0680/0755)]\tLoss Ss: 0.028953\n","\tEpoch:11 [003/005 (0700/0755)]\tLoss Ss: 0.013079\n","\tEpoch:11 [003/005 (0720/0755)]\tLoss Ss: 0.015858\n","\tEpoch:11 [003/005 (0740/0755)]\tLoss Ss: 0.018109\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tEpoch:11 [004/005 (0000/0693)]\tLoss Ss: 0.022449\n","\tEpoch:11 [004/005 (0020/0693)]\tLoss Ss: 0.026710\n","\tEpoch:11 [004/005 (0040/0693)]\tLoss Ss: 0.018243\n","\tEpoch:11 [004/005 (0060/0693)]\tLoss Ss: 0.028835\n","\tEpoch:11 [004/005 (0080/0693)]\tLoss Ss: 0.021036\n","\tEpoch:11 [004/005 (0100/0693)]\tLoss Ss: 0.020763\n","\tEpoch:11 [004/005 (0120/0693)]\tLoss Ss: 0.020098\n","\tEpoch:11 [004/005 (0140/0693)]\tLoss Ss: 0.024269\n","\tEpoch:11 [004/005 (0160/0693)]\tLoss Ss: 0.021008\n","\tEpoch:11 [004/005 (0180/0693)]\tLoss Ss: 0.020353\n","\tEpoch:11 [004/005 (0200/0693)]\tLoss Ss: 0.019075\n","\tEpoch:11 [004/005 (0220/0693)]\tLoss Ss: 0.016127\n","\tEpoch:11 [004/005 (0240/0693)]\tLoss Ss: 0.017282\n","\tEpoch:11 [004/005 (0260/0693)]\tLoss Ss: 0.014871\n","\tEpoch:11 [004/005 (0280/0693)]\tLoss Ss: 0.015577\n","\tEpoch:11 [004/005 (0300/0693)]\tLoss Ss: 0.017810\n","\tEpoch:11 [004/005 (0320/0693)]\tLoss Ss: 0.023852\n","\tEpoch:11 [004/005 (0340/0693)]\tLoss Ss: 0.014949\n","\tEpoch:11 [004/005 (0360/0693)]\tLoss Ss: 0.021123\n","\tEpoch:11 [004/005 (0380/0693)]\tLoss Ss: 0.011815\n","\tEpoch:11 [004/005 (0400/0693)]\tLoss Ss: 0.016015\n","\tEpoch:11 [004/005 (0420/0693)]\tLoss Ss: 0.011452\n","\tEpoch:11 [004/005 (0440/0693)]\tLoss Ss: 0.014221\n","\tEpoch:11 [004/005 (0460/0693)]\tLoss Ss: 0.015023\n","\tEpoch:11 [004/005 (0480/0693)]\tLoss Ss: 0.013668\n","\tEpoch:11 [004/005 (0500/0693)]\tLoss Ss: 0.014079\n","\tEpoch:11 [004/005 (0520/0693)]\tLoss Ss: 0.014894\n","\tEpoch:11 [004/005 (0540/0693)]\tLoss Ss: 0.015004\n","\tEpoch:11 [004/005 (0560/0693)]\tLoss Ss: 0.014362\n","\tEpoch:11 [004/005 (0580/0693)]\tLoss Ss: 0.019277\n","\tEpoch:11 [004/005 (0600/0693)]\tLoss Ss: 0.014208\n","\tEpoch:11 [004/005 (0620/0693)]\tLoss Ss: 0.018303\n","\tEpoch:11 [004/005 (0640/0693)]\tLoss Ss: 0.018151\n","\tEpoch:11 [004/005 (0660/0693)]\tLoss Ss: 0.012107\n","\tEpoch:11 [004/005 (0680/0693)]\tLoss Ss: 0.013984\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tEpoch:11 [005/005 (0000/0693)]\tLoss Ss: 0.015076\n","\tEpoch:11 [005/005 (0020/0693)]\tLoss Ss: 0.031097\n","\tEpoch:11 [005/005 (0040/0693)]\tLoss Ss: 0.025521\n","\tEpoch:11 [005/005 (0060/0693)]\tLoss Ss: 0.011737\n","\tEpoch:11 [005/005 (0080/0693)]\tLoss Ss: 0.014392\n","\tEpoch:11 [005/005 (0100/0693)]\tLoss Ss: 0.017621\n","\tEpoch:11 [005/005 (0120/0693)]\tLoss Ss: 0.020443\n","\tEpoch:11 [005/005 (0140/0693)]\tLoss Ss: 0.018324\n","\tEpoch:11 [005/005 (0160/0693)]\tLoss Ss: 0.017270\n","\tEpoch:11 [005/005 (0180/0693)]\tLoss Ss: 0.015628\n","\tEpoch:11 [005/005 (0200/0693)]\tLoss Ss: 0.014507\n","\tEpoch:11 [005/005 (0220/0693)]\tLoss Ss: 0.019113\n","\tEpoch:11 [005/005 (0240/0693)]\tLoss Ss: 0.011154\n","\tEpoch:11 [005/005 (0260/0693)]\tLoss Ss: 0.016752\n","\tEpoch:11 [005/005 (0280/0693)]\tLoss Ss: 0.019391\n","\tEpoch:11 [005/005 (0300/0693)]\tLoss Ss: 0.017553\n","\tEpoch:11 [005/005 (0320/0693)]\tLoss Ss: 0.014358\n","\tEpoch:11 [005/005 (0340/0693)]\tLoss Ss: 0.020728\n","\tEpoch:11 [005/005 (0360/0693)]\tLoss Ss: 0.017877\n","\tEpoch:11 [005/005 (0380/0693)]\tLoss Ss: 0.013798\n","\tEpoch:11 [005/005 (0400/0693)]\tLoss Ss: 0.011611\n","\tEpoch:11 [005/005 (0420/0693)]\tLoss Ss: 0.016654\n","\tEpoch:11 [005/005 (0440/0693)]\tLoss Ss: 0.014956\n","\tEpoch:11 [005/005 (0460/0693)]\tLoss Ss: 0.017141\n","\tEpoch:11 [005/005 (0480/0693)]\tLoss Ss: 0.021110\n","\tEpoch:11 [005/005 (0500/0693)]\tLoss Ss: 0.013998\n","\tEpoch:11 [005/005 (0520/0693)]\tLoss Ss: 0.015896\n","\tEpoch:11 [005/005 (0540/0693)]\tLoss Ss: 0.011569\n","\tEpoch:11 [005/005 (0560/0693)]\tLoss Ss: 0.011906\n","\tEpoch:11 [005/005 (0580/0693)]\tLoss Ss: 0.020739\n","\tEpoch:11 [005/005 (0600/0693)]\tLoss Ss: 0.011327\n","\tEpoch:11 [005/005 (0620/0693)]\tLoss Ss: 0.017562\n","\tEpoch:11 [005/005 (0640/0693)]\tLoss Ss: 0.014624\n","\tEpoch:11 [005/005 (0660/0693)]\tLoss Ss: 0.014328\n","\tEpoch:11 [005/005 (0680/0693)]\tLoss Ss: 0.013733\n","Now train the rotated image\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tRotated_Epoch:11 [000/005 (0000/0693)]\tLoss Ss: 0.022920\n","\tRotated_Epoch:11 [000/005 (0020/0693)]\tLoss Ss: 0.023809\n","\tRotated_Epoch:11 [000/005 (0040/0693)]\tLoss Ss: 0.025375\n","\tRotated_Epoch:11 [000/005 (0060/0693)]\tLoss Ss: 0.021144\n","\tRotated_Epoch:11 [000/005 (0080/0693)]\tLoss Ss: 0.023880\n","\tRotated_Epoch:11 [000/005 (0100/0693)]\tLoss Ss: 0.023242\n","\tRotated_Epoch:11 [000/005 (0120/0693)]\tLoss Ss: 0.025925\n","\tRotated_Epoch:11 [000/005 (0140/0693)]\tLoss Ss: 0.020286\n","\tRotated_Epoch:11 [000/005 (0160/0693)]\tLoss Ss: 0.014610\n","\tRotated_Epoch:11 [000/005 (0180/0693)]\tLoss Ss: 0.018125\n","\tRotated_Epoch:11 [000/005 (0200/0693)]\tLoss Ss: 0.017937\n","\tRotated_Epoch:11 [000/005 (0220/0693)]\tLoss Ss: 0.013808\n","\tRotated_Epoch:11 [000/005 (0240/0693)]\tLoss Ss: 0.024694\n","\tRotated_Epoch:11 [000/005 (0260/0693)]\tLoss Ss: 0.017616\n","\tRotated_Epoch:11 [000/005 (0280/0693)]\tLoss Ss: 0.016356\n","\tRotated_Epoch:11 [000/005 (0300/0693)]\tLoss Ss: 0.017437\n","\tRotated_Epoch:11 [000/005 (0320/0693)]\tLoss Ss: 0.015734\n","\tRotated_Epoch:11 [000/005 (0340/0693)]\tLoss Ss: 0.015512\n","\tRotated_Epoch:11 [000/005 (0360/0693)]\tLoss Ss: 0.015782\n","\tRotated_Epoch:11 [000/005 (0380/0693)]\tLoss Ss: 0.021596\n","\tRotated_Epoch:11 [000/005 (0400/0693)]\tLoss Ss: 0.016978\n","\tRotated_Epoch:11 [000/005 (0420/0693)]\tLoss Ss: 0.012914\n","\tRotated_Epoch:11 [000/005 (0440/0693)]\tLoss Ss: 0.011798\n","\tRotated_Epoch:11 [000/005 (0460/0693)]\tLoss Ss: 0.016563\n","\tRotated_Epoch:11 [000/005 (0480/0693)]\tLoss Ss: 0.014651\n","\tRotated_Epoch:11 [000/005 (0500/0693)]\tLoss Ss: 0.015121\n","\tRotated_Epoch:11 [000/005 (0520/0693)]\tLoss Ss: 0.016541\n","\tRotated_Epoch:11 [000/005 (0540/0693)]\tLoss Ss: 0.016788\n","\tRotated_Epoch:11 [000/005 (0560/0693)]\tLoss Ss: 0.016037\n","\tRotated_Epoch:11 [000/005 (0580/0693)]\tLoss Ss: 0.014664\n","\tRotated_Epoch:11 [000/005 (0600/0693)]\tLoss Ss: 0.014038\n","\tRotated_Epoch:11 [000/005 (0620/0693)]\tLoss Ss: 0.015249\n","\tRotated_Epoch:11 [000/005 (0640/0693)]\tLoss Ss: 0.017117\n","\tRotated_Epoch:11 [000/005 (0660/0693)]\tLoss Ss: 0.012591\n","\tRotated_Epoch:11 [000/005 (0680/0693)]\tLoss Ss: 0.015670\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tRotated_Epoch:11 [001/005 (0000/0588)]\tLoss Ss: 0.407421\n","\tRotated_Epoch:11 [001/005 (0020/0588)]\tLoss Ss: 0.408455\n","\tRotated_Epoch:11 [001/005 (0040/0588)]\tLoss Ss: 0.234136\n","\tRotated_Epoch:11 [001/005 (0060/0588)]\tLoss Ss: 0.117159\n","\tRotated_Epoch:11 [001/005 (0080/0588)]\tLoss Ss: 0.174593\n","\tRotated_Epoch:11 [001/005 (0100/0588)]\tLoss Ss: 0.153174\n","\tRotated_Epoch:11 [001/005 (0120/0588)]\tLoss Ss: 0.123047\n","\tRotated_Epoch:11 [001/005 (0140/0588)]\tLoss Ss: 0.138802\n","\tRotated_Epoch:11 [001/005 (0160/0588)]\tLoss Ss: 0.106223\n","\tRotated_Epoch:11 [001/005 (0180/0588)]\tLoss Ss: 0.132899\n","\tRotated_Epoch:11 [001/005 (0200/0588)]\tLoss Ss: 0.133992\n","\tRotated_Epoch:11 [001/005 (0220/0588)]\tLoss Ss: 0.116783\n","\tRotated_Epoch:11 [001/005 (0240/0588)]\tLoss Ss: 0.129218\n","\tRotated_Epoch:11 [001/005 (0260/0588)]\tLoss Ss: 0.096480\n","\tRotated_Epoch:11 [001/005 (0280/0588)]\tLoss Ss: 0.059283\n","\tRotated_Epoch:11 [001/005 (0300/0588)]\tLoss Ss: 0.077544\n","\tRotated_Epoch:11 [001/005 (0320/0588)]\tLoss Ss: 0.085996\n","\tRotated_Epoch:11 [001/005 (0340/0588)]\tLoss Ss: 0.060434\n","\tRotated_Epoch:11 [001/005 (0360/0588)]\tLoss Ss: 0.122101\n","\tRotated_Epoch:11 [001/005 (0380/0588)]\tLoss Ss: 0.069674\n","\tRotated_Epoch:11 [001/005 (0400/0588)]\tLoss Ss: 0.115593\n","\tRotated_Epoch:11 [001/005 (0420/0588)]\tLoss Ss: 0.063681\n","\tRotated_Epoch:11 [001/005 (0440/0588)]\tLoss Ss: 0.108260\n","\tRotated_Epoch:11 [001/005 (0460/0588)]\tLoss Ss: 0.069792\n","\tRotated_Epoch:11 [001/005 (0480/0588)]\tLoss Ss: 0.056731\n","\tRotated_Epoch:11 [001/005 (0500/0588)]\tLoss Ss: 0.108781\n","\tRotated_Epoch:11 [001/005 (0520/0588)]\tLoss Ss: 0.122275\n","\tRotated_Epoch:11 [001/005 (0540/0588)]\tLoss Ss: 0.073590\n","\tRotated_Epoch:11 [001/005 (0560/0588)]\tLoss Ss: 0.089537\n","\tRotated_Epoch:11 [001/005 (0580/0588)]\tLoss Ss: 0.077397\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:11 [002/005 (0000/0755)]\tLoss Ss: 0.111673\n","\tRotated_Epoch:11 [002/005 (0020/0755)]\tLoss Ss: 0.102084\n","\tRotated_Epoch:11 [002/005 (0040/0755)]\tLoss Ss: 0.078787\n","\tRotated_Epoch:11 [002/005 (0060/0755)]\tLoss Ss: 0.076639\n","\tRotated_Epoch:11 [002/005 (0080/0755)]\tLoss Ss: 0.081511\n","\tRotated_Epoch:11 [002/005 (0100/0755)]\tLoss Ss: 0.048021\n","\tRotated_Epoch:11 [002/005 (0120/0755)]\tLoss Ss: 0.038760\n","\tRotated_Epoch:11 [002/005 (0140/0755)]\tLoss Ss: 0.046731\n","\tRotated_Epoch:11 [002/005 (0160/0755)]\tLoss Ss: 0.030897\n","\tRotated_Epoch:11 [002/005 (0180/0755)]\tLoss Ss: 0.040214\n","\tRotated_Epoch:11 [002/005 (0200/0755)]\tLoss Ss: 0.039269\n","\tRotated_Epoch:11 [002/005 (0220/0755)]\tLoss Ss: 0.034220\n","\tRotated_Epoch:11 [002/005 (0240/0755)]\tLoss Ss: 0.044600\n","\tRotated_Epoch:11 [002/005 (0260/0755)]\tLoss Ss: 0.045665\n","\tRotated_Epoch:11 [002/005 (0280/0755)]\tLoss Ss: 0.048266\n","\tRotated_Epoch:11 [002/005 (0300/0755)]\tLoss Ss: 0.031163\n","\tRotated_Epoch:11 [002/005 (0320/0755)]\tLoss Ss: 0.049933\n","\tRotated_Epoch:11 [002/005 (0340/0755)]\tLoss Ss: 0.037716\n","\tRotated_Epoch:11 [002/005 (0360/0755)]\tLoss Ss: 0.038664\n","\tRotated_Epoch:11 [002/005 (0380/0755)]\tLoss Ss: 0.030663\n","\tRotated_Epoch:11 [002/005 (0400/0755)]\tLoss Ss: 0.029690\n","\tRotated_Epoch:11 [002/005 (0420/0755)]\tLoss Ss: 0.020848\n","\tRotated_Epoch:11 [002/005 (0440/0755)]\tLoss Ss: 0.038887\n","\tRotated_Epoch:11 [002/005 (0460/0755)]\tLoss Ss: 0.021458\n","\tRotated_Epoch:11 [002/005 (0480/0755)]\tLoss Ss: 0.032816\n","\tRotated_Epoch:11 [002/005 (0500/0755)]\tLoss Ss: 0.027124\n","\tRotated_Epoch:11 [002/005 (0520/0755)]\tLoss Ss: 0.025122\n","\tRotated_Epoch:11 [002/005 (0540/0755)]\tLoss Ss: 0.026765\n","\tRotated_Epoch:11 [002/005 (0560/0755)]\tLoss Ss: 0.020399\n","\tRotated_Epoch:11 [002/005 (0580/0755)]\tLoss Ss: 0.027630\n","\tRotated_Epoch:11 [002/005 (0600/0755)]\tLoss Ss: 0.040558\n","\tRotated_Epoch:11 [002/005 (0620/0755)]\tLoss Ss: 0.022580\n","\tRotated_Epoch:11 [002/005 (0640/0755)]\tLoss Ss: 0.020512\n","\tRotated_Epoch:11 [002/005 (0660/0755)]\tLoss Ss: 0.022305\n","\tRotated_Epoch:11 [002/005 (0680/0755)]\tLoss Ss: 0.023332\n","\tRotated_Epoch:11 [002/005 (0700/0755)]\tLoss Ss: 0.022201\n","\tRotated_Epoch:11 [002/005 (0720/0755)]\tLoss Ss: 0.019102\n","\tRotated_Epoch:11 [002/005 (0740/0755)]\tLoss Ss: 0.022022\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tRotated_Epoch:11 [003/005 (0000/0693)]\tLoss Ss: 0.030989\n","\tRotated_Epoch:11 [003/005 (0020/0693)]\tLoss Ss: 0.026325\n","\tRotated_Epoch:11 [003/005 (0040/0693)]\tLoss Ss: 0.019480\n","\tRotated_Epoch:11 [003/005 (0060/0693)]\tLoss Ss: 0.028151\n","\tRotated_Epoch:11 [003/005 (0080/0693)]\tLoss Ss: 0.025001\n","\tRotated_Epoch:11 [003/005 (0100/0693)]\tLoss Ss: 0.022043\n","\tRotated_Epoch:11 [003/005 (0120/0693)]\tLoss Ss: 0.020773\n","\tRotated_Epoch:11 [003/005 (0140/0693)]\tLoss Ss: 0.017267\n","\tRotated_Epoch:11 [003/005 (0160/0693)]\tLoss Ss: 0.020968\n","\tRotated_Epoch:11 [003/005 (0180/0693)]\tLoss Ss: 0.026568\n","\tRotated_Epoch:11 [003/005 (0200/0693)]\tLoss Ss: 0.016456\n","\tRotated_Epoch:11 [003/005 (0220/0693)]\tLoss Ss: 0.019439\n","\tRotated_Epoch:11 [003/005 (0240/0693)]\tLoss Ss: 0.024288\n","\tRotated_Epoch:11 [003/005 (0260/0693)]\tLoss Ss: 0.020925\n","\tRotated_Epoch:11 [003/005 (0280/0693)]\tLoss Ss: 0.016241\n","\tRotated_Epoch:11 [003/005 (0300/0693)]\tLoss Ss: 0.020574\n","\tRotated_Epoch:11 [003/005 (0320/0693)]\tLoss Ss: 0.019476\n","\tRotated_Epoch:11 [003/005 (0340/0693)]\tLoss Ss: 0.020694\n","\tRotated_Epoch:11 [003/005 (0360/0693)]\tLoss Ss: 0.021018\n","\tRotated_Epoch:11 [003/005 (0380/0693)]\tLoss Ss: 0.021105\n","\tRotated_Epoch:11 [003/005 (0400/0693)]\tLoss Ss: 0.027335\n","\tRotated_Epoch:11 [003/005 (0420/0693)]\tLoss Ss: 0.021932\n","\tRotated_Epoch:11 [003/005 (0440/0693)]\tLoss Ss: 0.022252\n","\tRotated_Epoch:11 [003/005 (0460/0693)]\tLoss Ss: 0.016570\n","\tRotated_Epoch:11 [003/005 (0480/0693)]\tLoss Ss: 0.018248\n","\tRotated_Epoch:11 [003/005 (0500/0693)]\tLoss Ss: 0.013433\n","\tRotated_Epoch:11 [003/005 (0520/0693)]\tLoss Ss: 0.021098\n","\tRotated_Epoch:11 [003/005 (0540/0693)]\tLoss Ss: 0.016677\n","\tRotated_Epoch:11 [003/005 (0560/0693)]\tLoss Ss: 0.015778\n","\tRotated_Epoch:11 [003/005 (0580/0693)]\tLoss Ss: 0.014732\n","\tRotated_Epoch:11 [003/005 (0600/0693)]\tLoss Ss: 0.023330\n","\tRotated_Epoch:11 [003/005 (0620/0693)]\tLoss Ss: 0.017342\n","\tRotated_Epoch:11 [003/005 (0640/0693)]\tLoss Ss: 0.014368\n","\tRotated_Epoch:11 [003/005 (0660/0693)]\tLoss Ss: 0.019388\n","\tRotated_Epoch:11 [003/005 (0680/0693)]\tLoss Ss: 0.025187\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:11 [004/005 (0000/0755)]\tLoss Ss: 0.394340\n","\tRotated_Epoch:11 [004/005 (0020/0755)]\tLoss Ss: 0.369346\n","\tRotated_Epoch:11 [004/005 (0040/0755)]\tLoss Ss: 0.173319\n","\tRotated_Epoch:11 [004/005 (0060/0755)]\tLoss Ss: 0.279945\n","\tRotated_Epoch:11 [004/005 (0080/0755)]\tLoss Ss: 0.208233\n","\tRotated_Epoch:11 [004/005 (0100/0755)]\tLoss Ss: 0.291416\n","\tRotated_Epoch:11 [004/005 (0120/0755)]\tLoss Ss: 0.323705\n","\tRotated_Epoch:11 [004/005 (0140/0755)]\tLoss Ss: 0.279070\n","\tRotated_Epoch:11 [004/005 (0160/0755)]\tLoss Ss: 0.109513\n","\tRotated_Epoch:11 [004/005 (0180/0755)]\tLoss Ss: 0.167819\n","\tRotated_Epoch:11 [004/005 (0200/0755)]\tLoss Ss: 0.167596\n","\tRotated_Epoch:11 [004/005 (0220/0755)]\tLoss Ss: 0.137076\n","\tRotated_Epoch:11 [004/005 (0240/0755)]\tLoss Ss: 0.156626\n","\tRotated_Epoch:11 [004/005 (0260/0755)]\tLoss Ss: 0.181294\n","\tRotated_Epoch:11 [004/005 (0280/0755)]\tLoss Ss: 0.107473\n","\tRotated_Epoch:11 [004/005 (0300/0755)]\tLoss Ss: 0.124427\n","\tRotated_Epoch:11 [004/005 (0320/0755)]\tLoss Ss: 0.153031\n","\tRotated_Epoch:11 [004/005 (0340/0755)]\tLoss Ss: 0.114526\n","\tRotated_Epoch:11 [004/005 (0360/0755)]\tLoss Ss: 0.123632\n","\tRotated_Epoch:11 [004/005 (0380/0755)]\tLoss Ss: 0.123955\n","\tRotated_Epoch:11 [004/005 (0400/0755)]\tLoss Ss: 0.098575\n","\tRotated_Epoch:11 [004/005 (0420/0755)]\tLoss Ss: 0.144749\n","\tRotated_Epoch:11 [004/005 (0440/0755)]\tLoss Ss: 0.193239\n","\tRotated_Epoch:11 [004/005 (0460/0755)]\tLoss Ss: 0.118299\n","\tRotated_Epoch:11 [004/005 (0480/0755)]\tLoss Ss: 0.102374\n","\tRotated_Epoch:11 [004/005 (0500/0755)]\tLoss Ss: 0.110717\n","\tRotated_Epoch:11 [004/005 (0520/0755)]\tLoss Ss: 0.144032\n","\tRotated_Epoch:11 [004/005 (0540/0755)]\tLoss Ss: 0.097322\n","\tRotated_Epoch:11 [004/005 (0560/0755)]\tLoss Ss: 0.163488\n","\tRotated_Epoch:11 [004/005 (0580/0755)]\tLoss Ss: 0.096433\n","\tRotated_Epoch:11 [004/005 (0600/0755)]\tLoss Ss: 0.086641\n","\tRotated_Epoch:11 [004/005 (0620/0755)]\tLoss Ss: 0.100072\n","\tRotated_Epoch:11 [004/005 (0640/0755)]\tLoss Ss: 0.090197\n","\tRotated_Epoch:11 [004/005 (0660/0755)]\tLoss Ss: 0.110219\n","\tRotated_Epoch:11 [004/005 (0680/0755)]\tLoss Ss: 0.053409\n","\tRotated_Epoch:11 [004/005 (0700/0755)]\tLoss Ss: 0.111237\n","\tRotated_Epoch:11 [004/005 (0720/0755)]\tLoss Ss: 0.092847\n","\tRotated_Epoch:11 [004/005 (0740/0755)]\tLoss Ss: 0.084345\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tRotated_Epoch:11 [005/005 (0000/0614)]\tLoss Ss: 0.091923\n","\tRotated_Epoch:11 [005/005 (0020/0614)]\tLoss Ss: 0.083078\n","\tRotated_Epoch:11 [005/005 (0040/0614)]\tLoss Ss: 0.065885\n","\tRotated_Epoch:11 [005/005 (0060/0614)]\tLoss Ss: 0.061366\n","\tRotated_Epoch:11 [005/005 (0080/0614)]\tLoss Ss: 0.048233\n","\tRotated_Epoch:11 [005/005 (0100/0614)]\tLoss Ss: 0.040991\n","\tRotated_Epoch:11 [005/005 (0120/0614)]\tLoss Ss: 0.024895\n","\tRotated_Epoch:11 [005/005 (0140/0614)]\tLoss Ss: 0.040095\n","\tRotated_Epoch:11 [005/005 (0160/0614)]\tLoss Ss: 0.033956\n","\tRotated_Epoch:11 [005/005 (0180/0614)]\tLoss Ss: 0.022238\n","\tRotated_Epoch:11 [005/005 (0200/0614)]\tLoss Ss: 0.022406\n","\tRotated_Epoch:11 [005/005 (0220/0614)]\tLoss Ss: 0.024622\n","\tRotated_Epoch:11 [005/005 (0240/0614)]\tLoss Ss: 0.024909\n","\tRotated_Epoch:11 [005/005 (0260/0614)]\tLoss Ss: 0.029249\n","\tRotated_Epoch:11 [005/005 (0280/0614)]\tLoss Ss: 0.021808\n","\tRotated_Epoch:11 [005/005 (0300/0614)]\tLoss Ss: 0.018323\n","\tRotated_Epoch:11 [005/005 (0320/0614)]\tLoss Ss: 0.019993\n","\tRotated_Epoch:11 [005/005 (0340/0614)]\tLoss Ss: 0.014857\n","\tRotated_Epoch:11 [005/005 (0360/0614)]\tLoss Ss: 0.015240\n","\tRotated_Epoch:11 [005/005 (0380/0614)]\tLoss Ss: 0.013118\n","\tRotated_Epoch:11 [005/005 (0400/0614)]\tLoss Ss: 0.015144\n","\tRotated_Epoch:11 [005/005 (0420/0614)]\tLoss Ss: 0.014410\n","\tRotated_Epoch:11 [005/005 (0440/0614)]\tLoss Ss: 0.014853\n","\tRotated_Epoch:11 [005/005 (0460/0614)]\tLoss Ss: 0.018608\n","\tRotated_Epoch:11 [005/005 (0480/0614)]\tLoss Ss: 0.016545\n","\tRotated_Epoch:11 [005/005 (0500/0614)]\tLoss Ss: 0.016417\n","\tRotated_Epoch:11 [005/005 (0520/0614)]\tLoss Ss: 0.012985\n","\tRotated_Epoch:11 [005/005 (0540/0614)]\tLoss Ss: 0.010557\n","\tRotated_Epoch:11 [005/005 (0560/0614)]\tLoss Ss: 0.011370\n","\tRotated_Epoch:11 [005/005 (0580/0614)]\tLoss Ss: 0.010496\n","\tRotated_Epoch:11 [005/005 (0600/0614)]\tLoss Ss: 0.013437\n","new_1_6_d.nii\n","new_1_6_m.nii\n","new_1_7_d.nii\n","new_1_7_m.nii\n","new_2_5_d.nii\n","new_2_5_m.nii\n","new_2_6_d.nii\n","new_2_6_m.nii\n","new_2_7_d.nii\n","new_2_7_m.nii\n","new_3_d (2).nii\n","new_3_m (2).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (3).nii\n","new_3_m (3).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (7).nii\n","new_3_m (7).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (8).nii\n","new_3_m (8).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (9).nii\n","new_3_m (9).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_4_10_d_163526.nii\n","new_4_10_m_163526.nii.gz\n","new_4_11_d_084336.nii\n","new_4_11_m_084336.nii.gz\n","new_4_7_d_184422.nii\n","new_4_7_m_184422.nii.gz\n","new_4_8_d_143412.nii\n","new_4_8_m_143412.nii.gz\n","new_4_9_d_155347.nii\n","new_4_9_m_155347.nii.gz\n","\tEpoch: 11; Dice: 0.9336 +/- 0.0163; Loss: 16.7755\n","Begin Epoch 12\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:12 [000/005 (0000/0755)]\tLoss Ss: 0.057319\n","\tEpoch:12 [000/005 (0020/0755)]\tLoss Ss: 0.051574\n","\tEpoch:12 [000/005 (0040/0755)]\tLoss Ss: 0.046446\n","\tEpoch:12 [000/005 (0060/0755)]\tLoss Ss: 0.042592\n","\tEpoch:12 [000/005 (0080/0755)]\tLoss Ss: 0.029919\n","\tEpoch:12 [000/005 (0100/0755)]\tLoss Ss: 0.030771\n","\tEpoch:12 [000/005 (0120/0755)]\tLoss Ss: 0.037946\n","\tEpoch:12 [000/005 (0140/0755)]\tLoss Ss: 0.038474\n","\tEpoch:12 [000/005 (0160/0755)]\tLoss Ss: 0.032451\n","\tEpoch:12 [000/005 (0180/0755)]\tLoss Ss: 0.047037\n","\tEpoch:12 [000/005 (0200/0755)]\tLoss Ss: 0.031388\n","\tEpoch:12 [000/005 (0220/0755)]\tLoss Ss: 0.038493\n","\tEpoch:12 [000/005 (0240/0755)]\tLoss Ss: 0.024727\n","\tEpoch:12 [000/005 (0260/0755)]\tLoss Ss: 0.022089\n","\tEpoch:12 [000/005 (0280/0755)]\tLoss Ss: 0.021662\n","\tEpoch:12 [000/005 (0300/0755)]\tLoss Ss: 0.018668\n","\tEpoch:12 [000/005 (0320/0755)]\tLoss Ss: 0.025119\n","\tEpoch:12 [000/005 (0340/0755)]\tLoss Ss: 0.031422\n","\tEpoch:12 [000/005 (0360/0755)]\tLoss Ss: 0.036416\n","\tEpoch:12 [000/005 (0380/0755)]\tLoss Ss: 0.033707\n","\tEpoch:12 [000/005 (0400/0755)]\tLoss Ss: 0.023429\n","\tEpoch:12 [000/005 (0420/0755)]\tLoss Ss: 0.021981\n","\tEpoch:12 [000/005 (0440/0755)]\tLoss Ss: 0.027572\n","\tEpoch:12 [000/005 (0460/0755)]\tLoss Ss: 0.024384\n","\tEpoch:12 [000/005 (0480/0755)]\tLoss Ss: 0.038281\n","\tEpoch:12 [000/005 (0500/0755)]\tLoss Ss: 0.025205\n","\tEpoch:12 [000/005 (0520/0755)]\tLoss Ss: 0.025581\n","\tEpoch:12 [000/005 (0540/0755)]\tLoss Ss: 0.016950\n","\tEpoch:12 [000/005 (0560/0755)]\tLoss Ss: 0.020674\n","\tEpoch:12 [000/005 (0580/0755)]\tLoss Ss: 0.018856\n","\tEpoch:12 [000/005 (0600/0755)]\tLoss Ss: 0.021970\n","\tEpoch:12 [000/005 (0620/0755)]\tLoss Ss: 0.023882\n","\tEpoch:12 [000/005 (0640/0755)]\tLoss Ss: 0.020929\n","\tEpoch:12 [000/005 (0660/0755)]\tLoss Ss: 0.026891\n","\tEpoch:12 [000/005 (0680/0755)]\tLoss Ss: 0.022930\n","\tEpoch:12 [000/005 (0700/0755)]\tLoss Ss: 0.019568\n","\tEpoch:12 [000/005 (0720/0755)]\tLoss Ss: 0.018921\n","\tEpoch:12 [000/005 (0740/0755)]\tLoss Ss: 0.030616\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tEpoch:12 [001/005 (0000/0614)]\tLoss Ss: 0.012407\n","\tEpoch:12 [001/005 (0020/0614)]\tLoss Ss: 0.010258\n","\tEpoch:12 [001/005 (0040/0614)]\tLoss Ss: 0.011122\n","\tEpoch:12 [001/005 (0060/0614)]\tLoss Ss: 0.008471\n","\tEpoch:12 [001/005 (0080/0614)]\tLoss Ss: 0.015679\n","\tEpoch:12 [001/005 (0100/0614)]\tLoss Ss: 0.011684\n","\tEpoch:12 [001/005 (0120/0614)]\tLoss Ss: 0.012811\n","\tEpoch:12 [001/005 (0140/0614)]\tLoss Ss: 0.013375\n","\tEpoch:12 [001/005 (0160/0614)]\tLoss Ss: 0.008680\n","\tEpoch:12 [001/005 (0180/0614)]\tLoss Ss: 0.010389\n","\tEpoch:12 [001/005 (0200/0614)]\tLoss Ss: 0.012437\n","\tEpoch:12 [001/005 (0220/0614)]\tLoss Ss: 0.009139\n","\tEpoch:12 [001/005 (0240/0614)]\tLoss Ss: 0.007959\n","\tEpoch:12 [001/005 (0260/0614)]\tLoss Ss: 0.008369\n","\tEpoch:12 [001/005 (0280/0614)]\tLoss Ss: 0.009289\n","\tEpoch:12 [001/005 (0300/0614)]\tLoss Ss: 0.010520\n","\tEpoch:12 [001/005 (0320/0614)]\tLoss Ss: 0.006485\n","\tEpoch:12 [001/005 (0340/0614)]\tLoss Ss: 0.008552\n","\tEpoch:12 [001/005 (0360/0614)]\tLoss Ss: 0.007571\n","\tEpoch:12 [001/005 (0380/0614)]\tLoss Ss: 0.010340\n","\tEpoch:12 [001/005 (0400/0614)]\tLoss Ss: 0.007368\n","\tEpoch:12 [001/005 (0420/0614)]\tLoss Ss: 0.012596\n","\tEpoch:12 [001/005 (0440/0614)]\tLoss Ss: 0.010623\n","\tEpoch:12 [001/005 (0460/0614)]\tLoss Ss: 0.008966\n","\tEpoch:12 [001/005 (0480/0614)]\tLoss Ss: 0.005625\n","\tEpoch:12 [001/005 (0500/0614)]\tLoss Ss: 0.009254\n","\tEpoch:12 [001/005 (0520/0614)]\tLoss Ss: 0.010962\n","\tEpoch:12 [001/005 (0540/0614)]\tLoss Ss: 0.006496\n","\tEpoch:12 [001/005 (0560/0614)]\tLoss Ss: 0.005228\n","\tEpoch:12 [001/005 (0580/0614)]\tLoss Ss: 0.008054\n","\tEpoch:12 [001/005 (0600/0614)]\tLoss Ss: 0.006263\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tEpoch:12 [002/005 (0000/0693)]\tLoss Ss: 0.015047\n","\tEpoch:12 [002/005 (0020/0693)]\tLoss Ss: 0.018136\n","\tEpoch:12 [002/005 (0040/0693)]\tLoss Ss: 0.012038\n","\tEpoch:12 [002/005 (0060/0693)]\tLoss Ss: 0.016834\n","\tEpoch:12 [002/005 (0080/0693)]\tLoss Ss: 0.030984\n","\tEpoch:12 [002/005 (0100/0693)]\tLoss Ss: 0.014026\n","\tEpoch:12 [002/005 (0120/0693)]\tLoss Ss: 0.023838\n","\tEpoch:12 [002/005 (0140/0693)]\tLoss Ss: 0.015459\n","\tEpoch:12 [002/005 (0160/0693)]\tLoss Ss: 0.017042\n","\tEpoch:12 [002/005 (0180/0693)]\tLoss Ss: 0.017449\n","\tEpoch:12 [002/005 (0200/0693)]\tLoss Ss: 0.021154\n","\tEpoch:12 [002/005 (0220/0693)]\tLoss Ss: 0.018854\n","\tEpoch:12 [002/005 (0240/0693)]\tLoss Ss: 0.018747\n","\tEpoch:12 [002/005 (0260/0693)]\tLoss Ss: 0.017950\n","\tEpoch:12 [002/005 (0280/0693)]\tLoss Ss: 0.015286\n","\tEpoch:12 [002/005 (0300/0693)]\tLoss Ss: 0.023121\n","\tEpoch:12 [002/005 (0320/0693)]\tLoss Ss: 0.025935\n","\tEpoch:12 [002/005 (0340/0693)]\tLoss Ss: 0.014713\n","\tEpoch:12 [002/005 (0360/0693)]\tLoss Ss: 0.016722\n","\tEpoch:12 [002/005 (0380/0693)]\tLoss Ss: 0.017328\n","\tEpoch:12 [002/005 (0400/0693)]\tLoss Ss: 0.019835\n","\tEpoch:12 [002/005 (0420/0693)]\tLoss Ss: 0.015067\n","\tEpoch:12 [002/005 (0440/0693)]\tLoss Ss: 0.016303\n","\tEpoch:12 [002/005 (0460/0693)]\tLoss Ss: 0.017322\n","\tEpoch:12 [002/005 (0480/0693)]\tLoss Ss: 0.016244\n","\tEpoch:12 [002/005 (0500/0693)]\tLoss Ss: 0.020405\n","\tEpoch:12 [002/005 (0520/0693)]\tLoss Ss: 0.012888\n","\tEpoch:12 [002/005 (0540/0693)]\tLoss Ss: 0.018489\n","\tEpoch:12 [002/005 (0560/0693)]\tLoss Ss: 0.012001\n","\tEpoch:12 [002/005 (0580/0693)]\tLoss Ss: 0.015125\n","\tEpoch:12 [002/005 (0600/0693)]\tLoss Ss: 0.017778\n","\tEpoch:12 [002/005 (0620/0693)]\tLoss Ss: 0.021649\n","\tEpoch:12 [002/005 (0640/0693)]\tLoss Ss: 0.016072\n","\tEpoch:12 [002/005 (0660/0693)]\tLoss Ss: 0.012787\n","\tEpoch:12 [002/005 (0680/0693)]\tLoss Ss: 0.015083\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tEpoch:12 [003/005 (0000/0588)]\tLoss Ss: 0.017941\n","\tEpoch:12 [003/005 (0020/0588)]\tLoss Ss: 0.011358\n","\tEpoch:12 [003/005 (0040/0588)]\tLoss Ss: 0.013457\n","\tEpoch:12 [003/005 (0060/0588)]\tLoss Ss: 0.013523\n","\tEpoch:12 [003/005 (0080/0588)]\tLoss Ss: 0.008240\n","\tEpoch:12 [003/005 (0100/0588)]\tLoss Ss: 0.007634\n","\tEpoch:12 [003/005 (0120/0588)]\tLoss Ss: 0.014334\n","\tEpoch:12 [003/005 (0140/0588)]\tLoss Ss: 0.010711\n","\tEpoch:12 [003/005 (0160/0588)]\tLoss Ss: 0.009144\n","\tEpoch:12 [003/005 (0180/0588)]\tLoss Ss: 0.008019\n","\tEpoch:12 [003/005 (0200/0588)]\tLoss Ss: 0.007805\n","\tEpoch:12 [003/005 (0220/0588)]\tLoss Ss: 0.006348\n","\tEpoch:12 [003/005 (0240/0588)]\tLoss Ss: 0.007157\n","\tEpoch:12 [003/005 (0260/0588)]\tLoss Ss: 0.008836\n","\tEpoch:12 [003/005 (0280/0588)]\tLoss Ss: 0.006367\n","\tEpoch:12 [003/005 (0300/0588)]\tLoss Ss: 0.008303\n","\tEpoch:12 [003/005 (0320/0588)]\tLoss Ss: 0.007090\n","\tEpoch:12 [003/005 (0340/0588)]\tLoss Ss: 0.005905\n","\tEpoch:12 [003/005 (0360/0588)]\tLoss Ss: 0.007293\n","\tEpoch:12 [003/005 (0380/0588)]\tLoss Ss: 0.005805\n","\tEpoch:12 [003/005 (0400/0588)]\tLoss Ss: 0.006028\n","\tEpoch:12 [003/005 (0420/0588)]\tLoss Ss: 0.006181\n","\tEpoch:12 [003/005 (0440/0588)]\tLoss Ss: 0.007926\n","\tEpoch:12 [003/005 (0460/0588)]\tLoss Ss: 0.006023\n","\tEpoch:12 [003/005 (0480/0588)]\tLoss Ss: 0.004922\n","\tEpoch:12 [003/005 (0500/0588)]\tLoss Ss: 0.007011\n","\tEpoch:12 [003/005 (0520/0588)]\tLoss Ss: 0.006747\n","\tEpoch:12 [003/005 (0540/0588)]\tLoss Ss: 0.007022\n","\tEpoch:12 [003/005 (0560/0588)]\tLoss Ss: 0.006520\n","\tEpoch:12 [003/005 (0580/0588)]\tLoss Ss: 0.006693\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:12 [004/005 (0000/0755)]\tLoss Ss: 0.036624\n","\tEpoch:12 [004/005 (0020/0755)]\tLoss Ss: 0.032134\n","\tEpoch:12 [004/005 (0040/0755)]\tLoss Ss: 0.037616\n","\tEpoch:12 [004/005 (0060/0755)]\tLoss Ss: 0.031968\n","\tEpoch:12 [004/005 (0080/0755)]\tLoss Ss: 0.024696\n","\tEpoch:12 [004/005 (0100/0755)]\tLoss Ss: 0.031982\n","\tEpoch:12 [004/005 (0120/0755)]\tLoss Ss: 0.020427\n","\tEpoch:12 [004/005 (0140/0755)]\tLoss Ss: 0.018665\n","\tEpoch:12 [004/005 (0160/0755)]\tLoss Ss: 0.022746\n","\tEpoch:12 [004/005 (0180/0755)]\tLoss Ss: 0.024528\n","\tEpoch:12 [004/005 (0200/0755)]\tLoss Ss: 0.025961\n","\tEpoch:12 [004/005 (0220/0755)]\tLoss Ss: 0.026077\n","\tEpoch:12 [004/005 (0240/0755)]\tLoss Ss: 0.022198\n","\tEpoch:12 [004/005 (0260/0755)]\tLoss Ss: 0.019703\n","\tEpoch:12 [004/005 (0280/0755)]\tLoss Ss: 0.049103\n","\tEpoch:12 [004/005 (0300/0755)]\tLoss Ss: 0.017801\n","\tEpoch:12 [004/005 (0320/0755)]\tLoss Ss: 0.015532\n","\tEpoch:12 [004/005 (0340/0755)]\tLoss Ss: 0.018135\n","\tEpoch:12 [004/005 (0360/0755)]\tLoss Ss: 0.023467\n","\tEpoch:12 [004/005 (0380/0755)]\tLoss Ss: 0.013872\n","\tEpoch:12 [004/005 (0400/0755)]\tLoss Ss: 0.027146\n","\tEpoch:12 [004/005 (0420/0755)]\tLoss Ss: 0.011141\n","\tEpoch:12 [004/005 (0440/0755)]\tLoss Ss: 0.020169\n","\tEpoch:12 [004/005 (0460/0755)]\tLoss Ss: 0.017103\n","\tEpoch:12 [004/005 (0480/0755)]\tLoss Ss: 0.013064\n","\tEpoch:12 [004/005 (0500/0755)]\tLoss Ss: 0.019162\n","\tEpoch:12 [004/005 (0520/0755)]\tLoss Ss: 0.014397\n","\tEpoch:12 [004/005 (0540/0755)]\tLoss Ss: 0.020187\n","\tEpoch:12 [004/005 (0560/0755)]\tLoss Ss: 0.011620\n","\tEpoch:12 [004/005 (0580/0755)]\tLoss Ss: 0.017495\n","\tEpoch:12 [004/005 (0600/0755)]\tLoss Ss: 0.024301\n","\tEpoch:12 [004/005 (0620/0755)]\tLoss Ss: 0.026190\n","\tEpoch:12 [004/005 (0640/0755)]\tLoss Ss: 0.018518\n","\tEpoch:12 [004/005 (0660/0755)]\tLoss Ss: 0.019679\n","\tEpoch:12 [004/005 (0680/0755)]\tLoss Ss: 0.019868\n","\tEpoch:12 [004/005 (0700/0755)]\tLoss Ss: 0.022639\n","\tEpoch:12 [004/005 (0720/0755)]\tLoss Ss: 0.014005\n","\tEpoch:12 [004/005 (0740/0755)]\tLoss Ss: 0.017309\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tEpoch:12 [005/005 (0000/0693)]\tLoss Ss: 0.025543\n","\tEpoch:12 [005/005 (0020/0693)]\tLoss Ss: 0.021439\n","\tEpoch:12 [005/005 (0040/0693)]\tLoss Ss: 0.028689\n","\tEpoch:12 [005/005 (0060/0693)]\tLoss Ss: 0.022402\n","\tEpoch:12 [005/005 (0080/0693)]\tLoss Ss: 0.015601\n","\tEpoch:12 [005/005 (0100/0693)]\tLoss Ss: 0.022074\n","\tEpoch:12 [005/005 (0120/0693)]\tLoss Ss: 0.021916\n","\tEpoch:12 [005/005 (0140/0693)]\tLoss Ss: 0.018631\n","\tEpoch:12 [005/005 (0160/0693)]\tLoss Ss: 0.012748\n","\tEpoch:12 [005/005 (0180/0693)]\tLoss Ss: 0.015975\n","\tEpoch:12 [005/005 (0200/0693)]\tLoss Ss: 0.022654\n","\tEpoch:12 [005/005 (0220/0693)]\tLoss Ss: 0.016011\n","\tEpoch:12 [005/005 (0240/0693)]\tLoss Ss: 0.017770\n","\tEpoch:12 [005/005 (0260/0693)]\tLoss Ss: 0.016659\n","\tEpoch:12 [005/005 (0280/0693)]\tLoss Ss: 0.015067\n","\tEpoch:12 [005/005 (0300/0693)]\tLoss Ss: 0.018689\n","\tEpoch:12 [005/005 (0320/0693)]\tLoss Ss: 0.018022\n","\tEpoch:12 [005/005 (0340/0693)]\tLoss Ss: 0.013698\n","\tEpoch:12 [005/005 (0360/0693)]\tLoss Ss: 0.016199\n","\tEpoch:12 [005/005 (0380/0693)]\tLoss Ss: 0.014422\n","\tEpoch:12 [005/005 (0400/0693)]\tLoss Ss: 0.014415\n","\tEpoch:12 [005/005 (0420/0693)]\tLoss Ss: 0.013205\n","\tEpoch:12 [005/005 (0440/0693)]\tLoss Ss: 0.012353\n","\tEpoch:12 [005/005 (0460/0693)]\tLoss Ss: 0.017228\n","\tEpoch:12 [005/005 (0480/0693)]\tLoss Ss: 0.016723\n","\tEpoch:12 [005/005 (0500/0693)]\tLoss Ss: 0.015088\n","\tEpoch:12 [005/005 (0520/0693)]\tLoss Ss: 0.016976\n","\tEpoch:12 [005/005 (0540/0693)]\tLoss Ss: 0.011695\n","\tEpoch:12 [005/005 (0560/0693)]\tLoss Ss: 0.010708\n","\tEpoch:12 [005/005 (0580/0693)]\tLoss Ss: 0.010942\n","\tEpoch:12 [005/005 (0600/0693)]\tLoss Ss: 0.015796\n","\tEpoch:12 [005/005 (0620/0693)]\tLoss Ss: 0.015376\n","\tEpoch:12 [005/005 (0640/0693)]\tLoss Ss: 0.013138\n","\tEpoch:12 [005/005 (0660/0693)]\tLoss Ss: 0.015997\n","\tEpoch:12 [005/005 (0680/0693)]\tLoss Ss: 0.011106\n","Now train the rotated image\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tRotated_Epoch:12 [000/005 (0000/0588)]\tLoss Ss: 0.292771\n","\tRotated_Epoch:12 [000/005 (0020/0588)]\tLoss Ss: 0.267927\n","\tRotated_Epoch:12 [000/005 (0040/0588)]\tLoss Ss: 0.187732\n","\tRotated_Epoch:12 [000/005 (0060/0588)]\tLoss Ss: 0.275884\n","\tRotated_Epoch:12 [000/005 (0080/0588)]\tLoss Ss: 0.201478\n","\tRotated_Epoch:12 [000/005 (0100/0588)]\tLoss Ss: 0.246867\n","\tRotated_Epoch:12 [000/005 (0120/0588)]\tLoss Ss: 0.170710\n","\tRotated_Epoch:12 [000/005 (0140/0588)]\tLoss Ss: 0.069501\n","\tRotated_Epoch:12 [000/005 (0160/0588)]\tLoss Ss: 0.115543\n","\tRotated_Epoch:12 [000/005 (0180/0588)]\tLoss Ss: 0.149804\n","\tRotated_Epoch:12 [000/005 (0200/0588)]\tLoss Ss: 0.146290\n","\tRotated_Epoch:12 [000/005 (0220/0588)]\tLoss Ss: 0.095433\n","\tRotated_Epoch:12 [000/005 (0240/0588)]\tLoss Ss: 0.085081\n","\tRotated_Epoch:12 [000/005 (0260/0588)]\tLoss Ss: 0.092651\n","\tRotated_Epoch:12 [000/005 (0280/0588)]\tLoss Ss: 0.124185\n","\tRotated_Epoch:12 [000/005 (0300/0588)]\tLoss Ss: 0.086144\n","\tRotated_Epoch:12 [000/005 (0320/0588)]\tLoss Ss: 0.080987\n","\tRotated_Epoch:12 [000/005 (0340/0588)]\tLoss Ss: 0.071971\n","\tRotated_Epoch:12 [000/005 (0360/0588)]\tLoss Ss: 0.116686\n","\tRotated_Epoch:12 [000/005 (0380/0588)]\tLoss Ss: 0.082126\n","\tRotated_Epoch:12 [000/005 (0400/0588)]\tLoss Ss: 0.098718\n","\tRotated_Epoch:12 [000/005 (0420/0588)]\tLoss Ss: 0.086310\n","\tRotated_Epoch:12 [000/005 (0440/0588)]\tLoss Ss: 0.088424\n","\tRotated_Epoch:12 [000/005 (0460/0588)]\tLoss Ss: 0.081829\n","\tRotated_Epoch:12 [000/005 (0480/0588)]\tLoss Ss: 0.071314\n","\tRotated_Epoch:12 [000/005 (0500/0588)]\tLoss Ss: 0.108745\n","\tRotated_Epoch:12 [000/005 (0520/0588)]\tLoss Ss: 0.053728\n","\tRotated_Epoch:12 [000/005 (0540/0588)]\tLoss Ss: 0.083538\n","\tRotated_Epoch:12 [000/005 (0560/0588)]\tLoss Ss: 0.058771\n","\tRotated_Epoch:12 [000/005 (0580/0588)]\tLoss Ss: 0.059926\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:12 [001/005 (0000/0755)]\tLoss Ss: 0.210039\n","\tRotated_Epoch:12 [001/005 (0020/0755)]\tLoss Ss: 0.203037\n","\tRotated_Epoch:12 [001/005 (0040/0755)]\tLoss Ss: 0.156783\n","\tRotated_Epoch:12 [001/005 (0060/0755)]\tLoss Ss: 0.112406\n","\tRotated_Epoch:12 [001/005 (0080/0755)]\tLoss Ss: 0.145588\n","\tRotated_Epoch:12 [001/005 (0100/0755)]\tLoss Ss: 0.085988\n","\tRotated_Epoch:12 [001/005 (0120/0755)]\tLoss Ss: 0.095185\n","\tRotated_Epoch:12 [001/005 (0140/0755)]\tLoss Ss: 0.098959\n","\tRotated_Epoch:12 [001/005 (0160/0755)]\tLoss Ss: 0.132785\n","\tRotated_Epoch:12 [001/005 (0180/0755)]\tLoss Ss: 0.073290\n","\tRotated_Epoch:12 [001/005 (0200/0755)]\tLoss Ss: 0.072088\n","\tRotated_Epoch:12 [001/005 (0220/0755)]\tLoss Ss: 0.084724\n","\tRotated_Epoch:12 [001/005 (0240/0755)]\tLoss Ss: 0.086202\n","\tRotated_Epoch:12 [001/005 (0260/0755)]\tLoss Ss: 0.092679\n","\tRotated_Epoch:12 [001/005 (0280/0755)]\tLoss Ss: 0.075920\n","\tRotated_Epoch:12 [001/005 (0300/0755)]\tLoss Ss: 0.094819\n","\tRotated_Epoch:12 [001/005 (0320/0755)]\tLoss Ss: 0.083448\n","\tRotated_Epoch:12 [001/005 (0340/0755)]\tLoss Ss: 0.063625\n","\tRotated_Epoch:12 [001/005 (0360/0755)]\tLoss Ss: 0.073489\n","\tRotated_Epoch:12 [001/005 (0380/0755)]\tLoss Ss: 0.076283\n","\tRotated_Epoch:12 [001/005 (0400/0755)]\tLoss Ss: 0.066222\n","\tRotated_Epoch:12 [001/005 (0420/0755)]\tLoss Ss: 0.066479\n","\tRotated_Epoch:12 [001/005 (0440/0755)]\tLoss Ss: 0.073039\n","\tRotated_Epoch:12 [001/005 (0460/0755)]\tLoss Ss: 0.069540\n","\tRotated_Epoch:12 [001/005 (0480/0755)]\tLoss Ss: 0.074282\n","\tRotated_Epoch:12 [001/005 (0500/0755)]\tLoss Ss: 0.067761\n","\tRotated_Epoch:12 [001/005 (0520/0755)]\tLoss Ss: 0.061644\n","\tRotated_Epoch:12 [001/005 (0540/0755)]\tLoss Ss: 0.059890\n","\tRotated_Epoch:12 [001/005 (0560/0755)]\tLoss Ss: 0.047714\n","\tRotated_Epoch:12 [001/005 (0580/0755)]\tLoss Ss: 0.080593\n","\tRotated_Epoch:12 [001/005 (0600/0755)]\tLoss Ss: 0.055345\n","\tRotated_Epoch:12 [001/005 (0620/0755)]\tLoss Ss: 0.055169\n","\tRotated_Epoch:12 [001/005 (0640/0755)]\tLoss Ss: 0.067250\n","\tRotated_Epoch:12 [001/005 (0660/0755)]\tLoss Ss: 0.071210\n","\tRotated_Epoch:12 [001/005 (0680/0755)]\tLoss Ss: 0.046310\n","\tRotated_Epoch:12 [001/005 (0700/0755)]\tLoss Ss: 0.059148\n","\tRotated_Epoch:12 [001/005 (0720/0755)]\tLoss Ss: 0.040671\n","\tRotated_Epoch:12 [001/005 (0740/0755)]\tLoss Ss: 0.073581\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tRotated_Epoch:12 [002/005 (0000/0693)]\tLoss Ss: 0.176290\n","\tRotated_Epoch:12 [002/005 (0020/0693)]\tLoss Ss: 0.098826\n","\tRotated_Epoch:12 [002/005 (0040/0693)]\tLoss Ss: 0.057733\n","\tRotated_Epoch:12 [002/005 (0060/0693)]\tLoss Ss: 0.045499\n","\tRotated_Epoch:12 [002/005 (0080/0693)]\tLoss Ss: 0.033761\n","\tRotated_Epoch:12 [002/005 (0100/0693)]\tLoss Ss: 0.047630\n","\tRotated_Epoch:12 [002/005 (0120/0693)]\tLoss Ss: 0.033208\n","\tRotated_Epoch:12 [002/005 (0140/0693)]\tLoss Ss: 0.033010\n","\tRotated_Epoch:12 [002/005 (0160/0693)]\tLoss Ss: 0.028106\n","\tRotated_Epoch:12 [002/005 (0180/0693)]\tLoss Ss: 0.029295\n","\tRotated_Epoch:12 [002/005 (0200/0693)]\tLoss Ss: 0.036929\n","\tRotated_Epoch:12 [002/005 (0220/0693)]\tLoss Ss: 0.028228\n","\tRotated_Epoch:12 [002/005 (0240/0693)]\tLoss Ss: 0.018977\n","\tRotated_Epoch:12 [002/005 (0260/0693)]\tLoss Ss: 0.023687\n","\tRotated_Epoch:12 [002/005 (0280/0693)]\tLoss Ss: 0.019215\n","\tRotated_Epoch:12 [002/005 (0300/0693)]\tLoss Ss: 0.018431\n","\tRotated_Epoch:12 [002/005 (0320/0693)]\tLoss Ss: 0.025025\n","\tRotated_Epoch:12 [002/005 (0340/0693)]\tLoss Ss: 0.019760\n","\tRotated_Epoch:12 [002/005 (0360/0693)]\tLoss Ss: 0.022427\n","\tRotated_Epoch:12 [002/005 (0380/0693)]\tLoss Ss: 0.017732\n","\tRotated_Epoch:12 [002/005 (0400/0693)]\tLoss Ss: 0.024436\n","\tRotated_Epoch:12 [002/005 (0420/0693)]\tLoss Ss: 0.017625\n","\tRotated_Epoch:12 [002/005 (0440/0693)]\tLoss Ss: 0.019361\n","\tRotated_Epoch:12 [002/005 (0460/0693)]\tLoss Ss: 0.014155\n","\tRotated_Epoch:12 [002/005 (0480/0693)]\tLoss Ss: 0.021293\n","\tRotated_Epoch:12 [002/005 (0500/0693)]\tLoss Ss: 0.022494\n","\tRotated_Epoch:12 [002/005 (0520/0693)]\tLoss Ss: 0.024099\n","\tRotated_Epoch:12 [002/005 (0540/0693)]\tLoss Ss: 0.017361\n","\tRotated_Epoch:12 [002/005 (0560/0693)]\tLoss Ss: 0.015806\n","\tRotated_Epoch:12 [002/005 (0580/0693)]\tLoss Ss: 0.018570\n","\tRotated_Epoch:12 [002/005 (0600/0693)]\tLoss Ss: 0.022751\n","\tRotated_Epoch:12 [002/005 (0620/0693)]\tLoss Ss: 0.019100\n","\tRotated_Epoch:12 [002/005 (0640/0693)]\tLoss Ss: 0.017252\n","\tRotated_Epoch:12 [002/005 (0660/0693)]\tLoss Ss: 0.026096\n","\tRotated_Epoch:12 [002/005 (0680/0693)]\tLoss Ss: 0.011950\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tRotated_Epoch:12 [003/005 (0000/0614)]\tLoss Ss: 0.056373\n","\tRotated_Epoch:12 [003/005 (0020/0614)]\tLoss Ss: 0.041535\n","\tRotated_Epoch:12 [003/005 (0040/0614)]\tLoss Ss: 0.022696\n","\tRotated_Epoch:12 [003/005 (0060/0614)]\tLoss Ss: 0.036666\n","\tRotated_Epoch:12 [003/005 (0080/0614)]\tLoss Ss: 0.023668\n","\tRotated_Epoch:12 [003/005 (0100/0614)]\tLoss Ss: 0.024025\n","\tRotated_Epoch:12 [003/005 (0120/0614)]\tLoss Ss: 0.030738\n","\tRotated_Epoch:12 [003/005 (0140/0614)]\tLoss Ss: 0.022482\n","\tRotated_Epoch:12 [003/005 (0160/0614)]\tLoss Ss: 0.013686\n","\tRotated_Epoch:12 [003/005 (0180/0614)]\tLoss Ss: 0.019300\n","\tRotated_Epoch:12 [003/005 (0200/0614)]\tLoss Ss: 0.013347\n","\tRotated_Epoch:12 [003/005 (0220/0614)]\tLoss Ss: 0.012443\n","\tRotated_Epoch:12 [003/005 (0240/0614)]\tLoss Ss: 0.014165\n","\tRotated_Epoch:12 [003/005 (0260/0614)]\tLoss Ss: 0.015151\n","\tRotated_Epoch:12 [003/005 (0280/0614)]\tLoss Ss: 0.013616\n","\tRotated_Epoch:12 [003/005 (0300/0614)]\tLoss Ss: 0.014272\n","\tRotated_Epoch:12 [003/005 (0320/0614)]\tLoss Ss: 0.017739\n","\tRotated_Epoch:12 [003/005 (0340/0614)]\tLoss Ss: 0.014343\n","\tRotated_Epoch:12 [003/005 (0360/0614)]\tLoss Ss: 0.014267\n","\tRotated_Epoch:12 [003/005 (0380/0614)]\tLoss Ss: 0.011925\n","\tRotated_Epoch:12 [003/005 (0400/0614)]\tLoss Ss: 0.011942\n","\tRotated_Epoch:12 [003/005 (0420/0614)]\tLoss Ss: 0.009833\n","\tRotated_Epoch:12 [003/005 (0440/0614)]\tLoss Ss: 0.012293\n","\tRotated_Epoch:12 [003/005 (0460/0614)]\tLoss Ss: 0.012860\n","\tRotated_Epoch:12 [003/005 (0480/0614)]\tLoss Ss: 0.012081\n","\tRotated_Epoch:12 [003/005 (0500/0614)]\tLoss Ss: 0.014189\n","\tRotated_Epoch:12 [003/005 (0520/0614)]\tLoss Ss: 0.008428\n","\tRotated_Epoch:12 [003/005 (0540/0614)]\tLoss Ss: 0.011723\n","\tRotated_Epoch:12 [003/005 (0560/0614)]\tLoss Ss: 0.010422\n","\tRotated_Epoch:12 [003/005 (0580/0614)]\tLoss Ss: 0.008766\n","\tRotated_Epoch:12 [003/005 (0600/0614)]\tLoss Ss: 0.011533\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:12 [004/005 (0000/0755)]\tLoss Ss: 0.046715\n","\tRotated_Epoch:12 [004/005 (0020/0755)]\tLoss Ss: 0.054810\n","\tRotated_Epoch:12 [004/005 (0040/0755)]\tLoss Ss: 0.042581\n","\tRotated_Epoch:12 [004/005 (0060/0755)]\tLoss Ss: 0.043405\n","\tRotated_Epoch:12 [004/005 (0080/0755)]\tLoss Ss: 0.037342\n","\tRotated_Epoch:12 [004/005 (0100/0755)]\tLoss Ss: 0.045238\n","\tRotated_Epoch:12 [004/005 (0120/0755)]\tLoss Ss: 0.034637\n","\tRotated_Epoch:12 [004/005 (0140/0755)]\tLoss Ss: 0.035312\n","\tRotated_Epoch:12 [004/005 (0160/0755)]\tLoss Ss: 0.025562\n","\tRotated_Epoch:12 [004/005 (0180/0755)]\tLoss Ss: 0.022484\n","\tRotated_Epoch:12 [004/005 (0200/0755)]\tLoss Ss: 0.016782\n","\tRotated_Epoch:12 [004/005 (0220/0755)]\tLoss Ss: 0.038278\n","\tRotated_Epoch:12 [004/005 (0240/0755)]\tLoss Ss: 0.036402\n","\tRotated_Epoch:12 [004/005 (0260/0755)]\tLoss Ss: 0.035593\n","\tRotated_Epoch:12 [004/005 (0280/0755)]\tLoss Ss: 0.026786\n","\tRotated_Epoch:12 [004/005 (0300/0755)]\tLoss Ss: 0.028019\n","\tRotated_Epoch:12 [004/005 (0320/0755)]\tLoss Ss: 0.018741\n","\tRotated_Epoch:12 [004/005 (0340/0755)]\tLoss Ss: 0.034632\n","\tRotated_Epoch:12 [004/005 (0360/0755)]\tLoss Ss: 0.031356\n","\tRotated_Epoch:12 [004/005 (0380/0755)]\tLoss Ss: 0.019336\n","\tRotated_Epoch:12 [004/005 (0400/0755)]\tLoss Ss: 0.025141\n","\tRotated_Epoch:12 [004/005 (0420/0755)]\tLoss Ss: 0.016942\n","\tRotated_Epoch:12 [004/005 (0440/0755)]\tLoss Ss: 0.023057\n","\tRotated_Epoch:12 [004/005 (0460/0755)]\tLoss Ss: 0.028876\n","\tRotated_Epoch:12 [004/005 (0480/0755)]\tLoss Ss: 0.019857\n","\tRotated_Epoch:12 [004/005 (0500/0755)]\tLoss Ss: 0.025582\n","\tRotated_Epoch:12 [004/005 (0520/0755)]\tLoss Ss: 0.034083\n","\tRotated_Epoch:12 [004/005 (0540/0755)]\tLoss Ss: 0.026716\n","\tRotated_Epoch:12 [004/005 (0560/0755)]\tLoss Ss: 0.025099\n","\tRotated_Epoch:12 [004/005 (0580/0755)]\tLoss Ss: 0.016689\n","\tRotated_Epoch:12 [004/005 (0600/0755)]\tLoss Ss: 0.017524\n","\tRotated_Epoch:12 [004/005 (0620/0755)]\tLoss Ss: 0.028468\n","\tRotated_Epoch:12 [004/005 (0640/0755)]\tLoss Ss: 0.015930\n","\tRotated_Epoch:12 [004/005 (0660/0755)]\tLoss Ss: 0.014625\n","\tRotated_Epoch:12 [004/005 (0680/0755)]\tLoss Ss: 0.016784\n","\tRotated_Epoch:12 [004/005 (0700/0755)]\tLoss Ss: 0.023964\n","\tRotated_Epoch:12 [004/005 (0720/0755)]\tLoss Ss: 0.015625\n","\tRotated_Epoch:12 [004/005 (0740/0755)]\tLoss Ss: 0.018552\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tRotated_Epoch:12 [005/005 (0000/0693)]\tLoss Ss: 0.028077\n","\tRotated_Epoch:12 [005/005 (0020/0693)]\tLoss Ss: 0.019314\n","\tRotated_Epoch:12 [005/005 (0040/0693)]\tLoss Ss: 0.025806\n","\tRotated_Epoch:12 [005/005 (0060/0693)]\tLoss Ss: 0.029849\n","\tRotated_Epoch:12 [005/005 (0080/0693)]\tLoss Ss: 0.020305\n","\tRotated_Epoch:12 [005/005 (0100/0693)]\tLoss Ss: 0.019396\n","\tRotated_Epoch:12 [005/005 (0120/0693)]\tLoss Ss: 0.021446\n","\tRotated_Epoch:12 [005/005 (0140/0693)]\tLoss Ss: 0.021860\n","\tRotated_Epoch:12 [005/005 (0160/0693)]\tLoss Ss: 0.017060\n","\tRotated_Epoch:12 [005/005 (0180/0693)]\tLoss Ss: 0.016483\n","\tRotated_Epoch:12 [005/005 (0200/0693)]\tLoss Ss: 0.018539\n","\tRotated_Epoch:12 [005/005 (0220/0693)]\tLoss Ss: 0.018192\n","\tRotated_Epoch:12 [005/005 (0240/0693)]\tLoss Ss: 0.020125\n","\tRotated_Epoch:12 [005/005 (0260/0693)]\tLoss Ss: 0.016926\n","\tRotated_Epoch:12 [005/005 (0280/0693)]\tLoss Ss: 0.017246\n","\tRotated_Epoch:12 [005/005 (0300/0693)]\tLoss Ss: 0.016869\n","\tRotated_Epoch:12 [005/005 (0320/0693)]\tLoss Ss: 0.016723\n","\tRotated_Epoch:12 [005/005 (0340/0693)]\tLoss Ss: 0.019751\n","\tRotated_Epoch:12 [005/005 (0360/0693)]\tLoss Ss: 0.019203\n","\tRotated_Epoch:12 [005/005 (0380/0693)]\tLoss Ss: 0.020690\n","\tRotated_Epoch:12 [005/005 (0400/0693)]\tLoss Ss: 0.015899\n","\tRotated_Epoch:12 [005/005 (0420/0693)]\tLoss Ss: 0.017999\n","\tRotated_Epoch:12 [005/005 (0440/0693)]\tLoss Ss: 0.017534\n","\tRotated_Epoch:12 [005/005 (0460/0693)]\tLoss Ss: 0.018001\n","\tRotated_Epoch:12 [005/005 (0480/0693)]\tLoss Ss: 0.018609\n","\tRotated_Epoch:12 [005/005 (0500/0693)]\tLoss Ss: 0.019035\n","\tRotated_Epoch:12 [005/005 (0520/0693)]\tLoss Ss: 0.014723\n","\tRotated_Epoch:12 [005/005 (0540/0693)]\tLoss Ss: 0.018783\n","\tRotated_Epoch:12 [005/005 (0560/0693)]\tLoss Ss: 0.018246\n","\tRotated_Epoch:12 [005/005 (0580/0693)]\tLoss Ss: 0.022093\n","\tRotated_Epoch:12 [005/005 (0600/0693)]\tLoss Ss: 0.017164\n","\tRotated_Epoch:12 [005/005 (0620/0693)]\tLoss Ss: 0.013964\n","\tRotated_Epoch:12 [005/005 (0640/0693)]\tLoss Ss: 0.021238\n","\tRotated_Epoch:12 [005/005 (0660/0693)]\tLoss Ss: 0.016457\n","\tRotated_Epoch:12 [005/005 (0680/0693)]\tLoss Ss: 0.018353\n","new_1_6_d.nii\n","new_1_6_m.nii\n","new_1_7_d.nii\n","new_1_7_m.nii\n","new_2_5_d.nii\n","new_2_5_m.nii\n","new_2_6_d.nii\n","new_2_6_m.nii\n","new_2_7_d.nii\n","new_2_7_m.nii\n","new_3_d (2).nii\n","new_3_m (2).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (3).nii\n","new_3_m (3).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (7).nii\n","new_3_m (7).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (8).nii\n","new_3_m (8).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (9).nii\n","new_3_m (9).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_4_10_d_163526.nii\n","new_4_10_m_163526.nii.gz\n","new_4_11_d_084336.nii\n","new_4_11_m_084336.nii.gz\n","new_4_7_d_184422.nii\n","new_4_7_m_184422.nii.gz\n","new_4_8_d_143412.nii\n","new_4_8_m_143412.nii.gz\n","new_4_9_d_155347.nii\n","new_4_9_m_155347.nii.gz\n","\tEpoch: 12; Dice: 0.9380 +/- 0.0188; Loss: 14.1304\n","Begin Epoch 13\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:13 [000/005 (0000/0755)]\tLoss Ss: 0.046708\n","\tEpoch:13 [000/005 (0020/0755)]\tLoss Ss: 0.037725\n","\tEpoch:13 [000/005 (0040/0755)]\tLoss Ss: 0.040801\n","\tEpoch:13 [000/005 (0060/0755)]\tLoss Ss: 0.022783\n","\tEpoch:13 [000/005 (0080/0755)]\tLoss Ss: 0.039085\n","\tEpoch:13 [000/005 (0100/0755)]\tLoss Ss: 0.031083\n","\tEpoch:13 [000/005 (0120/0755)]\tLoss Ss: 0.024076\n","\tEpoch:13 [000/005 (0140/0755)]\tLoss Ss: 0.028783\n","\tEpoch:13 [000/005 (0160/0755)]\tLoss Ss: 0.034075\n","\tEpoch:13 [000/005 (0180/0755)]\tLoss Ss: 0.015092\n","\tEpoch:13 [000/005 (0200/0755)]\tLoss Ss: 0.024991\n","\tEpoch:13 [000/005 (0220/0755)]\tLoss Ss: 0.029117\n","\tEpoch:13 [000/005 (0240/0755)]\tLoss Ss: 0.011177\n","\tEpoch:13 [000/005 (0260/0755)]\tLoss Ss: 0.017156\n","\tEpoch:13 [000/005 (0280/0755)]\tLoss Ss: 0.017861\n","\tEpoch:13 [000/005 (0300/0755)]\tLoss Ss: 0.020189\n","\tEpoch:13 [000/005 (0320/0755)]\tLoss Ss: 0.021099\n","\tEpoch:13 [000/005 (0340/0755)]\tLoss Ss: 0.013477\n","\tEpoch:13 [000/005 (0360/0755)]\tLoss Ss: 0.022978\n","\tEpoch:13 [000/005 (0380/0755)]\tLoss Ss: 0.032066\n","\tEpoch:13 [000/005 (0400/0755)]\tLoss Ss: 0.020630\n","\tEpoch:13 [000/005 (0420/0755)]\tLoss Ss: 0.016722\n","\tEpoch:13 [000/005 (0440/0755)]\tLoss Ss: 0.026424\n","\tEpoch:13 [000/005 (0460/0755)]\tLoss Ss: 0.023797\n","\tEpoch:13 [000/005 (0480/0755)]\tLoss Ss: 0.022716\n","\tEpoch:13 [000/005 (0500/0755)]\tLoss Ss: 0.017062\n","\tEpoch:13 [000/005 (0520/0755)]\tLoss Ss: 0.018546\n","\tEpoch:13 [000/005 (0540/0755)]\tLoss Ss: 0.025459\n","\tEpoch:13 [000/005 (0560/0755)]\tLoss Ss: 0.026061\n","\tEpoch:13 [000/005 (0580/0755)]\tLoss Ss: 0.025383\n","\tEpoch:13 [000/005 (0600/0755)]\tLoss Ss: 0.017075\n","\tEpoch:13 [000/005 (0620/0755)]\tLoss Ss: 0.014094\n","\tEpoch:13 [000/005 (0640/0755)]\tLoss Ss: 0.028846\n","\tEpoch:13 [000/005 (0660/0755)]\tLoss Ss: 0.016022\n","\tEpoch:13 [000/005 (0680/0755)]\tLoss Ss: 0.016703\n","\tEpoch:13 [000/005 (0700/0755)]\tLoss Ss: 0.022310\n","\tEpoch:13 [000/005 (0720/0755)]\tLoss Ss: 0.025779\n","\tEpoch:13 [000/005 (0740/0755)]\tLoss Ss: 0.015057\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tEpoch:13 [001/005 (0000/0614)]\tLoss Ss: 0.015060\n","\tEpoch:13 [001/005 (0020/0614)]\tLoss Ss: 0.010896\n","\tEpoch:13 [001/005 (0040/0614)]\tLoss Ss: 0.012244\n","\tEpoch:13 [001/005 (0060/0614)]\tLoss Ss: 0.010438\n","\tEpoch:13 [001/005 (0080/0614)]\tLoss Ss: 0.009615\n","\tEpoch:13 [001/005 (0100/0614)]\tLoss Ss: 0.013450\n","\tEpoch:13 [001/005 (0120/0614)]\tLoss Ss: 0.010590\n","\tEpoch:13 [001/005 (0140/0614)]\tLoss Ss: 0.008465\n","\tEpoch:13 [001/005 (0160/0614)]\tLoss Ss: 0.009902\n","\tEpoch:13 [001/005 (0180/0614)]\tLoss Ss: 0.007493\n","\tEpoch:13 [001/005 (0200/0614)]\tLoss Ss: 0.006738\n","\tEpoch:13 [001/005 (0220/0614)]\tLoss Ss: 0.007328\n","\tEpoch:13 [001/005 (0240/0614)]\tLoss Ss: 0.009044\n","\tEpoch:13 [001/005 (0260/0614)]\tLoss Ss: 0.007737\n","\tEpoch:13 [001/005 (0280/0614)]\tLoss Ss: 0.006766\n","\tEpoch:13 [001/005 (0300/0614)]\tLoss Ss: 0.007231\n","\tEpoch:13 [001/005 (0320/0614)]\tLoss Ss: 0.008225\n","\tEpoch:13 [001/005 (0340/0614)]\tLoss Ss: 0.007963\n","\tEpoch:13 [001/005 (0360/0614)]\tLoss Ss: 0.013405\n","\tEpoch:13 [001/005 (0380/0614)]\tLoss Ss: 0.006196\n","\tEpoch:13 [001/005 (0400/0614)]\tLoss Ss: 0.005888\n","\tEpoch:13 [001/005 (0420/0614)]\tLoss Ss: 0.008208\n","\tEpoch:13 [001/005 (0440/0614)]\tLoss Ss: 0.010523\n","\tEpoch:13 [001/005 (0460/0614)]\tLoss Ss: 0.004376\n","\tEpoch:13 [001/005 (0480/0614)]\tLoss Ss: 0.008121\n","\tEpoch:13 [001/005 (0500/0614)]\tLoss Ss: 0.005993\n","\tEpoch:13 [001/005 (0520/0614)]\tLoss Ss: 0.010010\n","\tEpoch:13 [001/005 (0540/0614)]\tLoss Ss: 0.005201\n","\tEpoch:13 [001/005 (0560/0614)]\tLoss Ss: 0.009004\n","\tEpoch:13 [001/005 (0580/0614)]\tLoss Ss: 0.008198\n","\tEpoch:13 [001/005 (0600/0614)]\tLoss Ss: 0.009985\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tEpoch:13 [002/005 (0000/0693)]\tLoss Ss: 0.013222\n","\tEpoch:13 [002/005 (0020/0693)]\tLoss Ss: 0.017274\n","\tEpoch:13 [002/005 (0040/0693)]\tLoss Ss: 0.016568\n","\tEpoch:13 [002/005 (0060/0693)]\tLoss Ss: 0.013038\n","\tEpoch:13 [002/005 (0080/0693)]\tLoss Ss: 0.020363\n","\tEpoch:13 [002/005 (0100/0693)]\tLoss Ss: 0.016711\n","\tEpoch:13 [002/005 (0120/0693)]\tLoss Ss: 0.021698\n","\tEpoch:13 [002/005 (0140/0693)]\tLoss Ss: 0.018038\n","\tEpoch:13 [002/005 (0160/0693)]\tLoss Ss: 0.015559\n","\tEpoch:13 [002/005 (0180/0693)]\tLoss Ss: 0.014601\n","\tEpoch:13 [002/005 (0200/0693)]\tLoss Ss: 0.019457\n","\tEpoch:13 [002/005 (0220/0693)]\tLoss Ss: 0.012050\n","\tEpoch:13 [002/005 (0240/0693)]\tLoss Ss: 0.024067\n","\tEpoch:13 [002/005 (0260/0693)]\tLoss Ss: 0.022670\n","\tEpoch:13 [002/005 (0280/0693)]\tLoss Ss: 0.016048\n","\tEpoch:13 [002/005 (0300/0693)]\tLoss Ss: 0.020193\n","\tEpoch:13 [002/005 (0320/0693)]\tLoss Ss: 0.016229\n","\tEpoch:13 [002/005 (0340/0693)]\tLoss Ss: 0.019119\n","\tEpoch:13 [002/005 (0360/0693)]\tLoss Ss: 0.012641\n","\tEpoch:13 [002/005 (0380/0693)]\tLoss Ss: 0.009257\n","\tEpoch:13 [002/005 (0400/0693)]\tLoss Ss: 0.017688\n","\tEpoch:13 [002/005 (0420/0693)]\tLoss Ss: 0.010614\n","\tEpoch:13 [002/005 (0440/0693)]\tLoss Ss: 0.016434\n","\tEpoch:13 [002/005 (0460/0693)]\tLoss Ss: 0.011479\n","\tEpoch:13 [002/005 (0480/0693)]\tLoss Ss: 0.013126\n","\tEpoch:13 [002/005 (0500/0693)]\tLoss Ss: 0.015143\n","\tEpoch:13 [002/005 (0520/0693)]\tLoss Ss: 0.009758\n","\tEpoch:13 [002/005 (0540/0693)]\tLoss Ss: 0.013664\n","\tEpoch:13 [002/005 (0560/0693)]\tLoss Ss: 0.011742\n","\tEpoch:13 [002/005 (0580/0693)]\tLoss Ss: 0.011149\n","\tEpoch:13 [002/005 (0600/0693)]\tLoss Ss: 0.010934\n","\tEpoch:13 [002/005 (0620/0693)]\tLoss Ss: 0.009860\n","\tEpoch:13 [002/005 (0640/0693)]\tLoss Ss: 0.018234\n","\tEpoch:13 [002/005 (0660/0693)]\tLoss Ss: 0.010547\n","\tEpoch:13 [002/005 (0680/0693)]\tLoss Ss: 0.015890\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tEpoch:13 [003/005 (0000/0693)]\tLoss Ss: 0.015055\n","\tEpoch:13 [003/005 (0020/0693)]\tLoss Ss: 0.014441\n","\tEpoch:13 [003/005 (0040/0693)]\tLoss Ss: 0.019935\n","\tEpoch:13 [003/005 (0060/0693)]\tLoss Ss: 0.013530\n","\tEpoch:13 [003/005 (0080/0693)]\tLoss Ss: 0.014392\n","\tEpoch:13 [003/005 (0100/0693)]\tLoss Ss: 0.019504\n","\tEpoch:13 [003/005 (0120/0693)]\tLoss Ss: 0.018931\n","\tEpoch:13 [003/005 (0140/0693)]\tLoss Ss: 0.012484\n","\tEpoch:13 [003/005 (0160/0693)]\tLoss Ss: 0.015811\n","\tEpoch:13 [003/005 (0180/0693)]\tLoss Ss: 0.016492\n","\tEpoch:13 [003/005 (0200/0693)]\tLoss Ss: 0.016629\n","\tEpoch:13 [003/005 (0220/0693)]\tLoss Ss: 0.024210\n","\tEpoch:13 [003/005 (0240/0693)]\tLoss Ss: 0.013526\n","\tEpoch:13 [003/005 (0260/0693)]\tLoss Ss: 0.016258\n","\tEpoch:13 [003/005 (0280/0693)]\tLoss Ss: 0.020016\n","\tEpoch:13 [003/005 (0300/0693)]\tLoss Ss: 0.012900\n","\tEpoch:13 [003/005 (0320/0693)]\tLoss Ss: 0.015112\n","\tEpoch:13 [003/005 (0340/0693)]\tLoss Ss: 0.013081\n","\tEpoch:13 [003/005 (0360/0693)]\tLoss Ss: 0.025530\n","\tEpoch:13 [003/005 (0380/0693)]\tLoss Ss: 0.016677\n","\tEpoch:13 [003/005 (0400/0693)]\tLoss Ss: 0.012387\n","\tEpoch:13 [003/005 (0420/0693)]\tLoss Ss: 0.015770\n","\tEpoch:13 [003/005 (0440/0693)]\tLoss Ss: 0.014847\n","\tEpoch:13 [003/005 (0460/0693)]\tLoss Ss: 0.016588\n","\tEpoch:13 [003/005 (0480/0693)]\tLoss Ss: 0.017252\n","\tEpoch:13 [003/005 (0500/0693)]\tLoss Ss: 0.012271\n","\tEpoch:13 [003/005 (0520/0693)]\tLoss Ss: 0.015070\n","\tEpoch:13 [003/005 (0540/0693)]\tLoss Ss: 0.017936\n","\tEpoch:13 [003/005 (0560/0693)]\tLoss Ss: 0.015992\n","\tEpoch:13 [003/005 (0580/0693)]\tLoss Ss: 0.014041\n","\tEpoch:13 [003/005 (0600/0693)]\tLoss Ss: 0.015677\n","\tEpoch:13 [003/005 (0620/0693)]\tLoss Ss: 0.011637\n","\tEpoch:13 [003/005 (0640/0693)]\tLoss Ss: 0.012796\n","\tEpoch:13 [003/005 (0660/0693)]\tLoss Ss: 0.018768\n","\tEpoch:13 [003/005 (0680/0693)]\tLoss Ss: 0.020556\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:13 [004/005 (0000/0755)]\tLoss Ss: 0.023847\n","\tEpoch:13 [004/005 (0020/0755)]\tLoss Ss: 0.015112\n","\tEpoch:13 [004/005 (0040/0755)]\tLoss Ss: 0.013687\n","\tEpoch:13 [004/005 (0060/0755)]\tLoss Ss: 0.018603\n","\tEpoch:13 [004/005 (0080/0755)]\tLoss Ss: 0.017168\n","\tEpoch:13 [004/005 (0100/0755)]\tLoss Ss: 0.019912\n","\tEpoch:13 [004/005 (0120/0755)]\tLoss Ss: 0.022946\n","\tEpoch:13 [004/005 (0140/0755)]\tLoss Ss: 0.015630\n","\tEpoch:13 [004/005 (0160/0755)]\tLoss Ss: 0.013411\n","\tEpoch:13 [004/005 (0180/0755)]\tLoss Ss: 0.017044\n","\tEpoch:13 [004/005 (0200/0755)]\tLoss Ss: 0.017133\n","\tEpoch:13 [004/005 (0220/0755)]\tLoss Ss: 0.012598\n","\tEpoch:13 [004/005 (0240/0755)]\tLoss Ss: 0.011533\n","\tEpoch:13 [004/005 (0260/0755)]\tLoss Ss: 0.011314\n","\tEpoch:13 [004/005 (0280/0755)]\tLoss Ss: 0.012391\n","\tEpoch:13 [004/005 (0300/0755)]\tLoss Ss: 0.018607\n","\tEpoch:13 [004/005 (0320/0755)]\tLoss Ss: 0.013016\n","\tEpoch:13 [004/005 (0340/0755)]\tLoss Ss: 0.017649\n","\tEpoch:13 [004/005 (0360/0755)]\tLoss Ss: 0.014782\n","\tEpoch:13 [004/005 (0380/0755)]\tLoss Ss: 0.016117\n","\tEpoch:13 [004/005 (0400/0755)]\tLoss Ss: 0.015638\n","\tEpoch:13 [004/005 (0420/0755)]\tLoss Ss: 0.017128\n","\tEpoch:13 [004/005 (0440/0755)]\tLoss Ss: 0.008680\n","\tEpoch:13 [004/005 (0460/0755)]\tLoss Ss: 0.014015\n","\tEpoch:13 [004/005 (0480/0755)]\tLoss Ss: 0.018230\n","\tEpoch:13 [004/005 (0500/0755)]\tLoss Ss: 0.018281\n","\tEpoch:13 [004/005 (0520/0755)]\tLoss Ss: 0.017163\n","\tEpoch:13 [004/005 (0540/0755)]\tLoss Ss: 0.014167\n","\tEpoch:13 [004/005 (0560/0755)]\tLoss Ss: 0.029120\n","\tEpoch:13 [004/005 (0580/0755)]\tLoss Ss: 0.014196\n","\tEpoch:13 [004/005 (0600/0755)]\tLoss Ss: 0.017897\n","\tEpoch:13 [004/005 (0620/0755)]\tLoss Ss: 0.017383\n","\tEpoch:13 [004/005 (0640/0755)]\tLoss Ss: 0.010054\n","\tEpoch:13 [004/005 (0660/0755)]\tLoss Ss: 0.019969\n","\tEpoch:13 [004/005 (0680/0755)]\tLoss Ss: 0.015168\n","\tEpoch:13 [004/005 (0700/0755)]\tLoss Ss: 0.009239\n","\tEpoch:13 [004/005 (0720/0755)]\tLoss Ss: 0.018893\n","\tEpoch:13 [004/005 (0740/0755)]\tLoss Ss: 0.012874\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tEpoch:13 [005/005 (0000/0588)]\tLoss Ss: 0.012757\n","\tEpoch:13 [005/005 (0020/0588)]\tLoss Ss: 0.010276\n","\tEpoch:13 [005/005 (0040/0588)]\tLoss Ss: 0.009996\n","\tEpoch:13 [005/005 (0060/0588)]\tLoss Ss: 0.006410\n","\tEpoch:13 [005/005 (0080/0588)]\tLoss Ss: 0.006486\n","\tEpoch:13 [005/005 (0100/0588)]\tLoss Ss: 0.009446\n","\tEpoch:13 [005/005 (0120/0588)]\tLoss Ss: 0.010538\n","\tEpoch:13 [005/005 (0140/0588)]\tLoss Ss: 0.008069\n","\tEpoch:13 [005/005 (0160/0588)]\tLoss Ss: 0.010895\n","\tEpoch:13 [005/005 (0180/0588)]\tLoss Ss: 0.007022\n","\tEpoch:13 [005/005 (0200/0588)]\tLoss Ss: 0.006462\n","\tEpoch:13 [005/005 (0220/0588)]\tLoss Ss: 0.008835\n","\tEpoch:13 [005/005 (0240/0588)]\tLoss Ss: 0.006929\n","\tEpoch:13 [005/005 (0260/0588)]\tLoss Ss: 0.005841\n","\tEpoch:13 [005/005 (0280/0588)]\tLoss Ss: 0.008085\n","\tEpoch:13 [005/005 (0300/0588)]\tLoss Ss: 0.010220\n","\tEpoch:13 [005/005 (0320/0588)]\tLoss Ss: 0.006405\n","\tEpoch:13 [005/005 (0340/0588)]\tLoss Ss: 0.005584\n","\tEpoch:13 [005/005 (0360/0588)]\tLoss Ss: 0.006657\n","\tEpoch:13 [005/005 (0380/0588)]\tLoss Ss: 0.013482\n","\tEpoch:13 [005/005 (0400/0588)]\tLoss Ss: 0.005323\n","\tEpoch:13 [005/005 (0420/0588)]\tLoss Ss: 0.003404\n","\tEpoch:13 [005/005 (0440/0588)]\tLoss Ss: 0.005826\n","\tEpoch:13 [005/005 (0460/0588)]\tLoss Ss: 0.006090\n","\tEpoch:13 [005/005 (0480/0588)]\tLoss Ss: 0.007327\n","\tEpoch:13 [005/005 (0500/0588)]\tLoss Ss: 0.006734\n","\tEpoch:13 [005/005 (0520/0588)]\tLoss Ss: 0.009690\n","\tEpoch:13 [005/005 (0540/0588)]\tLoss Ss: 0.005758\n","\tEpoch:13 [005/005 (0560/0588)]\tLoss Ss: 0.007267\n","\tEpoch:13 [005/005 (0580/0588)]\tLoss Ss: 0.012098\n","Now train the rotated image\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:13 [000/005 (0000/0755)]\tLoss Ss: 0.355885\n","\tRotated_Epoch:13 [000/005 (0020/0755)]\tLoss Ss: 0.264971\n","\tRotated_Epoch:13 [000/005 (0040/0755)]\tLoss Ss: 0.267781\n","\tRotated_Epoch:13 [000/005 (0060/0755)]\tLoss Ss: 0.197094\n","\tRotated_Epoch:13 [000/005 (0080/0755)]\tLoss Ss: 0.301689\n","\tRotated_Epoch:13 [000/005 (0100/0755)]\tLoss Ss: 0.222740\n","\tRotated_Epoch:13 [000/005 (0120/0755)]\tLoss Ss: 0.208190\n","\tRotated_Epoch:13 [000/005 (0140/0755)]\tLoss Ss: 0.136965\n","\tRotated_Epoch:13 [000/005 (0160/0755)]\tLoss Ss: 0.114029\n","\tRotated_Epoch:13 [000/005 (0180/0755)]\tLoss Ss: 0.113697\n","\tRotated_Epoch:13 [000/005 (0200/0755)]\tLoss Ss: 0.173255\n","\tRotated_Epoch:13 [000/005 (0220/0755)]\tLoss Ss: 0.120342\n","\tRotated_Epoch:13 [000/005 (0240/0755)]\tLoss Ss: 0.098869\n","\tRotated_Epoch:13 [000/005 (0260/0755)]\tLoss Ss: 0.097950\n","\tRotated_Epoch:13 [000/005 (0280/0755)]\tLoss Ss: 0.072671\n","\tRotated_Epoch:13 [000/005 (0300/0755)]\tLoss Ss: 0.076959\n","\tRotated_Epoch:13 [000/005 (0320/0755)]\tLoss Ss: 0.085147\n","\tRotated_Epoch:13 [000/005 (0340/0755)]\tLoss Ss: 0.082529\n","\tRotated_Epoch:13 [000/005 (0360/0755)]\tLoss Ss: 0.076181\n","\tRotated_Epoch:13 [000/005 (0380/0755)]\tLoss Ss: 0.076187\n","\tRotated_Epoch:13 [000/005 (0400/0755)]\tLoss Ss: 0.080418\n","\tRotated_Epoch:13 [000/005 (0420/0755)]\tLoss Ss: 0.061510\n","\tRotated_Epoch:13 [000/005 (0440/0755)]\tLoss Ss: 0.084228\n","\tRotated_Epoch:13 [000/005 (0460/0755)]\tLoss Ss: 0.065318\n","\tRotated_Epoch:13 [000/005 (0480/0755)]\tLoss Ss: 0.090263\n","\tRotated_Epoch:13 [000/005 (0500/0755)]\tLoss Ss: 0.082377\n","\tRotated_Epoch:13 [000/005 (0520/0755)]\tLoss Ss: 0.069675\n","\tRotated_Epoch:13 [000/005 (0540/0755)]\tLoss Ss: 0.077025\n","\tRotated_Epoch:13 [000/005 (0560/0755)]\tLoss Ss: 0.087158\n","\tRotated_Epoch:13 [000/005 (0580/0755)]\tLoss Ss: 0.084657\n","\tRotated_Epoch:13 [000/005 (0600/0755)]\tLoss Ss: 0.076465\n","\tRotated_Epoch:13 [000/005 (0620/0755)]\tLoss Ss: 0.082392\n","\tRotated_Epoch:13 [000/005 (0640/0755)]\tLoss Ss: 0.048045\n","\tRotated_Epoch:13 [000/005 (0660/0755)]\tLoss Ss: 0.068009\n","\tRotated_Epoch:13 [000/005 (0680/0755)]\tLoss Ss: 0.077736\n","\tRotated_Epoch:13 [000/005 (0700/0755)]\tLoss Ss: 0.066012\n","\tRotated_Epoch:13 [000/005 (0720/0755)]\tLoss Ss: 0.054202\n","\tRotated_Epoch:13 [000/005 (0740/0755)]\tLoss Ss: 0.075554\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tRotated_Epoch:13 [001/005 (0000/0588)]\tLoss Ss: 0.114162\n","\tRotated_Epoch:13 [001/005 (0020/0588)]\tLoss Ss: 0.102058\n","\tRotated_Epoch:13 [001/005 (0040/0588)]\tLoss Ss: 0.095345\n","\tRotated_Epoch:13 [001/005 (0060/0588)]\tLoss Ss: 0.126232\n","\tRotated_Epoch:13 [001/005 (0080/0588)]\tLoss Ss: 0.076033\n","\tRotated_Epoch:13 [001/005 (0100/0588)]\tLoss Ss: 0.108240\n","\tRotated_Epoch:13 [001/005 (0120/0588)]\tLoss Ss: 0.104355\n","\tRotated_Epoch:13 [001/005 (0140/0588)]\tLoss Ss: 0.104009\n","\tRotated_Epoch:13 [001/005 (0160/0588)]\tLoss Ss: 0.077233\n","\tRotated_Epoch:13 [001/005 (0180/0588)]\tLoss Ss: 0.044239\n","\tRotated_Epoch:13 [001/005 (0200/0588)]\tLoss Ss: 0.073172\n","\tRotated_Epoch:13 [001/005 (0220/0588)]\tLoss Ss: 0.080027\n","\tRotated_Epoch:13 [001/005 (0240/0588)]\tLoss Ss: 0.066289\n","\tRotated_Epoch:13 [001/005 (0260/0588)]\tLoss Ss: 0.077593\n","\tRotated_Epoch:13 [001/005 (0280/0588)]\tLoss Ss: 0.071538\n","\tRotated_Epoch:13 [001/005 (0300/0588)]\tLoss Ss: 0.080095\n","\tRotated_Epoch:13 [001/005 (0320/0588)]\tLoss Ss: 0.075522\n","\tRotated_Epoch:13 [001/005 (0340/0588)]\tLoss Ss: 0.068504\n","\tRotated_Epoch:13 [001/005 (0360/0588)]\tLoss Ss: 0.071729\n","\tRotated_Epoch:13 [001/005 (0380/0588)]\tLoss Ss: 0.087011\n","\tRotated_Epoch:13 [001/005 (0400/0588)]\tLoss Ss: 0.059855\n","\tRotated_Epoch:13 [001/005 (0420/0588)]\tLoss Ss: 0.072592\n","\tRotated_Epoch:13 [001/005 (0440/0588)]\tLoss Ss: 0.076530\n","\tRotated_Epoch:13 [001/005 (0460/0588)]\tLoss Ss: 0.049360\n","\tRotated_Epoch:13 [001/005 (0480/0588)]\tLoss Ss: 0.054928\n","\tRotated_Epoch:13 [001/005 (0500/0588)]\tLoss Ss: 0.077250\n","\tRotated_Epoch:13 [001/005 (0520/0588)]\tLoss Ss: 0.076578\n","\tRotated_Epoch:13 [001/005 (0540/0588)]\tLoss Ss: 0.060758\n","\tRotated_Epoch:13 [001/005 (0560/0588)]\tLoss Ss: 0.064687\n","\tRotated_Epoch:13 [001/005 (0580/0588)]\tLoss Ss: 0.046210\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:13 [002/005 (0000/0755)]\tLoss Ss: 0.203185\n","\tRotated_Epoch:13 [002/005 (0020/0755)]\tLoss Ss: 0.169876\n","\tRotated_Epoch:13 [002/005 (0040/0755)]\tLoss Ss: 0.143119\n","\tRotated_Epoch:13 [002/005 (0060/0755)]\tLoss Ss: 0.136545\n","\tRotated_Epoch:13 [002/005 (0080/0755)]\tLoss Ss: 0.126986\n","\tRotated_Epoch:13 [002/005 (0100/0755)]\tLoss Ss: 0.078042\n","\tRotated_Epoch:13 [002/005 (0120/0755)]\tLoss Ss: 0.065111\n","\tRotated_Epoch:13 [002/005 (0140/0755)]\tLoss Ss: 0.066689\n","\tRotated_Epoch:13 [002/005 (0160/0755)]\tLoss Ss: 0.063290\n","\tRotated_Epoch:13 [002/005 (0180/0755)]\tLoss Ss: 0.064346\n","\tRotated_Epoch:13 [002/005 (0200/0755)]\tLoss Ss: 0.081302\n","\tRotated_Epoch:13 [002/005 (0220/0755)]\tLoss Ss: 0.069153\n","\tRotated_Epoch:13 [002/005 (0240/0755)]\tLoss Ss: 0.046914\n","\tRotated_Epoch:13 [002/005 (0260/0755)]\tLoss Ss: 0.044037\n","\tRotated_Epoch:13 [002/005 (0280/0755)]\tLoss Ss: 0.056753\n","\tRotated_Epoch:13 [002/005 (0300/0755)]\tLoss Ss: 0.052450\n","\tRotated_Epoch:13 [002/005 (0320/0755)]\tLoss Ss: 0.069661\n","\tRotated_Epoch:13 [002/005 (0340/0755)]\tLoss Ss: 0.054268\n","\tRotated_Epoch:13 [002/005 (0360/0755)]\tLoss Ss: 0.033736\n","\tRotated_Epoch:13 [002/005 (0380/0755)]\tLoss Ss: 0.055882\n","\tRotated_Epoch:13 [002/005 (0400/0755)]\tLoss Ss: 0.031136\n","\tRotated_Epoch:13 [002/005 (0420/0755)]\tLoss Ss: 0.053934\n","\tRotated_Epoch:13 [002/005 (0440/0755)]\tLoss Ss: 0.049342\n","\tRotated_Epoch:13 [002/005 (0460/0755)]\tLoss Ss: 0.037624\n","\tRotated_Epoch:13 [002/005 (0480/0755)]\tLoss Ss: 0.046823\n","\tRotated_Epoch:13 [002/005 (0500/0755)]\tLoss Ss: 0.048353\n","\tRotated_Epoch:13 [002/005 (0520/0755)]\tLoss Ss: 0.027048\n","\tRotated_Epoch:13 [002/005 (0540/0755)]\tLoss Ss: 0.029372\n","\tRotated_Epoch:13 [002/005 (0560/0755)]\tLoss Ss: 0.044985\n","\tRotated_Epoch:13 [002/005 (0580/0755)]\tLoss Ss: 0.033503\n","\tRotated_Epoch:13 [002/005 (0600/0755)]\tLoss Ss: 0.045504\n","\tRotated_Epoch:13 [002/005 (0620/0755)]\tLoss Ss: 0.035817\n","\tRotated_Epoch:13 [002/005 (0640/0755)]\tLoss Ss: 0.030864\n","\tRotated_Epoch:13 [002/005 (0660/0755)]\tLoss Ss: 0.025380\n","\tRotated_Epoch:13 [002/005 (0680/0755)]\tLoss Ss: 0.038169\n","\tRotated_Epoch:13 [002/005 (0700/0755)]\tLoss Ss: 0.031403\n","\tRotated_Epoch:13 [002/005 (0720/0755)]\tLoss Ss: 0.030420\n","\tRotated_Epoch:13 [002/005 (0740/0755)]\tLoss Ss: 0.028692\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tRotated_Epoch:13 [003/005 (0000/0693)]\tLoss Ss: 0.036788\n","\tRotated_Epoch:13 [003/005 (0020/0693)]\tLoss Ss: 0.041668\n","\tRotated_Epoch:13 [003/005 (0040/0693)]\tLoss Ss: 0.029852\n","\tRotated_Epoch:13 [003/005 (0060/0693)]\tLoss Ss: 0.033611\n","\tRotated_Epoch:13 [003/005 (0080/0693)]\tLoss Ss: 0.031380\n","\tRotated_Epoch:13 [003/005 (0100/0693)]\tLoss Ss: 0.029370\n","\tRotated_Epoch:13 [003/005 (0120/0693)]\tLoss Ss: 0.026769\n","\tRotated_Epoch:13 [003/005 (0140/0693)]\tLoss Ss: 0.038921\n","\tRotated_Epoch:13 [003/005 (0160/0693)]\tLoss Ss: 0.018944\n","\tRotated_Epoch:13 [003/005 (0180/0693)]\tLoss Ss: 0.044628\n","\tRotated_Epoch:13 [003/005 (0200/0693)]\tLoss Ss: 0.029509\n","\tRotated_Epoch:13 [003/005 (0220/0693)]\tLoss Ss: 0.021442\n","\tRotated_Epoch:13 [003/005 (0240/0693)]\tLoss Ss: 0.028920\n","\tRotated_Epoch:13 [003/005 (0260/0693)]\tLoss Ss: 0.032610\n","\tRotated_Epoch:13 [003/005 (0280/0693)]\tLoss Ss: 0.028940\n","\tRotated_Epoch:13 [003/005 (0300/0693)]\tLoss Ss: 0.034311\n","\tRotated_Epoch:13 [003/005 (0320/0693)]\tLoss Ss: 0.014877\n","\tRotated_Epoch:13 [003/005 (0340/0693)]\tLoss Ss: 0.020845\n","\tRotated_Epoch:13 [003/005 (0360/0693)]\tLoss Ss: 0.025208\n","\tRotated_Epoch:13 [003/005 (0380/0693)]\tLoss Ss: 0.023163\n","\tRotated_Epoch:13 [003/005 (0400/0693)]\tLoss Ss: 0.024243\n","\tRotated_Epoch:13 [003/005 (0420/0693)]\tLoss Ss: 0.025238\n","\tRotated_Epoch:13 [003/005 (0440/0693)]\tLoss Ss: 0.014902\n","\tRotated_Epoch:13 [003/005 (0460/0693)]\tLoss Ss: 0.017182\n","\tRotated_Epoch:13 [003/005 (0480/0693)]\tLoss Ss: 0.026762\n","\tRotated_Epoch:13 [003/005 (0500/0693)]\tLoss Ss: 0.022615\n","\tRotated_Epoch:13 [003/005 (0520/0693)]\tLoss Ss: 0.015703\n","\tRotated_Epoch:13 [003/005 (0540/0693)]\tLoss Ss: 0.025743\n","\tRotated_Epoch:13 [003/005 (0560/0693)]\tLoss Ss: 0.017298\n","\tRotated_Epoch:13 [003/005 (0580/0693)]\tLoss Ss: 0.018997\n","\tRotated_Epoch:13 [003/005 (0600/0693)]\tLoss Ss: 0.024999\n","\tRotated_Epoch:13 [003/005 (0620/0693)]\tLoss Ss: 0.021548\n","\tRotated_Epoch:13 [003/005 (0640/0693)]\tLoss Ss: 0.026239\n","\tRotated_Epoch:13 [003/005 (0660/0693)]\tLoss Ss: 0.018612\n","\tRotated_Epoch:13 [003/005 (0680/0693)]\tLoss Ss: 0.021225\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tRotated_Epoch:13 [004/005 (0000/0693)]\tLoss Ss: 0.018786\n","\tRotated_Epoch:13 [004/005 (0020/0693)]\tLoss Ss: 0.024605\n","\tRotated_Epoch:13 [004/005 (0040/0693)]\tLoss Ss: 0.022492\n","\tRotated_Epoch:13 [004/005 (0060/0693)]\tLoss Ss: 0.018354\n","\tRotated_Epoch:13 [004/005 (0080/0693)]\tLoss Ss: 0.020928\n","\tRotated_Epoch:13 [004/005 (0100/0693)]\tLoss Ss: 0.018014\n","\tRotated_Epoch:13 [004/005 (0120/0693)]\tLoss Ss: 0.022254\n","\tRotated_Epoch:13 [004/005 (0140/0693)]\tLoss Ss: 0.021125\n","\tRotated_Epoch:13 [004/005 (0160/0693)]\tLoss Ss: 0.021667\n","\tRotated_Epoch:13 [004/005 (0180/0693)]\tLoss Ss: 0.022356\n","\tRotated_Epoch:13 [004/005 (0200/0693)]\tLoss Ss: 0.014908\n","\tRotated_Epoch:13 [004/005 (0220/0693)]\tLoss Ss: 0.018710\n","\tRotated_Epoch:13 [004/005 (0240/0693)]\tLoss Ss: 0.024383\n","\tRotated_Epoch:13 [004/005 (0260/0693)]\tLoss Ss: 0.026758\n","\tRotated_Epoch:13 [004/005 (0280/0693)]\tLoss Ss: 0.014776\n","\tRotated_Epoch:13 [004/005 (0300/0693)]\tLoss Ss: 0.013516\n","\tRotated_Epoch:13 [004/005 (0320/0693)]\tLoss Ss: 0.026997\n","\tRotated_Epoch:13 [004/005 (0340/0693)]\tLoss Ss: 0.016586\n","\tRotated_Epoch:13 [004/005 (0360/0693)]\tLoss Ss: 0.021390\n","\tRotated_Epoch:13 [004/005 (0380/0693)]\tLoss Ss: 0.025159\n","\tRotated_Epoch:13 [004/005 (0400/0693)]\tLoss Ss: 0.021895\n","\tRotated_Epoch:13 [004/005 (0420/0693)]\tLoss Ss: 0.023860\n","\tRotated_Epoch:13 [004/005 (0440/0693)]\tLoss Ss: 0.015357\n","\tRotated_Epoch:13 [004/005 (0460/0693)]\tLoss Ss: 0.016619\n","\tRotated_Epoch:13 [004/005 (0480/0693)]\tLoss Ss: 0.029128\n","\tRotated_Epoch:13 [004/005 (0500/0693)]\tLoss Ss: 0.012584\n","\tRotated_Epoch:13 [004/005 (0520/0693)]\tLoss Ss: 0.017699\n","\tRotated_Epoch:13 [004/005 (0540/0693)]\tLoss Ss: 0.017587\n","\tRotated_Epoch:13 [004/005 (0560/0693)]\tLoss Ss: 0.012903\n","\tRotated_Epoch:13 [004/005 (0580/0693)]\tLoss Ss: 0.012044\n","\tRotated_Epoch:13 [004/005 (0600/0693)]\tLoss Ss: 0.013644\n","\tRotated_Epoch:13 [004/005 (0620/0693)]\tLoss Ss: 0.017432\n","\tRotated_Epoch:13 [004/005 (0640/0693)]\tLoss Ss: 0.018046\n","\tRotated_Epoch:13 [004/005 (0660/0693)]\tLoss Ss: 0.017413\n","\tRotated_Epoch:13 [004/005 (0680/0693)]\tLoss Ss: 0.018581\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tRotated_Epoch:13 [005/005 (0000/0614)]\tLoss Ss: 0.041574\n","\tRotated_Epoch:13 [005/005 (0020/0614)]\tLoss Ss: 0.040791\n","\tRotated_Epoch:13 [005/005 (0040/0614)]\tLoss Ss: 0.023075\n","\tRotated_Epoch:13 [005/005 (0060/0614)]\tLoss Ss: 0.028886\n","\tRotated_Epoch:13 [005/005 (0080/0614)]\tLoss Ss: 0.020828\n","\tRotated_Epoch:13 [005/005 (0100/0614)]\tLoss Ss: 0.014336\n","\tRotated_Epoch:13 [005/005 (0120/0614)]\tLoss Ss: 0.012581\n","\tRotated_Epoch:13 [005/005 (0140/0614)]\tLoss Ss: 0.014592\n","\tRotated_Epoch:13 [005/005 (0160/0614)]\tLoss Ss: 0.013823\n","\tRotated_Epoch:13 [005/005 (0180/0614)]\tLoss Ss: 0.009807\n","\tRotated_Epoch:13 [005/005 (0200/0614)]\tLoss Ss: 0.014391\n","\tRotated_Epoch:13 [005/005 (0220/0614)]\tLoss Ss: 0.012103\n","\tRotated_Epoch:13 [005/005 (0240/0614)]\tLoss Ss: 0.009676\n","\tRotated_Epoch:13 [005/005 (0260/0614)]\tLoss Ss: 0.011466\n","\tRotated_Epoch:13 [005/005 (0280/0614)]\tLoss Ss: 0.010458\n","\tRotated_Epoch:13 [005/005 (0300/0614)]\tLoss Ss: 0.009191\n","\tRotated_Epoch:13 [005/005 (0320/0614)]\tLoss Ss: 0.018941\n","\tRotated_Epoch:13 [005/005 (0340/0614)]\tLoss Ss: 0.008617\n","\tRotated_Epoch:13 [005/005 (0360/0614)]\tLoss Ss: 0.006698\n","\tRotated_Epoch:13 [005/005 (0380/0614)]\tLoss Ss: 0.009221\n","\tRotated_Epoch:13 [005/005 (0400/0614)]\tLoss Ss: 0.009883\n","\tRotated_Epoch:13 [005/005 (0420/0614)]\tLoss Ss: 0.008481\n","\tRotated_Epoch:13 [005/005 (0440/0614)]\tLoss Ss: 0.010545\n","\tRotated_Epoch:13 [005/005 (0460/0614)]\tLoss Ss: 0.007238\n","\tRotated_Epoch:13 [005/005 (0480/0614)]\tLoss Ss: 0.006437\n","\tRotated_Epoch:13 [005/005 (0500/0614)]\tLoss Ss: 0.007196\n","\tRotated_Epoch:13 [005/005 (0520/0614)]\tLoss Ss: 0.006022\n","\tRotated_Epoch:13 [005/005 (0540/0614)]\tLoss Ss: 0.007150\n","\tRotated_Epoch:13 [005/005 (0560/0614)]\tLoss Ss: 0.009252\n","\tRotated_Epoch:13 [005/005 (0580/0614)]\tLoss Ss: 0.007524\n","\tRotated_Epoch:13 [005/005 (0600/0614)]\tLoss Ss: 0.006857\n","new_1_6_d.nii\n","new_1_6_m.nii\n","new_1_7_d.nii\n","new_1_7_m.nii\n","new_2_5_d.nii\n","new_2_5_m.nii\n","new_2_6_d.nii\n","new_2_6_m.nii\n","new_2_7_d.nii\n","new_2_7_m.nii\n","new_3_d (2).nii\n","new_3_m (2).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (3).nii\n","new_3_m (3).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (7).nii\n","new_3_m (7).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (8).nii\n","new_3_m (8).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (9).nii\n","new_3_m (9).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_4_10_d_163526.nii\n","new_4_10_m_163526.nii.gz\n","new_4_11_d_084336.nii\n","new_4_11_m_084336.nii.gz\n","new_4_7_d_184422.nii\n","new_4_7_m_184422.nii.gz\n","new_4_8_d_143412.nii\n","new_4_8_m_143412.nii.gz\n","new_4_9_d_155347.nii\n","new_4_9_m_155347.nii.gz\n","\tEpoch: 13; Dice: 0.9580 +/- 0.0095; Loss: 14.3103\n","Begin Epoch 14\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tEpoch:14 [000/005 (0000/0693)]\tLoss Ss: 0.017892\n","\tEpoch:14 [000/005 (0020/0693)]\tLoss Ss: 0.021504\n","\tEpoch:14 [000/005 (0040/0693)]\tLoss Ss: 0.017348\n","\tEpoch:14 [000/005 (0060/0693)]\tLoss Ss: 0.024269\n","\tEpoch:14 [000/005 (0080/0693)]\tLoss Ss: 0.017909\n","\tEpoch:14 [000/005 (0100/0693)]\tLoss Ss: 0.020413\n","\tEpoch:14 [000/005 (0120/0693)]\tLoss Ss: 0.016118\n","\tEpoch:14 [000/005 (0140/0693)]\tLoss Ss: 0.019602\n","\tEpoch:14 [000/005 (0160/0693)]\tLoss Ss: 0.015755\n","\tEpoch:14 [000/005 (0180/0693)]\tLoss Ss: 0.013991\n","\tEpoch:14 [000/005 (0200/0693)]\tLoss Ss: 0.016466\n","\tEpoch:14 [000/005 (0220/0693)]\tLoss Ss: 0.014693\n","\tEpoch:14 [000/005 (0240/0693)]\tLoss Ss: 0.017078\n","\tEpoch:14 [000/005 (0260/0693)]\tLoss Ss: 0.014086\n","\tEpoch:14 [000/005 (0280/0693)]\tLoss Ss: 0.014915\n","\tEpoch:14 [000/005 (0300/0693)]\tLoss Ss: 0.018283\n","\tEpoch:14 [000/005 (0320/0693)]\tLoss Ss: 0.014915\n","\tEpoch:14 [000/005 (0340/0693)]\tLoss Ss: 0.021485\n","\tEpoch:14 [000/005 (0360/0693)]\tLoss Ss: 0.013597\n","\tEpoch:14 [000/005 (0380/0693)]\tLoss Ss: 0.018940\n","\tEpoch:14 [000/005 (0400/0693)]\tLoss Ss: 0.013822\n","\tEpoch:14 [000/005 (0420/0693)]\tLoss Ss: 0.011216\n","\tEpoch:14 [000/005 (0440/0693)]\tLoss Ss: 0.016385\n","\tEpoch:14 [000/005 (0460/0693)]\tLoss Ss: 0.012238\n","\tEpoch:14 [000/005 (0480/0693)]\tLoss Ss: 0.013326\n","\tEpoch:14 [000/005 (0500/0693)]\tLoss Ss: 0.016947\n","\tEpoch:14 [000/005 (0520/0693)]\tLoss Ss: 0.016157\n","\tEpoch:14 [000/005 (0540/0693)]\tLoss Ss: 0.012461\n","\tEpoch:14 [000/005 (0560/0693)]\tLoss Ss: 0.011236\n","\tEpoch:14 [000/005 (0580/0693)]\tLoss Ss: 0.016682\n","\tEpoch:14 [000/005 (0600/0693)]\tLoss Ss: 0.016971\n","\tEpoch:14 [000/005 (0620/0693)]\tLoss Ss: 0.018197\n","\tEpoch:14 [000/005 (0640/0693)]\tLoss Ss: 0.009383\n","\tEpoch:14 [000/005 (0660/0693)]\tLoss Ss: 0.011710\n","\tEpoch:14 [000/005 (0680/0693)]\tLoss Ss: 0.016548\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tEpoch:14 [001/005 (0000/0588)]\tLoss Ss: 0.008740\n","\tEpoch:14 [001/005 (0020/0588)]\tLoss Ss: 0.008367\n","\tEpoch:14 [001/005 (0040/0588)]\tLoss Ss: 0.009319\n","\tEpoch:14 [001/005 (0060/0588)]\tLoss Ss: 0.009442\n","\tEpoch:14 [001/005 (0080/0588)]\tLoss Ss: 0.006873\n","\tEpoch:14 [001/005 (0100/0588)]\tLoss Ss: 0.007910\n","\tEpoch:14 [001/005 (0120/0588)]\tLoss Ss: 0.006341\n","\tEpoch:14 [001/005 (0140/0588)]\tLoss Ss: 0.011348\n","\tEpoch:14 [001/005 (0160/0588)]\tLoss Ss: 0.009575\n","\tEpoch:14 [001/005 (0180/0588)]\tLoss Ss: 0.008277\n","\tEpoch:14 [001/005 (0200/0588)]\tLoss Ss: 0.008062\n","\tEpoch:14 [001/005 (0220/0588)]\tLoss Ss: 0.006901\n","\tEpoch:14 [001/005 (0240/0588)]\tLoss Ss: 0.008286\n","\tEpoch:14 [001/005 (0260/0588)]\tLoss Ss: 0.006352\n","\tEpoch:14 [001/005 (0280/0588)]\tLoss Ss: 0.008710\n","\tEpoch:14 [001/005 (0300/0588)]\tLoss Ss: 0.007358\n","\tEpoch:14 [001/005 (0320/0588)]\tLoss Ss: 0.005542\n","\tEpoch:14 [001/005 (0340/0588)]\tLoss Ss: 0.006721\n","\tEpoch:14 [001/005 (0360/0588)]\tLoss Ss: 0.006361\n","\tEpoch:14 [001/005 (0380/0588)]\tLoss Ss: 0.013513\n","\tEpoch:14 [001/005 (0400/0588)]\tLoss Ss: 0.005650\n","\tEpoch:14 [001/005 (0420/0588)]\tLoss Ss: 0.009724\n","\tEpoch:14 [001/005 (0440/0588)]\tLoss Ss: 0.006621\n","\tEpoch:14 [001/005 (0460/0588)]\tLoss Ss: 0.009172\n","\tEpoch:14 [001/005 (0480/0588)]\tLoss Ss: 0.013633\n","\tEpoch:14 [001/005 (0500/0588)]\tLoss Ss: 0.007084\n","\tEpoch:14 [001/005 (0520/0588)]\tLoss Ss: 0.006028\n","\tEpoch:14 [001/005 (0540/0588)]\tLoss Ss: 0.005849\n","\tEpoch:14 [001/005 (0560/0588)]\tLoss Ss: 0.005231\n","\tEpoch:14 [001/005 (0580/0588)]\tLoss Ss: 0.004652\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:14 [002/005 (0000/0755)]\tLoss Ss: 0.031353\n","\tEpoch:14 [002/005 (0020/0755)]\tLoss Ss: 0.019214\n","\tEpoch:14 [002/005 (0040/0755)]\tLoss Ss: 0.030150\n","\tEpoch:14 [002/005 (0060/0755)]\tLoss Ss: 0.037193\n","\tEpoch:14 [002/005 (0080/0755)]\tLoss Ss: 0.025771\n","\tEpoch:14 [002/005 (0100/0755)]\tLoss Ss: 0.015076\n","\tEpoch:14 [002/005 (0120/0755)]\tLoss Ss: 0.016768\n","\tEpoch:14 [002/005 (0140/0755)]\tLoss Ss: 0.026615\n","\tEpoch:14 [002/005 (0160/0755)]\tLoss Ss: 0.018659\n","\tEpoch:14 [002/005 (0180/0755)]\tLoss Ss: 0.024452\n","\tEpoch:14 [002/005 (0200/0755)]\tLoss Ss: 0.015184\n","\tEpoch:14 [002/005 (0220/0755)]\tLoss Ss: 0.024313\n","\tEpoch:14 [002/005 (0240/0755)]\tLoss Ss: 0.024495\n","\tEpoch:14 [002/005 (0260/0755)]\tLoss Ss: 0.015392\n","\tEpoch:14 [002/005 (0280/0755)]\tLoss Ss: 0.016326\n","\tEpoch:14 [002/005 (0300/0755)]\tLoss Ss: 0.016921\n","\tEpoch:14 [002/005 (0320/0755)]\tLoss Ss: 0.014056\n","\tEpoch:14 [002/005 (0340/0755)]\tLoss Ss: 0.015049\n","\tEpoch:14 [002/005 (0360/0755)]\tLoss Ss: 0.018018\n","\tEpoch:14 [002/005 (0380/0755)]\tLoss Ss: 0.016096\n","\tEpoch:14 [002/005 (0400/0755)]\tLoss Ss: 0.018045\n","\tEpoch:14 [002/005 (0420/0755)]\tLoss Ss: 0.010729\n","\tEpoch:14 [002/005 (0440/0755)]\tLoss Ss: 0.017657\n","\tEpoch:14 [002/005 (0460/0755)]\tLoss Ss: 0.017375\n","\tEpoch:14 [002/005 (0480/0755)]\tLoss Ss: 0.019407\n","\tEpoch:14 [002/005 (0500/0755)]\tLoss Ss: 0.013335\n","\tEpoch:14 [002/005 (0520/0755)]\tLoss Ss: 0.016656\n","\tEpoch:14 [002/005 (0540/0755)]\tLoss Ss: 0.008816\n","\tEpoch:14 [002/005 (0560/0755)]\tLoss Ss: 0.020306\n","\tEpoch:14 [002/005 (0580/0755)]\tLoss Ss: 0.021761\n","\tEpoch:14 [002/005 (0600/0755)]\tLoss Ss: 0.015849\n","\tEpoch:14 [002/005 (0620/0755)]\tLoss Ss: 0.018641\n","\tEpoch:14 [002/005 (0640/0755)]\tLoss Ss: 0.018038\n","\tEpoch:14 [002/005 (0660/0755)]\tLoss Ss: 0.009388\n","\tEpoch:14 [002/005 (0680/0755)]\tLoss Ss: 0.011537\n","\tEpoch:14 [002/005 (0700/0755)]\tLoss Ss: 0.018913\n","\tEpoch:14 [002/005 (0720/0755)]\tLoss Ss: 0.015361\n","\tEpoch:14 [002/005 (0740/0755)]\tLoss Ss: 0.011577\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tEpoch:14 [003/005 (0000/0614)]\tLoss Ss: 0.010658\n","\tEpoch:14 [003/005 (0020/0614)]\tLoss Ss: 0.010835\n","\tEpoch:14 [003/005 (0040/0614)]\tLoss Ss: 0.007761\n","\tEpoch:14 [003/005 (0060/0614)]\tLoss Ss: 0.006701\n","\tEpoch:14 [003/005 (0080/0614)]\tLoss Ss: 0.008053\n","\tEpoch:14 [003/005 (0100/0614)]\tLoss Ss: 0.009177\n","\tEpoch:14 [003/005 (0120/0614)]\tLoss Ss: 0.010011\n","\tEpoch:14 [003/005 (0140/0614)]\tLoss Ss: 0.007691\n","\tEpoch:14 [003/005 (0160/0614)]\tLoss Ss: 0.005400\n","\tEpoch:14 [003/005 (0180/0614)]\tLoss Ss: 0.005343\n","\tEpoch:14 [003/005 (0200/0614)]\tLoss Ss: 0.010186\n","\tEpoch:14 [003/005 (0220/0614)]\tLoss Ss: 0.008048\n","\tEpoch:14 [003/005 (0240/0614)]\tLoss Ss: 0.008650\n","\tEpoch:14 [003/005 (0260/0614)]\tLoss Ss: 0.006223\n","\tEpoch:14 [003/005 (0280/0614)]\tLoss Ss: 0.005838\n","\tEpoch:14 [003/005 (0300/0614)]\tLoss Ss: 0.005449\n","\tEpoch:14 [003/005 (0320/0614)]\tLoss Ss: 0.006261\n","\tEpoch:14 [003/005 (0340/0614)]\tLoss Ss: 0.005993\n","\tEpoch:14 [003/005 (0360/0614)]\tLoss Ss: 0.011052\n","\tEpoch:14 [003/005 (0380/0614)]\tLoss Ss: 0.006819\n","\tEpoch:14 [003/005 (0400/0614)]\tLoss Ss: 0.009398\n","\tEpoch:14 [003/005 (0420/0614)]\tLoss Ss: 0.004820\n","\tEpoch:14 [003/005 (0440/0614)]\tLoss Ss: 0.005813\n","\tEpoch:14 [003/005 (0460/0614)]\tLoss Ss: 0.007645\n","\tEpoch:14 [003/005 (0480/0614)]\tLoss Ss: 0.007001\n","\tEpoch:14 [003/005 (0500/0614)]\tLoss Ss: 0.006267\n","\tEpoch:14 [003/005 (0520/0614)]\tLoss Ss: 0.008467\n","\tEpoch:14 [003/005 (0540/0614)]\tLoss Ss: 0.004715\n","\tEpoch:14 [003/005 (0560/0614)]\tLoss Ss: 0.007148\n","\tEpoch:14 [003/005 (0580/0614)]\tLoss Ss: 0.004929\n","\tEpoch:14 [003/005 (0600/0614)]\tLoss Ss: 0.008288\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:14 [004/005 (0000/0755)]\tLoss Ss: 0.028212\n","\tEpoch:14 [004/005 (0020/0755)]\tLoss Ss: 0.027530\n","\tEpoch:14 [004/005 (0040/0755)]\tLoss Ss: 0.019451\n","\tEpoch:14 [004/005 (0060/0755)]\tLoss Ss: 0.020170\n","\tEpoch:14 [004/005 (0080/0755)]\tLoss Ss: 0.021182\n","\tEpoch:14 [004/005 (0100/0755)]\tLoss Ss: 0.025141\n","\tEpoch:14 [004/005 (0120/0755)]\tLoss Ss: 0.025982\n","\tEpoch:14 [004/005 (0140/0755)]\tLoss Ss: 0.018650\n","\tEpoch:14 [004/005 (0160/0755)]\tLoss Ss: 0.021693\n","\tEpoch:14 [004/005 (0180/0755)]\tLoss Ss: 0.018878\n","\tEpoch:14 [004/005 (0200/0755)]\tLoss Ss: 0.021444\n","\tEpoch:14 [004/005 (0220/0755)]\tLoss Ss: 0.017205\n","\tEpoch:14 [004/005 (0240/0755)]\tLoss Ss: 0.016297\n","\tEpoch:14 [004/005 (0260/0755)]\tLoss Ss: 0.015221\n","\tEpoch:14 [004/005 (0280/0755)]\tLoss Ss: 0.012880\n","\tEpoch:14 [004/005 (0300/0755)]\tLoss Ss: 0.025901\n","\tEpoch:14 [004/005 (0320/0755)]\tLoss Ss: 0.015832\n","\tEpoch:14 [004/005 (0340/0755)]\tLoss Ss: 0.017154\n","\tEpoch:14 [004/005 (0360/0755)]\tLoss Ss: 0.020722\n","\tEpoch:14 [004/005 (0380/0755)]\tLoss Ss: 0.012213\n","\tEpoch:14 [004/005 (0400/0755)]\tLoss Ss: 0.022875\n","\tEpoch:14 [004/005 (0420/0755)]\tLoss Ss: 0.012599\n","\tEpoch:14 [004/005 (0440/0755)]\tLoss Ss: 0.014701\n","\tEpoch:14 [004/005 (0460/0755)]\tLoss Ss: 0.018853\n","\tEpoch:14 [004/005 (0480/0755)]\tLoss Ss: 0.017509\n","\tEpoch:14 [004/005 (0500/0755)]\tLoss Ss: 0.027622\n","\tEpoch:14 [004/005 (0520/0755)]\tLoss Ss: 0.021643\n","\tEpoch:14 [004/005 (0540/0755)]\tLoss Ss: 0.017815\n","\tEpoch:14 [004/005 (0560/0755)]\tLoss Ss: 0.016067\n","\tEpoch:14 [004/005 (0580/0755)]\tLoss Ss: 0.020003\n","\tEpoch:14 [004/005 (0600/0755)]\tLoss Ss: 0.017145\n","\tEpoch:14 [004/005 (0620/0755)]\tLoss Ss: 0.013095\n","\tEpoch:14 [004/005 (0640/0755)]\tLoss Ss: 0.012397\n","\tEpoch:14 [004/005 (0660/0755)]\tLoss Ss: 0.022027\n","\tEpoch:14 [004/005 (0680/0755)]\tLoss Ss: 0.016347\n","\tEpoch:14 [004/005 (0700/0755)]\tLoss Ss: 0.015568\n","\tEpoch:14 [004/005 (0720/0755)]\tLoss Ss: 0.025567\n","\tEpoch:14 [004/005 (0740/0755)]\tLoss Ss: 0.010978\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tEpoch:14 [005/005 (0000/0693)]\tLoss Ss: 0.020474\n","\tEpoch:14 [005/005 (0020/0693)]\tLoss Ss: 0.021868\n","\tEpoch:14 [005/005 (0040/0693)]\tLoss Ss: 0.022629\n","\tEpoch:14 [005/005 (0060/0693)]\tLoss Ss: 0.023266\n","\tEpoch:14 [005/005 (0080/0693)]\tLoss Ss: 0.015223\n","\tEpoch:14 [005/005 (0100/0693)]\tLoss Ss: 0.017200\n","\tEpoch:14 [005/005 (0120/0693)]\tLoss Ss: 0.011284\n","\tEpoch:14 [005/005 (0140/0693)]\tLoss Ss: 0.015784\n","\tEpoch:14 [005/005 (0160/0693)]\tLoss Ss: 0.014966\n","\tEpoch:14 [005/005 (0180/0693)]\tLoss Ss: 0.010584\n","\tEpoch:14 [005/005 (0200/0693)]\tLoss Ss: 0.015933\n","\tEpoch:14 [005/005 (0220/0693)]\tLoss Ss: 0.017502\n","\tEpoch:14 [005/005 (0240/0693)]\tLoss Ss: 0.016353\n","\tEpoch:14 [005/005 (0260/0693)]\tLoss Ss: 0.019149\n","\tEpoch:14 [005/005 (0280/0693)]\tLoss Ss: 0.017817\n","\tEpoch:14 [005/005 (0300/0693)]\tLoss Ss: 0.014219\n","\tEpoch:14 [005/005 (0320/0693)]\tLoss Ss: 0.022319\n","\tEpoch:14 [005/005 (0340/0693)]\tLoss Ss: 0.019785\n","\tEpoch:14 [005/005 (0360/0693)]\tLoss Ss: 0.013661\n","\tEpoch:14 [005/005 (0380/0693)]\tLoss Ss: 0.020588\n","\tEpoch:14 [005/005 (0400/0693)]\tLoss Ss: 0.014386\n","\tEpoch:14 [005/005 (0420/0693)]\tLoss Ss: 0.012599\n","\tEpoch:14 [005/005 (0440/0693)]\tLoss Ss: 0.020881\n","\tEpoch:14 [005/005 (0460/0693)]\tLoss Ss: 0.013164\n","\tEpoch:14 [005/005 (0480/0693)]\tLoss Ss: 0.010647\n","\tEpoch:14 [005/005 (0500/0693)]\tLoss Ss: 0.013860\n","\tEpoch:14 [005/005 (0520/0693)]\tLoss Ss: 0.017948\n","\tEpoch:14 [005/005 (0540/0693)]\tLoss Ss: 0.014664\n","\tEpoch:14 [005/005 (0560/0693)]\tLoss Ss: 0.013466\n","\tEpoch:14 [005/005 (0580/0693)]\tLoss Ss: 0.020277\n","\tEpoch:14 [005/005 (0600/0693)]\tLoss Ss: 0.017135\n","\tEpoch:14 [005/005 (0620/0693)]\tLoss Ss: 0.024255\n","\tEpoch:14 [005/005 (0640/0693)]\tLoss Ss: 0.014344\n","\tEpoch:14 [005/005 (0660/0693)]\tLoss Ss: 0.012768\n","\tEpoch:14 [005/005 (0680/0693)]\tLoss Ss: 0.018202\n","Now train the rotated image\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tRotated_Epoch:14 [000/005 (0000/0614)]\tLoss Ss: 0.006627\n","\tRotated_Epoch:14 [000/005 (0020/0614)]\tLoss Ss: 0.014287\n","\tRotated_Epoch:14 [000/005 (0040/0614)]\tLoss Ss: 0.009832\n","\tRotated_Epoch:14 [000/005 (0060/0614)]\tLoss Ss: 0.006636\n","\tRotated_Epoch:14 [000/005 (0080/0614)]\tLoss Ss: 0.009225\n","\tRotated_Epoch:14 [000/005 (0100/0614)]\tLoss Ss: 0.017839\n","\tRotated_Epoch:14 [000/005 (0120/0614)]\tLoss Ss: 0.013329\n","\tRotated_Epoch:14 [000/005 (0140/0614)]\tLoss Ss: 0.005724\n","\tRotated_Epoch:14 [000/005 (0160/0614)]\tLoss Ss: 0.006671\n","\tRotated_Epoch:14 [000/005 (0180/0614)]\tLoss Ss: 0.010101\n","\tRotated_Epoch:14 [000/005 (0200/0614)]\tLoss Ss: 0.006779\n","\tRotated_Epoch:14 [000/005 (0220/0614)]\tLoss Ss: 0.007013\n","\tRotated_Epoch:14 [000/005 (0240/0614)]\tLoss Ss: 0.005450\n","\tRotated_Epoch:14 [000/005 (0260/0614)]\tLoss Ss: 0.007509\n","\tRotated_Epoch:14 [000/005 (0280/0614)]\tLoss Ss: 0.009431\n","\tRotated_Epoch:14 [000/005 (0300/0614)]\tLoss Ss: 0.006300\n","\tRotated_Epoch:14 [000/005 (0320/0614)]\tLoss Ss: 0.008632\n","\tRotated_Epoch:14 [000/005 (0340/0614)]\tLoss Ss: 0.004377\n","\tRotated_Epoch:14 [000/005 (0360/0614)]\tLoss Ss: 0.005669\n","\tRotated_Epoch:14 [000/005 (0380/0614)]\tLoss Ss: 0.005669\n","\tRotated_Epoch:14 [000/005 (0400/0614)]\tLoss Ss: 0.005432\n","\tRotated_Epoch:14 [000/005 (0420/0614)]\tLoss Ss: 0.008054\n","\tRotated_Epoch:14 [000/005 (0440/0614)]\tLoss Ss: 0.006977\n","\tRotated_Epoch:14 [000/005 (0460/0614)]\tLoss Ss: 0.005617\n","\tRotated_Epoch:14 [000/005 (0480/0614)]\tLoss Ss: 0.003708\n","\tRotated_Epoch:14 [000/005 (0500/0614)]\tLoss Ss: 0.004725\n","\tRotated_Epoch:14 [000/005 (0520/0614)]\tLoss Ss: 0.006613\n","\tRotated_Epoch:14 [000/005 (0540/0614)]\tLoss Ss: 0.004360\n","\tRotated_Epoch:14 [000/005 (0560/0614)]\tLoss Ss: 0.007284\n","\tRotated_Epoch:14 [000/005 (0580/0614)]\tLoss Ss: 0.008617\n","\tRotated_Epoch:14 [000/005 (0600/0614)]\tLoss Ss: 0.008114\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tRotated_Epoch:14 [001/005 (0000/0693)]\tLoss Ss: 0.020560\n","\tRotated_Epoch:14 [001/005 (0020/0693)]\tLoss Ss: 0.015591\n","\tRotated_Epoch:14 [001/005 (0040/0693)]\tLoss Ss: 0.015544\n","\tRotated_Epoch:14 [001/005 (0060/0693)]\tLoss Ss: 0.017303\n","\tRotated_Epoch:14 [001/005 (0080/0693)]\tLoss Ss: 0.018676\n","\tRotated_Epoch:14 [001/005 (0100/0693)]\tLoss Ss: 0.022378\n","\tRotated_Epoch:14 [001/005 (0120/0693)]\tLoss Ss: 0.023850\n","\tRotated_Epoch:14 [001/005 (0140/0693)]\tLoss Ss: 0.018916\n","\tRotated_Epoch:14 [001/005 (0160/0693)]\tLoss Ss: 0.017155\n","\tRotated_Epoch:14 [001/005 (0180/0693)]\tLoss Ss: 0.022841\n","\tRotated_Epoch:14 [001/005 (0200/0693)]\tLoss Ss: 0.019599\n","\tRotated_Epoch:14 [001/005 (0220/0693)]\tLoss Ss: 0.021412\n","\tRotated_Epoch:14 [001/005 (0240/0693)]\tLoss Ss: 0.018351\n","\tRotated_Epoch:14 [001/005 (0260/0693)]\tLoss Ss: 0.022336\n","\tRotated_Epoch:14 [001/005 (0280/0693)]\tLoss Ss: 0.014888\n","\tRotated_Epoch:14 [001/005 (0300/0693)]\tLoss Ss: 0.012504\n","\tRotated_Epoch:14 [001/005 (0320/0693)]\tLoss Ss: 0.011417\n","\tRotated_Epoch:14 [001/005 (0340/0693)]\tLoss Ss: 0.013685\n","\tRotated_Epoch:14 [001/005 (0360/0693)]\tLoss Ss: 0.018621\n","\tRotated_Epoch:14 [001/005 (0380/0693)]\tLoss Ss: 0.014742\n","\tRotated_Epoch:14 [001/005 (0400/0693)]\tLoss Ss: 0.014725\n","\tRotated_Epoch:14 [001/005 (0420/0693)]\tLoss Ss: 0.014631\n","\tRotated_Epoch:14 [001/005 (0440/0693)]\tLoss Ss: 0.014428\n","\tRotated_Epoch:14 [001/005 (0460/0693)]\tLoss Ss: 0.011962\n","\tRotated_Epoch:14 [001/005 (0480/0693)]\tLoss Ss: 0.014243\n","\tRotated_Epoch:14 [001/005 (0500/0693)]\tLoss Ss: 0.014295\n","\tRotated_Epoch:14 [001/005 (0520/0693)]\tLoss Ss: 0.018305\n","\tRotated_Epoch:14 [001/005 (0540/0693)]\tLoss Ss: 0.016082\n","\tRotated_Epoch:14 [001/005 (0560/0693)]\tLoss Ss: 0.014460\n","\tRotated_Epoch:14 [001/005 (0580/0693)]\tLoss Ss: 0.012946\n","\tRotated_Epoch:14 [001/005 (0600/0693)]\tLoss Ss: 0.012116\n","\tRotated_Epoch:14 [001/005 (0620/0693)]\tLoss Ss: 0.019139\n","\tRotated_Epoch:14 [001/005 (0640/0693)]\tLoss Ss: 0.017786\n","\tRotated_Epoch:14 [001/005 (0660/0693)]\tLoss Ss: 0.014115\n","\tRotated_Epoch:14 [001/005 (0680/0693)]\tLoss Ss: 0.015675\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:14 [002/005 (0000/0755)]\tLoss Ss: 0.342793\n","\tRotated_Epoch:14 [002/005 (0020/0755)]\tLoss Ss: 0.402422\n","\tRotated_Epoch:14 [002/005 (0040/0755)]\tLoss Ss: 0.350939\n","\tRotated_Epoch:14 [002/005 (0060/0755)]\tLoss Ss: 0.187874\n","\tRotated_Epoch:14 [002/005 (0080/0755)]\tLoss Ss: 0.232972\n","\tRotated_Epoch:14 [002/005 (0100/0755)]\tLoss Ss: 0.286528\n","\tRotated_Epoch:14 [002/005 (0120/0755)]\tLoss Ss: 0.126319\n","\tRotated_Epoch:14 [002/005 (0140/0755)]\tLoss Ss: 0.239160\n","\tRotated_Epoch:14 [002/005 (0160/0755)]\tLoss Ss: 0.104653\n","\tRotated_Epoch:14 [002/005 (0180/0755)]\tLoss Ss: 0.092468\n","\tRotated_Epoch:14 [002/005 (0200/0755)]\tLoss Ss: 0.082970\n","\tRotated_Epoch:14 [002/005 (0220/0755)]\tLoss Ss: 0.196978\n","\tRotated_Epoch:14 [002/005 (0240/0755)]\tLoss Ss: 0.060361\n","\tRotated_Epoch:14 [002/005 (0260/0755)]\tLoss Ss: 0.065881\n","\tRotated_Epoch:14 [002/005 (0280/0755)]\tLoss Ss: 0.050002\n","\tRotated_Epoch:14 [002/005 (0300/0755)]\tLoss Ss: 0.113805\n","\tRotated_Epoch:14 [002/005 (0320/0755)]\tLoss Ss: 0.095879\n","\tRotated_Epoch:14 [002/005 (0340/0755)]\tLoss Ss: 0.066865\n","\tRotated_Epoch:14 [002/005 (0360/0755)]\tLoss Ss: 0.087999\n","\tRotated_Epoch:14 [002/005 (0380/0755)]\tLoss Ss: 0.062117\n","\tRotated_Epoch:14 [002/005 (0400/0755)]\tLoss Ss: 0.122546\n","\tRotated_Epoch:14 [002/005 (0420/0755)]\tLoss Ss: 0.085687\n","\tRotated_Epoch:14 [002/005 (0440/0755)]\tLoss Ss: 0.098567\n","\tRotated_Epoch:14 [002/005 (0460/0755)]\tLoss Ss: 0.092797\n","\tRotated_Epoch:14 [002/005 (0480/0755)]\tLoss Ss: 0.103836\n","\tRotated_Epoch:14 [002/005 (0500/0755)]\tLoss Ss: 0.106214\n","\tRotated_Epoch:14 [002/005 (0520/0755)]\tLoss Ss: 0.072895\n","\tRotated_Epoch:14 [002/005 (0540/0755)]\tLoss Ss: 0.111833\n","\tRotated_Epoch:14 [002/005 (0560/0755)]\tLoss Ss: 0.096410\n","\tRotated_Epoch:14 [002/005 (0580/0755)]\tLoss Ss: 0.068126\n","\tRotated_Epoch:14 [002/005 (0600/0755)]\tLoss Ss: 0.058065\n","\tRotated_Epoch:14 [002/005 (0620/0755)]\tLoss Ss: 0.059902\n","\tRotated_Epoch:14 [002/005 (0640/0755)]\tLoss Ss: 0.076962\n","\tRotated_Epoch:14 [002/005 (0660/0755)]\tLoss Ss: 0.092243\n","\tRotated_Epoch:14 [002/005 (0680/0755)]\tLoss Ss: 0.057470\n","\tRotated_Epoch:14 [002/005 (0700/0755)]\tLoss Ss: 0.082726\n","\tRotated_Epoch:14 [002/005 (0720/0755)]\tLoss Ss: 0.057099\n","\tRotated_Epoch:14 [002/005 (0740/0755)]\tLoss Ss: 0.088636\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:14 [003/005 (0000/0755)]\tLoss Ss: 0.150520\n","\tRotated_Epoch:14 [003/005 (0020/0755)]\tLoss Ss: 0.103520\n","\tRotated_Epoch:14 [003/005 (0040/0755)]\tLoss Ss: 0.146402\n","\tRotated_Epoch:14 [003/005 (0060/0755)]\tLoss Ss: 0.077219\n","\tRotated_Epoch:14 [003/005 (0080/0755)]\tLoss Ss: 0.105337\n","\tRotated_Epoch:14 [003/005 (0100/0755)]\tLoss Ss: 0.071574\n","\tRotated_Epoch:14 [003/005 (0120/0755)]\tLoss Ss: 0.063351\n","\tRotated_Epoch:14 [003/005 (0140/0755)]\tLoss Ss: 0.065845\n","\tRotated_Epoch:14 [003/005 (0160/0755)]\tLoss Ss: 0.059108\n","\tRotated_Epoch:14 [003/005 (0180/0755)]\tLoss Ss: 0.072657\n","\tRotated_Epoch:14 [003/005 (0200/0755)]\tLoss Ss: 0.063347\n","\tRotated_Epoch:14 [003/005 (0220/0755)]\tLoss Ss: 0.051925\n","\tRotated_Epoch:14 [003/005 (0240/0755)]\tLoss Ss: 0.063964\n","\tRotated_Epoch:14 [003/005 (0260/0755)]\tLoss Ss: 0.041408\n","\tRotated_Epoch:14 [003/005 (0280/0755)]\tLoss Ss: 0.034840\n","\tRotated_Epoch:14 [003/005 (0300/0755)]\tLoss Ss: 0.050558\n","\tRotated_Epoch:14 [003/005 (0320/0755)]\tLoss Ss: 0.040767\n","\tRotated_Epoch:14 [003/005 (0340/0755)]\tLoss Ss: 0.034692\n","\tRotated_Epoch:14 [003/005 (0360/0755)]\tLoss Ss: 0.033692\n","\tRotated_Epoch:14 [003/005 (0380/0755)]\tLoss Ss: 0.042440\n","\tRotated_Epoch:14 [003/005 (0400/0755)]\tLoss Ss: 0.033696\n","\tRotated_Epoch:14 [003/005 (0420/0755)]\tLoss Ss: 0.037732\n","\tRotated_Epoch:14 [003/005 (0440/0755)]\tLoss Ss: 0.050148\n","\tRotated_Epoch:14 [003/005 (0460/0755)]\tLoss Ss: 0.036298\n","\tRotated_Epoch:14 [003/005 (0480/0755)]\tLoss Ss: 0.050291\n","\tRotated_Epoch:14 [003/005 (0500/0755)]\tLoss Ss: 0.044091\n","\tRotated_Epoch:14 [003/005 (0520/0755)]\tLoss Ss: 0.036075\n","\tRotated_Epoch:14 [003/005 (0540/0755)]\tLoss Ss: 0.045719\n","\tRotated_Epoch:14 [003/005 (0560/0755)]\tLoss Ss: 0.029714\n","\tRotated_Epoch:14 [003/005 (0580/0755)]\tLoss Ss: 0.049429\n","\tRotated_Epoch:14 [003/005 (0600/0755)]\tLoss Ss: 0.027267\n","\tRotated_Epoch:14 [003/005 (0620/0755)]\tLoss Ss: 0.041841\n","\tRotated_Epoch:14 [003/005 (0640/0755)]\tLoss Ss: 0.041311\n","\tRotated_Epoch:14 [003/005 (0660/0755)]\tLoss Ss: 0.036014\n","\tRotated_Epoch:14 [003/005 (0680/0755)]\tLoss Ss: 0.042435\n","\tRotated_Epoch:14 [003/005 (0700/0755)]\tLoss Ss: 0.025881\n","\tRotated_Epoch:14 [003/005 (0720/0755)]\tLoss Ss: 0.031891\n","\tRotated_Epoch:14 [003/005 (0740/0755)]\tLoss Ss: 0.028705\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tRotated_Epoch:14 [004/005 (0000/0588)]\tLoss Ss: 0.206714\n","\tRotated_Epoch:14 [004/005 (0020/0588)]\tLoss Ss: 0.141305\n","\tRotated_Epoch:14 [004/005 (0040/0588)]\tLoss Ss: 0.197823\n","\tRotated_Epoch:14 [004/005 (0060/0588)]\tLoss Ss: 0.121972\n","\tRotated_Epoch:14 [004/005 (0080/0588)]\tLoss Ss: 0.116282\n","\tRotated_Epoch:14 [004/005 (0100/0588)]\tLoss Ss: 0.114510\n","\tRotated_Epoch:14 [004/005 (0120/0588)]\tLoss Ss: 0.085052\n","\tRotated_Epoch:14 [004/005 (0140/0588)]\tLoss Ss: 0.131032\n","\tRotated_Epoch:14 [004/005 (0160/0588)]\tLoss Ss: 0.073103\n","\tRotated_Epoch:14 [004/005 (0180/0588)]\tLoss Ss: 0.060947\n","\tRotated_Epoch:14 [004/005 (0200/0588)]\tLoss Ss: 0.076938\n","\tRotated_Epoch:14 [004/005 (0220/0588)]\tLoss Ss: 0.071775\n","\tRotated_Epoch:14 [004/005 (0240/0588)]\tLoss Ss: 0.064144\n","\tRotated_Epoch:14 [004/005 (0260/0588)]\tLoss Ss: 0.071012\n","\tRotated_Epoch:14 [004/005 (0280/0588)]\tLoss Ss: 0.087819\n","\tRotated_Epoch:14 [004/005 (0300/0588)]\tLoss Ss: 0.099956\n","\tRotated_Epoch:14 [004/005 (0320/0588)]\tLoss Ss: 0.065269\n","\tRotated_Epoch:14 [004/005 (0340/0588)]\tLoss Ss: 0.062154\n","\tRotated_Epoch:14 [004/005 (0360/0588)]\tLoss Ss: 0.079678\n","\tRotated_Epoch:14 [004/005 (0380/0588)]\tLoss Ss: 0.090452\n","\tRotated_Epoch:14 [004/005 (0400/0588)]\tLoss Ss: 0.069733\n","\tRotated_Epoch:14 [004/005 (0420/0588)]\tLoss Ss: 0.058969\n","\tRotated_Epoch:14 [004/005 (0440/0588)]\tLoss Ss: 0.074699\n","\tRotated_Epoch:14 [004/005 (0460/0588)]\tLoss Ss: 0.063288\n","\tRotated_Epoch:14 [004/005 (0480/0588)]\tLoss Ss: 0.087723\n","\tRotated_Epoch:14 [004/005 (0500/0588)]\tLoss Ss: 0.092112\n","\tRotated_Epoch:14 [004/005 (0520/0588)]\tLoss Ss: 0.090036\n","\tRotated_Epoch:14 [004/005 (0540/0588)]\tLoss Ss: 0.055339\n","\tRotated_Epoch:14 [004/005 (0560/0588)]\tLoss Ss: 0.049800\n","\tRotated_Epoch:14 [004/005 (0580/0588)]\tLoss Ss: 0.090154\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tRotated_Epoch:14 [005/005 (0000/0693)]\tLoss Ss: 0.050660\n","\tRotated_Epoch:14 [005/005 (0020/0693)]\tLoss Ss: 0.066509\n","\tRotated_Epoch:14 [005/005 (0040/0693)]\tLoss Ss: 0.048565\n","\tRotated_Epoch:14 [005/005 (0060/0693)]\tLoss Ss: 0.032030\n","\tRotated_Epoch:14 [005/005 (0080/0693)]\tLoss Ss: 0.046758\n","\tRotated_Epoch:14 [005/005 (0100/0693)]\tLoss Ss: 0.031604\n","\tRotated_Epoch:14 [005/005 (0120/0693)]\tLoss Ss: 0.030307\n","\tRotated_Epoch:14 [005/005 (0140/0693)]\tLoss Ss: 0.027279\n","\tRotated_Epoch:14 [005/005 (0160/0693)]\tLoss Ss: 0.025534\n","\tRotated_Epoch:14 [005/005 (0180/0693)]\tLoss Ss: 0.034725\n","\tRotated_Epoch:14 [005/005 (0200/0693)]\tLoss Ss: 0.034188\n","\tRotated_Epoch:14 [005/005 (0220/0693)]\tLoss Ss: 0.022979\n","\tRotated_Epoch:14 [005/005 (0240/0693)]\tLoss Ss: 0.028972\n","\tRotated_Epoch:14 [005/005 (0260/0693)]\tLoss Ss: 0.032653\n","\tRotated_Epoch:14 [005/005 (0280/0693)]\tLoss Ss: 0.026705\n","\tRotated_Epoch:14 [005/005 (0300/0693)]\tLoss Ss: 0.022553\n","\tRotated_Epoch:14 [005/005 (0320/0693)]\tLoss Ss: 0.027973\n","\tRotated_Epoch:14 [005/005 (0340/0693)]\tLoss Ss: 0.028106\n","\tRotated_Epoch:14 [005/005 (0360/0693)]\tLoss Ss: 0.023872\n","\tRotated_Epoch:14 [005/005 (0380/0693)]\tLoss Ss: 0.019932\n","\tRotated_Epoch:14 [005/005 (0400/0693)]\tLoss Ss: 0.025969\n","\tRotated_Epoch:14 [005/005 (0420/0693)]\tLoss Ss: 0.031630\n","\tRotated_Epoch:14 [005/005 (0440/0693)]\tLoss Ss: 0.020212\n","\tRotated_Epoch:14 [005/005 (0460/0693)]\tLoss Ss: 0.014765\n","\tRotated_Epoch:14 [005/005 (0480/0693)]\tLoss Ss: 0.019573\n","\tRotated_Epoch:14 [005/005 (0500/0693)]\tLoss Ss: 0.015721\n","\tRotated_Epoch:14 [005/005 (0520/0693)]\tLoss Ss: 0.019620\n","\tRotated_Epoch:14 [005/005 (0540/0693)]\tLoss Ss: 0.016769\n","\tRotated_Epoch:14 [005/005 (0560/0693)]\tLoss Ss: 0.021136\n","\tRotated_Epoch:14 [005/005 (0580/0693)]\tLoss Ss: 0.016867\n","\tRotated_Epoch:14 [005/005 (0600/0693)]\tLoss Ss: 0.018336\n","\tRotated_Epoch:14 [005/005 (0620/0693)]\tLoss Ss: 0.014117\n","\tRotated_Epoch:14 [005/005 (0640/0693)]\tLoss Ss: 0.020205\n","\tRotated_Epoch:14 [005/005 (0660/0693)]\tLoss Ss: 0.021746\n","\tRotated_Epoch:14 [005/005 (0680/0693)]\tLoss Ss: 0.023782\n","new_1_6_d.nii\n","new_1_6_m.nii\n","new_1_7_d.nii\n","new_1_7_m.nii\n","new_2_5_d.nii\n","new_2_5_m.nii\n","new_2_6_d.nii\n","new_2_6_m.nii\n","new_2_7_d.nii\n","new_2_7_m.nii\n","new_3_d (2).nii\n","new_3_m (2).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (3).nii\n","new_3_m (3).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (7).nii\n","new_3_m (7).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (8).nii\n","new_3_m (8).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (9).nii\n","new_3_m (9).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_4_10_d_163526.nii\n","new_4_10_m_163526.nii.gz\n","new_4_11_d_084336.nii\n","new_4_11_m_084336.nii.gz\n","new_4_7_d_184422.nii\n","new_4_7_m_184422.nii.gz\n","new_4_8_d_143412.nii\n","new_4_8_m_143412.nii.gz\n","new_4_9_d_155347.nii\n","new_4_9_m_155347.nii.gz\n","\tEpoch: 14; Dice: 0.9593 +/- 0.0057; Loss: 14.4258\n","Begin Epoch 15\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tEpoch:15 [000/005 (0000/0614)]\tLoss Ss: 0.041808\n","\tEpoch:15 [000/005 (0020/0614)]\tLoss Ss: 0.022123\n","\tEpoch:15 [000/005 (0040/0614)]\tLoss Ss: 0.039422\n","\tEpoch:15 [000/005 (0060/0614)]\tLoss Ss: 0.016176\n","\tEpoch:15 [000/005 (0080/0614)]\tLoss Ss: 0.017576\n","\tEpoch:15 [000/005 (0100/0614)]\tLoss Ss: 0.014642\n","\tEpoch:15 [000/005 (0120/0614)]\tLoss Ss: 0.015155\n","\tEpoch:15 [000/005 (0140/0614)]\tLoss Ss: 0.011635\n","\tEpoch:15 [000/005 (0160/0614)]\tLoss Ss: 0.012270\n","\tEpoch:15 [000/005 (0180/0614)]\tLoss Ss: 0.015827\n","\tEpoch:15 [000/005 (0200/0614)]\tLoss Ss: 0.018766\n","\tEpoch:15 [000/005 (0220/0614)]\tLoss Ss: 0.008036\n","\tEpoch:15 [000/005 (0240/0614)]\tLoss Ss: 0.013708\n","\tEpoch:15 [000/005 (0260/0614)]\tLoss Ss: 0.009073\n","\tEpoch:15 [000/005 (0280/0614)]\tLoss Ss: 0.011804\n","\tEpoch:15 [000/005 (0300/0614)]\tLoss Ss: 0.011372\n","\tEpoch:15 [000/005 (0320/0614)]\tLoss Ss: 0.019736\n","\tEpoch:15 [000/005 (0340/0614)]\tLoss Ss: 0.011804\n","\tEpoch:15 [000/005 (0360/0614)]\tLoss Ss: 0.007894\n","\tEpoch:15 [000/005 (0380/0614)]\tLoss Ss: 0.008473\n","\tEpoch:15 [000/005 (0400/0614)]\tLoss Ss: 0.014429\n","\tEpoch:15 [000/005 (0420/0614)]\tLoss Ss: 0.005704\n","\tEpoch:15 [000/005 (0440/0614)]\tLoss Ss: 0.007516\n","\tEpoch:15 [000/005 (0460/0614)]\tLoss Ss: 0.010089\n","\tEpoch:15 [000/005 (0480/0614)]\tLoss Ss: 0.010118\n","\tEpoch:15 [000/005 (0500/0614)]\tLoss Ss: 0.010745\n","\tEpoch:15 [000/005 (0520/0614)]\tLoss Ss: 0.009636\n","\tEpoch:15 [000/005 (0540/0614)]\tLoss Ss: 0.007206\n","\tEpoch:15 [000/005 (0560/0614)]\tLoss Ss: 0.006131\n","\tEpoch:15 [000/005 (0580/0614)]\tLoss Ss: 0.010176\n","\tEpoch:15 [000/005 (0600/0614)]\tLoss Ss: 0.007444\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tEpoch:15 [001/005 (0000/0693)]\tLoss Ss: 0.017050\n","\tEpoch:15 [001/005 (0020/0693)]\tLoss Ss: 0.039895\n","\tEpoch:15 [001/005 (0040/0693)]\tLoss Ss: 0.014113\n","\tEpoch:15 [001/005 (0060/0693)]\tLoss Ss: 0.017402\n","\tEpoch:15 [001/005 (0080/0693)]\tLoss Ss: 0.016633\n","\tEpoch:15 [001/005 (0100/0693)]\tLoss Ss: 0.015302\n","\tEpoch:15 [001/005 (0120/0693)]\tLoss Ss: 0.010301\n","\tEpoch:15 [001/005 (0140/0693)]\tLoss Ss: 0.009330\n","\tEpoch:15 [001/005 (0160/0693)]\tLoss Ss: 0.016336\n","\tEpoch:15 [001/005 (0180/0693)]\tLoss Ss: 0.026102\n","\tEpoch:15 [001/005 (0200/0693)]\tLoss Ss: 0.016585\n","\tEpoch:15 [001/005 (0220/0693)]\tLoss Ss: 0.016853\n","\tEpoch:15 [001/005 (0240/0693)]\tLoss Ss: 0.011601\n","\tEpoch:15 [001/005 (0260/0693)]\tLoss Ss: 0.031721\n","\tEpoch:15 [001/005 (0280/0693)]\tLoss Ss: 0.011473\n","\tEpoch:15 [001/005 (0300/0693)]\tLoss Ss: 0.014639\n","\tEpoch:15 [001/005 (0320/0693)]\tLoss Ss: 0.013275\n","\tEpoch:15 [001/005 (0340/0693)]\tLoss Ss: 0.016077\n","\tEpoch:15 [001/005 (0360/0693)]\tLoss Ss: 0.017483\n","\tEpoch:15 [001/005 (0380/0693)]\tLoss Ss: 0.015796\n","\tEpoch:15 [001/005 (0400/0693)]\tLoss Ss: 0.018666\n","\tEpoch:15 [001/005 (0420/0693)]\tLoss Ss: 0.018175\n","\tEpoch:15 [001/005 (0440/0693)]\tLoss Ss: 0.014411\n","\tEpoch:15 [001/005 (0460/0693)]\tLoss Ss: 0.014756\n","\tEpoch:15 [001/005 (0480/0693)]\tLoss Ss: 0.011901\n","\tEpoch:15 [001/005 (0500/0693)]\tLoss Ss: 0.020197\n","\tEpoch:15 [001/005 (0520/0693)]\tLoss Ss: 0.024692\n","\tEpoch:15 [001/005 (0540/0693)]\tLoss Ss: 0.013492\n","\tEpoch:15 [001/005 (0560/0693)]\tLoss Ss: 0.014176\n","\tEpoch:15 [001/005 (0580/0693)]\tLoss Ss: 0.013332\n","\tEpoch:15 [001/005 (0600/0693)]\tLoss Ss: 0.012902\n","\tEpoch:15 [001/005 (0620/0693)]\tLoss Ss: 0.020007\n","\tEpoch:15 [001/005 (0640/0693)]\tLoss Ss: 0.014099\n","\tEpoch:15 [001/005 (0660/0693)]\tLoss Ss: 0.010912\n","\tEpoch:15 [001/005 (0680/0693)]\tLoss Ss: 0.023749\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tEpoch:15 [002/005 (0000/0588)]\tLoss Ss: 0.015662\n","\tEpoch:15 [002/005 (0020/0588)]\tLoss Ss: 0.021072\n","\tEpoch:15 [002/005 (0040/0588)]\tLoss Ss: 0.006266\n","\tEpoch:15 [002/005 (0060/0588)]\tLoss Ss: 0.011538\n","\tEpoch:15 [002/005 (0080/0588)]\tLoss Ss: 0.007308\n","\tEpoch:15 [002/005 (0100/0588)]\tLoss Ss: 0.007534\n","\tEpoch:15 [002/005 (0120/0588)]\tLoss Ss: 0.008145\n","\tEpoch:15 [002/005 (0140/0588)]\tLoss Ss: 0.007857\n","\tEpoch:15 [002/005 (0160/0588)]\tLoss Ss: 0.006374\n","\tEpoch:15 [002/005 (0180/0588)]\tLoss Ss: 0.008065\n","\tEpoch:15 [002/005 (0200/0588)]\tLoss Ss: 0.008849\n","\tEpoch:15 [002/005 (0220/0588)]\tLoss Ss: 0.005236\n","\tEpoch:15 [002/005 (0240/0588)]\tLoss Ss: 0.007206\n","\tEpoch:15 [002/005 (0260/0588)]\tLoss Ss: 0.005659\n","\tEpoch:15 [002/005 (0280/0588)]\tLoss Ss: 0.008095\n","\tEpoch:15 [002/005 (0300/0588)]\tLoss Ss: 0.010181\n","\tEpoch:15 [002/005 (0320/0588)]\tLoss Ss: 0.005594\n","\tEpoch:15 [002/005 (0340/0588)]\tLoss Ss: 0.006908\n","\tEpoch:15 [002/005 (0360/0588)]\tLoss Ss: 0.006228\n","\tEpoch:15 [002/005 (0380/0588)]\tLoss Ss: 0.007883\n","\tEpoch:15 [002/005 (0400/0588)]\tLoss Ss: 0.007207\n","\tEpoch:15 [002/005 (0420/0588)]\tLoss Ss: 0.004931\n","\tEpoch:15 [002/005 (0440/0588)]\tLoss Ss: 0.007964\n","\tEpoch:15 [002/005 (0460/0588)]\tLoss Ss: 0.004320\n","\tEpoch:15 [002/005 (0480/0588)]\tLoss Ss: 0.005138\n","\tEpoch:15 [002/005 (0500/0588)]\tLoss Ss: 0.006590\n","\tEpoch:15 [002/005 (0520/0588)]\tLoss Ss: 0.007352\n","\tEpoch:15 [002/005 (0540/0588)]\tLoss Ss: 0.007374\n","\tEpoch:15 [002/005 (0560/0588)]\tLoss Ss: 0.003626\n","\tEpoch:15 [002/005 (0580/0588)]\tLoss Ss: 0.008086\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:15 [003/005 (0000/0755)]\tLoss Ss: 0.025650\n","\tEpoch:15 [003/005 (0020/0755)]\tLoss Ss: 0.028617\n","\tEpoch:15 [003/005 (0040/0755)]\tLoss Ss: 0.026787\n","\tEpoch:15 [003/005 (0060/0755)]\tLoss Ss: 0.026306\n","\tEpoch:15 [003/005 (0080/0755)]\tLoss Ss: 0.044218\n","\tEpoch:15 [003/005 (0100/0755)]\tLoss Ss: 0.022160\n","\tEpoch:15 [003/005 (0120/0755)]\tLoss Ss: 0.018955\n","\tEpoch:15 [003/005 (0140/0755)]\tLoss Ss: 0.023359\n","\tEpoch:15 [003/005 (0160/0755)]\tLoss Ss: 0.020255\n","\tEpoch:15 [003/005 (0180/0755)]\tLoss Ss: 0.020051\n","\tEpoch:15 [003/005 (0200/0755)]\tLoss Ss: 0.019205\n","\tEpoch:15 [003/005 (0220/0755)]\tLoss Ss: 0.012726\n","\tEpoch:15 [003/005 (0240/0755)]\tLoss Ss: 0.016684\n","\tEpoch:15 [003/005 (0260/0755)]\tLoss Ss: 0.020029\n","\tEpoch:15 [003/005 (0280/0755)]\tLoss Ss: 0.026992\n","\tEpoch:15 [003/005 (0300/0755)]\tLoss Ss: 0.014783\n","\tEpoch:15 [003/005 (0320/0755)]\tLoss Ss: 0.016127\n","\tEpoch:15 [003/005 (0340/0755)]\tLoss Ss: 0.014421\n","\tEpoch:15 [003/005 (0360/0755)]\tLoss Ss: 0.017628\n","\tEpoch:15 [003/005 (0380/0755)]\tLoss Ss: 0.017079\n","\tEpoch:15 [003/005 (0400/0755)]\tLoss Ss: 0.017273\n","\tEpoch:15 [003/005 (0420/0755)]\tLoss Ss: 0.028239\n","\tEpoch:15 [003/005 (0440/0755)]\tLoss Ss: 0.017395\n","\tEpoch:15 [003/005 (0460/0755)]\tLoss Ss: 0.012462\n","\tEpoch:15 [003/005 (0480/0755)]\tLoss Ss: 0.011729\n","\tEpoch:15 [003/005 (0500/0755)]\tLoss Ss: 0.014085\n","\tEpoch:15 [003/005 (0520/0755)]\tLoss Ss: 0.015468\n","\tEpoch:15 [003/005 (0540/0755)]\tLoss Ss: 0.013417\n","\tEpoch:15 [003/005 (0560/0755)]\tLoss Ss: 0.013202\n","\tEpoch:15 [003/005 (0580/0755)]\tLoss Ss: 0.020351\n","\tEpoch:15 [003/005 (0600/0755)]\tLoss Ss: 0.018443\n","\tEpoch:15 [003/005 (0620/0755)]\tLoss Ss: 0.015571\n","\tEpoch:15 [003/005 (0640/0755)]\tLoss Ss: 0.014605\n","\tEpoch:15 [003/005 (0660/0755)]\tLoss Ss: 0.013621\n","\tEpoch:15 [003/005 (0680/0755)]\tLoss Ss: 0.016197\n","\tEpoch:15 [003/005 (0700/0755)]\tLoss Ss: 0.015402\n","\tEpoch:15 [003/005 (0720/0755)]\tLoss Ss: 0.017079\n","\tEpoch:15 [003/005 (0740/0755)]\tLoss Ss: 0.010070\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:15 [004/005 (0000/0755)]\tLoss Ss: 0.025645\n","\tEpoch:15 [004/005 (0020/0755)]\tLoss Ss: 0.029512\n","\tEpoch:15 [004/005 (0040/0755)]\tLoss Ss: 0.021679\n","\tEpoch:15 [004/005 (0060/0755)]\tLoss Ss: 0.029020\n","\tEpoch:15 [004/005 (0080/0755)]\tLoss Ss: 0.014207\n","\tEpoch:15 [004/005 (0100/0755)]\tLoss Ss: 0.021032\n","\tEpoch:15 [004/005 (0120/0755)]\tLoss Ss: 0.019652\n","\tEpoch:15 [004/005 (0140/0755)]\tLoss Ss: 0.026066\n","\tEpoch:15 [004/005 (0160/0755)]\tLoss Ss: 0.029086\n","\tEpoch:15 [004/005 (0180/0755)]\tLoss Ss: 0.019752\n","\tEpoch:15 [004/005 (0200/0755)]\tLoss Ss: 0.018064\n","\tEpoch:15 [004/005 (0220/0755)]\tLoss Ss: 0.019677\n","\tEpoch:15 [004/005 (0240/0755)]\tLoss Ss: 0.019682\n","\tEpoch:15 [004/005 (0260/0755)]\tLoss Ss: 0.017707\n","\tEpoch:15 [004/005 (0280/0755)]\tLoss Ss: 0.030931\n","\tEpoch:15 [004/005 (0300/0755)]\tLoss Ss: 0.016752\n","\tEpoch:15 [004/005 (0320/0755)]\tLoss Ss: 0.015582\n","\tEpoch:15 [004/005 (0340/0755)]\tLoss Ss: 0.020382\n","\tEpoch:15 [004/005 (0360/0755)]\tLoss Ss: 0.016210\n","\tEpoch:15 [004/005 (0380/0755)]\tLoss Ss: 0.018744\n","\tEpoch:15 [004/005 (0400/0755)]\tLoss Ss: 0.015077\n","\tEpoch:15 [004/005 (0420/0755)]\tLoss Ss: 0.020825\n","\tEpoch:15 [004/005 (0440/0755)]\tLoss Ss: 0.015997\n","\tEpoch:15 [004/005 (0460/0755)]\tLoss Ss: 0.018241\n","\tEpoch:15 [004/005 (0480/0755)]\tLoss Ss: 0.014568\n","\tEpoch:15 [004/005 (0500/0755)]\tLoss Ss: 0.021384\n","\tEpoch:15 [004/005 (0520/0755)]\tLoss Ss: 0.010573\n","\tEpoch:15 [004/005 (0540/0755)]\tLoss Ss: 0.010853\n","\tEpoch:15 [004/005 (0560/0755)]\tLoss Ss: 0.021428\n","\tEpoch:15 [004/005 (0580/0755)]\tLoss Ss: 0.021982\n","\tEpoch:15 [004/005 (0600/0755)]\tLoss Ss: 0.014754\n","\tEpoch:15 [004/005 (0620/0755)]\tLoss Ss: 0.020859\n","\tEpoch:15 [004/005 (0640/0755)]\tLoss Ss: 0.019953\n","\tEpoch:15 [004/005 (0660/0755)]\tLoss Ss: 0.017717\n","\tEpoch:15 [004/005 (0680/0755)]\tLoss Ss: 0.015818\n","\tEpoch:15 [004/005 (0700/0755)]\tLoss Ss: 0.010736\n","\tEpoch:15 [004/005 (0720/0755)]\tLoss Ss: 0.024738\n","\tEpoch:15 [004/005 (0740/0755)]\tLoss Ss: 0.019057\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tEpoch:15 [005/005 (0000/0693)]\tLoss Ss: 0.015290\n","\tEpoch:15 [005/005 (0020/0693)]\tLoss Ss: 0.023036\n","\tEpoch:15 [005/005 (0040/0693)]\tLoss Ss: 0.026130\n","\tEpoch:15 [005/005 (0060/0693)]\tLoss Ss: 0.010541\n","\tEpoch:15 [005/005 (0080/0693)]\tLoss Ss: 0.014630\n","\tEpoch:15 [005/005 (0100/0693)]\tLoss Ss: 0.020875\n","\tEpoch:15 [005/005 (0120/0693)]\tLoss Ss: 0.016754\n","\tEpoch:15 [005/005 (0140/0693)]\tLoss Ss: 0.021188\n","\tEpoch:15 [005/005 (0160/0693)]\tLoss Ss: 0.011516\n","\tEpoch:15 [005/005 (0180/0693)]\tLoss Ss: 0.015355\n","\tEpoch:15 [005/005 (0200/0693)]\tLoss Ss: 0.015920\n","\tEpoch:15 [005/005 (0220/0693)]\tLoss Ss: 0.022157\n","\tEpoch:15 [005/005 (0240/0693)]\tLoss Ss: 0.019639\n","\tEpoch:15 [005/005 (0260/0693)]\tLoss Ss: 0.016023\n","\tEpoch:15 [005/005 (0280/0693)]\tLoss Ss: 0.015497\n","\tEpoch:15 [005/005 (0300/0693)]\tLoss Ss: 0.016404\n","\tEpoch:15 [005/005 (0320/0693)]\tLoss Ss: 0.012598\n","\tEpoch:15 [005/005 (0340/0693)]\tLoss Ss: 0.014784\n","\tEpoch:15 [005/005 (0360/0693)]\tLoss Ss: 0.020317\n","\tEpoch:15 [005/005 (0380/0693)]\tLoss Ss: 0.012816\n","\tEpoch:15 [005/005 (0400/0693)]\tLoss Ss: 0.018337\n","\tEpoch:15 [005/005 (0420/0693)]\tLoss Ss: 0.018404\n","\tEpoch:15 [005/005 (0440/0693)]\tLoss Ss: 0.015155\n","\tEpoch:15 [005/005 (0460/0693)]\tLoss Ss: 0.012322\n","\tEpoch:15 [005/005 (0480/0693)]\tLoss Ss: 0.010467\n","\tEpoch:15 [005/005 (0500/0693)]\tLoss Ss: 0.015197\n","\tEpoch:15 [005/005 (0520/0693)]\tLoss Ss: 0.016014\n","\tEpoch:15 [005/005 (0540/0693)]\tLoss Ss: 0.019161\n","\tEpoch:15 [005/005 (0560/0693)]\tLoss Ss: 0.014987\n","\tEpoch:15 [005/005 (0580/0693)]\tLoss Ss: 0.013934\n","\tEpoch:15 [005/005 (0600/0693)]\tLoss Ss: 0.015433\n","\tEpoch:15 [005/005 (0620/0693)]\tLoss Ss: 0.012910\n","\tEpoch:15 [005/005 (0640/0693)]\tLoss Ss: 0.028033\n","\tEpoch:15 [005/005 (0660/0693)]\tLoss Ss: 0.015886\n","\tEpoch:15 [005/005 (0680/0693)]\tLoss Ss: 0.014042\n","Now train the rotated image\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:15 [000/005 (0000/0755)]\tLoss Ss: 0.024540\n","\tRotated_Epoch:15 [000/005 (0020/0755)]\tLoss Ss: 0.030725\n","\tRotated_Epoch:15 [000/005 (0040/0755)]\tLoss Ss: 0.030058\n","\tRotated_Epoch:15 [000/005 (0060/0755)]\tLoss Ss: 0.026349\n","\tRotated_Epoch:15 [000/005 (0080/0755)]\tLoss Ss: 0.018619\n","\tRotated_Epoch:15 [000/005 (0100/0755)]\tLoss Ss: 0.021307\n","\tRotated_Epoch:15 [000/005 (0120/0755)]\tLoss Ss: 0.029276\n","\tRotated_Epoch:15 [000/005 (0140/0755)]\tLoss Ss: 0.019870\n","\tRotated_Epoch:15 [000/005 (0160/0755)]\tLoss Ss: 0.016423\n","\tRotated_Epoch:15 [000/005 (0180/0755)]\tLoss Ss: 0.032183\n","\tRotated_Epoch:15 [000/005 (0200/0755)]\tLoss Ss: 0.020451\n","\tRotated_Epoch:15 [000/005 (0220/0755)]\tLoss Ss: 0.027740\n","\tRotated_Epoch:15 [000/005 (0240/0755)]\tLoss Ss: 0.020216\n","\tRotated_Epoch:15 [000/005 (0260/0755)]\tLoss Ss: 0.023043\n","\tRotated_Epoch:15 [000/005 (0280/0755)]\tLoss Ss: 0.023056\n","\tRotated_Epoch:15 [000/005 (0300/0755)]\tLoss Ss: 0.021002\n","\tRotated_Epoch:15 [000/005 (0320/0755)]\tLoss Ss: 0.016250\n","\tRotated_Epoch:15 [000/005 (0340/0755)]\tLoss Ss: 0.015946\n","\tRotated_Epoch:15 [000/005 (0360/0755)]\tLoss Ss: 0.016825\n","\tRotated_Epoch:15 [000/005 (0380/0755)]\tLoss Ss: 0.022003\n","\tRotated_Epoch:15 [000/005 (0400/0755)]\tLoss Ss: 0.013840\n","\tRotated_Epoch:15 [000/005 (0420/0755)]\tLoss Ss: 0.023018\n","\tRotated_Epoch:15 [000/005 (0440/0755)]\tLoss Ss: 0.013905\n","\tRotated_Epoch:15 [000/005 (0460/0755)]\tLoss Ss: 0.024159\n","\tRotated_Epoch:15 [000/005 (0480/0755)]\tLoss Ss: 0.020787\n","\tRotated_Epoch:15 [000/005 (0500/0755)]\tLoss Ss: 0.025869\n","\tRotated_Epoch:15 [000/005 (0520/0755)]\tLoss Ss: 0.016011\n","\tRotated_Epoch:15 [000/005 (0540/0755)]\tLoss Ss: 0.021780\n","\tRotated_Epoch:15 [000/005 (0560/0755)]\tLoss Ss: 0.022432\n","\tRotated_Epoch:15 [000/005 (0580/0755)]\tLoss Ss: 0.018422\n","\tRotated_Epoch:15 [000/005 (0600/0755)]\tLoss Ss: 0.018981\n","\tRotated_Epoch:15 [000/005 (0620/0755)]\tLoss Ss: 0.012701\n","\tRotated_Epoch:15 [000/005 (0640/0755)]\tLoss Ss: 0.020240\n","\tRotated_Epoch:15 [000/005 (0660/0755)]\tLoss Ss: 0.016532\n","\tRotated_Epoch:15 [000/005 (0680/0755)]\tLoss Ss: 0.018049\n","\tRotated_Epoch:15 [000/005 (0700/0755)]\tLoss Ss: 0.025381\n","\tRotated_Epoch:15 [000/005 (0720/0755)]\tLoss Ss: 0.017539\n","\tRotated_Epoch:15 [000/005 (0740/0755)]\tLoss Ss: 0.013735\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tRotated_Epoch:15 [001/005 (0000/0588)]\tLoss Ss: 0.097766\n","\tRotated_Epoch:15 [001/005 (0020/0588)]\tLoss Ss: 0.339339\n","\tRotated_Epoch:15 [001/005 (0040/0588)]\tLoss Ss: 0.126172\n","\tRotated_Epoch:15 [001/005 (0060/0588)]\tLoss Ss: 0.141693\n","\tRotated_Epoch:15 [001/005 (0080/0588)]\tLoss Ss: 0.104521\n","\tRotated_Epoch:15 [001/005 (0100/0588)]\tLoss Ss: 0.101686\n","\tRotated_Epoch:15 [001/005 (0120/0588)]\tLoss Ss: 0.073701\n","\tRotated_Epoch:15 [001/005 (0140/0588)]\tLoss Ss: 0.112843\n","\tRotated_Epoch:15 [001/005 (0160/0588)]\tLoss Ss: 0.076237\n","\tRotated_Epoch:15 [001/005 (0180/0588)]\tLoss Ss: 0.087517\n","\tRotated_Epoch:15 [001/005 (0200/0588)]\tLoss Ss: 0.091911\n","\tRotated_Epoch:15 [001/005 (0220/0588)]\tLoss Ss: 0.062288\n","\tRotated_Epoch:15 [001/005 (0240/0588)]\tLoss Ss: 0.056309\n","\tRotated_Epoch:15 [001/005 (0260/0588)]\tLoss Ss: 0.091798\n","\tRotated_Epoch:15 [001/005 (0280/0588)]\tLoss Ss: 0.102210\n","\tRotated_Epoch:15 [001/005 (0300/0588)]\tLoss Ss: 0.085905\n","\tRotated_Epoch:15 [001/005 (0320/0588)]\tLoss Ss: 0.060344\n","\tRotated_Epoch:15 [001/005 (0340/0588)]\tLoss Ss: 0.054599\n","\tRotated_Epoch:15 [001/005 (0360/0588)]\tLoss Ss: 0.076854\n","\tRotated_Epoch:15 [001/005 (0380/0588)]\tLoss Ss: 0.070595\n","\tRotated_Epoch:15 [001/005 (0400/0588)]\tLoss Ss: 0.076166\n","\tRotated_Epoch:15 [001/005 (0420/0588)]\tLoss Ss: 0.060770\n","\tRotated_Epoch:15 [001/005 (0440/0588)]\tLoss Ss: 0.093870\n","\tRotated_Epoch:15 [001/005 (0460/0588)]\tLoss Ss: 0.090210\n","\tRotated_Epoch:15 [001/005 (0480/0588)]\tLoss Ss: 0.054655\n","\tRotated_Epoch:15 [001/005 (0500/0588)]\tLoss Ss: 0.090359\n","\tRotated_Epoch:15 [001/005 (0520/0588)]\tLoss Ss: 0.066357\n","\tRotated_Epoch:15 [001/005 (0540/0588)]\tLoss Ss: 0.050534\n","\tRotated_Epoch:15 [001/005 (0560/0588)]\tLoss Ss: 0.084808\n","\tRotated_Epoch:15 [001/005 (0580/0588)]\tLoss Ss: 0.060895\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tRotated_Epoch:15 [002/005 (0000/0693)]\tLoss Ss: 0.168072\n","\tRotated_Epoch:15 [002/005 (0020/0693)]\tLoss Ss: 0.164122\n","\tRotated_Epoch:15 [002/005 (0040/0693)]\tLoss Ss: 0.053256\n","\tRotated_Epoch:15 [002/005 (0060/0693)]\tLoss Ss: 0.042551\n","\tRotated_Epoch:15 [002/005 (0080/0693)]\tLoss Ss: 0.024206\n","\tRotated_Epoch:15 [002/005 (0100/0693)]\tLoss Ss: 0.029755\n","\tRotated_Epoch:15 [002/005 (0120/0693)]\tLoss Ss: 0.025573\n","\tRotated_Epoch:15 [002/005 (0140/0693)]\tLoss Ss: 0.020990\n","\tRotated_Epoch:15 [002/005 (0160/0693)]\tLoss Ss: 0.029435\n","\tRotated_Epoch:15 [002/005 (0180/0693)]\tLoss Ss: 0.022997\n","\tRotated_Epoch:15 [002/005 (0200/0693)]\tLoss Ss: 0.039623\n","\tRotated_Epoch:15 [002/005 (0220/0693)]\tLoss Ss: 0.034610\n","\tRotated_Epoch:15 [002/005 (0240/0693)]\tLoss Ss: 0.030683\n","\tRotated_Epoch:15 [002/005 (0260/0693)]\tLoss Ss: 0.024803\n","\tRotated_Epoch:15 [002/005 (0280/0693)]\tLoss Ss: 0.023666\n","\tRotated_Epoch:15 [002/005 (0300/0693)]\tLoss Ss: 0.037067\n","\tRotated_Epoch:15 [002/005 (0320/0693)]\tLoss Ss: 0.025699\n","\tRotated_Epoch:15 [002/005 (0340/0693)]\tLoss Ss: 0.026906\n","\tRotated_Epoch:15 [002/005 (0360/0693)]\tLoss Ss: 0.021484\n","\tRotated_Epoch:15 [002/005 (0380/0693)]\tLoss Ss: 0.027930\n","\tRotated_Epoch:15 [002/005 (0400/0693)]\tLoss Ss: 0.024788\n","\tRotated_Epoch:15 [002/005 (0420/0693)]\tLoss Ss: 0.018276\n","\tRotated_Epoch:15 [002/005 (0440/0693)]\tLoss Ss: 0.021190\n","\tRotated_Epoch:15 [002/005 (0460/0693)]\tLoss Ss: 0.024723\n","\tRotated_Epoch:15 [002/005 (0480/0693)]\tLoss Ss: 0.023987\n","\tRotated_Epoch:15 [002/005 (0500/0693)]\tLoss Ss: 0.018862\n","\tRotated_Epoch:15 [002/005 (0520/0693)]\tLoss Ss: 0.025640\n","\tRotated_Epoch:15 [002/005 (0540/0693)]\tLoss Ss: 0.021399\n","\tRotated_Epoch:15 [002/005 (0560/0693)]\tLoss Ss: 0.024682\n","\tRotated_Epoch:15 [002/005 (0580/0693)]\tLoss Ss: 0.019198\n","\tRotated_Epoch:15 [002/005 (0600/0693)]\tLoss Ss: 0.029706\n","\tRotated_Epoch:15 [002/005 (0620/0693)]\tLoss Ss: 0.015965\n","\tRotated_Epoch:15 [002/005 (0640/0693)]\tLoss Ss: 0.021708\n","\tRotated_Epoch:15 [002/005 (0660/0693)]\tLoss Ss: 0.013964\n","\tRotated_Epoch:15 [002/005 (0680/0693)]\tLoss Ss: 0.016184\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tRotated_Epoch:15 [003/005 (0000/0693)]\tLoss Ss: 0.026899\n","\tRotated_Epoch:15 [003/005 (0020/0693)]\tLoss Ss: 0.014846\n","\tRotated_Epoch:15 [003/005 (0040/0693)]\tLoss Ss: 0.018788\n","\tRotated_Epoch:15 [003/005 (0060/0693)]\tLoss Ss: 0.011288\n","\tRotated_Epoch:15 [003/005 (0080/0693)]\tLoss Ss: 0.028760\n","\tRotated_Epoch:15 [003/005 (0100/0693)]\tLoss Ss: 0.024498\n","\tRotated_Epoch:15 [003/005 (0120/0693)]\tLoss Ss: 0.019762\n","\tRotated_Epoch:15 [003/005 (0140/0693)]\tLoss Ss: 0.016054\n","\tRotated_Epoch:15 [003/005 (0160/0693)]\tLoss Ss: 0.017501\n","\tRotated_Epoch:15 [003/005 (0180/0693)]\tLoss Ss: 0.018615\n","\tRotated_Epoch:15 [003/005 (0200/0693)]\tLoss Ss: 0.016997\n","\tRotated_Epoch:15 [003/005 (0220/0693)]\tLoss Ss: 0.022872\n","\tRotated_Epoch:15 [003/005 (0240/0693)]\tLoss Ss: 0.020166\n","\tRotated_Epoch:15 [003/005 (0260/0693)]\tLoss Ss: 0.021483\n","\tRotated_Epoch:15 [003/005 (0280/0693)]\tLoss Ss: 0.020533\n","\tRotated_Epoch:15 [003/005 (0300/0693)]\tLoss Ss: 0.020406\n","\tRotated_Epoch:15 [003/005 (0320/0693)]\tLoss Ss: 0.016069\n","\tRotated_Epoch:15 [003/005 (0340/0693)]\tLoss Ss: 0.016332\n","\tRotated_Epoch:15 [003/005 (0360/0693)]\tLoss Ss: 0.019261\n","\tRotated_Epoch:15 [003/005 (0380/0693)]\tLoss Ss: 0.021893\n","\tRotated_Epoch:15 [003/005 (0400/0693)]\tLoss Ss: 0.015299\n","\tRotated_Epoch:15 [003/005 (0420/0693)]\tLoss Ss: 0.012162\n","\tRotated_Epoch:15 [003/005 (0440/0693)]\tLoss Ss: 0.021454\n","\tRotated_Epoch:15 [003/005 (0460/0693)]\tLoss Ss: 0.013169\n","\tRotated_Epoch:15 [003/005 (0480/0693)]\tLoss Ss: 0.022314\n","\tRotated_Epoch:15 [003/005 (0500/0693)]\tLoss Ss: 0.019527\n","\tRotated_Epoch:15 [003/005 (0520/0693)]\tLoss Ss: 0.014792\n","\tRotated_Epoch:15 [003/005 (0540/0693)]\tLoss Ss: 0.014347\n","\tRotated_Epoch:15 [003/005 (0560/0693)]\tLoss Ss: 0.012599\n","\tRotated_Epoch:15 [003/005 (0580/0693)]\tLoss Ss: 0.013245\n","\tRotated_Epoch:15 [003/005 (0600/0693)]\tLoss Ss: 0.019870\n","\tRotated_Epoch:15 [003/005 (0620/0693)]\tLoss Ss: 0.012506\n","\tRotated_Epoch:15 [003/005 (0640/0693)]\tLoss Ss: 0.016358\n","\tRotated_Epoch:15 [003/005 (0660/0693)]\tLoss Ss: 0.022786\n","\tRotated_Epoch:15 [003/005 (0680/0693)]\tLoss Ss: 0.015701\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tRotated_Epoch:15 [004/005 (0000/0614)]\tLoss Ss: 0.038137\n","\tRotated_Epoch:15 [004/005 (0020/0614)]\tLoss Ss: 0.031367\n","\tRotated_Epoch:15 [004/005 (0040/0614)]\tLoss Ss: 0.016772\n","\tRotated_Epoch:15 [004/005 (0060/0614)]\tLoss Ss: 0.029913\n","\tRotated_Epoch:15 [004/005 (0080/0614)]\tLoss Ss: 0.028338\n","\tRotated_Epoch:15 [004/005 (0100/0614)]\tLoss Ss: 0.016238\n","\tRotated_Epoch:15 [004/005 (0120/0614)]\tLoss Ss: 0.019119\n","\tRotated_Epoch:15 [004/005 (0140/0614)]\tLoss Ss: 0.014597\n","\tRotated_Epoch:15 [004/005 (0160/0614)]\tLoss Ss: 0.009782\n","\tRotated_Epoch:15 [004/005 (0180/0614)]\tLoss Ss: 0.018056\n","\tRotated_Epoch:15 [004/005 (0200/0614)]\tLoss Ss: 0.011633\n","\tRotated_Epoch:15 [004/005 (0220/0614)]\tLoss Ss: 0.014574\n","\tRotated_Epoch:15 [004/005 (0240/0614)]\tLoss Ss: 0.008732\n","\tRotated_Epoch:15 [004/005 (0260/0614)]\tLoss Ss: 0.014296\n","\tRotated_Epoch:15 [004/005 (0280/0614)]\tLoss Ss: 0.008773\n","\tRotated_Epoch:15 [004/005 (0300/0614)]\tLoss Ss: 0.014477\n","\tRotated_Epoch:15 [004/005 (0320/0614)]\tLoss Ss: 0.007930\n","\tRotated_Epoch:15 [004/005 (0340/0614)]\tLoss Ss: 0.011400\n","\tRotated_Epoch:15 [004/005 (0360/0614)]\tLoss Ss: 0.007595\n","\tRotated_Epoch:15 [004/005 (0380/0614)]\tLoss Ss: 0.010545\n","\tRotated_Epoch:15 [004/005 (0400/0614)]\tLoss Ss: 0.011637\n","\tRotated_Epoch:15 [004/005 (0420/0614)]\tLoss Ss: 0.007656\n","\tRotated_Epoch:15 [004/005 (0440/0614)]\tLoss Ss: 0.013410\n","\tRotated_Epoch:15 [004/005 (0460/0614)]\tLoss Ss: 0.007613\n","\tRotated_Epoch:15 [004/005 (0480/0614)]\tLoss Ss: 0.010514\n","\tRotated_Epoch:15 [004/005 (0500/0614)]\tLoss Ss: 0.007423\n","\tRotated_Epoch:15 [004/005 (0520/0614)]\tLoss Ss: 0.007669\n","\tRotated_Epoch:15 [004/005 (0540/0614)]\tLoss Ss: 0.008527\n","\tRotated_Epoch:15 [004/005 (0560/0614)]\tLoss Ss: 0.006768\n","\tRotated_Epoch:15 [004/005 (0580/0614)]\tLoss Ss: 0.008466\n","\tRotated_Epoch:15 [004/005 (0600/0614)]\tLoss Ss: 0.009397\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:15 [005/005 (0000/0755)]\tLoss Ss: 0.355660\n","\tRotated_Epoch:15 [005/005 (0020/0755)]\tLoss Ss: 0.283648\n","\tRotated_Epoch:15 [005/005 (0040/0755)]\tLoss Ss: 0.233886\n","\tRotated_Epoch:15 [005/005 (0060/0755)]\tLoss Ss: 0.242814\n","\tRotated_Epoch:15 [005/005 (0080/0755)]\tLoss Ss: 0.197679\n","\tRotated_Epoch:15 [005/005 (0100/0755)]\tLoss Ss: 0.227512\n","\tRotated_Epoch:15 [005/005 (0120/0755)]\tLoss Ss: 0.201218\n","\tRotated_Epoch:15 [005/005 (0140/0755)]\tLoss Ss: 0.137025\n","\tRotated_Epoch:15 [005/005 (0160/0755)]\tLoss Ss: 0.154151\n","\tRotated_Epoch:15 [005/005 (0180/0755)]\tLoss Ss: 0.114455\n","\tRotated_Epoch:15 [005/005 (0200/0755)]\tLoss Ss: 0.127209\n","\tRotated_Epoch:15 [005/005 (0220/0755)]\tLoss Ss: 0.132883\n","\tRotated_Epoch:15 [005/005 (0240/0755)]\tLoss Ss: 0.127752\n","\tRotated_Epoch:15 [005/005 (0260/0755)]\tLoss Ss: 0.120390\n","\tRotated_Epoch:15 [005/005 (0280/0755)]\tLoss Ss: 0.111643\n","\tRotated_Epoch:15 [005/005 (0300/0755)]\tLoss Ss: 0.123971\n","\tRotated_Epoch:15 [005/005 (0320/0755)]\tLoss Ss: 0.099161\n","\tRotated_Epoch:15 [005/005 (0340/0755)]\tLoss Ss: 0.098090\n","\tRotated_Epoch:15 [005/005 (0360/0755)]\tLoss Ss: 0.084726\n","\tRotated_Epoch:15 [005/005 (0380/0755)]\tLoss Ss: 0.098359\n","\tRotated_Epoch:15 [005/005 (0400/0755)]\tLoss Ss: 0.077672\n","\tRotated_Epoch:15 [005/005 (0420/0755)]\tLoss Ss: 0.104953\n","\tRotated_Epoch:15 [005/005 (0440/0755)]\tLoss Ss: 0.077348\n","\tRotated_Epoch:15 [005/005 (0460/0755)]\tLoss Ss: 0.082017\n","\tRotated_Epoch:15 [005/005 (0480/0755)]\tLoss Ss: 0.076298\n","\tRotated_Epoch:15 [005/005 (0500/0755)]\tLoss Ss: 0.091410\n","\tRotated_Epoch:15 [005/005 (0520/0755)]\tLoss Ss: 0.109195\n","\tRotated_Epoch:15 [005/005 (0540/0755)]\tLoss Ss: 0.105229\n","\tRotated_Epoch:15 [005/005 (0560/0755)]\tLoss Ss: 0.090480\n","\tRotated_Epoch:15 [005/005 (0580/0755)]\tLoss Ss: 0.068734\n","\tRotated_Epoch:15 [005/005 (0600/0755)]\tLoss Ss: 0.098953\n","\tRotated_Epoch:15 [005/005 (0620/0755)]\tLoss Ss: 0.078629\n","\tRotated_Epoch:15 [005/005 (0640/0755)]\tLoss Ss: 0.089784\n","\tRotated_Epoch:15 [005/005 (0660/0755)]\tLoss Ss: 0.062897\n","\tRotated_Epoch:15 [005/005 (0680/0755)]\tLoss Ss: 0.095166\n","\tRotated_Epoch:15 [005/005 (0700/0755)]\tLoss Ss: 0.062425\n","\tRotated_Epoch:15 [005/005 (0720/0755)]\tLoss Ss: 0.065700\n","\tRotated_Epoch:15 [005/005 (0740/0755)]\tLoss Ss: 0.074798\n","new_1_6_d.nii\n","new_1_6_m.nii\n","new_1_7_d.nii\n","new_1_7_m.nii\n","new_2_5_d.nii\n","new_2_5_m.nii\n","new_2_6_d.nii\n","new_2_6_m.nii\n","new_2_7_d.nii\n","new_2_7_m.nii\n","new_3_d (2).nii\n","new_3_m (2).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (3).nii\n","new_3_m (3).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (7).nii\n","new_3_m (7).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (8).nii\n","new_3_m (8).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (9).nii\n","new_3_m (9).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_4_10_d_163526.nii\n","new_4_10_m_163526.nii.gz\n","new_4_11_d_084336.nii\n","new_4_11_m_084336.nii.gz\n","new_4_7_d_184422.nii\n","new_4_7_m_184422.nii.gz\n","new_4_8_d_143412.nii\n","new_4_8_m_143412.nii.gz\n","new_4_9_d_155347.nii\n","new_4_9_m_155347.nii.gz\n","\tEpoch: 15; Dice: 0.8092 +/- 0.0510; Loss: 13.8868\n","Begin Epoch 16\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tEpoch:16 [000/005 (0000/0693)]\tLoss Ss: 0.125569\n","\tEpoch:16 [000/005 (0020/0693)]\tLoss Ss: 0.082284\n","\tEpoch:16 [000/005 (0040/0693)]\tLoss Ss: 0.042858\n","\tEpoch:16 [000/005 (0060/0693)]\tLoss Ss: 0.050712\n","\tEpoch:16 [000/005 (0080/0693)]\tLoss Ss: 0.035274\n","\tEpoch:16 [000/005 (0100/0693)]\tLoss Ss: 0.032336\n","\tEpoch:16 [000/005 (0120/0693)]\tLoss Ss: 0.026611\n","\tEpoch:16 [000/005 (0140/0693)]\tLoss Ss: 0.015893\n","\tEpoch:16 [000/005 (0160/0693)]\tLoss Ss: 0.018749\n","\tEpoch:16 [000/005 (0180/0693)]\tLoss Ss: 0.021311\n","\tEpoch:16 [000/005 (0200/0693)]\tLoss Ss: 0.014757\n","\tEpoch:16 [000/005 (0220/0693)]\tLoss Ss: 0.015493\n","\tEpoch:16 [000/005 (0240/0693)]\tLoss Ss: 0.023217\n","\tEpoch:16 [000/005 (0260/0693)]\tLoss Ss: 0.021353\n","\tEpoch:16 [000/005 (0280/0693)]\tLoss Ss: 0.019672\n","\tEpoch:16 [000/005 (0300/0693)]\tLoss Ss: 0.015063\n","\tEpoch:16 [000/005 (0320/0693)]\tLoss Ss: 0.017827\n","\tEpoch:16 [000/005 (0340/0693)]\tLoss Ss: 0.019863\n","\tEpoch:16 [000/005 (0360/0693)]\tLoss Ss: 0.020564\n","\tEpoch:16 [000/005 (0380/0693)]\tLoss Ss: 0.019634\n","\tEpoch:16 [000/005 (0400/0693)]\tLoss Ss: 0.013295\n","\tEpoch:16 [000/005 (0420/0693)]\tLoss Ss: 0.018802\n","\tEpoch:16 [000/005 (0440/0693)]\tLoss Ss: 0.012565\n","\tEpoch:16 [000/005 (0460/0693)]\tLoss Ss: 0.022219\n","\tEpoch:16 [000/005 (0480/0693)]\tLoss Ss: 0.018595\n","\tEpoch:16 [000/005 (0500/0693)]\tLoss Ss: 0.016582\n","\tEpoch:16 [000/005 (0520/0693)]\tLoss Ss: 0.013309\n","\tEpoch:16 [000/005 (0540/0693)]\tLoss Ss: 0.014826\n","\tEpoch:16 [000/005 (0560/0693)]\tLoss Ss: 0.016337\n","\tEpoch:16 [000/005 (0580/0693)]\tLoss Ss: 0.012542\n","\tEpoch:16 [000/005 (0600/0693)]\tLoss Ss: 0.011403\n","\tEpoch:16 [000/005 (0620/0693)]\tLoss Ss: 0.019999\n","\tEpoch:16 [000/005 (0640/0693)]\tLoss Ss: 0.014664\n","\tEpoch:16 [000/005 (0660/0693)]\tLoss Ss: 0.014992\n","\tEpoch:16 [000/005 (0680/0693)]\tLoss Ss: 0.014037\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tEpoch:16 [001/005 (0000/0614)]\tLoss Ss: 0.010375\n","\tEpoch:16 [001/005 (0020/0614)]\tLoss Ss: 0.012773\n","\tEpoch:16 [001/005 (0040/0614)]\tLoss Ss: 0.006951\n","\tEpoch:16 [001/005 (0060/0614)]\tLoss Ss: 0.011651\n","\tEpoch:16 [001/005 (0080/0614)]\tLoss Ss: 0.010693\n","\tEpoch:16 [001/005 (0100/0614)]\tLoss Ss: 0.011378\n","\tEpoch:16 [001/005 (0120/0614)]\tLoss Ss: 0.010717\n","\tEpoch:16 [001/005 (0140/0614)]\tLoss Ss: 0.012068\n","\tEpoch:16 [001/005 (0160/0614)]\tLoss Ss: 0.011544\n","\tEpoch:16 [001/005 (0180/0614)]\tLoss Ss: 0.008231\n","\tEpoch:16 [001/005 (0200/0614)]\tLoss Ss: 0.009348\n","\tEpoch:16 [001/005 (0220/0614)]\tLoss Ss: 0.010399\n","\tEpoch:16 [001/005 (0240/0614)]\tLoss Ss: 0.006968\n","\tEpoch:16 [001/005 (0260/0614)]\tLoss Ss: 0.006805\n","\tEpoch:16 [001/005 (0280/0614)]\tLoss Ss: 0.010054\n","\tEpoch:16 [001/005 (0300/0614)]\tLoss Ss: 0.006029\n","\tEpoch:16 [001/005 (0320/0614)]\tLoss Ss: 0.009505\n","\tEpoch:16 [001/005 (0340/0614)]\tLoss Ss: 0.008921\n","\tEpoch:16 [001/005 (0360/0614)]\tLoss Ss: 0.006673\n","\tEpoch:16 [001/005 (0380/0614)]\tLoss Ss: 0.011903\n","\tEpoch:16 [001/005 (0400/0614)]\tLoss Ss: 0.009268\n","\tEpoch:16 [001/005 (0420/0614)]\tLoss Ss: 0.011468\n","\tEpoch:16 [001/005 (0440/0614)]\tLoss Ss: 0.010485\n","\tEpoch:16 [001/005 (0460/0614)]\tLoss Ss: 0.007457\n","\tEpoch:16 [001/005 (0480/0614)]\tLoss Ss: 0.007061\n","\tEpoch:16 [001/005 (0500/0614)]\tLoss Ss: 0.006937\n","\tEpoch:16 [001/005 (0520/0614)]\tLoss Ss: 0.007057\n","\tEpoch:16 [001/005 (0540/0614)]\tLoss Ss: 0.007041\n","\tEpoch:16 [001/005 (0560/0614)]\tLoss Ss: 0.007189\n","\tEpoch:16 [001/005 (0580/0614)]\tLoss Ss: 0.008990\n","\tEpoch:16 [001/005 (0600/0614)]\tLoss Ss: 0.009244\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tEpoch:16 [002/005 (0000/0588)]\tLoss Ss: 0.013641\n","\tEpoch:16 [002/005 (0020/0588)]\tLoss Ss: 0.016941\n","\tEpoch:16 [002/005 (0040/0588)]\tLoss Ss: 0.012970\n","\tEpoch:16 [002/005 (0060/0588)]\tLoss Ss: 0.011307\n","\tEpoch:16 [002/005 (0080/0588)]\tLoss Ss: 0.011832\n","\tEpoch:16 [002/005 (0100/0588)]\tLoss Ss: 0.010014\n","\tEpoch:16 [002/005 (0120/0588)]\tLoss Ss: 0.010659\n","\tEpoch:16 [002/005 (0140/0588)]\tLoss Ss: 0.008987\n","\tEpoch:16 [002/005 (0160/0588)]\tLoss Ss: 0.009271\n","\tEpoch:16 [002/005 (0180/0588)]\tLoss Ss: 0.011068\n","\tEpoch:16 [002/005 (0200/0588)]\tLoss Ss: 0.008397\n","\tEpoch:16 [002/005 (0220/0588)]\tLoss Ss: 0.009222\n","\tEpoch:16 [002/005 (0240/0588)]\tLoss Ss: 0.011174\n","\tEpoch:16 [002/005 (0260/0588)]\tLoss Ss: 0.007008\n","\tEpoch:16 [002/005 (0280/0588)]\tLoss Ss: 0.007571\n","\tEpoch:16 [002/005 (0300/0588)]\tLoss Ss: 0.007856\n","\tEpoch:16 [002/005 (0320/0588)]\tLoss Ss: 0.007402\n","\tEpoch:16 [002/005 (0340/0588)]\tLoss Ss: 0.006582\n","\tEpoch:16 [002/005 (0360/0588)]\tLoss Ss: 0.007351\n","\tEpoch:16 [002/005 (0380/0588)]\tLoss Ss: 0.007318\n","\tEpoch:16 [002/005 (0400/0588)]\tLoss Ss: 0.006124\n","\tEpoch:16 [002/005 (0420/0588)]\tLoss Ss: 0.006006\n","\tEpoch:16 [002/005 (0440/0588)]\tLoss Ss: 0.007089\n","\tEpoch:16 [002/005 (0460/0588)]\tLoss Ss: 0.005977\n","\tEpoch:16 [002/005 (0480/0588)]\tLoss Ss: 0.005623\n","\tEpoch:16 [002/005 (0500/0588)]\tLoss Ss: 0.007683\n","\tEpoch:16 [002/005 (0520/0588)]\tLoss Ss: 0.006092\n","\tEpoch:16 [002/005 (0540/0588)]\tLoss Ss: 0.006632\n","\tEpoch:16 [002/005 (0560/0588)]\tLoss Ss: 0.007698\n","\tEpoch:16 [002/005 (0580/0588)]\tLoss Ss: 0.005820\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tEpoch:16 [003/005 (0000/0693)]\tLoss Ss: 0.016666\n","\tEpoch:16 [003/005 (0020/0693)]\tLoss Ss: 0.022518\n","\tEpoch:16 [003/005 (0040/0693)]\tLoss Ss: 0.022097\n","\tEpoch:16 [003/005 (0060/0693)]\tLoss Ss: 0.025337\n","\tEpoch:16 [003/005 (0080/0693)]\tLoss Ss: 0.023423\n","\tEpoch:16 [003/005 (0100/0693)]\tLoss Ss: 0.014965\n","\tEpoch:16 [003/005 (0120/0693)]\tLoss Ss: 0.019158\n","\tEpoch:16 [003/005 (0140/0693)]\tLoss Ss: 0.018318\n","\tEpoch:16 [003/005 (0160/0693)]\tLoss Ss: 0.009474\n","\tEpoch:16 [003/005 (0180/0693)]\tLoss Ss: 0.016534\n","\tEpoch:16 [003/005 (0200/0693)]\tLoss Ss: 0.021206\n","\tEpoch:16 [003/005 (0220/0693)]\tLoss Ss: 0.017795\n","\tEpoch:16 [003/005 (0240/0693)]\tLoss Ss: 0.021067\n","\tEpoch:16 [003/005 (0260/0693)]\tLoss Ss: 0.011553\n","\tEpoch:16 [003/005 (0280/0693)]\tLoss Ss: 0.013804\n","\tEpoch:16 [003/005 (0300/0693)]\tLoss Ss: 0.016311\n","\tEpoch:16 [003/005 (0320/0693)]\tLoss Ss: 0.019023\n","\tEpoch:16 [003/005 (0340/0693)]\tLoss Ss: 0.018604\n","\tEpoch:16 [003/005 (0360/0693)]\tLoss Ss: 0.016964\n","\tEpoch:16 [003/005 (0380/0693)]\tLoss Ss: 0.018043\n","\tEpoch:16 [003/005 (0400/0693)]\tLoss Ss: 0.013888\n","\tEpoch:16 [003/005 (0420/0693)]\tLoss Ss: 0.011962\n","\tEpoch:16 [003/005 (0440/0693)]\tLoss Ss: 0.013340\n","\tEpoch:16 [003/005 (0460/0693)]\tLoss Ss: 0.013966\n","\tEpoch:16 [003/005 (0480/0693)]\tLoss Ss: 0.021855\n","\tEpoch:16 [003/005 (0500/0693)]\tLoss Ss: 0.008441\n","\tEpoch:16 [003/005 (0520/0693)]\tLoss Ss: 0.011955\n","\tEpoch:16 [003/005 (0540/0693)]\tLoss Ss: 0.015249\n","\tEpoch:16 [003/005 (0560/0693)]\tLoss Ss: 0.017246\n","\tEpoch:16 [003/005 (0580/0693)]\tLoss Ss: 0.016766\n","\tEpoch:16 [003/005 (0600/0693)]\tLoss Ss: 0.016103\n","\tEpoch:16 [003/005 (0620/0693)]\tLoss Ss: 0.014878\n","\tEpoch:16 [003/005 (0640/0693)]\tLoss Ss: 0.016605\n","\tEpoch:16 [003/005 (0660/0693)]\tLoss Ss: 0.016850\n","\tEpoch:16 [003/005 (0680/0693)]\tLoss Ss: 0.016262\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:16 [004/005 (0000/0755)]\tLoss Ss: 0.028105\n","\tEpoch:16 [004/005 (0020/0755)]\tLoss Ss: 0.028872\n","\tEpoch:16 [004/005 (0040/0755)]\tLoss Ss: 0.029545\n","\tEpoch:16 [004/005 (0060/0755)]\tLoss Ss: 0.019467\n","\tEpoch:16 [004/005 (0080/0755)]\tLoss Ss: 0.022848\n","\tEpoch:16 [004/005 (0100/0755)]\tLoss Ss: 0.022786\n","\tEpoch:16 [004/005 (0120/0755)]\tLoss Ss: 0.018243\n","\tEpoch:16 [004/005 (0140/0755)]\tLoss Ss: 0.019361\n","\tEpoch:16 [004/005 (0160/0755)]\tLoss Ss: 0.017789\n","\tEpoch:16 [004/005 (0180/0755)]\tLoss Ss: 0.025589\n","\tEpoch:16 [004/005 (0200/0755)]\tLoss Ss: 0.024143\n","\tEpoch:16 [004/005 (0220/0755)]\tLoss Ss: 0.017846\n","\tEpoch:16 [004/005 (0240/0755)]\tLoss Ss: 0.022324\n","\tEpoch:16 [004/005 (0260/0755)]\tLoss Ss: 0.017802\n","\tEpoch:16 [004/005 (0280/0755)]\tLoss Ss: 0.020898\n","\tEpoch:16 [004/005 (0300/0755)]\tLoss Ss: 0.014469\n","\tEpoch:16 [004/005 (0320/0755)]\tLoss Ss: 0.013693\n","\tEpoch:16 [004/005 (0340/0755)]\tLoss Ss: 0.022628\n","\tEpoch:16 [004/005 (0360/0755)]\tLoss Ss: 0.022569\n","\tEpoch:16 [004/005 (0380/0755)]\tLoss Ss: 0.012792\n","\tEpoch:16 [004/005 (0400/0755)]\tLoss Ss: 0.012063\n","\tEpoch:16 [004/005 (0420/0755)]\tLoss Ss: 0.016438\n","\tEpoch:16 [004/005 (0440/0755)]\tLoss Ss: 0.019591\n","\tEpoch:16 [004/005 (0460/0755)]\tLoss Ss: 0.020754\n","\tEpoch:16 [004/005 (0480/0755)]\tLoss Ss: 0.016596\n","\tEpoch:16 [004/005 (0500/0755)]\tLoss Ss: 0.013147\n","\tEpoch:16 [004/005 (0520/0755)]\tLoss Ss: 0.021830\n","\tEpoch:16 [004/005 (0540/0755)]\tLoss Ss: 0.017815\n","\tEpoch:16 [004/005 (0560/0755)]\tLoss Ss: 0.014306\n","\tEpoch:16 [004/005 (0580/0755)]\tLoss Ss: 0.013607\n","\tEpoch:16 [004/005 (0600/0755)]\tLoss Ss: 0.017269\n","\tEpoch:16 [004/005 (0620/0755)]\tLoss Ss: 0.016182\n","\tEpoch:16 [004/005 (0640/0755)]\tLoss Ss: 0.012048\n","\tEpoch:16 [004/005 (0660/0755)]\tLoss Ss: 0.019071\n","\tEpoch:16 [004/005 (0680/0755)]\tLoss Ss: 0.011287\n","\tEpoch:16 [004/005 (0700/0755)]\tLoss Ss: 0.011755\n","\tEpoch:16 [004/005 (0720/0755)]\tLoss Ss: 0.012825\n","\tEpoch:16 [004/005 (0740/0755)]\tLoss Ss: 0.017769\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:16 [005/005 (0000/0755)]\tLoss Ss: 0.025963\n","\tEpoch:16 [005/005 (0020/0755)]\tLoss Ss: 0.021380\n","\tEpoch:16 [005/005 (0040/0755)]\tLoss Ss: 0.031746\n","\tEpoch:16 [005/005 (0060/0755)]\tLoss Ss: 0.021321\n","\tEpoch:16 [005/005 (0080/0755)]\tLoss Ss: 0.022279\n","\tEpoch:16 [005/005 (0100/0755)]\tLoss Ss: 0.020467\n","\tEpoch:16 [005/005 (0120/0755)]\tLoss Ss: 0.018218\n","\tEpoch:16 [005/005 (0140/0755)]\tLoss Ss: 0.025557\n","\tEpoch:16 [005/005 (0160/0755)]\tLoss Ss: 0.020969\n","\tEpoch:16 [005/005 (0180/0755)]\tLoss Ss: 0.022811\n","\tEpoch:16 [005/005 (0200/0755)]\tLoss Ss: 0.035558\n","\tEpoch:16 [005/005 (0220/0755)]\tLoss Ss: 0.011452\n","\tEpoch:16 [005/005 (0240/0755)]\tLoss Ss: 0.018875\n","\tEpoch:16 [005/005 (0260/0755)]\tLoss Ss: 0.015847\n","\tEpoch:16 [005/005 (0280/0755)]\tLoss Ss: 0.021179\n","\tEpoch:16 [005/005 (0300/0755)]\tLoss Ss: 0.020447\n","\tEpoch:16 [005/005 (0320/0755)]\tLoss Ss: 0.012342\n","\tEpoch:16 [005/005 (0340/0755)]\tLoss Ss: 0.023461\n","\tEpoch:16 [005/005 (0360/0755)]\tLoss Ss: 0.015409\n","\tEpoch:16 [005/005 (0380/0755)]\tLoss Ss: 0.020621\n","\tEpoch:16 [005/005 (0400/0755)]\tLoss Ss: 0.016846\n","\tEpoch:16 [005/005 (0420/0755)]\tLoss Ss: 0.020740\n","\tEpoch:16 [005/005 (0440/0755)]\tLoss Ss: 0.017920\n","\tEpoch:16 [005/005 (0460/0755)]\tLoss Ss: 0.013312\n","\tEpoch:16 [005/005 (0480/0755)]\tLoss Ss: 0.012281\n","\tEpoch:16 [005/005 (0500/0755)]\tLoss Ss: 0.016089\n","\tEpoch:16 [005/005 (0520/0755)]\tLoss Ss: 0.015579\n","\tEpoch:16 [005/005 (0540/0755)]\tLoss Ss: 0.015956\n","\tEpoch:16 [005/005 (0560/0755)]\tLoss Ss: 0.017616\n","\tEpoch:16 [005/005 (0580/0755)]\tLoss Ss: 0.018530\n","\tEpoch:16 [005/005 (0600/0755)]\tLoss Ss: 0.011747\n","\tEpoch:16 [005/005 (0620/0755)]\tLoss Ss: 0.021618\n","\tEpoch:16 [005/005 (0640/0755)]\tLoss Ss: 0.018688\n","\tEpoch:16 [005/005 (0660/0755)]\tLoss Ss: 0.019259\n","\tEpoch:16 [005/005 (0680/0755)]\tLoss Ss: 0.010267\n","\tEpoch:16 [005/005 (0700/0755)]\tLoss Ss: 0.018233\n","\tEpoch:16 [005/005 (0720/0755)]\tLoss Ss: 0.010195\n","\tEpoch:16 [005/005 (0740/0755)]\tLoss Ss: 0.020111\n","Now train the rotated image\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tRotated_Epoch:16 [000/005 (0000/0614)]\tLoss Ss: 0.022926\n","\tRotated_Epoch:16 [000/005 (0020/0614)]\tLoss Ss: 0.029203\n","\tRotated_Epoch:16 [000/005 (0040/0614)]\tLoss Ss: 0.020057\n","\tRotated_Epoch:16 [000/005 (0060/0614)]\tLoss Ss: 0.015524\n","\tRotated_Epoch:16 [000/005 (0080/0614)]\tLoss Ss: 0.017163\n","\tRotated_Epoch:16 [000/005 (0100/0614)]\tLoss Ss: 0.011580\n","\tRotated_Epoch:16 [000/005 (0120/0614)]\tLoss Ss: 0.010848\n","\tRotated_Epoch:16 [000/005 (0140/0614)]\tLoss Ss: 0.006095\n","\tRotated_Epoch:16 [000/005 (0160/0614)]\tLoss Ss: 0.009009\n","\tRotated_Epoch:16 [000/005 (0180/0614)]\tLoss Ss: 0.007446\n","\tRotated_Epoch:16 [000/005 (0200/0614)]\tLoss Ss: 0.011145\n","\tRotated_Epoch:16 [000/005 (0220/0614)]\tLoss Ss: 0.010297\n","\tRotated_Epoch:16 [000/005 (0240/0614)]\tLoss Ss: 0.017246\n","\tRotated_Epoch:16 [000/005 (0260/0614)]\tLoss Ss: 0.007158\n","\tRotated_Epoch:16 [000/005 (0280/0614)]\tLoss Ss: 0.007442\n","\tRotated_Epoch:16 [000/005 (0300/0614)]\tLoss Ss: 0.009979\n","\tRotated_Epoch:16 [000/005 (0320/0614)]\tLoss Ss: 0.007183\n","\tRotated_Epoch:16 [000/005 (0340/0614)]\tLoss Ss: 0.007383\n","\tRotated_Epoch:16 [000/005 (0360/0614)]\tLoss Ss: 0.006892\n","\tRotated_Epoch:16 [000/005 (0380/0614)]\tLoss Ss: 0.006625\n","\tRotated_Epoch:16 [000/005 (0400/0614)]\tLoss Ss: 0.011782\n","\tRotated_Epoch:16 [000/005 (0420/0614)]\tLoss Ss: 0.004593\n","\tRotated_Epoch:16 [000/005 (0440/0614)]\tLoss Ss: 0.007624\n","\tRotated_Epoch:16 [000/005 (0460/0614)]\tLoss Ss: 0.005309\n","\tRotated_Epoch:16 [000/005 (0480/0614)]\tLoss Ss: 0.009674\n","\tRotated_Epoch:16 [000/005 (0500/0614)]\tLoss Ss: 0.008110\n","\tRotated_Epoch:16 [000/005 (0520/0614)]\tLoss Ss: 0.007898\n","\tRotated_Epoch:16 [000/005 (0540/0614)]\tLoss Ss: 0.007023\n","\tRotated_Epoch:16 [000/005 (0560/0614)]\tLoss Ss: 0.005388\n","\tRotated_Epoch:16 [000/005 (0580/0614)]\tLoss Ss: 0.007359\n","\tRotated_Epoch:16 [000/005 (0600/0614)]\tLoss Ss: 0.003854\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tRotated_Epoch:16 [001/005 (0000/0588)]\tLoss Ss: 0.325577\n","\tRotated_Epoch:16 [001/005 (0020/0588)]\tLoss Ss: 0.213219\n","\tRotated_Epoch:16 [001/005 (0040/0588)]\tLoss Ss: 0.149326\n","\tRotated_Epoch:16 [001/005 (0060/0588)]\tLoss Ss: 0.180844\n","\tRotated_Epoch:16 [001/005 (0080/0588)]\tLoss Ss: 0.110305\n","\tRotated_Epoch:16 [001/005 (0100/0588)]\tLoss Ss: 0.098161\n","\tRotated_Epoch:16 [001/005 (0120/0588)]\tLoss Ss: 0.081763\n","\tRotated_Epoch:16 [001/005 (0140/0588)]\tLoss Ss: 0.082168\n","\tRotated_Epoch:16 [001/005 (0160/0588)]\tLoss Ss: 0.074112\n","\tRotated_Epoch:16 [001/005 (0180/0588)]\tLoss Ss: 0.071369\n","\tRotated_Epoch:16 [001/005 (0200/0588)]\tLoss Ss: 0.057423\n","\tRotated_Epoch:16 [001/005 (0220/0588)]\tLoss Ss: 0.102435\n","\tRotated_Epoch:16 [001/005 (0240/0588)]\tLoss Ss: 0.069255\n","\tRotated_Epoch:16 [001/005 (0260/0588)]\tLoss Ss: 0.071929\n","\tRotated_Epoch:16 [001/005 (0280/0588)]\tLoss Ss: 0.066872\n","\tRotated_Epoch:16 [001/005 (0300/0588)]\tLoss Ss: 0.067853\n","\tRotated_Epoch:16 [001/005 (0320/0588)]\tLoss Ss: 0.072757\n","\tRotated_Epoch:16 [001/005 (0340/0588)]\tLoss Ss: 0.049425\n","\tRotated_Epoch:16 [001/005 (0360/0588)]\tLoss Ss: 0.072696\n","\tRotated_Epoch:16 [001/005 (0380/0588)]\tLoss Ss: 0.069582\n","\tRotated_Epoch:16 [001/005 (0400/0588)]\tLoss Ss: 0.081345\n","\tRotated_Epoch:16 [001/005 (0420/0588)]\tLoss Ss: 0.073519\n","\tRotated_Epoch:16 [001/005 (0440/0588)]\tLoss Ss: 0.063216\n","\tRotated_Epoch:16 [001/005 (0460/0588)]\tLoss Ss: 0.064609\n","\tRotated_Epoch:16 [001/005 (0480/0588)]\tLoss Ss: 0.048072\n","\tRotated_Epoch:16 [001/005 (0500/0588)]\tLoss Ss: 0.072670\n","\tRotated_Epoch:16 [001/005 (0520/0588)]\tLoss Ss: 0.055276\n","\tRotated_Epoch:16 [001/005 (0540/0588)]\tLoss Ss: 0.068643\n","\tRotated_Epoch:16 [001/005 (0560/0588)]\tLoss Ss: 0.073097\n","\tRotated_Epoch:16 [001/005 (0580/0588)]\tLoss Ss: 0.060492\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:16 [002/005 (0000/0755)]\tLoss Ss: 0.288319\n","\tRotated_Epoch:16 [002/005 (0020/0755)]\tLoss Ss: 0.146256\n","\tRotated_Epoch:16 [002/005 (0040/0755)]\tLoss Ss: 0.111249\n","\tRotated_Epoch:16 [002/005 (0060/0755)]\tLoss Ss: 0.128736\n","\tRotated_Epoch:16 [002/005 (0080/0755)]\tLoss Ss: 0.107965\n","\tRotated_Epoch:16 [002/005 (0100/0755)]\tLoss Ss: 0.079294\n","\tRotated_Epoch:16 [002/005 (0120/0755)]\tLoss Ss: 0.068647\n","\tRotated_Epoch:16 [002/005 (0140/0755)]\tLoss Ss: 0.053481\n","\tRotated_Epoch:16 [002/005 (0160/0755)]\tLoss Ss: 0.042764\n","\tRotated_Epoch:16 [002/005 (0180/0755)]\tLoss Ss: 0.077099\n","\tRotated_Epoch:16 [002/005 (0200/0755)]\tLoss Ss: 0.044674\n","\tRotated_Epoch:16 [002/005 (0220/0755)]\tLoss Ss: 0.112065\n","\tRotated_Epoch:16 [002/005 (0240/0755)]\tLoss Ss: 0.103898\n","\tRotated_Epoch:16 [002/005 (0260/0755)]\tLoss Ss: 0.048616\n","\tRotated_Epoch:16 [002/005 (0280/0755)]\tLoss Ss: 0.046866\n","\tRotated_Epoch:16 [002/005 (0300/0755)]\tLoss Ss: 0.092706\n","\tRotated_Epoch:16 [002/005 (0320/0755)]\tLoss Ss: 0.065881\n","\tRotated_Epoch:16 [002/005 (0340/0755)]\tLoss Ss: 0.062165\n","\tRotated_Epoch:16 [002/005 (0360/0755)]\tLoss Ss: 0.075067\n","\tRotated_Epoch:16 [002/005 (0380/0755)]\tLoss Ss: 0.050356\n","\tRotated_Epoch:16 [002/005 (0400/0755)]\tLoss Ss: 0.067686\n","\tRotated_Epoch:16 [002/005 (0420/0755)]\tLoss Ss: 0.047090\n","\tRotated_Epoch:16 [002/005 (0440/0755)]\tLoss Ss: 0.036128\n","\tRotated_Epoch:16 [002/005 (0460/0755)]\tLoss Ss: 0.066057\n","\tRotated_Epoch:16 [002/005 (0480/0755)]\tLoss Ss: 0.063344\n","\tRotated_Epoch:16 [002/005 (0500/0755)]\tLoss Ss: 0.069837\n","\tRotated_Epoch:16 [002/005 (0520/0755)]\tLoss Ss: 0.038337\n","\tRotated_Epoch:16 [002/005 (0540/0755)]\tLoss Ss: 0.045129\n","\tRotated_Epoch:16 [002/005 (0560/0755)]\tLoss Ss: 0.051215\n","\tRotated_Epoch:16 [002/005 (0580/0755)]\tLoss Ss: 0.053533\n","\tRotated_Epoch:16 [002/005 (0600/0755)]\tLoss Ss: 0.054371\n","\tRotated_Epoch:16 [002/005 (0620/0755)]\tLoss Ss: 0.066392\n","\tRotated_Epoch:16 [002/005 (0640/0755)]\tLoss Ss: 0.040265\n","\tRotated_Epoch:16 [002/005 (0660/0755)]\tLoss Ss: 0.057761\n","\tRotated_Epoch:16 [002/005 (0680/0755)]\tLoss Ss: 0.058140\n","\tRotated_Epoch:16 [002/005 (0700/0755)]\tLoss Ss: 0.074524\n","\tRotated_Epoch:16 [002/005 (0720/0755)]\tLoss Ss: 0.060996\n","\tRotated_Epoch:16 [002/005 (0740/0755)]\tLoss Ss: 0.070960\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tRotated_Epoch:16 [003/005 (0000/0693)]\tLoss Ss: 0.174966\n","\tRotated_Epoch:16 [003/005 (0020/0693)]\tLoss Ss: 0.155435\n","\tRotated_Epoch:16 [003/005 (0040/0693)]\tLoss Ss: 0.052719\n","\tRotated_Epoch:16 [003/005 (0060/0693)]\tLoss Ss: 0.039138\n","\tRotated_Epoch:16 [003/005 (0080/0693)]\tLoss Ss: 0.035661\n","\tRotated_Epoch:16 [003/005 (0100/0693)]\tLoss Ss: 0.038728\n","\tRotated_Epoch:16 [003/005 (0120/0693)]\tLoss Ss: 0.038645\n","\tRotated_Epoch:16 [003/005 (0140/0693)]\tLoss Ss: 0.028865\n","\tRotated_Epoch:16 [003/005 (0160/0693)]\tLoss Ss: 0.032183\n","\tRotated_Epoch:16 [003/005 (0180/0693)]\tLoss Ss: 0.017568\n","\tRotated_Epoch:16 [003/005 (0200/0693)]\tLoss Ss: 0.025098\n","\tRotated_Epoch:16 [003/005 (0220/0693)]\tLoss Ss: 0.027557\n","\tRotated_Epoch:16 [003/005 (0240/0693)]\tLoss Ss: 0.027333\n","\tRotated_Epoch:16 [003/005 (0260/0693)]\tLoss Ss: 0.020755\n","\tRotated_Epoch:16 [003/005 (0280/0693)]\tLoss Ss: 0.017018\n","\tRotated_Epoch:16 [003/005 (0300/0693)]\tLoss Ss: 0.014303\n","\tRotated_Epoch:16 [003/005 (0320/0693)]\tLoss Ss: 0.016180\n","\tRotated_Epoch:16 [003/005 (0340/0693)]\tLoss Ss: 0.027842\n","\tRotated_Epoch:16 [003/005 (0360/0693)]\tLoss Ss: 0.025547\n","\tRotated_Epoch:16 [003/005 (0380/0693)]\tLoss Ss: 0.010116\n","\tRotated_Epoch:16 [003/005 (0400/0693)]\tLoss Ss: 0.023431\n","\tRotated_Epoch:16 [003/005 (0420/0693)]\tLoss Ss: 0.019398\n","\tRotated_Epoch:16 [003/005 (0440/0693)]\tLoss Ss: 0.016563\n","\tRotated_Epoch:16 [003/005 (0460/0693)]\tLoss Ss: 0.014385\n","\tRotated_Epoch:16 [003/005 (0480/0693)]\tLoss Ss: 0.022979\n","\tRotated_Epoch:16 [003/005 (0500/0693)]\tLoss Ss: 0.017007\n","\tRotated_Epoch:16 [003/005 (0520/0693)]\tLoss Ss: 0.025120\n","\tRotated_Epoch:16 [003/005 (0540/0693)]\tLoss Ss: 0.016517\n","\tRotated_Epoch:16 [003/005 (0560/0693)]\tLoss Ss: 0.017509\n","\tRotated_Epoch:16 [003/005 (0580/0693)]\tLoss Ss: 0.023001\n","\tRotated_Epoch:16 [003/005 (0600/0693)]\tLoss Ss: 0.015508\n","\tRotated_Epoch:16 [003/005 (0620/0693)]\tLoss Ss: 0.020916\n","\tRotated_Epoch:16 [003/005 (0640/0693)]\tLoss Ss: 0.019409\n","\tRotated_Epoch:16 [003/005 (0660/0693)]\tLoss Ss: 0.017564\n","\tRotated_Epoch:16 [003/005 (0680/0693)]\tLoss Ss: 0.027870\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tRotated_Epoch:16 [004/005 (0000/0693)]\tLoss Ss: 0.022820\n","\tRotated_Epoch:16 [004/005 (0020/0693)]\tLoss Ss: 0.024390\n","\tRotated_Epoch:16 [004/005 (0040/0693)]\tLoss Ss: 0.020336\n","\tRotated_Epoch:16 [004/005 (0060/0693)]\tLoss Ss: 0.023837\n","\tRotated_Epoch:16 [004/005 (0080/0693)]\tLoss Ss: 0.016781\n","\tRotated_Epoch:16 [004/005 (0100/0693)]\tLoss Ss: 0.015612\n","\tRotated_Epoch:16 [004/005 (0120/0693)]\tLoss Ss: 0.014588\n","\tRotated_Epoch:16 [004/005 (0140/0693)]\tLoss Ss: 0.021521\n","\tRotated_Epoch:16 [004/005 (0160/0693)]\tLoss Ss: 0.018608\n","\tRotated_Epoch:16 [004/005 (0180/0693)]\tLoss Ss: 0.021422\n","\tRotated_Epoch:16 [004/005 (0200/0693)]\tLoss Ss: 0.014438\n","\tRotated_Epoch:16 [004/005 (0220/0693)]\tLoss Ss: 0.018735\n","\tRotated_Epoch:16 [004/005 (0240/0693)]\tLoss Ss: 0.022932\n","\tRotated_Epoch:16 [004/005 (0260/0693)]\tLoss Ss: 0.021836\n","\tRotated_Epoch:16 [004/005 (0280/0693)]\tLoss Ss: 0.020224\n","\tRotated_Epoch:16 [004/005 (0300/0693)]\tLoss Ss: 0.018935\n","\tRotated_Epoch:16 [004/005 (0320/0693)]\tLoss Ss: 0.024700\n","\tRotated_Epoch:16 [004/005 (0340/0693)]\tLoss Ss: 0.018082\n","\tRotated_Epoch:16 [004/005 (0360/0693)]\tLoss Ss: 0.015414\n","\tRotated_Epoch:16 [004/005 (0380/0693)]\tLoss Ss: 0.018091\n","\tRotated_Epoch:16 [004/005 (0400/0693)]\tLoss Ss: 0.017294\n","\tRotated_Epoch:16 [004/005 (0420/0693)]\tLoss Ss: 0.016769\n","\tRotated_Epoch:16 [004/005 (0440/0693)]\tLoss Ss: 0.013994\n","\tRotated_Epoch:16 [004/005 (0460/0693)]\tLoss Ss: 0.013781\n","\tRotated_Epoch:16 [004/005 (0480/0693)]\tLoss Ss: 0.013456\n","\tRotated_Epoch:16 [004/005 (0500/0693)]\tLoss Ss: 0.013453\n","\tRotated_Epoch:16 [004/005 (0520/0693)]\tLoss Ss: 0.013404\n","\tRotated_Epoch:16 [004/005 (0540/0693)]\tLoss Ss: 0.019148\n","\tRotated_Epoch:16 [004/005 (0560/0693)]\tLoss Ss: 0.013951\n","\tRotated_Epoch:16 [004/005 (0580/0693)]\tLoss Ss: 0.013437\n","\tRotated_Epoch:16 [004/005 (0600/0693)]\tLoss Ss: 0.018675\n","\tRotated_Epoch:16 [004/005 (0620/0693)]\tLoss Ss: 0.014847\n","\tRotated_Epoch:16 [004/005 (0640/0693)]\tLoss Ss: 0.011667\n","\tRotated_Epoch:16 [004/005 (0660/0693)]\tLoss Ss: 0.015389\n","\tRotated_Epoch:16 [004/005 (0680/0693)]\tLoss Ss: 0.017512\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:16 [005/005 (0000/0755)]\tLoss Ss: 0.039574\n","\tRotated_Epoch:16 [005/005 (0020/0755)]\tLoss Ss: 0.061633\n","\tRotated_Epoch:16 [005/005 (0040/0755)]\tLoss Ss: 0.045657\n","\tRotated_Epoch:16 [005/005 (0060/0755)]\tLoss Ss: 0.047013\n","\tRotated_Epoch:16 [005/005 (0080/0755)]\tLoss Ss: 0.027034\n","\tRotated_Epoch:16 [005/005 (0100/0755)]\tLoss Ss: 0.048841\n","\tRotated_Epoch:16 [005/005 (0120/0755)]\tLoss Ss: 0.025038\n","\tRotated_Epoch:16 [005/005 (0140/0755)]\tLoss Ss: 0.028584\n","\tRotated_Epoch:16 [005/005 (0160/0755)]\tLoss Ss: 0.027225\n","\tRotated_Epoch:16 [005/005 (0180/0755)]\tLoss Ss: 0.020060\n","\tRotated_Epoch:16 [005/005 (0200/0755)]\tLoss Ss: 0.031496\n","\tRotated_Epoch:16 [005/005 (0220/0755)]\tLoss Ss: 0.028555\n","\tRotated_Epoch:16 [005/005 (0240/0755)]\tLoss Ss: 0.023353\n","\tRotated_Epoch:16 [005/005 (0260/0755)]\tLoss Ss: 0.026367\n","\tRotated_Epoch:16 [005/005 (0280/0755)]\tLoss Ss: 0.028064\n","\tRotated_Epoch:16 [005/005 (0300/0755)]\tLoss Ss: 0.025199\n","\tRotated_Epoch:16 [005/005 (0320/0755)]\tLoss Ss: 0.019986\n","\tRotated_Epoch:16 [005/005 (0340/0755)]\tLoss Ss: 0.042258\n","\tRotated_Epoch:16 [005/005 (0360/0755)]\tLoss Ss: 0.021418\n","\tRotated_Epoch:16 [005/005 (0380/0755)]\tLoss Ss: 0.026371\n","\tRotated_Epoch:16 [005/005 (0400/0755)]\tLoss Ss: 0.026219\n","\tRotated_Epoch:16 [005/005 (0420/0755)]\tLoss Ss: 0.033567\n","\tRotated_Epoch:16 [005/005 (0440/0755)]\tLoss Ss: 0.023250\n","\tRotated_Epoch:16 [005/005 (0460/0755)]\tLoss Ss: 0.025003\n","\tRotated_Epoch:16 [005/005 (0480/0755)]\tLoss Ss: 0.019642\n","\tRotated_Epoch:16 [005/005 (0500/0755)]\tLoss Ss: 0.024822\n","\tRotated_Epoch:16 [005/005 (0520/0755)]\tLoss Ss: 0.018974\n","\tRotated_Epoch:16 [005/005 (0540/0755)]\tLoss Ss: 0.030121\n","\tRotated_Epoch:16 [005/005 (0560/0755)]\tLoss Ss: 0.021456\n","\tRotated_Epoch:16 [005/005 (0580/0755)]\tLoss Ss: 0.015508\n","\tRotated_Epoch:16 [005/005 (0600/0755)]\tLoss Ss: 0.013416\n","\tRotated_Epoch:16 [005/005 (0620/0755)]\tLoss Ss: 0.011910\n","\tRotated_Epoch:16 [005/005 (0640/0755)]\tLoss Ss: 0.025293\n","\tRotated_Epoch:16 [005/005 (0660/0755)]\tLoss Ss: 0.039719\n","\tRotated_Epoch:16 [005/005 (0680/0755)]\tLoss Ss: 0.020388\n","\tRotated_Epoch:16 [005/005 (0700/0755)]\tLoss Ss: 0.017769\n","\tRotated_Epoch:16 [005/005 (0720/0755)]\tLoss Ss: 0.011917\n","\tRotated_Epoch:16 [005/005 (0740/0755)]\tLoss Ss: 0.009584\n","new_1_6_d.nii\n","new_1_6_m.nii\n","new_1_7_d.nii\n","new_1_7_m.nii\n","new_2_5_d.nii\n","new_2_5_m.nii\n","new_2_6_d.nii\n","new_2_6_m.nii\n","new_2_7_d.nii\n","new_2_7_m.nii\n","new_3_d (2).nii\n","new_3_m (2).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (3).nii\n","new_3_m (3).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (7).nii\n","new_3_m (7).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (8).nii\n","new_3_m (8).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (9).nii\n","new_3_m (9).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_4_10_d_163526.nii\n","new_4_10_m_163526.nii.gz\n","new_4_11_d_084336.nii\n","new_4_11_m_084336.nii.gz\n","new_4_7_d_184422.nii\n","new_4_7_m_184422.nii.gz\n","new_4_8_d_143412.nii\n","new_4_8_m_143412.nii.gz\n","new_4_9_d_155347.nii\n","new_4_9_m_155347.nii.gz\n","\tEpoch: 16; Dice: 0.9314 +/- 0.0207; Loss: 12.1058\n","Begin Epoch 17\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tEpoch:17 [000/005 (0000/0614)]\tLoss Ss: 0.033664\n","\tEpoch:17 [000/005 (0020/0614)]\tLoss Ss: 0.021398\n","\tEpoch:17 [000/005 (0040/0614)]\tLoss Ss: 0.022708\n","\tEpoch:17 [000/005 (0060/0614)]\tLoss Ss: 0.014975\n","\tEpoch:17 [000/005 (0080/0614)]\tLoss Ss: 0.018543\n","\tEpoch:17 [000/005 (0100/0614)]\tLoss Ss: 0.012938\n","\tEpoch:17 [000/005 (0120/0614)]\tLoss Ss: 0.010722\n","\tEpoch:17 [000/005 (0140/0614)]\tLoss Ss: 0.013187\n","\tEpoch:17 [000/005 (0160/0614)]\tLoss Ss: 0.013107\n","\tEpoch:17 [000/005 (0180/0614)]\tLoss Ss: 0.009638\n","\tEpoch:17 [000/005 (0200/0614)]\tLoss Ss: 0.010774\n","\tEpoch:17 [000/005 (0220/0614)]\tLoss Ss: 0.009480\n","\tEpoch:17 [000/005 (0240/0614)]\tLoss Ss: 0.008407\n","\tEpoch:17 [000/005 (0260/0614)]\tLoss Ss: 0.006581\n","\tEpoch:17 [000/005 (0280/0614)]\tLoss Ss: 0.012408\n","\tEpoch:17 [000/005 (0300/0614)]\tLoss Ss: 0.009520\n","\tEpoch:17 [000/005 (0320/0614)]\tLoss Ss: 0.010943\n","\tEpoch:17 [000/005 (0340/0614)]\tLoss Ss: 0.006962\n","\tEpoch:17 [000/005 (0360/0614)]\tLoss Ss: 0.008511\n","\tEpoch:17 [000/005 (0380/0614)]\tLoss Ss: 0.009612\n","\tEpoch:17 [000/005 (0400/0614)]\tLoss Ss: 0.006610\n","\tEpoch:17 [000/005 (0420/0614)]\tLoss Ss: 0.009383\n","\tEpoch:17 [000/005 (0440/0614)]\tLoss Ss: 0.010157\n","\tEpoch:17 [000/005 (0460/0614)]\tLoss Ss: 0.006131\n","\tEpoch:17 [000/005 (0480/0614)]\tLoss Ss: 0.006822\n","\tEpoch:17 [000/005 (0500/0614)]\tLoss Ss: 0.005306\n","\tEpoch:17 [000/005 (0520/0614)]\tLoss Ss: 0.008565\n","\tEpoch:17 [000/005 (0540/0614)]\tLoss Ss: 0.012184\n","\tEpoch:17 [000/005 (0560/0614)]\tLoss Ss: 0.005090\n","\tEpoch:17 [000/005 (0580/0614)]\tLoss Ss: 0.005754\n","\tEpoch:17 [000/005 (0600/0614)]\tLoss Ss: 0.005908\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tEpoch:17 [001/005 (0000/0588)]\tLoss Ss: 0.013488\n","\tEpoch:17 [001/005 (0020/0588)]\tLoss Ss: 0.015717\n","\tEpoch:17 [001/005 (0040/0588)]\tLoss Ss: 0.011078\n","\tEpoch:17 [001/005 (0060/0588)]\tLoss Ss: 0.009360\n","\tEpoch:17 [001/005 (0080/0588)]\tLoss Ss: 0.009835\n","\tEpoch:17 [001/005 (0100/0588)]\tLoss Ss: 0.006472\n","\tEpoch:17 [001/005 (0120/0588)]\tLoss Ss: 0.008182\n","\tEpoch:17 [001/005 (0140/0588)]\tLoss Ss: 0.010447\n","\tEpoch:17 [001/005 (0160/0588)]\tLoss Ss: 0.016826\n","\tEpoch:17 [001/005 (0180/0588)]\tLoss Ss: 0.010380\n","\tEpoch:17 [001/005 (0200/0588)]\tLoss Ss: 0.006953\n","\tEpoch:17 [001/005 (0220/0588)]\tLoss Ss: 0.007986\n","\tEpoch:17 [001/005 (0240/0588)]\tLoss Ss: 0.005997\n","\tEpoch:17 [001/005 (0260/0588)]\tLoss Ss: 0.007492\n","\tEpoch:17 [001/005 (0280/0588)]\tLoss Ss: 0.007441\n","\tEpoch:17 [001/005 (0300/0588)]\tLoss Ss: 0.008668\n","\tEpoch:17 [001/005 (0320/0588)]\tLoss Ss: 0.006235\n","\tEpoch:17 [001/005 (0340/0588)]\tLoss Ss: 0.006973\n","\tEpoch:17 [001/005 (0360/0588)]\tLoss Ss: 0.008485\n","\tEpoch:17 [001/005 (0380/0588)]\tLoss Ss: 0.006642\n","\tEpoch:17 [001/005 (0400/0588)]\tLoss Ss: 0.010406\n","\tEpoch:17 [001/005 (0420/0588)]\tLoss Ss: 0.006664\n","\tEpoch:17 [001/005 (0440/0588)]\tLoss Ss: 0.005490\n","\tEpoch:17 [001/005 (0460/0588)]\tLoss Ss: 0.005558\n","\tEpoch:17 [001/005 (0480/0588)]\tLoss Ss: 0.006460\n","\tEpoch:17 [001/005 (0500/0588)]\tLoss Ss: 0.005556\n","\tEpoch:17 [001/005 (0520/0588)]\tLoss Ss: 0.003999\n","\tEpoch:17 [001/005 (0540/0588)]\tLoss Ss: 0.007685\n","\tEpoch:17 [001/005 (0560/0588)]\tLoss Ss: 0.005299\n","\tEpoch:17 [001/005 (0580/0588)]\tLoss Ss: 0.004279\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:17 [002/005 (0000/0755)]\tLoss Ss: 0.019464\n","\tEpoch:17 [002/005 (0020/0755)]\tLoss Ss: 0.016966\n","\tEpoch:17 [002/005 (0040/0755)]\tLoss Ss: 0.016403\n","\tEpoch:17 [002/005 (0060/0755)]\tLoss Ss: 0.025442\n","\tEpoch:17 [002/005 (0080/0755)]\tLoss Ss: 0.024349\n","\tEpoch:17 [002/005 (0100/0755)]\tLoss Ss: 0.036715\n","\tEpoch:17 [002/005 (0120/0755)]\tLoss Ss: 0.015448\n","\tEpoch:17 [002/005 (0140/0755)]\tLoss Ss: 0.021560\n","\tEpoch:17 [002/005 (0160/0755)]\tLoss Ss: 0.018664\n","\tEpoch:17 [002/005 (0180/0755)]\tLoss Ss: 0.016091\n","\tEpoch:17 [002/005 (0200/0755)]\tLoss Ss: 0.015591\n","\tEpoch:17 [002/005 (0220/0755)]\tLoss Ss: 0.009271\n","\tEpoch:17 [002/005 (0240/0755)]\tLoss Ss: 0.013464\n","\tEpoch:17 [002/005 (0260/0755)]\tLoss Ss: 0.011971\n","\tEpoch:17 [002/005 (0280/0755)]\tLoss Ss: 0.015927\n","\tEpoch:17 [002/005 (0300/0755)]\tLoss Ss: 0.017314\n","\tEpoch:17 [002/005 (0320/0755)]\tLoss Ss: 0.020755\n","\tEpoch:17 [002/005 (0340/0755)]\tLoss Ss: 0.018059\n","\tEpoch:17 [002/005 (0360/0755)]\tLoss Ss: 0.015222\n","\tEpoch:17 [002/005 (0380/0755)]\tLoss Ss: 0.016744\n","\tEpoch:17 [002/005 (0400/0755)]\tLoss Ss: 0.017692\n","\tEpoch:17 [002/005 (0420/0755)]\tLoss Ss: 0.016557\n","\tEpoch:17 [002/005 (0440/0755)]\tLoss Ss: 0.017850\n","\tEpoch:17 [002/005 (0460/0755)]\tLoss Ss: 0.021292\n","\tEpoch:17 [002/005 (0480/0755)]\tLoss Ss: 0.011886\n","\tEpoch:17 [002/005 (0500/0755)]\tLoss Ss: 0.019740\n","\tEpoch:17 [002/005 (0520/0755)]\tLoss Ss: 0.017420\n","\tEpoch:17 [002/005 (0540/0755)]\tLoss Ss: 0.014950\n","\tEpoch:17 [002/005 (0560/0755)]\tLoss Ss: 0.013541\n","\tEpoch:17 [002/005 (0580/0755)]\tLoss Ss: 0.012460\n","\tEpoch:17 [002/005 (0600/0755)]\tLoss Ss: 0.008899\n","\tEpoch:17 [002/005 (0620/0755)]\tLoss Ss: 0.014397\n","\tEpoch:17 [002/005 (0640/0755)]\tLoss Ss: 0.014234\n","\tEpoch:17 [002/005 (0660/0755)]\tLoss Ss: 0.015027\n","\tEpoch:17 [002/005 (0680/0755)]\tLoss Ss: 0.019105\n","\tEpoch:17 [002/005 (0700/0755)]\tLoss Ss: 0.013196\n","\tEpoch:17 [002/005 (0720/0755)]\tLoss Ss: 0.015455\n","\tEpoch:17 [002/005 (0740/0755)]\tLoss Ss: 0.013927\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tEpoch:17 [003/005 (0000/0693)]\tLoss Ss: 0.017782\n","\tEpoch:17 [003/005 (0020/0693)]\tLoss Ss: 0.021698\n","\tEpoch:17 [003/005 (0040/0693)]\tLoss Ss: 0.014654\n","\tEpoch:17 [003/005 (0060/0693)]\tLoss Ss: 0.011584\n","\tEpoch:17 [003/005 (0080/0693)]\tLoss Ss: 0.023264\n","\tEpoch:17 [003/005 (0100/0693)]\tLoss Ss: 0.014832\n","\tEpoch:17 [003/005 (0120/0693)]\tLoss Ss: 0.014488\n","\tEpoch:17 [003/005 (0140/0693)]\tLoss Ss: 0.015771\n","\tEpoch:17 [003/005 (0160/0693)]\tLoss Ss: 0.011556\n","\tEpoch:17 [003/005 (0180/0693)]\tLoss Ss: 0.013972\n","\tEpoch:17 [003/005 (0200/0693)]\tLoss Ss: 0.015504\n","\tEpoch:17 [003/005 (0220/0693)]\tLoss Ss: 0.019684\n","\tEpoch:17 [003/005 (0240/0693)]\tLoss Ss: 0.017863\n","\tEpoch:17 [003/005 (0260/0693)]\tLoss Ss: 0.017943\n","\tEpoch:17 [003/005 (0280/0693)]\tLoss Ss: 0.019835\n","\tEpoch:17 [003/005 (0300/0693)]\tLoss Ss: 0.018503\n","\tEpoch:17 [003/005 (0320/0693)]\tLoss Ss: 0.011690\n","\tEpoch:17 [003/005 (0340/0693)]\tLoss Ss: 0.010664\n","\tEpoch:17 [003/005 (0360/0693)]\tLoss Ss: 0.015981\n","\tEpoch:17 [003/005 (0380/0693)]\tLoss Ss: 0.017477\n","\tEpoch:17 [003/005 (0400/0693)]\tLoss Ss: 0.023859\n","\tEpoch:17 [003/005 (0420/0693)]\tLoss Ss: 0.017343\n","\tEpoch:17 [003/005 (0440/0693)]\tLoss Ss: 0.017837\n","\tEpoch:17 [003/005 (0460/0693)]\tLoss Ss: 0.018440\n","\tEpoch:17 [003/005 (0480/0693)]\tLoss Ss: 0.020351\n","\tEpoch:17 [003/005 (0500/0693)]\tLoss Ss: 0.021086\n","\tEpoch:17 [003/005 (0520/0693)]\tLoss Ss: 0.009645\n","\tEpoch:17 [003/005 (0540/0693)]\tLoss Ss: 0.015547\n","\tEpoch:17 [003/005 (0560/0693)]\tLoss Ss: 0.015977\n","\tEpoch:17 [003/005 (0580/0693)]\tLoss Ss: 0.012747\n","\tEpoch:17 [003/005 (0600/0693)]\tLoss Ss: 0.015482\n","\tEpoch:17 [003/005 (0620/0693)]\tLoss Ss: 0.019961\n","\tEpoch:17 [003/005 (0640/0693)]\tLoss Ss: 0.012337\n","\tEpoch:17 [003/005 (0660/0693)]\tLoss Ss: 0.012504\n","\tEpoch:17 [003/005 (0680/0693)]\tLoss Ss: 0.021538\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tEpoch:17 [004/005 (0000/0693)]\tLoss Ss: 0.013813\n","\tEpoch:17 [004/005 (0020/0693)]\tLoss Ss: 0.015997\n","\tEpoch:17 [004/005 (0040/0693)]\tLoss Ss: 0.014758\n","\tEpoch:17 [004/005 (0060/0693)]\tLoss Ss: 0.013734\n","\tEpoch:17 [004/005 (0080/0693)]\tLoss Ss: 0.011576\n","\tEpoch:17 [004/005 (0100/0693)]\tLoss Ss: 0.017705\n","\tEpoch:17 [004/005 (0120/0693)]\tLoss Ss: 0.015378\n","\tEpoch:17 [004/005 (0140/0693)]\tLoss Ss: 0.014424\n","\tEpoch:17 [004/005 (0160/0693)]\tLoss Ss: 0.011229\n","\tEpoch:17 [004/005 (0180/0693)]\tLoss Ss: 0.019174\n","\tEpoch:17 [004/005 (0200/0693)]\tLoss Ss: 0.018741\n","\tEpoch:17 [004/005 (0220/0693)]\tLoss Ss: 0.018766\n","\tEpoch:17 [004/005 (0240/0693)]\tLoss Ss: 0.014663\n","\tEpoch:17 [004/005 (0260/0693)]\tLoss Ss: 0.010006\n","\tEpoch:17 [004/005 (0280/0693)]\tLoss Ss: 0.016585\n","\tEpoch:17 [004/005 (0300/0693)]\tLoss Ss: 0.013669\n","\tEpoch:17 [004/005 (0320/0693)]\tLoss Ss: 0.014360\n","\tEpoch:17 [004/005 (0340/0693)]\tLoss Ss: 0.017599\n","\tEpoch:17 [004/005 (0360/0693)]\tLoss Ss: 0.015203\n","\tEpoch:17 [004/005 (0380/0693)]\tLoss Ss: 0.010693\n","\tEpoch:17 [004/005 (0400/0693)]\tLoss Ss: 0.015052\n","\tEpoch:17 [004/005 (0420/0693)]\tLoss Ss: 0.009401\n","\tEpoch:17 [004/005 (0440/0693)]\tLoss Ss: 0.013477\n","\tEpoch:17 [004/005 (0460/0693)]\tLoss Ss: 0.012587\n","\tEpoch:17 [004/005 (0480/0693)]\tLoss Ss: 0.015910\n","\tEpoch:17 [004/005 (0500/0693)]\tLoss Ss: 0.014089\n","\tEpoch:17 [004/005 (0520/0693)]\tLoss Ss: 0.015954\n","\tEpoch:17 [004/005 (0540/0693)]\tLoss Ss: 0.010658\n","\tEpoch:17 [004/005 (0560/0693)]\tLoss Ss: 0.009679\n","\tEpoch:17 [004/005 (0580/0693)]\tLoss Ss: 0.010811\n","\tEpoch:17 [004/005 (0600/0693)]\tLoss Ss: 0.012371\n","\tEpoch:17 [004/005 (0620/0693)]\tLoss Ss: 0.013326\n","\tEpoch:17 [004/005 (0640/0693)]\tLoss Ss: 0.010112\n","\tEpoch:17 [004/005 (0660/0693)]\tLoss Ss: 0.013306\n","\tEpoch:17 [004/005 (0680/0693)]\tLoss Ss: 0.012047\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:17 [005/005 (0000/0755)]\tLoss Ss: 0.021196\n","\tEpoch:17 [005/005 (0020/0755)]\tLoss Ss: 0.019975\n","\tEpoch:17 [005/005 (0040/0755)]\tLoss Ss: 0.021791\n","\tEpoch:17 [005/005 (0060/0755)]\tLoss Ss: 0.028255\n","\tEpoch:17 [005/005 (0080/0755)]\tLoss Ss: 0.024959\n","\tEpoch:17 [005/005 (0100/0755)]\tLoss Ss: 0.026988\n","\tEpoch:17 [005/005 (0120/0755)]\tLoss Ss: 0.020074\n","\tEpoch:17 [005/005 (0140/0755)]\tLoss Ss: 0.023120\n","\tEpoch:17 [005/005 (0160/0755)]\tLoss Ss: 0.018545\n","\tEpoch:17 [005/005 (0180/0755)]\tLoss Ss: 0.017560\n","\tEpoch:17 [005/005 (0200/0755)]\tLoss Ss: 0.017477\n","\tEpoch:17 [005/005 (0220/0755)]\tLoss Ss: 0.027151\n","\tEpoch:17 [005/005 (0240/0755)]\tLoss Ss: 0.026276\n","\tEpoch:17 [005/005 (0260/0755)]\tLoss Ss: 0.017211\n","\tEpoch:17 [005/005 (0280/0755)]\tLoss Ss: 0.019952\n","\tEpoch:17 [005/005 (0300/0755)]\tLoss Ss: 0.014706\n","\tEpoch:17 [005/005 (0320/0755)]\tLoss Ss: 0.015075\n","\tEpoch:17 [005/005 (0340/0755)]\tLoss Ss: 0.019128\n","\tEpoch:17 [005/005 (0360/0755)]\tLoss Ss: 0.015343\n","\tEpoch:17 [005/005 (0380/0755)]\tLoss Ss: 0.017111\n","\tEpoch:17 [005/005 (0400/0755)]\tLoss Ss: 0.019124\n","\tEpoch:17 [005/005 (0420/0755)]\tLoss Ss: 0.013924\n","\tEpoch:17 [005/005 (0440/0755)]\tLoss Ss: 0.012921\n","\tEpoch:17 [005/005 (0460/0755)]\tLoss Ss: 0.013049\n","\tEpoch:17 [005/005 (0480/0755)]\tLoss Ss: 0.021496\n","\tEpoch:17 [005/005 (0500/0755)]\tLoss Ss: 0.024414\n","\tEpoch:17 [005/005 (0520/0755)]\tLoss Ss: 0.018006\n","\tEpoch:17 [005/005 (0540/0755)]\tLoss Ss: 0.020350\n","\tEpoch:17 [005/005 (0560/0755)]\tLoss Ss: 0.018183\n","\tEpoch:17 [005/005 (0580/0755)]\tLoss Ss: 0.017003\n","\tEpoch:17 [005/005 (0600/0755)]\tLoss Ss: 0.016957\n","\tEpoch:17 [005/005 (0620/0755)]\tLoss Ss: 0.016138\n","\tEpoch:17 [005/005 (0640/0755)]\tLoss Ss: 0.016924\n","\tEpoch:17 [005/005 (0660/0755)]\tLoss Ss: 0.027482\n","\tEpoch:17 [005/005 (0680/0755)]\tLoss Ss: 0.017253\n","\tEpoch:17 [005/005 (0700/0755)]\tLoss Ss: 0.009902\n","\tEpoch:17 [005/005 (0720/0755)]\tLoss Ss: 0.017742\n","\tEpoch:17 [005/005 (0740/0755)]\tLoss Ss: 0.021557\n","Now train the rotated image\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:17 [000/005 (0000/0755)]\tLoss Ss: 0.422509\n","\tRotated_Epoch:17 [000/005 (0020/0755)]\tLoss Ss: 0.227500\n","\tRotated_Epoch:17 [000/005 (0040/0755)]\tLoss Ss: 0.191993\n","\tRotated_Epoch:17 [000/005 (0060/0755)]\tLoss Ss: 0.247482\n","\tRotated_Epoch:17 [000/005 (0080/0755)]\tLoss Ss: 0.152340\n","\tRotated_Epoch:17 [000/005 (0100/0755)]\tLoss Ss: 0.141660\n","\tRotated_Epoch:17 [000/005 (0120/0755)]\tLoss Ss: 0.143293\n","\tRotated_Epoch:17 [000/005 (0140/0755)]\tLoss Ss: 0.099169\n","\tRotated_Epoch:17 [000/005 (0160/0755)]\tLoss Ss: 0.100605\n","\tRotated_Epoch:17 [000/005 (0180/0755)]\tLoss Ss: 0.070294\n","\tRotated_Epoch:17 [000/005 (0200/0755)]\tLoss Ss: 0.034162\n","\tRotated_Epoch:17 [000/005 (0220/0755)]\tLoss Ss: 0.101199\n","\tRotated_Epoch:17 [000/005 (0240/0755)]\tLoss Ss: 0.073139\n","\tRotated_Epoch:17 [000/005 (0260/0755)]\tLoss Ss: 0.102163\n","\tRotated_Epoch:17 [000/005 (0280/0755)]\tLoss Ss: 0.079699\n","\tRotated_Epoch:17 [000/005 (0300/0755)]\tLoss Ss: 0.066071\n","\tRotated_Epoch:17 [000/005 (0320/0755)]\tLoss Ss: 0.054020\n","\tRotated_Epoch:17 [000/005 (0340/0755)]\tLoss Ss: 0.051755\n","\tRotated_Epoch:17 [000/005 (0360/0755)]\tLoss Ss: 0.055942\n","\tRotated_Epoch:17 [000/005 (0380/0755)]\tLoss Ss: 0.072345\n","\tRotated_Epoch:17 [000/005 (0400/0755)]\tLoss Ss: 0.066009\n","\tRotated_Epoch:17 [000/005 (0420/0755)]\tLoss Ss: 0.063373\n","\tRotated_Epoch:17 [000/005 (0440/0755)]\tLoss Ss: 0.056947\n","\tRotated_Epoch:17 [000/005 (0460/0755)]\tLoss Ss: 0.061751\n","\tRotated_Epoch:17 [000/005 (0480/0755)]\tLoss Ss: 0.043978\n","\tRotated_Epoch:17 [000/005 (0500/0755)]\tLoss Ss: 0.052134\n","\tRotated_Epoch:17 [000/005 (0520/0755)]\tLoss Ss: 0.050794\n","\tRotated_Epoch:17 [000/005 (0540/0755)]\tLoss Ss: 0.041931\n","\tRotated_Epoch:17 [000/005 (0560/0755)]\tLoss Ss: 0.051724\n","\tRotated_Epoch:17 [000/005 (0580/0755)]\tLoss Ss: 0.063885\n","\tRotated_Epoch:17 [000/005 (0600/0755)]\tLoss Ss: 0.064384\n","\tRotated_Epoch:17 [000/005 (0620/0755)]\tLoss Ss: 0.051827\n","\tRotated_Epoch:17 [000/005 (0640/0755)]\tLoss Ss: 0.043163\n","\tRotated_Epoch:17 [000/005 (0660/0755)]\tLoss Ss: 0.040505\n","\tRotated_Epoch:17 [000/005 (0680/0755)]\tLoss Ss: 0.047682\n","\tRotated_Epoch:17 [000/005 (0700/0755)]\tLoss Ss: 0.035868\n","\tRotated_Epoch:17 [000/005 (0720/0755)]\tLoss Ss: 0.054429\n","\tRotated_Epoch:17 [000/005 (0740/0755)]\tLoss Ss: 0.055957\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tRotated_Epoch:17 [001/005 (0000/0693)]\tLoss Ss: 0.129964\n","\tRotated_Epoch:17 [001/005 (0020/0693)]\tLoss Ss: 0.069444\n","\tRotated_Epoch:17 [001/005 (0040/0693)]\tLoss Ss: 0.074238\n","\tRotated_Epoch:17 [001/005 (0060/0693)]\tLoss Ss: 0.042494\n","\tRotated_Epoch:17 [001/005 (0080/0693)]\tLoss Ss: 0.035863\n","\tRotated_Epoch:17 [001/005 (0100/0693)]\tLoss Ss: 0.030171\n","\tRotated_Epoch:17 [001/005 (0120/0693)]\tLoss Ss: 0.034011\n","\tRotated_Epoch:17 [001/005 (0140/0693)]\tLoss Ss: 0.030698\n","\tRotated_Epoch:17 [001/005 (0160/0693)]\tLoss Ss: 0.023699\n","\tRotated_Epoch:17 [001/005 (0180/0693)]\tLoss Ss: 0.032304\n","\tRotated_Epoch:17 [001/005 (0200/0693)]\tLoss Ss: 0.021830\n","\tRotated_Epoch:17 [001/005 (0220/0693)]\tLoss Ss: 0.020674\n","\tRotated_Epoch:17 [001/005 (0240/0693)]\tLoss Ss: 0.019558\n","\tRotated_Epoch:17 [001/005 (0260/0693)]\tLoss Ss: 0.025365\n","\tRotated_Epoch:17 [001/005 (0280/0693)]\tLoss Ss: 0.026910\n","\tRotated_Epoch:17 [001/005 (0300/0693)]\tLoss Ss: 0.015697\n","\tRotated_Epoch:17 [001/005 (0320/0693)]\tLoss Ss: 0.020536\n","\tRotated_Epoch:17 [001/005 (0340/0693)]\tLoss Ss: 0.020194\n","\tRotated_Epoch:17 [001/005 (0360/0693)]\tLoss Ss: 0.017364\n","\tRotated_Epoch:17 [001/005 (0380/0693)]\tLoss Ss: 0.023475\n","\tRotated_Epoch:17 [001/005 (0400/0693)]\tLoss Ss: 0.022712\n","\tRotated_Epoch:17 [001/005 (0420/0693)]\tLoss Ss: 0.015390\n","\tRotated_Epoch:17 [001/005 (0440/0693)]\tLoss Ss: 0.017776\n","\tRotated_Epoch:17 [001/005 (0460/0693)]\tLoss Ss: 0.020899\n","\tRotated_Epoch:17 [001/005 (0480/0693)]\tLoss Ss: 0.013858\n","\tRotated_Epoch:17 [001/005 (0500/0693)]\tLoss Ss: 0.015186\n","\tRotated_Epoch:17 [001/005 (0520/0693)]\tLoss Ss: 0.019863\n","\tRotated_Epoch:17 [001/005 (0540/0693)]\tLoss Ss: 0.015064\n","\tRotated_Epoch:17 [001/005 (0560/0693)]\tLoss Ss: 0.010392\n","\tRotated_Epoch:17 [001/005 (0580/0693)]\tLoss Ss: 0.014520\n","\tRotated_Epoch:17 [001/005 (0600/0693)]\tLoss Ss: 0.016265\n","\tRotated_Epoch:17 [001/005 (0620/0693)]\tLoss Ss: 0.017293\n","\tRotated_Epoch:17 [001/005 (0640/0693)]\tLoss Ss: 0.020413\n","\tRotated_Epoch:17 [001/005 (0660/0693)]\tLoss Ss: 0.023202\n","\tRotated_Epoch:17 [001/005 (0680/0693)]\tLoss Ss: 0.013670\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tRotated_Epoch:17 [002/005 (0000/0614)]\tLoss Ss: 0.035506\n","\tRotated_Epoch:17 [002/005 (0020/0614)]\tLoss Ss: 0.032158\n","\tRotated_Epoch:17 [002/005 (0040/0614)]\tLoss Ss: 0.032848\n","\tRotated_Epoch:17 [002/005 (0060/0614)]\tLoss Ss: 0.016319\n","\tRotated_Epoch:17 [002/005 (0080/0614)]\tLoss Ss: 0.019223\n","\tRotated_Epoch:17 [002/005 (0100/0614)]\tLoss Ss: 0.012755\n","\tRotated_Epoch:17 [002/005 (0120/0614)]\tLoss Ss: 0.010815\n","\tRotated_Epoch:17 [002/005 (0140/0614)]\tLoss Ss: 0.011897\n","\tRotated_Epoch:17 [002/005 (0160/0614)]\tLoss Ss: 0.019775\n","\tRotated_Epoch:17 [002/005 (0180/0614)]\tLoss Ss: 0.011699\n","\tRotated_Epoch:17 [002/005 (0200/0614)]\tLoss Ss: 0.009840\n","\tRotated_Epoch:17 [002/005 (0220/0614)]\tLoss Ss: 0.012529\n","\tRotated_Epoch:17 [002/005 (0240/0614)]\tLoss Ss: 0.013046\n","\tRotated_Epoch:17 [002/005 (0260/0614)]\tLoss Ss: 0.008650\n","\tRotated_Epoch:17 [002/005 (0280/0614)]\tLoss Ss: 0.010368\n","\tRotated_Epoch:17 [002/005 (0300/0614)]\tLoss Ss: 0.007580\n","\tRotated_Epoch:17 [002/005 (0320/0614)]\tLoss Ss: 0.009679\n","\tRotated_Epoch:17 [002/005 (0340/0614)]\tLoss Ss: 0.008443\n","\tRotated_Epoch:17 [002/005 (0360/0614)]\tLoss Ss: 0.006084\n","\tRotated_Epoch:17 [002/005 (0380/0614)]\tLoss Ss: 0.007592\n","\tRotated_Epoch:17 [002/005 (0400/0614)]\tLoss Ss: 0.010065\n","\tRotated_Epoch:17 [002/005 (0420/0614)]\tLoss Ss: 0.008150\n","\tRotated_Epoch:17 [002/005 (0440/0614)]\tLoss Ss: 0.007519\n","\tRotated_Epoch:17 [002/005 (0460/0614)]\tLoss Ss: 0.005701\n","\tRotated_Epoch:17 [002/005 (0480/0614)]\tLoss Ss: 0.007897\n","\tRotated_Epoch:17 [002/005 (0500/0614)]\tLoss Ss: 0.009204\n","\tRotated_Epoch:17 [002/005 (0520/0614)]\tLoss Ss: 0.007900\n","\tRotated_Epoch:17 [002/005 (0540/0614)]\tLoss Ss: 0.005586\n","\tRotated_Epoch:17 [002/005 (0560/0614)]\tLoss Ss: 0.011981\n","\tRotated_Epoch:17 [002/005 (0580/0614)]\tLoss Ss: 0.004913\n","\tRotated_Epoch:17 [002/005 (0600/0614)]\tLoss Ss: 0.006439\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tRotated_Epoch:17 [003/005 (0000/0693)]\tLoss Ss: 0.025708\n","\tRotated_Epoch:17 [003/005 (0020/0693)]\tLoss Ss: 0.021502\n","\tRotated_Epoch:17 [003/005 (0040/0693)]\tLoss Ss: 0.029350\n","\tRotated_Epoch:17 [003/005 (0060/0693)]\tLoss Ss: 0.019229\n","\tRotated_Epoch:17 [003/005 (0080/0693)]\tLoss Ss: 0.019535\n","\tRotated_Epoch:17 [003/005 (0100/0693)]\tLoss Ss: 0.022195\n","\tRotated_Epoch:17 [003/005 (0120/0693)]\tLoss Ss: 0.015634\n","\tRotated_Epoch:17 [003/005 (0140/0693)]\tLoss Ss: 0.020391\n","\tRotated_Epoch:17 [003/005 (0160/0693)]\tLoss Ss: 0.014493\n","\tRotated_Epoch:17 [003/005 (0180/0693)]\tLoss Ss: 0.014887\n","\tRotated_Epoch:17 [003/005 (0200/0693)]\tLoss Ss: 0.017421\n","\tRotated_Epoch:17 [003/005 (0220/0693)]\tLoss Ss: 0.014597\n","\tRotated_Epoch:17 [003/005 (0240/0693)]\tLoss Ss: 0.013417\n","\tRotated_Epoch:17 [003/005 (0260/0693)]\tLoss Ss: 0.025556\n","\tRotated_Epoch:17 [003/005 (0280/0693)]\tLoss Ss: 0.019926\n","\tRotated_Epoch:17 [003/005 (0300/0693)]\tLoss Ss: 0.012197\n","\tRotated_Epoch:17 [003/005 (0320/0693)]\tLoss Ss: 0.010611\n","\tRotated_Epoch:17 [003/005 (0340/0693)]\tLoss Ss: 0.019032\n","\tRotated_Epoch:17 [003/005 (0360/0693)]\tLoss Ss: 0.019578\n","\tRotated_Epoch:17 [003/005 (0380/0693)]\tLoss Ss: 0.013338\n","\tRotated_Epoch:17 [003/005 (0400/0693)]\tLoss Ss: 0.012657\n","\tRotated_Epoch:17 [003/005 (0420/0693)]\tLoss Ss: 0.014124\n","\tRotated_Epoch:17 [003/005 (0440/0693)]\tLoss Ss: 0.015333\n","\tRotated_Epoch:17 [003/005 (0460/0693)]\tLoss Ss: 0.025213\n","\tRotated_Epoch:17 [003/005 (0480/0693)]\tLoss Ss: 0.019424\n","\tRotated_Epoch:17 [003/005 (0500/0693)]\tLoss Ss: 0.020590\n","\tRotated_Epoch:17 [003/005 (0520/0693)]\tLoss Ss: 0.015226\n","\tRotated_Epoch:17 [003/005 (0540/0693)]\tLoss Ss: 0.015270\n","\tRotated_Epoch:17 [003/005 (0560/0693)]\tLoss Ss: 0.021203\n","\tRotated_Epoch:17 [003/005 (0580/0693)]\tLoss Ss: 0.015848\n","\tRotated_Epoch:17 [003/005 (0600/0693)]\tLoss Ss: 0.020071\n","\tRotated_Epoch:17 [003/005 (0620/0693)]\tLoss Ss: 0.013630\n","\tRotated_Epoch:17 [003/005 (0640/0693)]\tLoss Ss: 0.012756\n","\tRotated_Epoch:17 [003/005 (0660/0693)]\tLoss Ss: 0.012250\n","\tRotated_Epoch:17 [003/005 (0680/0693)]\tLoss Ss: 0.015213\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tRotated_Epoch:17 [004/005 (0000/0588)]\tLoss Ss: 0.151194\n","\tRotated_Epoch:17 [004/005 (0020/0588)]\tLoss Ss: 0.226279\n","\tRotated_Epoch:17 [004/005 (0040/0588)]\tLoss Ss: 0.183941\n","\tRotated_Epoch:17 [004/005 (0060/0588)]\tLoss Ss: 0.124185\n","\tRotated_Epoch:17 [004/005 (0080/0588)]\tLoss Ss: 0.078277\n","\tRotated_Epoch:17 [004/005 (0100/0588)]\tLoss Ss: 0.058622\n","\tRotated_Epoch:17 [004/005 (0120/0588)]\tLoss Ss: 0.073709\n","\tRotated_Epoch:17 [004/005 (0140/0588)]\tLoss Ss: 0.066539\n","\tRotated_Epoch:17 [004/005 (0160/0588)]\tLoss Ss: 0.112879\n","\tRotated_Epoch:17 [004/005 (0180/0588)]\tLoss Ss: 0.062091\n","\tRotated_Epoch:17 [004/005 (0200/0588)]\tLoss Ss: 0.067936\n","\tRotated_Epoch:17 [004/005 (0220/0588)]\tLoss Ss: 0.061385\n","\tRotated_Epoch:17 [004/005 (0240/0588)]\tLoss Ss: 0.071521\n","\tRotated_Epoch:17 [004/005 (0260/0588)]\tLoss Ss: 0.061641\n","\tRotated_Epoch:17 [004/005 (0280/0588)]\tLoss Ss: 0.064160\n","\tRotated_Epoch:17 [004/005 (0300/0588)]\tLoss Ss: 0.070945\n","\tRotated_Epoch:17 [004/005 (0320/0588)]\tLoss Ss: 0.061543\n","\tRotated_Epoch:17 [004/005 (0340/0588)]\tLoss Ss: 0.078728\n","\tRotated_Epoch:17 [004/005 (0360/0588)]\tLoss Ss: 0.057959\n","\tRotated_Epoch:17 [004/005 (0380/0588)]\tLoss Ss: 0.063162\n","\tRotated_Epoch:17 [004/005 (0400/0588)]\tLoss Ss: 0.080751\n","\tRotated_Epoch:17 [004/005 (0420/0588)]\tLoss Ss: 0.063129\n","\tRotated_Epoch:17 [004/005 (0440/0588)]\tLoss Ss: 0.051477\n","\tRotated_Epoch:17 [004/005 (0460/0588)]\tLoss Ss: 0.065391\n","\tRotated_Epoch:17 [004/005 (0480/0588)]\tLoss Ss: 0.062947\n","\tRotated_Epoch:17 [004/005 (0500/0588)]\tLoss Ss: 0.066431\n","\tRotated_Epoch:17 [004/005 (0520/0588)]\tLoss Ss: 0.056491\n","\tRotated_Epoch:17 [004/005 (0540/0588)]\tLoss Ss: 0.061080\n","\tRotated_Epoch:17 [004/005 (0560/0588)]\tLoss Ss: 0.060119\n","\tRotated_Epoch:17 [004/005 (0580/0588)]\tLoss Ss: 0.052667\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:17 [005/005 (0000/0755)]\tLoss Ss: 0.050348\n","\tRotated_Epoch:17 [005/005 (0020/0755)]\tLoss Ss: 0.033363\n","\tRotated_Epoch:17 [005/005 (0040/0755)]\tLoss Ss: 0.081321\n","\tRotated_Epoch:17 [005/005 (0060/0755)]\tLoss Ss: 0.057407\n","\tRotated_Epoch:17 [005/005 (0080/0755)]\tLoss Ss: 0.061921\n","\tRotated_Epoch:17 [005/005 (0100/0755)]\tLoss Ss: 0.027556\n","\tRotated_Epoch:17 [005/005 (0120/0755)]\tLoss Ss: 0.046035\n","\tRotated_Epoch:17 [005/005 (0140/0755)]\tLoss Ss: 0.043734\n","\tRotated_Epoch:17 [005/005 (0160/0755)]\tLoss Ss: 0.030728\n","\tRotated_Epoch:17 [005/005 (0180/0755)]\tLoss Ss: 0.040087\n","\tRotated_Epoch:17 [005/005 (0200/0755)]\tLoss Ss: 0.046740\n","\tRotated_Epoch:17 [005/005 (0220/0755)]\tLoss Ss: 0.030191\n","\tRotated_Epoch:17 [005/005 (0240/0755)]\tLoss Ss: 0.023930\n","\tRotated_Epoch:17 [005/005 (0260/0755)]\tLoss Ss: 0.034624\n","\tRotated_Epoch:17 [005/005 (0280/0755)]\tLoss Ss: 0.027118\n","\tRotated_Epoch:17 [005/005 (0300/0755)]\tLoss Ss: 0.027259\n","\tRotated_Epoch:17 [005/005 (0320/0755)]\tLoss Ss: 0.022075\n","\tRotated_Epoch:17 [005/005 (0340/0755)]\tLoss Ss: 0.025355\n","\tRotated_Epoch:17 [005/005 (0360/0755)]\tLoss Ss: 0.027438\n","\tRotated_Epoch:17 [005/005 (0380/0755)]\tLoss Ss: 0.019490\n","\tRotated_Epoch:17 [005/005 (0400/0755)]\tLoss Ss: 0.031395\n","\tRotated_Epoch:17 [005/005 (0420/0755)]\tLoss Ss: 0.024729\n","\tRotated_Epoch:17 [005/005 (0440/0755)]\tLoss Ss: 0.021702\n","\tRotated_Epoch:17 [005/005 (0460/0755)]\tLoss Ss: 0.022332\n","\tRotated_Epoch:17 [005/005 (0480/0755)]\tLoss Ss: 0.020181\n","\tRotated_Epoch:17 [005/005 (0500/0755)]\tLoss Ss: 0.018266\n","\tRotated_Epoch:17 [005/005 (0520/0755)]\tLoss Ss: 0.014955\n","\tRotated_Epoch:17 [005/005 (0540/0755)]\tLoss Ss: 0.030177\n","\tRotated_Epoch:17 [005/005 (0560/0755)]\tLoss Ss: 0.021685\n","\tRotated_Epoch:17 [005/005 (0580/0755)]\tLoss Ss: 0.023000\n","\tRotated_Epoch:17 [005/005 (0600/0755)]\tLoss Ss: 0.013201\n","\tRotated_Epoch:17 [005/005 (0620/0755)]\tLoss Ss: 0.024105\n","\tRotated_Epoch:17 [005/005 (0640/0755)]\tLoss Ss: 0.022324\n","\tRotated_Epoch:17 [005/005 (0660/0755)]\tLoss Ss: 0.021528\n","\tRotated_Epoch:17 [005/005 (0680/0755)]\tLoss Ss: 0.022364\n","\tRotated_Epoch:17 [005/005 (0700/0755)]\tLoss Ss: 0.020902\n","\tRotated_Epoch:17 [005/005 (0720/0755)]\tLoss Ss: 0.015636\n","\tRotated_Epoch:17 [005/005 (0740/0755)]\tLoss Ss: 0.020943\n","new_1_6_d.nii\n","new_1_6_m.nii\n","new_1_7_d.nii\n","new_1_7_m.nii\n","new_2_5_d.nii\n","new_2_5_m.nii\n","new_2_6_d.nii\n","new_2_6_m.nii\n","new_2_7_d.nii\n","new_2_7_m.nii\n","new_3_d (2).nii\n","new_3_m (2).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (3).nii\n","new_3_m (3).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (7).nii\n","new_3_m (7).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (8).nii\n","new_3_m (8).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (9).nii\n","new_3_m (9).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_4_10_d_163526.nii\n","new_4_10_m_163526.nii.gz\n","new_4_11_d_084336.nii\n","new_4_11_m_084336.nii.gz\n","new_4_7_d_184422.nii\n","new_4_7_m_184422.nii.gz\n","new_4_8_d_143412.nii\n","new_4_8_m_143412.nii.gz\n","new_4_9_d_155347.nii\n","new_4_9_m_155347.nii.gz\n","\tEpoch: 17; Dice: 0.9296 +/- 0.0484; Loss: 12.0032\n","Begin Epoch 18\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tEpoch:18 [000/005 (0000/0588)]\tLoss Ss: 0.032450\n","\tEpoch:18 [000/005 (0020/0588)]\tLoss Ss: 0.037141\n","\tEpoch:18 [000/005 (0040/0588)]\tLoss Ss: 0.037899\n","\tEpoch:18 [000/005 (0060/0588)]\tLoss Ss: 0.028995\n","\tEpoch:18 [000/005 (0080/0588)]\tLoss Ss: 0.029412\n","\tEpoch:18 [000/005 (0100/0588)]\tLoss Ss: 0.019024\n","\tEpoch:18 [000/005 (0120/0588)]\tLoss Ss: 0.018836\n","\tEpoch:18 [000/005 (0140/0588)]\tLoss Ss: 0.016321\n","\tEpoch:18 [000/005 (0160/0588)]\tLoss Ss: 0.010161\n","\tEpoch:18 [000/005 (0180/0588)]\tLoss Ss: 0.011479\n","\tEpoch:18 [000/005 (0200/0588)]\tLoss Ss: 0.011136\n","\tEpoch:18 [000/005 (0220/0588)]\tLoss Ss: 0.012697\n","\tEpoch:18 [000/005 (0240/0588)]\tLoss Ss: 0.010420\n","\tEpoch:18 [000/005 (0260/0588)]\tLoss Ss: 0.012694\n","\tEpoch:18 [000/005 (0280/0588)]\tLoss Ss: 0.011869\n","\tEpoch:18 [000/005 (0300/0588)]\tLoss Ss: 0.010383\n","\tEpoch:18 [000/005 (0320/0588)]\tLoss Ss: 0.013659\n","\tEpoch:18 [000/005 (0340/0588)]\tLoss Ss: 0.009857\n","\tEpoch:18 [000/005 (0360/0588)]\tLoss Ss: 0.009446\n","\tEpoch:18 [000/005 (0380/0588)]\tLoss Ss: 0.011964\n","\tEpoch:18 [000/005 (0400/0588)]\tLoss Ss: 0.010747\n","\tEpoch:18 [000/005 (0420/0588)]\tLoss Ss: 0.014035\n","\tEpoch:18 [000/005 (0440/0588)]\tLoss Ss: 0.010549\n","\tEpoch:18 [000/005 (0460/0588)]\tLoss Ss: 0.006701\n","\tEpoch:18 [000/005 (0480/0588)]\tLoss Ss: 0.009846\n","\tEpoch:18 [000/005 (0500/0588)]\tLoss Ss: 0.010554\n","\tEpoch:18 [000/005 (0520/0588)]\tLoss Ss: 0.006585\n","\tEpoch:18 [000/005 (0540/0588)]\tLoss Ss: 0.008595\n","\tEpoch:18 [000/005 (0560/0588)]\tLoss Ss: 0.006509\n","\tEpoch:18 [000/005 (0580/0588)]\tLoss Ss: 0.012949\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:18 [001/005 (0000/0755)]\tLoss Ss: 0.016584\n","\tEpoch:18 [001/005 (0020/0755)]\tLoss Ss: 0.019670\n","\tEpoch:18 [001/005 (0040/0755)]\tLoss Ss: 0.026572\n","\tEpoch:18 [001/005 (0060/0755)]\tLoss Ss: 0.028617\n","\tEpoch:18 [001/005 (0080/0755)]\tLoss Ss: 0.026760\n","\tEpoch:18 [001/005 (0100/0755)]\tLoss Ss: 0.020492\n","\tEpoch:18 [001/005 (0120/0755)]\tLoss Ss: 0.018515\n","\tEpoch:18 [001/005 (0140/0755)]\tLoss Ss: 0.014863\n","\tEpoch:18 [001/005 (0160/0755)]\tLoss Ss: 0.012367\n","\tEpoch:18 [001/005 (0180/0755)]\tLoss Ss: 0.014518\n","\tEpoch:18 [001/005 (0200/0755)]\tLoss Ss: 0.021177\n","\tEpoch:18 [001/005 (0220/0755)]\tLoss Ss: 0.014939\n","\tEpoch:18 [001/005 (0240/0755)]\tLoss Ss: 0.021372\n","\tEpoch:18 [001/005 (0260/0755)]\tLoss Ss: 0.020631\n","\tEpoch:18 [001/005 (0280/0755)]\tLoss Ss: 0.012311\n","\tEpoch:18 [001/005 (0300/0755)]\tLoss Ss: 0.013311\n","\tEpoch:18 [001/005 (0320/0755)]\tLoss Ss: 0.015180\n","\tEpoch:18 [001/005 (0340/0755)]\tLoss Ss: 0.013835\n","\tEpoch:18 [001/005 (0360/0755)]\tLoss Ss: 0.014993\n","\tEpoch:18 [001/005 (0380/0755)]\tLoss Ss: 0.008739\n","\tEpoch:18 [001/005 (0400/0755)]\tLoss Ss: 0.025216\n","\tEpoch:18 [001/005 (0420/0755)]\tLoss Ss: 0.016238\n","\tEpoch:18 [001/005 (0440/0755)]\tLoss Ss: 0.021536\n","\tEpoch:18 [001/005 (0460/0755)]\tLoss Ss: 0.017342\n","\tEpoch:18 [001/005 (0480/0755)]\tLoss Ss: 0.012377\n","\tEpoch:18 [001/005 (0500/0755)]\tLoss Ss: 0.021310\n","\tEpoch:18 [001/005 (0520/0755)]\tLoss Ss: 0.025063\n","\tEpoch:18 [001/005 (0540/0755)]\tLoss Ss: 0.015956\n","\tEpoch:18 [001/005 (0560/0755)]\tLoss Ss: 0.016399\n","\tEpoch:18 [001/005 (0580/0755)]\tLoss Ss: 0.011778\n","\tEpoch:18 [001/005 (0600/0755)]\tLoss Ss: 0.019134\n","\tEpoch:18 [001/005 (0620/0755)]\tLoss Ss: 0.009155\n","\tEpoch:18 [001/005 (0640/0755)]\tLoss Ss: 0.016548\n","\tEpoch:18 [001/005 (0660/0755)]\tLoss Ss: 0.010953\n","\tEpoch:18 [001/005 (0680/0755)]\tLoss Ss: 0.011984\n","\tEpoch:18 [001/005 (0700/0755)]\tLoss Ss: 0.014420\n","\tEpoch:18 [001/005 (0720/0755)]\tLoss Ss: 0.014930\n","\tEpoch:18 [001/005 (0740/0755)]\tLoss Ss: 0.014763\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:18 [002/005 (0000/0755)]\tLoss Ss: 0.024482\n","\tEpoch:18 [002/005 (0020/0755)]\tLoss Ss: 0.024863\n","\tEpoch:18 [002/005 (0040/0755)]\tLoss Ss: 0.022925\n","\tEpoch:18 [002/005 (0060/0755)]\tLoss Ss: 0.019349\n","\tEpoch:18 [002/005 (0080/0755)]\tLoss Ss: 0.025262\n","\tEpoch:18 [002/005 (0100/0755)]\tLoss Ss: 0.028970\n","\tEpoch:18 [002/005 (0120/0755)]\tLoss Ss: 0.020924\n","\tEpoch:18 [002/005 (0140/0755)]\tLoss Ss: 0.019358\n","\tEpoch:18 [002/005 (0160/0755)]\tLoss Ss: 0.018748\n","\tEpoch:18 [002/005 (0180/0755)]\tLoss Ss: 0.021677\n","\tEpoch:18 [002/005 (0200/0755)]\tLoss Ss: 0.011085\n","\tEpoch:18 [002/005 (0220/0755)]\tLoss Ss: 0.027936\n","\tEpoch:18 [002/005 (0240/0755)]\tLoss Ss: 0.018533\n","\tEpoch:18 [002/005 (0260/0755)]\tLoss Ss: 0.018740\n","\tEpoch:18 [002/005 (0280/0755)]\tLoss Ss: 0.020307\n","\tEpoch:18 [002/005 (0300/0755)]\tLoss Ss: 0.016564\n","\tEpoch:18 [002/005 (0320/0755)]\tLoss Ss: 0.012345\n","\tEpoch:18 [002/005 (0340/0755)]\tLoss Ss: 0.022307\n","\tEpoch:18 [002/005 (0360/0755)]\tLoss Ss: 0.020239\n","\tEpoch:18 [002/005 (0380/0755)]\tLoss Ss: 0.015003\n","\tEpoch:18 [002/005 (0400/0755)]\tLoss Ss: 0.025073\n","\tEpoch:18 [002/005 (0420/0755)]\tLoss Ss: 0.017927\n","\tEpoch:18 [002/005 (0440/0755)]\tLoss Ss: 0.016140\n","\tEpoch:18 [002/005 (0460/0755)]\tLoss Ss: 0.012121\n","\tEpoch:18 [002/005 (0480/0755)]\tLoss Ss: 0.017710\n","\tEpoch:18 [002/005 (0500/0755)]\tLoss Ss: 0.023209\n","\tEpoch:18 [002/005 (0520/0755)]\tLoss Ss: 0.016410\n","\tEpoch:18 [002/005 (0540/0755)]\tLoss Ss: 0.012516\n","\tEpoch:18 [002/005 (0560/0755)]\tLoss Ss: 0.015797\n","\tEpoch:18 [002/005 (0580/0755)]\tLoss Ss: 0.013777\n","\tEpoch:18 [002/005 (0600/0755)]\tLoss Ss: 0.014463\n","\tEpoch:18 [002/005 (0620/0755)]\tLoss Ss: 0.018671\n","\tEpoch:18 [002/005 (0640/0755)]\tLoss Ss: 0.024118\n","\tEpoch:18 [002/005 (0660/0755)]\tLoss Ss: 0.014079\n","\tEpoch:18 [002/005 (0680/0755)]\tLoss Ss: 0.014033\n","\tEpoch:18 [002/005 (0700/0755)]\tLoss Ss: 0.014152\n","\tEpoch:18 [002/005 (0720/0755)]\tLoss Ss: 0.015825\n","\tEpoch:18 [002/005 (0740/0755)]\tLoss Ss: 0.014104\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tEpoch:18 [003/005 (0000/0693)]\tLoss Ss: 0.023928\n","\tEpoch:18 [003/005 (0020/0693)]\tLoss Ss: 0.030637\n","\tEpoch:18 [003/005 (0040/0693)]\tLoss Ss: 0.018618\n","\tEpoch:18 [003/005 (0060/0693)]\tLoss Ss: 0.022023\n","\tEpoch:18 [003/005 (0080/0693)]\tLoss Ss: 0.017337\n","\tEpoch:18 [003/005 (0100/0693)]\tLoss Ss: 0.022632\n","\tEpoch:18 [003/005 (0120/0693)]\tLoss Ss: 0.013965\n","\tEpoch:18 [003/005 (0140/0693)]\tLoss Ss: 0.014591\n","\tEpoch:18 [003/005 (0160/0693)]\tLoss Ss: 0.011656\n","\tEpoch:18 [003/005 (0180/0693)]\tLoss Ss: 0.020899\n","\tEpoch:18 [003/005 (0200/0693)]\tLoss Ss: 0.015972\n","\tEpoch:18 [003/005 (0220/0693)]\tLoss Ss: 0.016009\n","\tEpoch:18 [003/005 (0240/0693)]\tLoss Ss: 0.013326\n","\tEpoch:18 [003/005 (0260/0693)]\tLoss Ss: 0.014901\n","\tEpoch:18 [003/005 (0280/0693)]\tLoss Ss: 0.012125\n","\tEpoch:18 [003/005 (0300/0693)]\tLoss Ss: 0.015898\n","\tEpoch:18 [003/005 (0320/0693)]\tLoss Ss: 0.017058\n","\tEpoch:18 [003/005 (0340/0693)]\tLoss Ss: 0.017125\n","\tEpoch:18 [003/005 (0360/0693)]\tLoss Ss: 0.011910\n","\tEpoch:18 [003/005 (0380/0693)]\tLoss Ss: 0.017232\n","\tEpoch:18 [003/005 (0400/0693)]\tLoss Ss: 0.011814\n","\tEpoch:18 [003/005 (0420/0693)]\tLoss Ss: 0.018806\n","\tEpoch:18 [003/005 (0440/0693)]\tLoss Ss: 0.008800\n","\tEpoch:18 [003/005 (0460/0693)]\tLoss Ss: 0.020035\n","\tEpoch:18 [003/005 (0480/0693)]\tLoss Ss: 0.011868\n","\tEpoch:18 [003/005 (0500/0693)]\tLoss Ss: 0.012050\n","\tEpoch:18 [003/005 (0520/0693)]\tLoss Ss: 0.011452\n","\tEpoch:18 [003/005 (0540/0693)]\tLoss Ss: 0.010614\n","\tEpoch:18 [003/005 (0560/0693)]\tLoss Ss: 0.014258\n","\tEpoch:18 [003/005 (0580/0693)]\tLoss Ss: 0.015587\n","\tEpoch:18 [003/005 (0600/0693)]\tLoss Ss: 0.015384\n","\tEpoch:18 [003/005 (0620/0693)]\tLoss Ss: 0.011176\n","\tEpoch:18 [003/005 (0640/0693)]\tLoss Ss: 0.010904\n","\tEpoch:18 [003/005 (0660/0693)]\tLoss Ss: 0.008512\n","\tEpoch:18 [003/005 (0680/0693)]\tLoss Ss: 0.019906\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tEpoch:18 [004/005 (0000/0614)]\tLoss Ss: 0.010577\n","\tEpoch:18 [004/005 (0020/0614)]\tLoss Ss: 0.006661\n","\tEpoch:18 [004/005 (0040/0614)]\tLoss Ss: 0.009137\n","\tEpoch:18 [004/005 (0060/0614)]\tLoss Ss: 0.010901\n","\tEpoch:18 [004/005 (0080/0614)]\tLoss Ss: 0.011751\n","\tEpoch:18 [004/005 (0100/0614)]\tLoss Ss: 0.007427\n","\tEpoch:18 [004/005 (0120/0614)]\tLoss Ss: 0.005063\n","\tEpoch:18 [004/005 (0140/0614)]\tLoss Ss: 0.006394\n","\tEpoch:18 [004/005 (0160/0614)]\tLoss Ss: 0.004997\n","\tEpoch:18 [004/005 (0180/0614)]\tLoss Ss: 0.008578\n","\tEpoch:18 [004/005 (0200/0614)]\tLoss Ss: 0.007333\n","\tEpoch:18 [004/005 (0220/0614)]\tLoss Ss: 0.006007\n","\tEpoch:18 [004/005 (0240/0614)]\tLoss Ss: 0.008129\n","\tEpoch:18 [004/005 (0260/0614)]\tLoss Ss: 0.008116\n","\tEpoch:18 [004/005 (0280/0614)]\tLoss Ss: 0.004980\n","\tEpoch:18 [004/005 (0300/0614)]\tLoss Ss: 0.006297\n","\tEpoch:18 [004/005 (0320/0614)]\tLoss Ss: 0.006983\n","\tEpoch:18 [004/005 (0340/0614)]\tLoss Ss: 0.007658\n","\tEpoch:18 [004/005 (0360/0614)]\tLoss Ss: 0.009395\n","\tEpoch:18 [004/005 (0380/0614)]\tLoss Ss: 0.007927\n","\tEpoch:18 [004/005 (0400/0614)]\tLoss Ss: 0.006796\n","\tEpoch:18 [004/005 (0420/0614)]\tLoss Ss: 0.010725\n","\tEpoch:18 [004/005 (0440/0614)]\tLoss Ss: 0.004825\n","\tEpoch:18 [004/005 (0460/0614)]\tLoss Ss: 0.006840\n","\tEpoch:18 [004/005 (0480/0614)]\tLoss Ss: 0.010120\n","\tEpoch:18 [004/005 (0500/0614)]\tLoss Ss: 0.008253\n","\tEpoch:18 [004/005 (0520/0614)]\tLoss Ss: 0.007636\n","\tEpoch:18 [004/005 (0540/0614)]\tLoss Ss: 0.004844\n","\tEpoch:18 [004/005 (0560/0614)]\tLoss Ss: 0.004770\n","\tEpoch:18 [004/005 (0580/0614)]\tLoss Ss: 0.007171\n","\tEpoch:18 [004/005 (0600/0614)]\tLoss Ss: 0.007337\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tEpoch:18 [005/005 (0000/0693)]\tLoss Ss: 0.013917\n","\tEpoch:18 [005/005 (0020/0693)]\tLoss Ss: 0.020097\n","\tEpoch:18 [005/005 (0040/0693)]\tLoss Ss: 0.017949\n","\tEpoch:18 [005/005 (0060/0693)]\tLoss Ss: 0.012197\n","\tEpoch:18 [005/005 (0080/0693)]\tLoss Ss: 0.021323\n","\tEpoch:18 [005/005 (0100/0693)]\tLoss Ss: 0.015079\n","\tEpoch:18 [005/005 (0120/0693)]\tLoss Ss: 0.018416\n","\tEpoch:18 [005/005 (0140/0693)]\tLoss Ss: 0.021529\n","\tEpoch:18 [005/005 (0160/0693)]\tLoss Ss: 0.020643\n","\tEpoch:18 [005/005 (0180/0693)]\tLoss Ss: 0.010010\n","\tEpoch:18 [005/005 (0200/0693)]\tLoss Ss: 0.017550\n","\tEpoch:18 [005/005 (0220/0693)]\tLoss Ss: 0.012989\n","\tEpoch:18 [005/005 (0240/0693)]\tLoss Ss: 0.013959\n","\tEpoch:18 [005/005 (0260/0693)]\tLoss Ss: 0.019943\n","\tEpoch:18 [005/005 (0280/0693)]\tLoss Ss: 0.016631\n","\tEpoch:18 [005/005 (0300/0693)]\tLoss Ss: 0.012408\n","\tEpoch:18 [005/005 (0320/0693)]\tLoss Ss: 0.017634\n","\tEpoch:18 [005/005 (0340/0693)]\tLoss Ss: 0.010391\n","\tEpoch:18 [005/005 (0360/0693)]\tLoss Ss: 0.014113\n","\tEpoch:18 [005/005 (0380/0693)]\tLoss Ss: 0.013765\n","\tEpoch:18 [005/005 (0400/0693)]\tLoss Ss: 0.012459\n","\tEpoch:18 [005/005 (0420/0693)]\tLoss Ss: 0.020257\n","\tEpoch:18 [005/005 (0440/0693)]\tLoss Ss: 0.018244\n","\tEpoch:18 [005/005 (0460/0693)]\tLoss Ss: 0.014261\n","\tEpoch:18 [005/005 (0480/0693)]\tLoss Ss: 0.013577\n","\tEpoch:18 [005/005 (0500/0693)]\tLoss Ss: 0.017214\n","\tEpoch:18 [005/005 (0520/0693)]\tLoss Ss: 0.011297\n","\tEpoch:18 [005/005 (0540/0693)]\tLoss Ss: 0.012825\n","\tEpoch:18 [005/005 (0560/0693)]\tLoss Ss: 0.014528\n","\tEpoch:18 [005/005 (0580/0693)]\tLoss Ss: 0.021990\n","\tEpoch:18 [005/005 (0600/0693)]\tLoss Ss: 0.010486\n","\tEpoch:18 [005/005 (0620/0693)]\tLoss Ss: 0.015644\n","\tEpoch:18 [005/005 (0640/0693)]\tLoss Ss: 0.011911\n","\tEpoch:18 [005/005 (0660/0693)]\tLoss Ss: 0.012081\n","\tEpoch:18 [005/005 (0680/0693)]\tLoss Ss: 0.013180\n","Now train the rotated image\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tRotated_Epoch:18 [000/005 (0000/0614)]\tLoss Ss: 0.015887\n","\tRotated_Epoch:18 [000/005 (0020/0614)]\tLoss Ss: 0.018692\n","\tRotated_Epoch:18 [000/005 (0040/0614)]\tLoss Ss: 0.031722\n","\tRotated_Epoch:18 [000/005 (0060/0614)]\tLoss Ss: 0.011150\n","\tRotated_Epoch:18 [000/005 (0080/0614)]\tLoss Ss: 0.016232\n","\tRotated_Epoch:18 [000/005 (0100/0614)]\tLoss Ss: 0.013067\n","\tRotated_Epoch:18 [000/005 (0120/0614)]\tLoss Ss: 0.014984\n","\tRotated_Epoch:18 [000/005 (0140/0614)]\tLoss Ss: 0.007007\n","\tRotated_Epoch:18 [000/005 (0160/0614)]\tLoss Ss: 0.011222\n","\tRotated_Epoch:18 [000/005 (0180/0614)]\tLoss Ss: 0.010273\n","\tRotated_Epoch:18 [000/005 (0200/0614)]\tLoss Ss: 0.007406\n","\tRotated_Epoch:18 [000/005 (0220/0614)]\tLoss Ss: 0.011565\n","\tRotated_Epoch:18 [000/005 (0240/0614)]\tLoss Ss: 0.007102\n","\tRotated_Epoch:18 [000/005 (0260/0614)]\tLoss Ss: 0.007151\n","\tRotated_Epoch:18 [000/005 (0280/0614)]\tLoss Ss: 0.008204\n","\tRotated_Epoch:18 [000/005 (0300/0614)]\tLoss Ss: 0.008067\n","\tRotated_Epoch:18 [000/005 (0320/0614)]\tLoss Ss: 0.008073\n","\tRotated_Epoch:18 [000/005 (0340/0614)]\tLoss Ss: 0.011597\n","\tRotated_Epoch:18 [000/005 (0360/0614)]\tLoss Ss: 0.008949\n","\tRotated_Epoch:18 [000/005 (0380/0614)]\tLoss Ss: 0.008739\n","\tRotated_Epoch:18 [000/005 (0400/0614)]\tLoss Ss: 0.008910\n","\tRotated_Epoch:18 [000/005 (0420/0614)]\tLoss Ss: 0.007553\n","\tRotated_Epoch:18 [000/005 (0440/0614)]\tLoss Ss: 0.006912\n","\tRotated_Epoch:18 [000/005 (0460/0614)]\tLoss Ss: 0.009213\n","\tRotated_Epoch:18 [000/005 (0480/0614)]\tLoss Ss: 0.007336\n","\tRotated_Epoch:18 [000/005 (0500/0614)]\tLoss Ss: 0.005119\n","\tRotated_Epoch:18 [000/005 (0520/0614)]\tLoss Ss: 0.006948\n","\tRotated_Epoch:18 [000/005 (0540/0614)]\tLoss Ss: 0.008230\n","\tRotated_Epoch:18 [000/005 (0560/0614)]\tLoss Ss: 0.007557\n","\tRotated_Epoch:18 [000/005 (0580/0614)]\tLoss Ss: 0.008095\n","\tRotated_Epoch:18 [000/005 (0600/0614)]\tLoss Ss: 0.013447\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:18 [001/005 (0000/0755)]\tLoss Ss: 0.024149\n","\tRotated_Epoch:18 [001/005 (0020/0755)]\tLoss Ss: 0.111278\n","\tRotated_Epoch:18 [001/005 (0040/0755)]\tLoss Ss: 0.028195\n","\tRotated_Epoch:18 [001/005 (0060/0755)]\tLoss Ss: 0.024546\n","\tRotated_Epoch:18 [001/005 (0080/0755)]\tLoss Ss: 0.027863\n","\tRotated_Epoch:18 [001/005 (0100/0755)]\tLoss Ss: 0.028513\n","\tRotated_Epoch:18 [001/005 (0120/0755)]\tLoss Ss: 0.020015\n","\tRotated_Epoch:18 [001/005 (0140/0755)]\tLoss Ss: 0.019126\n","\tRotated_Epoch:18 [001/005 (0160/0755)]\tLoss Ss: 0.017155\n","\tRotated_Epoch:18 [001/005 (0180/0755)]\tLoss Ss: 0.020158\n","\tRotated_Epoch:18 [001/005 (0200/0755)]\tLoss Ss: 0.014739\n","\tRotated_Epoch:18 [001/005 (0220/0755)]\tLoss Ss: 0.026186\n","\tRotated_Epoch:18 [001/005 (0240/0755)]\tLoss Ss: 0.025068\n","\tRotated_Epoch:18 [001/005 (0260/0755)]\tLoss Ss: 0.016401\n","\tRotated_Epoch:18 [001/005 (0280/0755)]\tLoss Ss: 0.014001\n","\tRotated_Epoch:18 [001/005 (0300/0755)]\tLoss Ss: 0.017168\n","\tRotated_Epoch:18 [001/005 (0320/0755)]\tLoss Ss: 0.016198\n","\tRotated_Epoch:18 [001/005 (0340/0755)]\tLoss Ss: 0.019525\n","\tRotated_Epoch:18 [001/005 (0360/0755)]\tLoss Ss: 0.012961\n","\tRotated_Epoch:18 [001/005 (0380/0755)]\tLoss Ss: 0.018223\n","\tRotated_Epoch:18 [001/005 (0400/0755)]\tLoss Ss: 0.019441\n","\tRotated_Epoch:18 [001/005 (0420/0755)]\tLoss Ss: 0.024383\n","\tRotated_Epoch:18 [001/005 (0440/0755)]\tLoss Ss: 0.011558\n","\tRotated_Epoch:18 [001/005 (0460/0755)]\tLoss Ss: 0.013452\n","\tRotated_Epoch:18 [001/005 (0480/0755)]\tLoss Ss: 0.019125\n","\tRotated_Epoch:18 [001/005 (0500/0755)]\tLoss Ss: 0.022277\n","\tRotated_Epoch:18 [001/005 (0520/0755)]\tLoss Ss: 0.019867\n","\tRotated_Epoch:18 [001/005 (0540/0755)]\tLoss Ss: 0.017092\n","\tRotated_Epoch:18 [001/005 (0560/0755)]\tLoss Ss: 0.014459\n","\tRotated_Epoch:18 [001/005 (0580/0755)]\tLoss Ss: 0.015167\n","\tRotated_Epoch:18 [001/005 (0600/0755)]\tLoss Ss: 0.018828\n","\tRotated_Epoch:18 [001/005 (0620/0755)]\tLoss Ss: 0.016200\n","\tRotated_Epoch:18 [001/005 (0640/0755)]\tLoss Ss: 0.016802\n","\tRotated_Epoch:18 [001/005 (0660/0755)]\tLoss Ss: 0.019442\n","\tRotated_Epoch:18 [001/005 (0680/0755)]\tLoss Ss: 0.022035\n","\tRotated_Epoch:18 [001/005 (0700/0755)]\tLoss Ss: 0.019114\n","\tRotated_Epoch:18 [001/005 (0720/0755)]\tLoss Ss: 0.018053\n","\tRotated_Epoch:18 [001/005 (0740/0755)]\tLoss Ss: 0.011009\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tRotated_Epoch:18 [002/005 (0000/0693)]\tLoss Ss: 0.018770\n","\tRotated_Epoch:18 [002/005 (0020/0693)]\tLoss Ss: 0.016421\n","\tRotated_Epoch:18 [002/005 (0040/0693)]\tLoss Ss: 0.024851\n","\tRotated_Epoch:18 [002/005 (0060/0693)]\tLoss Ss: 0.024634\n","\tRotated_Epoch:18 [002/005 (0080/0693)]\tLoss Ss: 0.019278\n","\tRotated_Epoch:18 [002/005 (0100/0693)]\tLoss Ss: 0.018669\n","\tRotated_Epoch:18 [002/005 (0120/0693)]\tLoss Ss: 0.017546\n","\tRotated_Epoch:18 [002/005 (0140/0693)]\tLoss Ss: 0.016042\n","\tRotated_Epoch:18 [002/005 (0160/0693)]\tLoss Ss: 0.024742\n","\tRotated_Epoch:18 [002/005 (0180/0693)]\tLoss Ss: 0.014725\n","\tRotated_Epoch:18 [002/005 (0200/0693)]\tLoss Ss: 0.018027\n","\tRotated_Epoch:18 [002/005 (0220/0693)]\tLoss Ss: 0.013745\n","\tRotated_Epoch:18 [002/005 (0240/0693)]\tLoss Ss: 0.020751\n","\tRotated_Epoch:18 [002/005 (0260/0693)]\tLoss Ss: 0.017267\n","\tRotated_Epoch:18 [002/005 (0280/0693)]\tLoss Ss: 0.015318\n","\tRotated_Epoch:18 [002/005 (0300/0693)]\tLoss Ss: 0.017531\n","\tRotated_Epoch:18 [002/005 (0320/0693)]\tLoss Ss: 0.015205\n","\tRotated_Epoch:18 [002/005 (0340/0693)]\tLoss Ss: 0.017988\n","\tRotated_Epoch:18 [002/005 (0360/0693)]\tLoss Ss: 0.012718\n","\tRotated_Epoch:18 [002/005 (0380/0693)]\tLoss Ss: 0.011470\n","\tRotated_Epoch:18 [002/005 (0400/0693)]\tLoss Ss: 0.013764\n","\tRotated_Epoch:18 [002/005 (0420/0693)]\tLoss Ss: 0.017677\n","\tRotated_Epoch:18 [002/005 (0440/0693)]\tLoss Ss: 0.009364\n","\tRotated_Epoch:18 [002/005 (0460/0693)]\tLoss Ss: 0.017326\n","\tRotated_Epoch:18 [002/005 (0480/0693)]\tLoss Ss: 0.013036\n","\tRotated_Epoch:18 [002/005 (0500/0693)]\tLoss Ss: 0.013608\n","\tRotated_Epoch:18 [002/005 (0520/0693)]\tLoss Ss: 0.010682\n","\tRotated_Epoch:18 [002/005 (0540/0693)]\tLoss Ss: 0.015848\n","\tRotated_Epoch:18 [002/005 (0560/0693)]\tLoss Ss: 0.018888\n","\tRotated_Epoch:18 [002/005 (0580/0693)]\tLoss Ss: 0.012773\n","\tRotated_Epoch:18 [002/005 (0600/0693)]\tLoss Ss: 0.018036\n","\tRotated_Epoch:18 [002/005 (0620/0693)]\tLoss Ss: 0.009018\n","\tRotated_Epoch:18 [002/005 (0640/0693)]\tLoss Ss: 0.014715\n","\tRotated_Epoch:18 [002/005 (0660/0693)]\tLoss Ss: 0.012242\n","\tRotated_Epoch:18 [002/005 (0680/0693)]\tLoss Ss: 0.019673\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:18 [003/005 (0000/0755)]\tLoss Ss: 0.342649\n","\tRotated_Epoch:18 [003/005 (0020/0755)]\tLoss Ss: 0.284560\n","\tRotated_Epoch:18 [003/005 (0040/0755)]\tLoss Ss: 0.120357\n","\tRotated_Epoch:18 [003/005 (0060/0755)]\tLoss Ss: 0.192346\n","\tRotated_Epoch:18 [003/005 (0080/0755)]\tLoss Ss: 0.163408\n","\tRotated_Epoch:18 [003/005 (0100/0755)]\tLoss Ss: 0.163155\n","\tRotated_Epoch:18 [003/005 (0120/0755)]\tLoss Ss: 0.069286\n","\tRotated_Epoch:18 [003/005 (0140/0755)]\tLoss Ss: 0.069489\n","\tRotated_Epoch:18 [003/005 (0160/0755)]\tLoss Ss: 0.072575\n","\tRotated_Epoch:18 [003/005 (0180/0755)]\tLoss Ss: 0.086216\n","\tRotated_Epoch:18 [003/005 (0200/0755)]\tLoss Ss: 0.065464\n","\tRotated_Epoch:18 [003/005 (0220/0755)]\tLoss Ss: 0.062530\n","\tRotated_Epoch:18 [003/005 (0240/0755)]\tLoss Ss: 0.038629\n","\tRotated_Epoch:18 [003/005 (0260/0755)]\tLoss Ss: 0.047996\n","\tRotated_Epoch:18 [003/005 (0280/0755)]\tLoss Ss: 0.049090\n","\tRotated_Epoch:18 [003/005 (0300/0755)]\tLoss Ss: 0.090858\n","\tRotated_Epoch:18 [003/005 (0320/0755)]\tLoss Ss: 0.053913\n","\tRotated_Epoch:18 [003/005 (0340/0755)]\tLoss Ss: 0.071562\n","\tRotated_Epoch:18 [003/005 (0360/0755)]\tLoss Ss: 0.048822\n","\tRotated_Epoch:18 [003/005 (0380/0755)]\tLoss Ss: 0.071215\n","\tRotated_Epoch:18 [003/005 (0400/0755)]\tLoss Ss: 0.067719\n","\tRotated_Epoch:18 [003/005 (0420/0755)]\tLoss Ss: 0.079327\n","\tRotated_Epoch:18 [003/005 (0440/0755)]\tLoss Ss: 0.036974\n","\tRotated_Epoch:18 [003/005 (0460/0755)]\tLoss Ss: 0.048012\n","\tRotated_Epoch:18 [003/005 (0480/0755)]\tLoss Ss: 0.050242\n","\tRotated_Epoch:18 [003/005 (0500/0755)]\tLoss Ss: 0.053460\n","\tRotated_Epoch:18 [003/005 (0520/0755)]\tLoss Ss: 0.053350\n","\tRotated_Epoch:18 [003/005 (0540/0755)]\tLoss Ss: 0.064464\n","\tRotated_Epoch:18 [003/005 (0560/0755)]\tLoss Ss: 0.061648\n","\tRotated_Epoch:18 [003/005 (0580/0755)]\tLoss Ss: 0.048602\n","\tRotated_Epoch:18 [003/005 (0600/0755)]\tLoss Ss: 0.069263\n","\tRotated_Epoch:18 [003/005 (0620/0755)]\tLoss Ss: 0.050375\n","\tRotated_Epoch:18 [003/005 (0640/0755)]\tLoss Ss: 0.029060\n","\tRotated_Epoch:18 [003/005 (0660/0755)]\tLoss Ss: 0.046473\n","\tRotated_Epoch:18 [003/005 (0680/0755)]\tLoss Ss: 0.056372\n","\tRotated_Epoch:18 [003/005 (0700/0755)]\tLoss Ss: 0.041509\n","\tRotated_Epoch:18 [003/005 (0720/0755)]\tLoss Ss: 0.037537\n","\tRotated_Epoch:18 [003/005 (0740/0755)]\tLoss Ss: 0.070689\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tRotated_Epoch:18 [004/005 (0000/0588)]\tLoss Ss: 0.102471\n","\tRotated_Epoch:18 [004/005 (0020/0588)]\tLoss Ss: 0.091942\n","\tRotated_Epoch:18 [004/005 (0040/0588)]\tLoss Ss: 0.135013\n","\tRotated_Epoch:18 [004/005 (0060/0588)]\tLoss Ss: 0.175599\n","\tRotated_Epoch:18 [004/005 (0080/0588)]\tLoss Ss: 0.083978\n","\tRotated_Epoch:18 [004/005 (0100/0588)]\tLoss Ss: 0.113619\n","\tRotated_Epoch:18 [004/005 (0120/0588)]\tLoss Ss: 0.074601\n","\tRotated_Epoch:18 [004/005 (0140/0588)]\tLoss Ss: 0.088636\n","\tRotated_Epoch:18 [004/005 (0160/0588)]\tLoss Ss: 0.080086\n","\tRotated_Epoch:18 [004/005 (0180/0588)]\tLoss Ss: 0.085125\n","\tRotated_Epoch:18 [004/005 (0200/0588)]\tLoss Ss: 0.079709\n","\tRotated_Epoch:18 [004/005 (0220/0588)]\tLoss Ss: 0.062591\n","\tRotated_Epoch:18 [004/005 (0240/0588)]\tLoss Ss: 0.040993\n","\tRotated_Epoch:18 [004/005 (0260/0588)]\tLoss Ss: 0.058420\n","\tRotated_Epoch:18 [004/005 (0280/0588)]\tLoss Ss: 0.051165\n","\tRotated_Epoch:18 [004/005 (0300/0588)]\tLoss Ss: 0.063034\n","\tRotated_Epoch:18 [004/005 (0320/0588)]\tLoss Ss: 0.063039\n","\tRotated_Epoch:18 [004/005 (0340/0588)]\tLoss Ss: 0.067080\n","\tRotated_Epoch:18 [004/005 (0360/0588)]\tLoss Ss: 0.067413\n","\tRotated_Epoch:18 [004/005 (0380/0588)]\tLoss Ss: 0.062560\n","\tRotated_Epoch:18 [004/005 (0400/0588)]\tLoss Ss: 0.051085\n","\tRotated_Epoch:18 [004/005 (0420/0588)]\tLoss Ss: 0.058031\n","\tRotated_Epoch:18 [004/005 (0440/0588)]\tLoss Ss: 0.060080\n","\tRotated_Epoch:18 [004/005 (0460/0588)]\tLoss Ss: 0.061966\n","\tRotated_Epoch:18 [004/005 (0480/0588)]\tLoss Ss: 0.068539\n","\tRotated_Epoch:18 [004/005 (0500/0588)]\tLoss Ss: 0.062215\n","\tRotated_Epoch:18 [004/005 (0520/0588)]\tLoss Ss: 0.080138\n","\tRotated_Epoch:18 [004/005 (0540/0588)]\tLoss Ss: 0.064483\n","\tRotated_Epoch:18 [004/005 (0560/0588)]\tLoss Ss: 0.056883\n","\tRotated_Epoch:18 [004/005 (0580/0588)]\tLoss Ss: 0.053355\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tRotated_Epoch:18 [005/005 (0000/0693)]\tLoss Ss: 0.057441\n","\tRotated_Epoch:18 [005/005 (0020/0693)]\tLoss Ss: 0.048128\n","\tRotated_Epoch:18 [005/005 (0040/0693)]\tLoss Ss: 0.048146\n","\tRotated_Epoch:18 [005/005 (0060/0693)]\tLoss Ss: 0.032247\n","\tRotated_Epoch:18 [005/005 (0080/0693)]\tLoss Ss: 0.030423\n","\tRotated_Epoch:18 [005/005 (0100/0693)]\tLoss Ss: 0.020313\n","\tRotated_Epoch:18 [005/005 (0120/0693)]\tLoss Ss: 0.032300\n","\tRotated_Epoch:18 [005/005 (0140/0693)]\tLoss Ss: 0.030736\n","\tRotated_Epoch:18 [005/005 (0160/0693)]\tLoss Ss: 0.020200\n","\tRotated_Epoch:18 [005/005 (0180/0693)]\tLoss Ss: 0.020615\n","\tRotated_Epoch:18 [005/005 (0200/0693)]\tLoss Ss: 0.020480\n","\tRotated_Epoch:18 [005/005 (0220/0693)]\tLoss Ss: 0.025240\n","\tRotated_Epoch:18 [005/005 (0240/0693)]\tLoss Ss: 0.019698\n","\tRotated_Epoch:18 [005/005 (0260/0693)]\tLoss Ss: 0.016464\n","\tRotated_Epoch:18 [005/005 (0280/0693)]\tLoss Ss: 0.021219\n","\tRotated_Epoch:18 [005/005 (0300/0693)]\tLoss Ss: 0.016014\n","\tRotated_Epoch:18 [005/005 (0320/0693)]\tLoss Ss: 0.013310\n","\tRotated_Epoch:18 [005/005 (0340/0693)]\tLoss Ss: 0.021731\n","\tRotated_Epoch:18 [005/005 (0360/0693)]\tLoss Ss: 0.016249\n","\tRotated_Epoch:18 [005/005 (0380/0693)]\tLoss Ss: 0.018520\n","\tRotated_Epoch:18 [005/005 (0400/0693)]\tLoss Ss: 0.015142\n","\tRotated_Epoch:18 [005/005 (0420/0693)]\tLoss Ss: 0.019602\n","\tRotated_Epoch:18 [005/005 (0440/0693)]\tLoss Ss: 0.014943\n","\tRotated_Epoch:18 [005/005 (0460/0693)]\tLoss Ss: 0.020225\n","\tRotated_Epoch:18 [005/005 (0480/0693)]\tLoss Ss: 0.016340\n","\tRotated_Epoch:18 [005/005 (0500/0693)]\tLoss Ss: 0.017254\n","\tRotated_Epoch:18 [005/005 (0520/0693)]\tLoss Ss: 0.019101\n","\tRotated_Epoch:18 [005/005 (0540/0693)]\tLoss Ss: 0.018089\n","\tRotated_Epoch:18 [005/005 (0560/0693)]\tLoss Ss: 0.017423\n","\tRotated_Epoch:18 [005/005 (0580/0693)]\tLoss Ss: 0.014296\n","\tRotated_Epoch:18 [005/005 (0600/0693)]\tLoss Ss: 0.016747\n","\tRotated_Epoch:18 [005/005 (0620/0693)]\tLoss Ss: 0.015986\n","\tRotated_Epoch:18 [005/005 (0640/0693)]\tLoss Ss: 0.016161\n","\tRotated_Epoch:18 [005/005 (0660/0693)]\tLoss Ss: 0.019008\n","\tRotated_Epoch:18 [005/005 (0680/0693)]\tLoss Ss: 0.017841\n","new_1_6_d.nii\n","new_1_6_m.nii\n","new_1_7_d.nii\n","new_1_7_m.nii\n","new_2_5_d.nii\n","new_2_5_m.nii\n","new_2_6_d.nii\n","new_2_6_m.nii\n","new_2_7_d.nii\n","new_2_7_m.nii\n","new_3_d (2).nii\n","new_3_m (2).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (3).nii\n","new_3_m (3).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (7).nii\n","new_3_m (7).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (8).nii\n","new_3_m (8).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (9).nii\n","new_3_m (9).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_4_10_d_163526.nii\n","new_4_10_m_163526.nii.gz\n","new_4_11_d_084336.nii\n","new_4_11_m_084336.nii.gz\n","new_4_7_d_184422.nii\n","new_4_7_m_184422.nii.gz\n","new_4_8_d_143412.nii\n","new_4_8_m_143412.nii.gz\n","new_4_9_d_155347.nii\n","new_4_9_m_155347.nii.gz\n","\tEpoch: 18; Dice: 0.9242 +/- 0.0217; Loss: 11.0355\n","Begin Epoch 19\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:19 [000/005 (0000/0755)]\tLoss Ss: 0.052499\n","\tEpoch:19 [000/005 (0020/0755)]\tLoss Ss: 0.055252\n","\tEpoch:19 [000/005 (0040/0755)]\tLoss Ss: 0.036437\n","\tEpoch:19 [000/005 (0060/0755)]\tLoss Ss: 0.029959\n","\tEpoch:19 [000/005 (0080/0755)]\tLoss Ss: 0.024598\n","\tEpoch:19 [000/005 (0100/0755)]\tLoss Ss: 0.024355\n","\tEpoch:19 [000/005 (0120/0755)]\tLoss Ss: 0.019936\n","\tEpoch:19 [000/005 (0140/0755)]\tLoss Ss: 0.018546\n","\tEpoch:19 [000/005 (0160/0755)]\tLoss Ss: 0.027658\n","\tEpoch:19 [000/005 (0180/0755)]\tLoss Ss: 0.025594\n","\tEpoch:19 [000/005 (0200/0755)]\tLoss Ss: 0.015831\n","\tEpoch:19 [000/005 (0220/0755)]\tLoss Ss: 0.016690\n","\tEpoch:19 [000/005 (0240/0755)]\tLoss Ss: 0.022958\n","\tEpoch:19 [000/005 (0260/0755)]\tLoss Ss: 0.021495\n","\tEpoch:19 [000/005 (0280/0755)]\tLoss Ss: 0.022274\n","\tEpoch:19 [000/005 (0300/0755)]\tLoss Ss: 0.033136\n","\tEpoch:19 [000/005 (0320/0755)]\tLoss Ss: 0.020169\n","\tEpoch:19 [000/005 (0340/0755)]\tLoss Ss: 0.023658\n","\tEpoch:19 [000/005 (0360/0755)]\tLoss Ss: 0.017983\n","\tEpoch:19 [000/005 (0380/0755)]\tLoss Ss: 0.023546\n","\tEpoch:19 [000/005 (0400/0755)]\tLoss Ss: 0.023511\n","\tEpoch:19 [000/005 (0420/0755)]\tLoss Ss: 0.025882\n","\tEpoch:19 [000/005 (0440/0755)]\tLoss Ss: 0.019365\n","\tEpoch:19 [000/005 (0460/0755)]\tLoss Ss: 0.024180\n","\tEpoch:19 [000/005 (0480/0755)]\tLoss Ss: 0.009013\n","\tEpoch:19 [000/005 (0500/0755)]\tLoss Ss: 0.013009\n","\tEpoch:19 [000/005 (0520/0755)]\tLoss Ss: 0.013069\n","\tEpoch:19 [000/005 (0540/0755)]\tLoss Ss: 0.016954\n","\tEpoch:19 [000/005 (0560/0755)]\tLoss Ss: 0.018091\n","\tEpoch:19 [000/005 (0580/0755)]\tLoss Ss: 0.020497\n","\tEpoch:19 [000/005 (0600/0755)]\tLoss Ss: 0.015894\n","\tEpoch:19 [000/005 (0620/0755)]\tLoss Ss: 0.017310\n","\tEpoch:19 [000/005 (0640/0755)]\tLoss Ss: 0.015397\n","\tEpoch:19 [000/005 (0660/0755)]\tLoss Ss: 0.025810\n","\tEpoch:19 [000/005 (0680/0755)]\tLoss Ss: 0.012109\n","\tEpoch:19 [000/005 (0700/0755)]\tLoss Ss: 0.016097\n","\tEpoch:19 [000/005 (0720/0755)]\tLoss Ss: 0.017459\n","\tEpoch:19 [000/005 (0740/0755)]\tLoss Ss: 0.020313\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tEpoch:19 [001/005 (0000/0588)]\tLoss Ss: 0.006999\n","\tEpoch:19 [001/005 (0020/0588)]\tLoss Ss: 0.009177\n","\tEpoch:19 [001/005 (0040/0588)]\tLoss Ss: 0.010714\n","\tEpoch:19 [001/005 (0060/0588)]\tLoss Ss: 0.013128\n","\tEpoch:19 [001/005 (0080/0588)]\tLoss Ss: 0.010439\n","\tEpoch:19 [001/005 (0100/0588)]\tLoss Ss: 0.011564\n","\tEpoch:19 [001/005 (0120/0588)]\tLoss Ss: 0.007130\n","\tEpoch:19 [001/005 (0140/0588)]\tLoss Ss: 0.007169\n","\tEpoch:19 [001/005 (0160/0588)]\tLoss Ss: 0.005900\n","\tEpoch:19 [001/005 (0180/0588)]\tLoss Ss: 0.010127\n","\tEpoch:19 [001/005 (0200/0588)]\tLoss Ss: 0.009449\n","\tEpoch:19 [001/005 (0220/0588)]\tLoss Ss: 0.007127\n","\tEpoch:19 [001/005 (0240/0588)]\tLoss Ss: 0.008417\n","\tEpoch:19 [001/005 (0260/0588)]\tLoss Ss: 0.006906\n","\tEpoch:19 [001/005 (0280/0588)]\tLoss Ss: 0.007245\n","\tEpoch:19 [001/005 (0300/0588)]\tLoss Ss: 0.006672\n","\tEpoch:19 [001/005 (0320/0588)]\tLoss Ss: 0.005179\n","\tEpoch:19 [001/005 (0340/0588)]\tLoss Ss: 0.008772\n","\tEpoch:19 [001/005 (0360/0588)]\tLoss Ss: 0.004166\n","\tEpoch:19 [001/005 (0380/0588)]\tLoss Ss: 0.005932\n","\tEpoch:19 [001/005 (0400/0588)]\tLoss Ss: 0.007846\n","\tEpoch:19 [001/005 (0420/0588)]\tLoss Ss: 0.004539\n","\tEpoch:19 [001/005 (0440/0588)]\tLoss Ss: 0.005032\n","\tEpoch:19 [001/005 (0460/0588)]\tLoss Ss: 0.006423\n","\tEpoch:19 [001/005 (0480/0588)]\tLoss Ss: 0.007963\n","\tEpoch:19 [001/005 (0500/0588)]\tLoss Ss: 0.006784\n","\tEpoch:19 [001/005 (0520/0588)]\tLoss Ss: 0.006246\n","\tEpoch:19 [001/005 (0540/0588)]\tLoss Ss: 0.005196\n","\tEpoch:19 [001/005 (0560/0588)]\tLoss Ss: 0.009369\n","\tEpoch:19 [001/005 (0580/0588)]\tLoss Ss: 0.004153\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tEpoch:19 [002/005 (0000/0693)]\tLoss Ss: 0.024102\n","\tEpoch:19 [002/005 (0020/0693)]\tLoss Ss: 0.016039\n","\tEpoch:19 [002/005 (0040/0693)]\tLoss Ss: 0.011593\n","\tEpoch:19 [002/005 (0060/0693)]\tLoss Ss: 0.020517\n","\tEpoch:19 [002/005 (0080/0693)]\tLoss Ss: 0.011505\n","\tEpoch:19 [002/005 (0100/0693)]\tLoss Ss: 0.012789\n","\tEpoch:19 [002/005 (0120/0693)]\tLoss Ss: 0.012449\n","\tEpoch:19 [002/005 (0140/0693)]\tLoss Ss: 0.009431\n","\tEpoch:19 [002/005 (0160/0693)]\tLoss Ss: 0.016611\n","\tEpoch:19 [002/005 (0180/0693)]\tLoss Ss: 0.010094\n","\tEpoch:19 [002/005 (0200/0693)]\tLoss Ss: 0.014386\n","\tEpoch:19 [002/005 (0220/0693)]\tLoss Ss: 0.011158\n","\tEpoch:19 [002/005 (0240/0693)]\tLoss Ss: 0.013852\n","\tEpoch:19 [002/005 (0260/0693)]\tLoss Ss: 0.014153\n","\tEpoch:19 [002/005 (0280/0693)]\tLoss Ss: 0.015184\n","\tEpoch:19 [002/005 (0300/0693)]\tLoss Ss: 0.013309\n","\tEpoch:19 [002/005 (0320/0693)]\tLoss Ss: 0.014139\n","\tEpoch:19 [002/005 (0340/0693)]\tLoss Ss: 0.015157\n","\tEpoch:19 [002/005 (0360/0693)]\tLoss Ss: 0.013701\n","\tEpoch:19 [002/005 (0380/0693)]\tLoss Ss: 0.013303\n","\tEpoch:19 [002/005 (0400/0693)]\tLoss Ss: 0.012077\n","\tEpoch:19 [002/005 (0420/0693)]\tLoss Ss: 0.021682\n","\tEpoch:19 [002/005 (0440/0693)]\tLoss Ss: 0.011909\n","\tEpoch:19 [002/005 (0460/0693)]\tLoss Ss: 0.018922\n","\tEpoch:19 [002/005 (0480/0693)]\tLoss Ss: 0.014796\n","\tEpoch:19 [002/005 (0500/0693)]\tLoss Ss: 0.008541\n","\tEpoch:19 [002/005 (0520/0693)]\tLoss Ss: 0.012430\n","\tEpoch:19 [002/005 (0540/0693)]\tLoss Ss: 0.010782\n","\tEpoch:19 [002/005 (0560/0693)]\tLoss Ss: 0.013950\n","\tEpoch:19 [002/005 (0580/0693)]\tLoss Ss: 0.013260\n","\tEpoch:19 [002/005 (0600/0693)]\tLoss Ss: 0.009317\n","\tEpoch:19 [002/005 (0620/0693)]\tLoss Ss: 0.014588\n","\tEpoch:19 [002/005 (0640/0693)]\tLoss Ss: 0.011766\n","\tEpoch:19 [002/005 (0660/0693)]\tLoss Ss: 0.017122\n","\tEpoch:19 [002/005 (0680/0693)]\tLoss Ss: 0.012799\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tEpoch:19 [003/005 (0000/0693)]\tLoss Ss: 0.017115\n","\tEpoch:19 [003/005 (0020/0693)]\tLoss Ss: 0.020128\n","\tEpoch:19 [003/005 (0040/0693)]\tLoss Ss: 0.019570\n","\tEpoch:19 [003/005 (0060/0693)]\tLoss Ss: 0.013516\n","\tEpoch:19 [003/005 (0080/0693)]\tLoss Ss: 0.020641\n","\tEpoch:19 [003/005 (0100/0693)]\tLoss Ss: 0.012610\n","\tEpoch:19 [003/005 (0120/0693)]\tLoss Ss: 0.013378\n","\tEpoch:19 [003/005 (0140/0693)]\tLoss Ss: 0.018810\n","\tEpoch:19 [003/005 (0160/0693)]\tLoss Ss: 0.013759\n","\tEpoch:19 [003/005 (0180/0693)]\tLoss Ss: 0.013442\n","\tEpoch:19 [003/005 (0200/0693)]\tLoss Ss: 0.015954\n","\tEpoch:19 [003/005 (0220/0693)]\tLoss Ss: 0.014915\n","\tEpoch:19 [003/005 (0240/0693)]\tLoss Ss: 0.012128\n","\tEpoch:19 [003/005 (0260/0693)]\tLoss Ss: 0.011245\n","\tEpoch:19 [003/005 (0280/0693)]\tLoss Ss: 0.017230\n","\tEpoch:19 [003/005 (0300/0693)]\tLoss Ss: 0.011122\n","\tEpoch:19 [003/005 (0320/0693)]\tLoss Ss: 0.010067\n","\tEpoch:19 [003/005 (0340/0693)]\tLoss Ss: 0.015879\n","\tEpoch:19 [003/005 (0360/0693)]\tLoss Ss: 0.017822\n","\tEpoch:19 [003/005 (0380/0693)]\tLoss Ss: 0.020834\n","\tEpoch:19 [003/005 (0400/0693)]\tLoss Ss: 0.013472\n","\tEpoch:19 [003/005 (0420/0693)]\tLoss Ss: 0.012228\n","\tEpoch:19 [003/005 (0440/0693)]\tLoss Ss: 0.014265\n","\tEpoch:19 [003/005 (0460/0693)]\tLoss Ss: 0.014138\n","\tEpoch:19 [003/005 (0480/0693)]\tLoss Ss: 0.011387\n","\tEpoch:19 [003/005 (0500/0693)]\tLoss Ss: 0.008744\n","\tEpoch:19 [003/005 (0520/0693)]\tLoss Ss: 0.015923\n","\tEpoch:19 [003/005 (0540/0693)]\tLoss Ss: 0.013722\n","\tEpoch:19 [003/005 (0560/0693)]\tLoss Ss: 0.019093\n","\tEpoch:19 [003/005 (0580/0693)]\tLoss Ss: 0.017120\n","\tEpoch:19 [003/005 (0600/0693)]\tLoss Ss: 0.022768\n","\tEpoch:19 [003/005 (0620/0693)]\tLoss Ss: 0.014463\n","\tEpoch:19 [003/005 (0640/0693)]\tLoss Ss: 0.015797\n","\tEpoch:19 [003/005 (0660/0693)]\tLoss Ss: 0.017098\n","\tEpoch:19 [003/005 (0680/0693)]\tLoss Ss: 0.012924\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tEpoch:19 [004/005 (0000/0614)]\tLoss Ss: 0.005744\n","\tEpoch:19 [004/005 (0020/0614)]\tLoss Ss: 0.006130\n","\tEpoch:19 [004/005 (0040/0614)]\tLoss Ss: 0.008624\n","\tEpoch:19 [004/005 (0060/0614)]\tLoss Ss: 0.007459\n","\tEpoch:19 [004/005 (0080/0614)]\tLoss Ss: 0.007560\n","\tEpoch:19 [004/005 (0100/0614)]\tLoss Ss: 0.006718\n","\tEpoch:19 [004/005 (0120/0614)]\tLoss Ss: 0.007054\n","\tEpoch:19 [004/005 (0140/0614)]\tLoss Ss: 0.006983\n","\tEpoch:19 [004/005 (0160/0614)]\tLoss Ss: 0.005229\n","\tEpoch:19 [004/005 (0180/0614)]\tLoss Ss: 0.005224\n","\tEpoch:19 [004/005 (0200/0614)]\tLoss Ss: 0.004993\n","\tEpoch:19 [004/005 (0220/0614)]\tLoss Ss: 0.008261\n","\tEpoch:19 [004/005 (0240/0614)]\tLoss Ss: 0.006167\n","\tEpoch:19 [004/005 (0260/0614)]\tLoss Ss: 0.007065\n","\tEpoch:19 [004/005 (0280/0614)]\tLoss Ss: 0.006765\n","\tEpoch:19 [004/005 (0300/0614)]\tLoss Ss: 0.007760\n","\tEpoch:19 [004/005 (0320/0614)]\tLoss Ss: 0.005715\n","\tEpoch:19 [004/005 (0340/0614)]\tLoss Ss: 0.007607\n","\tEpoch:19 [004/005 (0360/0614)]\tLoss Ss: 0.006883\n","\tEpoch:19 [004/005 (0380/0614)]\tLoss Ss: 0.007274\n","\tEpoch:19 [004/005 (0400/0614)]\tLoss Ss: 0.005683\n","\tEpoch:19 [004/005 (0420/0614)]\tLoss Ss: 0.005560\n","\tEpoch:19 [004/005 (0440/0614)]\tLoss Ss: 0.007652\n","\tEpoch:19 [004/005 (0460/0614)]\tLoss Ss: 0.004495\n","\tEpoch:19 [004/005 (0480/0614)]\tLoss Ss: 0.006563\n","\tEpoch:19 [004/005 (0500/0614)]\tLoss Ss: 0.007013\n","\tEpoch:19 [004/005 (0520/0614)]\tLoss Ss: 0.004431\n","\tEpoch:19 [004/005 (0540/0614)]\tLoss Ss: 0.004142\n","\tEpoch:19 [004/005 (0560/0614)]\tLoss Ss: 0.006609\n","\tEpoch:19 [004/005 (0580/0614)]\tLoss Ss: 0.006167\n","\tEpoch:19 [004/005 (0600/0614)]\tLoss Ss: 0.005201\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:19 [005/005 (0000/0755)]\tLoss Ss: 0.016969\n","\tEpoch:19 [005/005 (0020/0755)]\tLoss Ss: 0.018160\n","\tEpoch:19 [005/005 (0040/0755)]\tLoss Ss: 0.022365\n","\tEpoch:19 [005/005 (0060/0755)]\tLoss Ss: 0.020883\n","\tEpoch:19 [005/005 (0080/0755)]\tLoss Ss: 0.028355\n","\tEpoch:19 [005/005 (0100/0755)]\tLoss Ss: 0.022127\n","\tEpoch:19 [005/005 (0120/0755)]\tLoss Ss: 0.012416\n","\tEpoch:19 [005/005 (0140/0755)]\tLoss Ss: 0.023519\n","\tEpoch:19 [005/005 (0160/0755)]\tLoss Ss: 0.030719\n","\tEpoch:19 [005/005 (0180/0755)]\tLoss Ss: 0.019887\n","\tEpoch:19 [005/005 (0200/0755)]\tLoss Ss: 0.020296\n","\tEpoch:19 [005/005 (0220/0755)]\tLoss Ss: 0.021537\n","\tEpoch:19 [005/005 (0240/0755)]\tLoss Ss: 0.010331\n","\tEpoch:19 [005/005 (0260/0755)]\tLoss Ss: 0.023157\n","\tEpoch:19 [005/005 (0280/0755)]\tLoss Ss: 0.019185\n","\tEpoch:19 [005/005 (0300/0755)]\tLoss Ss: 0.023030\n","\tEpoch:19 [005/005 (0320/0755)]\tLoss Ss: 0.020157\n","\tEpoch:19 [005/005 (0340/0755)]\tLoss Ss: 0.014532\n","\tEpoch:19 [005/005 (0360/0755)]\tLoss Ss: 0.015103\n","\tEpoch:19 [005/005 (0380/0755)]\tLoss Ss: 0.019437\n","\tEpoch:19 [005/005 (0400/0755)]\tLoss Ss: 0.016287\n","\tEpoch:19 [005/005 (0420/0755)]\tLoss Ss: 0.014088\n","\tEpoch:19 [005/005 (0440/0755)]\tLoss Ss: 0.011253\n","\tEpoch:19 [005/005 (0460/0755)]\tLoss Ss: 0.014398\n","\tEpoch:19 [005/005 (0480/0755)]\tLoss Ss: 0.018414\n","\tEpoch:19 [005/005 (0500/0755)]\tLoss Ss: 0.014580\n","\tEpoch:19 [005/005 (0520/0755)]\tLoss Ss: 0.017813\n","\tEpoch:19 [005/005 (0540/0755)]\tLoss Ss: 0.024208\n","\tEpoch:19 [005/005 (0560/0755)]\tLoss Ss: 0.015142\n","\tEpoch:19 [005/005 (0580/0755)]\tLoss Ss: 0.020141\n","\tEpoch:19 [005/005 (0600/0755)]\tLoss Ss: 0.012890\n","\tEpoch:19 [005/005 (0620/0755)]\tLoss Ss: 0.018136\n","\tEpoch:19 [005/005 (0640/0755)]\tLoss Ss: 0.010231\n","\tEpoch:19 [005/005 (0660/0755)]\tLoss Ss: 0.013159\n","\tEpoch:19 [005/005 (0680/0755)]\tLoss Ss: 0.021512\n","\tEpoch:19 [005/005 (0700/0755)]\tLoss Ss: 0.013825\n","\tEpoch:19 [005/005 (0720/0755)]\tLoss Ss: 0.011195\n","\tEpoch:19 [005/005 (0740/0755)]\tLoss Ss: 0.013191\n","Now train the rotated image\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:19 [000/005 (0000/0755)]\tLoss Ss: 0.178263\n","\tRotated_Epoch:19 [000/005 (0020/0755)]\tLoss Ss: 0.092410\n","\tRotated_Epoch:19 [000/005 (0040/0755)]\tLoss Ss: 0.229192\n","\tRotated_Epoch:19 [000/005 (0060/0755)]\tLoss Ss: 0.173429\n","\tRotated_Epoch:19 [000/005 (0080/0755)]\tLoss Ss: 0.203887\n","\tRotated_Epoch:19 [000/005 (0100/0755)]\tLoss Ss: 0.159311\n","\tRotated_Epoch:19 [000/005 (0120/0755)]\tLoss Ss: 0.085094\n","\tRotated_Epoch:19 [000/005 (0140/0755)]\tLoss Ss: 0.092279\n","\tRotated_Epoch:19 [000/005 (0160/0755)]\tLoss Ss: 0.091616\n","\tRotated_Epoch:19 [000/005 (0180/0755)]\tLoss Ss: 0.068366\n","\tRotated_Epoch:19 [000/005 (0200/0755)]\tLoss Ss: 0.072574\n","\tRotated_Epoch:19 [000/005 (0220/0755)]\tLoss Ss: 0.057358\n","\tRotated_Epoch:19 [000/005 (0240/0755)]\tLoss Ss: 0.078512\n","\tRotated_Epoch:19 [000/005 (0260/0755)]\tLoss Ss: 0.092083\n","\tRotated_Epoch:19 [000/005 (0280/0755)]\tLoss Ss: 0.084008\n","\tRotated_Epoch:19 [000/005 (0300/0755)]\tLoss Ss: 0.054762\n","\tRotated_Epoch:19 [000/005 (0320/0755)]\tLoss Ss: 0.073929\n","\tRotated_Epoch:19 [000/005 (0340/0755)]\tLoss Ss: 0.049203\n","\tRotated_Epoch:19 [000/005 (0360/0755)]\tLoss Ss: 0.061232\n","\tRotated_Epoch:19 [000/005 (0380/0755)]\tLoss Ss: 0.059412\n","\tRotated_Epoch:19 [000/005 (0400/0755)]\tLoss Ss: 0.059285\n","\tRotated_Epoch:19 [000/005 (0420/0755)]\tLoss Ss: 0.055094\n","\tRotated_Epoch:19 [000/005 (0440/0755)]\tLoss Ss: 0.048652\n","\tRotated_Epoch:19 [000/005 (0460/0755)]\tLoss Ss: 0.040362\n","\tRotated_Epoch:19 [000/005 (0480/0755)]\tLoss Ss: 0.036272\n","\tRotated_Epoch:19 [000/005 (0500/0755)]\tLoss Ss: 0.054781\n","\tRotated_Epoch:19 [000/005 (0520/0755)]\tLoss Ss: 0.052769\n","\tRotated_Epoch:19 [000/005 (0540/0755)]\tLoss Ss: 0.057794\n","\tRotated_Epoch:19 [000/005 (0560/0755)]\tLoss Ss: 0.045512\n","\tRotated_Epoch:19 [000/005 (0580/0755)]\tLoss Ss: 0.073674\n","\tRotated_Epoch:19 [000/005 (0600/0755)]\tLoss Ss: 0.041873\n","\tRotated_Epoch:19 [000/005 (0620/0755)]\tLoss Ss: 0.032608\n","\tRotated_Epoch:19 [000/005 (0640/0755)]\tLoss Ss: 0.031126\n","\tRotated_Epoch:19 [000/005 (0660/0755)]\tLoss Ss: 0.037191\n","\tRotated_Epoch:19 [000/005 (0680/0755)]\tLoss Ss: 0.055709\n","\tRotated_Epoch:19 [000/005 (0700/0755)]\tLoss Ss: 0.051021\n","\tRotated_Epoch:19 [000/005 (0720/0755)]\tLoss Ss: 0.040089\n","\tRotated_Epoch:19 [000/005 (0740/0755)]\tLoss Ss: 0.035989\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tRotated_Epoch:19 [001/005 (0000/0614)]\tLoss Ss: 0.042379\n","\tRotated_Epoch:19 [001/005 (0020/0614)]\tLoss Ss: 0.058518\n","\tRotated_Epoch:19 [001/005 (0040/0614)]\tLoss Ss: 0.020167\n","\tRotated_Epoch:19 [001/005 (0060/0614)]\tLoss Ss: 0.017805\n","\tRotated_Epoch:19 [001/005 (0080/0614)]\tLoss Ss: 0.024456\n","\tRotated_Epoch:19 [001/005 (0100/0614)]\tLoss Ss: 0.020283\n","\tRotated_Epoch:19 [001/005 (0120/0614)]\tLoss Ss: 0.014966\n","\tRotated_Epoch:19 [001/005 (0140/0614)]\tLoss Ss: 0.015552\n","\tRotated_Epoch:19 [001/005 (0160/0614)]\tLoss Ss: 0.016496\n","\tRotated_Epoch:19 [001/005 (0180/0614)]\tLoss Ss: 0.022837\n","\tRotated_Epoch:19 [001/005 (0200/0614)]\tLoss Ss: 0.008082\n","\tRotated_Epoch:19 [001/005 (0220/0614)]\tLoss Ss: 0.008239\n","\tRotated_Epoch:19 [001/005 (0240/0614)]\tLoss Ss: 0.012676\n","\tRotated_Epoch:19 [001/005 (0260/0614)]\tLoss Ss: 0.009776\n","\tRotated_Epoch:19 [001/005 (0280/0614)]\tLoss Ss: 0.013686\n","\tRotated_Epoch:19 [001/005 (0300/0614)]\tLoss Ss: 0.011546\n","\tRotated_Epoch:19 [001/005 (0320/0614)]\tLoss Ss: 0.009426\n","\tRotated_Epoch:19 [001/005 (0340/0614)]\tLoss Ss: 0.006102\n","\tRotated_Epoch:19 [001/005 (0360/0614)]\tLoss Ss: 0.007622\n","\tRotated_Epoch:19 [001/005 (0380/0614)]\tLoss Ss: 0.008377\n","\tRotated_Epoch:19 [001/005 (0400/0614)]\tLoss Ss: 0.007834\n","\tRotated_Epoch:19 [001/005 (0420/0614)]\tLoss Ss: 0.010124\n","\tRotated_Epoch:19 [001/005 (0440/0614)]\tLoss Ss: 0.008568\n","\tRotated_Epoch:19 [001/005 (0460/0614)]\tLoss Ss: 0.006329\n","\tRotated_Epoch:19 [001/005 (0480/0614)]\tLoss Ss: 0.007426\n","\tRotated_Epoch:19 [001/005 (0500/0614)]\tLoss Ss: 0.006603\n","\tRotated_Epoch:19 [001/005 (0520/0614)]\tLoss Ss: 0.008035\n","\tRotated_Epoch:19 [001/005 (0540/0614)]\tLoss Ss: 0.007020\n","\tRotated_Epoch:19 [001/005 (0560/0614)]\tLoss Ss: 0.005791\n","\tRotated_Epoch:19 [001/005 (0580/0614)]\tLoss Ss: 0.010142\n","\tRotated_Epoch:19 [001/005 (0600/0614)]\tLoss Ss: 0.007926\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tRotated_Epoch:19 [002/005 (0000/0588)]\tLoss Ss: 0.146583\n","\tRotated_Epoch:19 [002/005 (0020/0588)]\tLoss Ss: 0.101473\n","\tRotated_Epoch:19 [002/005 (0040/0588)]\tLoss Ss: 0.115000\n","\tRotated_Epoch:19 [002/005 (0060/0588)]\tLoss Ss: 0.085777\n","\tRotated_Epoch:19 [002/005 (0080/0588)]\tLoss Ss: 0.055523\n","\tRotated_Epoch:19 [002/005 (0100/0588)]\tLoss Ss: 0.084092\n","\tRotated_Epoch:19 [002/005 (0120/0588)]\tLoss Ss: 0.089334\n","\tRotated_Epoch:19 [002/005 (0140/0588)]\tLoss Ss: 0.061916\n","\tRotated_Epoch:19 [002/005 (0160/0588)]\tLoss Ss: 0.061504\n","\tRotated_Epoch:19 [002/005 (0180/0588)]\tLoss Ss: 0.074023\n","\tRotated_Epoch:19 [002/005 (0200/0588)]\tLoss Ss: 0.066736\n","\tRotated_Epoch:19 [002/005 (0220/0588)]\tLoss Ss: 0.072205\n","\tRotated_Epoch:19 [002/005 (0240/0588)]\tLoss Ss: 0.056129\n","\tRotated_Epoch:19 [002/005 (0260/0588)]\tLoss Ss: 0.066657\n","\tRotated_Epoch:19 [002/005 (0280/0588)]\tLoss Ss: 0.057791\n","\tRotated_Epoch:19 [002/005 (0300/0588)]\tLoss Ss: 0.064163\n","\tRotated_Epoch:19 [002/005 (0320/0588)]\tLoss Ss: 0.049872\n","\tRotated_Epoch:19 [002/005 (0340/0588)]\tLoss Ss: 0.059879\n","\tRotated_Epoch:19 [002/005 (0360/0588)]\tLoss Ss: 0.065087\n","\tRotated_Epoch:19 [002/005 (0380/0588)]\tLoss Ss: 0.051149\n","\tRotated_Epoch:19 [002/005 (0400/0588)]\tLoss Ss: 0.048565\n","\tRotated_Epoch:19 [002/005 (0420/0588)]\tLoss Ss: 0.045339\n","\tRotated_Epoch:19 [002/005 (0440/0588)]\tLoss Ss: 0.063430\n","\tRotated_Epoch:19 [002/005 (0460/0588)]\tLoss Ss: 0.055423\n","\tRotated_Epoch:19 [002/005 (0480/0588)]\tLoss Ss: 0.061478\n","\tRotated_Epoch:19 [002/005 (0500/0588)]\tLoss Ss: 0.062962\n","\tRotated_Epoch:19 [002/005 (0520/0588)]\tLoss Ss: 0.052000\n","\tRotated_Epoch:19 [002/005 (0540/0588)]\tLoss Ss: 0.069374\n","\tRotated_Epoch:19 [002/005 (0560/0588)]\tLoss Ss: 0.068884\n","\tRotated_Epoch:19 [002/005 (0580/0588)]\tLoss Ss: 0.060923\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tRotated_Epoch:19 [003/005 (0000/0693)]\tLoss Ss: 0.020343\n","\tRotated_Epoch:19 [003/005 (0020/0693)]\tLoss Ss: 0.018944\n","\tRotated_Epoch:19 [003/005 (0040/0693)]\tLoss Ss: 0.027195\n","\tRotated_Epoch:19 [003/005 (0060/0693)]\tLoss Ss: 0.023764\n","\tRotated_Epoch:19 [003/005 (0080/0693)]\tLoss Ss: 0.024394\n","\tRotated_Epoch:19 [003/005 (0100/0693)]\tLoss Ss: 0.021703\n","\tRotated_Epoch:19 [003/005 (0120/0693)]\tLoss Ss: 0.017892\n","\tRotated_Epoch:19 [003/005 (0140/0693)]\tLoss Ss: 0.018411\n","\tRotated_Epoch:19 [003/005 (0160/0693)]\tLoss Ss: 0.017108\n","\tRotated_Epoch:19 [003/005 (0180/0693)]\tLoss Ss: 0.015755\n","\tRotated_Epoch:19 [003/005 (0200/0693)]\tLoss Ss: 0.017453\n","\tRotated_Epoch:19 [003/005 (0220/0693)]\tLoss Ss: 0.024196\n","\tRotated_Epoch:19 [003/005 (0240/0693)]\tLoss Ss: 0.020347\n","\tRotated_Epoch:19 [003/005 (0260/0693)]\tLoss Ss: 0.014414\n","\tRotated_Epoch:19 [003/005 (0280/0693)]\tLoss Ss: 0.016350\n","\tRotated_Epoch:19 [003/005 (0300/0693)]\tLoss Ss: 0.014570\n","\tRotated_Epoch:19 [003/005 (0320/0693)]\tLoss Ss: 0.020757\n","\tRotated_Epoch:19 [003/005 (0340/0693)]\tLoss Ss: 0.016408\n","\tRotated_Epoch:19 [003/005 (0360/0693)]\tLoss Ss: 0.016511\n","\tRotated_Epoch:19 [003/005 (0380/0693)]\tLoss Ss: 0.015878\n","\tRotated_Epoch:19 [003/005 (0400/0693)]\tLoss Ss: 0.020409\n","\tRotated_Epoch:19 [003/005 (0420/0693)]\tLoss Ss: 0.015579\n","\tRotated_Epoch:19 [003/005 (0440/0693)]\tLoss Ss: 0.015137\n","\tRotated_Epoch:19 [003/005 (0460/0693)]\tLoss Ss: 0.015800\n","\tRotated_Epoch:19 [003/005 (0480/0693)]\tLoss Ss: 0.020831\n","\tRotated_Epoch:19 [003/005 (0500/0693)]\tLoss Ss: 0.010276\n","\tRotated_Epoch:19 [003/005 (0520/0693)]\tLoss Ss: 0.015708\n","\tRotated_Epoch:19 [003/005 (0540/0693)]\tLoss Ss: 0.020158\n","\tRotated_Epoch:19 [003/005 (0560/0693)]\tLoss Ss: 0.012450\n","\tRotated_Epoch:19 [003/005 (0580/0693)]\tLoss Ss: 0.014647\n","\tRotated_Epoch:19 [003/005 (0600/0693)]\tLoss Ss: 0.017270\n","\tRotated_Epoch:19 [003/005 (0620/0693)]\tLoss Ss: 0.010951\n","\tRotated_Epoch:19 [003/005 (0640/0693)]\tLoss Ss: 0.021296\n","\tRotated_Epoch:19 [003/005 (0660/0693)]\tLoss Ss: 0.020175\n","\tRotated_Epoch:19 [003/005 (0680/0693)]\tLoss Ss: 0.021045\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tRotated_Epoch:19 [004/005 (0000/0693)]\tLoss Ss: 0.017465\n","\tRotated_Epoch:19 [004/005 (0020/0693)]\tLoss Ss: 0.019955\n","\tRotated_Epoch:19 [004/005 (0040/0693)]\tLoss Ss: 0.026553\n","\tRotated_Epoch:19 [004/005 (0060/0693)]\tLoss Ss: 0.013825\n","\tRotated_Epoch:19 [004/005 (0080/0693)]\tLoss Ss: 0.020851\n","\tRotated_Epoch:19 [004/005 (0100/0693)]\tLoss Ss: 0.016197\n","\tRotated_Epoch:19 [004/005 (0120/0693)]\tLoss Ss: 0.024866\n","\tRotated_Epoch:19 [004/005 (0140/0693)]\tLoss Ss: 0.019901\n","\tRotated_Epoch:19 [004/005 (0160/0693)]\tLoss Ss: 0.018701\n","\tRotated_Epoch:19 [004/005 (0180/0693)]\tLoss Ss: 0.014687\n","\tRotated_Epoch:19 [004/005 (0200/0693)]\tLoss Ss: 0.013856\n","\tRotated_Epoch:19 [004/005 (0220/0693)]\tLoss Ss: 0.013068\n","\tRotated_Epoch:19 [004/005 (0240/0693)]\tLoss Ss: 0.012966\n","\tRotated_Epoch:19 [004/005 (0260/0693)]\tLoss Ss: 0.011137\n","\tRotated_Epoch:19 [004/005 (0280/0693)]\tLoss Ss: 0.015400\n","\tRotated_Epoch:19 [004/005 (0300/0693)]\tLoss Ss: 0.018954\n","\tRotated_Epoch:19 [004/005 (0320/0693)]\tLoss Ss: 0.018008\n","\tRotated_Epoch:19 [004/005 (0340/0693)]\tLoss Ss: 0.013605\n","\tRotated_Epoch:19 [004/005 (0360/0693)]\tLoss Ss: 0.015163\n","\tRotated_Epoch:19 [004/005 (0380/0693)]\tLoss Ss: 0.025030\n","\tRotated_Epoch:19 [004/005 (0400/0693)]\tLoss Ss: 0.013940\n","\tRotated_Epoch:19 [004/005 (0420/0693)]\tLoss Ss: 0.019871\n","\tRotated_Epoch:19 [004/005 (0440/0693)]\tLoss Ss: 0.014534\n","\tRotated_Epoch:19 [004/005 (0460/0693)]\tLoss Ss: 0.012025\n","\tRotated_Epoch:19 [004/005 (0480/0693)]\tLoss Ss: 0.014604\n","\tRotated_Epoch:19 [004/005 (0500/0693)]\tLoss Ss: 0.013668\n","\tRotated_Epoch:19 [004/005 (0520/0693)]\tLoss Ss: 0.017801\n","\tRotated_Epoch:19 [004/005 (0540/0693)]\tLoss Ss: 0.015709\n","\tRotated_Epoch:19 [004/005 (0560/0693)]\tLoss Ss: 0.019158\n","\tRotated_Epoch:19 [004/005 (0580/0693)]\tLoss Ss: 0.011925\n","\tRotated_Epoch:19 [004/005 (0600/0693)]\tLoss Ss: 0.016684\n","\tRotated_Epoch:19 [004/005 (0620/0693)]\tLoss Ss: 0.012299\n","\tRotated_Epoch:19 [004/005 (0640/0693)]\tLoss Ss: 0.016932\n","\tRotated_Epoch:19 [004/005 (0660/0693)]\tLoss Ss: 0.018498\n","\tRotated_Epoch:19 [004/005 (0680/0693)]\tLoss Ss: 0.013519\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:19 [005/005 (0000/0755)]\tLoss Ss: 0.070075\n","\tRotated_Epoch:19 [005/005 (0020/0755)]\tLoss Ss: 0.071605\n","\tRotated_Epoch:19 [005/005 (0040/0755)]\tLoss Ss: 0.042151\n","\tRotated_Epoch:19 [005/005 (0060/0755)]\tLoss Ss: 0.070460\n","\tRotated_Epoch:19 [005/005 (0080/0755)]\tLoss Ss: 0.073093\n","\tRotated_Epoch:19 [005/005 (0100/0755)]\tLoss Ss: 0.034084\n","\tRotated_Epoch:19 [005/005 (0120/0755)]\tLoss Ss: 0.053030\n","\tRotated_Epoch:19 [005/005 (0140/0755)]\tLoss Ss: 0.023097\n","\tRotated_Epoch:19 [005/005 (0160/0755)]\tLoss Ss: 0.027077\n","\tRotated_Epoch:19 [005/005 (0180/0755)]\tLoss Ss: 0.037059\n","\tRotated_Epoch:19 [005/005 (0200/0755)]\tLoss Ss: 0.026974\n","\tRotated_Epoch:19 [005/005 (0220/0755)]\tLoss Ss: 0.021472\n","\tRotated_Epoch:19 [005/005 (0240/0755)]\tLoss Ss: 0.037892\n","\tRotated_Epoch:19 [005/005 (0260/0755)]\tLoss Ss: 0.022769\n","\tRotated_Epoch:19 [005/005 (0280/0755)]\tLoss Ss: 0.021697\n","\tRotated_Epoch:19 [005/005 (0300/0755)]\tLoss Ss: 0.028700\n","\tRotated_Epoch:19 [005/005 (0320/0755)]\tLoss Ss: 0.024221\n","\tRotated_Epoch:19 [005/005 (0340/0755)]\tLoss Ss: 0.025543\n","\tRotated_Epoch:19 [005/005 (0360/0755)]\tLoss Ss: 0.020540\n","\tRotated_Epoch:19 [005/005 (0380/0755)]\tLoss Ss: 0.012324\n","\tRotated_Epoch:19 [005/005 (0400/0755)]\tLoss Ss: 0.020541\n","\tRotated_Epoch:19 [005/005 (0420/0755)]\tLoss Ss: 0.017903\n","\tRotated_Epoch:19 [005/005 (0440/0755)]\tLoss Ss: 0.026534\n","\tRotated_Epoch:19 [005/005 (0460/0755)]\tLoss Ss: 0.018452\n","\tRotated_Epoch:19 [005/005 (0480/0755)]\tLoss Ss: 0.022507\n","\tRotated_Epoch:19 [005/005 (0500/0755)]\tLoss Ss: 0.016000\n","\tRotated_Epoch:19 [005/005 (0520/0755)]\tLoss Ss: 0.019548\n","\tRotated_Epoch:19 [005/005 (0540/0755)]\tLoss Ss: 0.019808\n","\tRotated_Epoch:19 [005/005 (0560/0755)]\tLoss Ss: 0.015628\n","\tRotated_Epoch:19 [005/005 (0580/0755)]\tLoss Ss: 0.028968\n","\tRotated_Epoch:19 [005/005 (0600/0755)]\tLoss Ss: 0.024586\n","\tRotated_Epoch:19 [005/005 (0620/0755)]\tLoss Ss: 0.016353\n","\tRotated_Epoch:19 [005/005 (0640/0755)]\tLoss Ss: 0.020659\n","\tRotated_Epoch:19 [005/005 (0660/0755)]\tLoss Ss: 0.025644\n","\tRotated_Epoch:19 [005/005 (0680/0755)]\tLoss Ss: 0.025815\n","\tRotated_Epoch:19 [005/005 (0700/0755)]\tLoss Ss: 0.014050\n","\tRotated_Epoch:19 [005/005 (0720/0755)]\tLoss Ss: 0.025482\n","\tRotated_Epoch:19 [005/005 (0740/0755)]\tLoss Ss: 0.018210\n","new_1_6_d.nii\n","new_1_6_m.nii\n","new_1_7_d.nii\n","new_1_7_m.nii\n","new_2_5_d.nii\n","new_2_5_m.nii\n","new_2_6_d.nii\n","new_2_6_m.nii\n","new_2_7_d.nii\n","new_2_7_m.nii\n","new_3_d (2).nii\n","new_3_m (2).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (3).nii\n","new_3_m (3).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (7).nii\n","new_3_m (7).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (8).nii\n","new_3_m (8).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (9).nii\n","new_3_m (9).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_4_10_d_163526.nii\n","new_4_10_m_163526.nii.gz\n","new_4_11_d_084336.nii\n","new_4_11_m_084336.nii.gz\n","new_4_7_d_184422.nii\n","new_4_7_m_184422.nii.gz\n","new_4_8_d_143412.nii\n","new_4_8_m_143412.nii.gz\n","new_4_9_d_155347.nii\n","new_4_9_m_155347.nii.gz\n","\tEpoch: 19; Dice: 0.9395 +/- 0.0349; Loss: 10.7352\n","Begin Epoch 20\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tEpoch:20 [000/005 (0000/0588)]\tLoss Ss: 0.031817\n","\tEpoch:20 [000/005 (0020/0588)]\tLoss Ss: 0.021660\n","\tEpoch:20 [000/005 (0040/0588)]\tLoss Ss: 0.016813\n","\tEpoch:20 [000/005 (0060/0588)]\tLoss Ss: 0.016169\n","\tEpoch:20 [000/005 (0080/0588)]\tLoss Ss: 0.012019\n","\tEpoch:20 [000/005 (0100/0588)]\tLoss Ss: 0.010281\n","\tEpoch:20 [000/005 (0120/0588)]\tLoss Ss: 0.009161\n","\tEpoch:20 [000/005 (0140/0588)]\tLoss Ss: 0.015243\n","\tEpoch:20 [000/005 (0160/0588)]\tLoss Ss: 0.007186\n","\tEpoch:20 [000/005 (0180/0588)]\tLoss Ss: 0.009982\n","\tEpoch:20 [000/005 (0200/0588)]\tLoss Ss: 0.011848\n","\tEpoch:20 [000/005 (0220/0588)]\tLoss Ss: 0.011505\n","\tEpoch:20 [000/005 (0240/0588)]\tLoss Ss: 0.008364\n","\tEpoch:20 [000/005 (0260/0588)]\tLoss Ss: 0.009394\n","\tEpoch:20 [000/005 (0280/0588)]\tLoss Ss: 0.009219\n","\tEpoch:20 [000/005 (0300/0588)]\tLoss Ss: 0.009657\n","\tEpoch:20 [000/005 (0320/0588)]\tLoss Ss: 0.008431\n","\tEpoch:20 [000/005 (0340/0588)]\tLoss Ss: 0.012612\n","\tEpoch:20 [000/005 (0360/0588)]\tLoss Ss: 0.008605\n","\tEpoch:20 [000/005 (0380/0588)]\tLoss Ss: 0.006096\n","\tEpoch:20 [000/005 (0400/0588)]\tLoss Ss: 0.008738\n","\tEpoch:20 [000/005 (0420/0588)]\tLoss Ss: 0.006734\n","\tEpoch:20 [000/005 (0440/0588)]\tLoss Ss: 0.010414\n","\tEpoch:20 [000/005 (0460/0588)]\tLoss Ss: 0.008707\n","\tEpoch:20 [000/005 (0480/0588)]\tLoss Ss: 0.007981\n","\tEpoch:20 [000/005 (0500/0588)]\tLoss Ss: 0.006762\n","\tEpoch:20 [000/005 (0520/0588)]\tLoss Ss: 0.006973\n","\tEpoch:20 [000/005 (0540/0588)]\tLoss Ss: 0.005615\n","\tEpoch:20 [000/005 (0560/0588)]\tLoss Ss: 0.007865\n","\tEpoch:20 [000/005 (0580/0588)]\tLoss Ss: 0.005302\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tEpoch:20 [001/005 (0000/0693)]\tLoss Ss: 0.017549\n","\tEpoch:20 [001/005 (0020/0693)]\tLoss Ss: 0.017830\n","\tEpoch:20 [001/005 (0040/0693)]\tLoss Ss: 0.018433\n","\tEpoch:20 [001/005 (0060/0693)]\tLoss Ss: 0.018539\n","\tEpoch:20 [001/005 (0080/0693)]\tLoss Ss: 0.022856\n","\tEpoch:20 [001/005 (0100/0693)]\tLoss Ss: 0.026950\n","\tEpoch:20 [001/005 (0120/0693)]\tLoss Ss: 0.013516\n","\tEpoch:20 [001/005 (0140/0693)]\tLoss Ss: 0.015907\n","\tEpoch:20 [001/005 (0160/0693)]\tLoss Ss: 0.016155\n","\tEpoch:20 [001/005 (0180/0693)]\tLoss Ss: 0.015458\n","\tEpoch:20 [001/005 (0200/0693)]\tLoss Ss: 0.013197\n","\tEpoch:20 [001/005 (0220/0693)]\tLoss Ss: 0.020656\n","\tEpoch:20 [001/005 (0240/0693)]\tLoss Ss: 0.015459\n","\tEpoch:20 [001/005 (0260/0693)]\tLoss Ss: 0.031483\n","\tEpoch:20 [001/005 (0280/0693)]\tLoss Ss: 0.020282\n","\tEpoch:20 [001/005 (0300/0693)]\tLoss Ss: 0.007224\n","\tEpoch:20 [001/005 (0320/0693)]\tLoss Ss: 0.015659\n","\tEpoch:20 [001/005 (0340/0693)]\tLoss Ss: 0.013008\n","\tEpoch:20 [001/005 (0360/0693)]\tLoss Ss: 0.015906\n","\tEpoch:20 [001/005 (0380/0693)]\tLoss Ss: 0.010310\n","\tEpoch:20 [001/005 (0400/0693)]\tLoss Ss: 0.020638\n","\tEpoch:20 [001/005 (0420/0693)]\tLoss Ss: 0.019452\n","\tEpoch:20 [001/005 (0440/0693)]\tLoss Ss: 0.019262\n","\tEpoch:20 [001/005 (0460/0693)]\tLoss Ss: 0.020630\n","\tEpoch:20 [001/005 (0480/0693)]\tLoss Ss: 0.017374\n","\tEpoch:20 [001/005 (0500/0693)]\tLoss Ss: 0.013247\n","\tEpoch:20 [001/005 (0520/0693)]\tLoss Ss: 0.012975\n","\tEpoch:20 [001/005 (0540/0693)]\tLoss Ss: 0.013977\n","\tEpoch:20 [001/005 (0560/0693)]\tLoss Ss: 0.015295\n","\tEpoch:20 [001/005 (0580/0693)]\tLoss Ss: 0.018273\n","\tEpoch:20 [001/005 (0600/0693)]\tLoss Ss: 0.014117\n","\tEpoch:20 [001/005 (0620/0693)]\tLoss Ss: 0.019123\n","\tEpoch:20 [001/005 (0640/0693)]\tLoss Ss: 0.013183\n","\tEpoch:20 [001/005 (0660/0693)]\tLoss Ss: 0.018036\n","\tEpoch:20 [001/005 (0680/0693)]\tLoss Ss: 0.011026\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tEpoch:20 [002/005 (0000/0614)]\tLoss Ss: 0.007185\n","\tEpoch:20 [002/005 (0020/0614)]\tLoss Ss: 0.006172\n","\tEpoch:20 [002/005 (0040/0614)]\tLoss Ss: 0.004563\n","\tEpoch:20 [002/005 (0060/0614)]\tLoss Ss: 0.005698\n","\tEpoch:20 [002/005 (0080/0614)]\tLoss Ss: 0.005682\n","\tEpoch:20 [002/005 (0100/0614)]\tLoss Ss: 0.005183\n","\tEpoch:20 [002/005 (0120/0614)]\tLoss Ss: 0.004479\n","\tEpoch:20 [002/005 (0140/0614)]\tLoss Ss: 0.007052\n","\tEpoch:20 [002/005 (0160/0614)]\tLoss Ss: 0.008624\n","\tEpoch:20 [002/005 (0180/0614)]\tLoss Ss: 0.007649\n","\tEpoch:20 [002/005 (0200/0614)]\tLoss Ss: 0.006371\n","\tEpoch:20 [002/005 (0220/0614)]\tLoss Ss: 0.008436\n","\tEpoch:20 [002/005 (0240/0614)]\tLoss Ss: 0.007458\n","\tEpoch:20 [002/005 (0260/0614)]\tLoss Ss: 0.007361\n","\tEpoch:20 [002/005 (0280/0614)]\tLoss Ss: 0.005689\n","\tEpoch:20 [002/005 (0300/0614)]\tLoss Ss: 0.005225\n","\tEpoch:20 [002/005 (0320/0614)]\tLoss Ss: 0.005935\n","\tEpoch:20 [002/005 (0340/0614)]\tLoss Ss: 0.005889\n","\tEpoch:20 [002/005 (0360/0614)]\tLoss Ss: 0.006404\n","\tEpoch:20 [002/005 (0380/0614)]\tLoss Ss: 0.004657\n","\tEpoch:20 [002/005 (0400/0614)]\tLoss Ss: 0.006116\n","\tEpoch:20 [002/005 (0420/0614)]\tLoss Ss: 0.005858\n","\tEpoch:20 [002/005 (0440/0614)]\tLoss Ss: 0.007130\n","\tEpoch:20 [002/005 (0460/0614)]\tLoss Ss: 0.006481\n","\tEpoch:20 [002/005 (0480/0614)]\tLoss Ss: 0.007964\n","\tEpoch:20 [002/005 (0500/0614)]\tLoss Ss: 0.005792\n","\tEpoch:20 [002/005 (0520/0614)]\tLoss Ss: 0.008208\n","\tEpoch:20 [002/005 (0540/0614)]\tLoss Ss: 0.008370\n","\tEpoch:20 [002/005 (0560/0614)]\tLoss Ss: 0.007085\n","\tEpoch:20 [002/005 (0580/0614)]\tLoss Ss: 0.005111\n","\tEpoch:20 [002/005 (0600/0614)]\tLoss Ss: 0.006870\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:20 [003/005 (0000/0755)]\tLoss Ss: 0.018189\n","\tEpoch:20 [003/005 (0020/0755)]\tLoss Ss: 0.018290\n","\tEpoch:20 [003/005 (0040/0755)]\tLoss Ss: 0.017385\n","\tEpoch:20 [003/005 (0060/0755)]\tLoss Ss: 0.011929\n","\tEpoch:20 [003/005 (0080/0755)]\tLoss Ss: 0.012930\n","\tEpoch:20 [003/005 (0100/0755)]\tLoss Ss: 0.012247\n","\tEpoch:20 [003/005 (0120/0755)]\tLoss Ss: 0.017633\n","\tEpoch:20 [003/005 (0140/0755)]\tLoss Ss: 0.009911\n","\tEpoch:20 [003/005 (0160/0755)]\tLoss Ss: 0.023674\n","\tEpoch:20 [003/005 (0180/0755)]\tLoss Ss: 0.017247\n","\tEpoch:20 [003/005 (0200/0755)]\tLoss Ss: 0.018920\n","\tEpoch:20 [003/005 (0220/0755)]\tLoss Ss: 0.015913\n","\tEpoch:20 [003/005 (0240/0755)]\tLoss Ss: 0.009955\n","\tEpoch:20 [003/005 (0260/0755)]\tLoss Ss: 0.016262\n","\tEpoch:20 [003/005 (0280/0755)]\tLoss Ss: 0.016716\n","\tEpoch:20 [003/005 (0300/0755)]\tLoss Ss: 0.013295\n","\tEpoch:20 [003/005 (0320/0755)]\tLoss Ss: 0.018979\n","\tEpoch:20 [003/005 (0340/0755)]\tLoss Ss: 0.009039\n","\tEpoch:20 [003/005 (0360/0755)]\tLoss Ss: 0.015071\n","\tEpoch:20 [003/005 (0380/0755)]\tLoss Ss: 0.011354\n","\tEpoch:20 [003/005 (0400/0755)]\tLoss Ss: 0.008852\n","\tEpoch:20 [003/005 (0420/0755)]\tLoss Ss: 0.017302\n","\tEpoch:20 [003/005 (0440/0755)]\tLoss Ss: 0.014931\n","\tEpoch:20 [003/005 (0460/0755)]\tLoss Ss: 0.017156\n","\tEpoch:20 [003/005 (0480/0755)]\tLoss Ss: 0.014646\n","\tEpoch:20 [003/005 (0500/0755)]\tLoss Ss: 0.015618\n","\tEpoch:20 [003/005 (0520/0755)]\tLoss Ss: 0.012991\n","\tEpoch:20 [003/005 (0540/0755)]\tLoss Ss: 0.013230\n","\tEpoch:20 [003/005 (0560/0755)]\tLoss Ss: 0.016930\n","\tEpoch:20 [003/005 (0580/0755)]\tLoss Ss: 0.021547\n","\tEpoch:20 [003/005 (0600/0755)]\tLoss Ss: 0.018235\n","\tEpoch:20 [003/005 (0620/0755)]\tLoss Ss: 0.011627\n","\tEpoch:20 [003/005 (0640/0755)]\tLoss Ss: 0.008872\n","\tEpoch:20 [003/005 (0660/0755)]\tLoss Ss: 0.011570\n","\tEpoch:20 [003/005 (0680/0755)]\tLoss Ss: 0.014334\n","\tEpoch:20 [003/005 (0700/0755)]\tLoss Ss: 0.013871\n","\tEpoch:20 [003/005 (0720/0755)]\tLoss Ss: 0.015764\n","\tEpoch:20 [003/005 (0740/0755)]\tLoss Ss: 0.010701\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tEpoch:20 [004/005 (0000/0693)]\tLoss Ss: 0.014394\n","\tEpoch:20 [004/005 (0020/0693)]\tLoss Ss: 0.009635\n","\tEpoch:20 [004/005 (0040/0693)]\tLoss Ss: 0.028504\n","\tEpoch:20 [004/005 (0060/0693)]\tLoss Ss: 0.013389\n","\tEpoch:20 [004/005 (0080/0693)]\tLoss Ss: 0.014406\n","\tEpoch:20 [004/005 (0100/0693)]\tLoss Ss: 0.012726\n","\tEpoch:20 [004/005 (0120/0693)]\tLoss Ss: 0.012876\n","\tEpoch:20 [004/005 (0140/0693)]\tLoss Ss: 0.017339\n","\tEpoch:20 [004/005 (0160/0693)]\tLoss Ss: 0.018413\n","\tEpoch:20 [004/005 (0180/0693)]\tLoss Ss: 0.016364\n","\tEpoch:20 [004/005 (0200/0693)]\tLoss Ss: 0.013136\n","\tEpoch:20 [004/005 (0220/0693)]\tLoss Ss: 0.011361\n","\tEpoch:20 [004/005 (0240/0693)]\tLoss Ss: 0.009707\n","\tEpoch:20 [004/005 (0260/0693)]\tLoss Ss: 0.012414\n","\tEpoch:20 [004/005 (0280/0693)]\tLoss Ss: 0.012979\n","\tEpoch:20 [004/005 (0300/0693)]\tLoss Ss: 0.016314\n","\tEpoch:20 [004/005 (0320/0693)]\tLoss Ss: 0.013443\n","\tEpoch:20 [004/005 (0340/0693)]\tLoss Ss: 0.010230\n","\tEpoch:20 [004/005 (0360/0693)]\tLoss Ss: 0.014611\n","\tEpoch:20 [004/005 (0380/0693)]\tLoss Ss: 0.008186\n","\tEpoch:20 [004/005 (0400/0693)]\tLoss Ss: 0.016689\n","\tEpoch:20 [004/005 (0420/0693)]\tLoss Ss: 0.011357\n","\tEpoch:20 [004/005 (0440/0693)]\tLoss Ss: 0.016253\n","\tEpoch:20 [004/005 (0460/0693)]\tLoss Ss: 0.013539\n","\tEpoch:20 [004/005 (0480/0693)]\tLoss Ss: 0.017115\n","\tEpoch:20 [004/005 (0500/0693)]\tLoss Ss: 0.008377\n","\tEpoch:20 [004/005 (0520/0693)]\tLoss Ss: 0.012406\n","\tEpoch:20 [004/005 (0540/0693)]\tLoss Ss: 0.014490\n","\tEpoch:20 [004/005 (0560/0693)]\tLoss Ss: 0.009988\n","\tEpoch:20 [004/005 (0580/0693)]\tLoss Ss: 0.011498\n","\tEpoch:20 [004/005 (0600/0693)]\tLoss Ss: 0.010915\n","\tEpoch:20 [004/005 (0620/0693)]\tLoss Ss: 0.013590\n","\tEpoch:20 [004/005 (0640/0693)]\tLoss Ss: 0.008385\n","\tEpoch:20 [004/005 (0660/0693)]\tLoss Ss: 0.009039\n","\tEpoch:20 [004/005 (0680/0693)]\tLoss Ss: 0.013850\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:20 [005/005 (0000/0755)]\tLoss Ss: 0.026190\n","\tEpoch:20 [005/005 (0020/0755)]\tLoss Ss: 0.021474\n","\tEpoch:20 [005/005 (0040/0755)]\tLoss Ss: 0.017867\n","\tEpoch:20 [005/005 (0060/0755)]\tLoss Ss: 0.018451\n","\tEpoch:20 [005/005 (0080/0755)]\tLoss Ss: 0.017776\n","\tEpoch:20 [005/005 (0100/0755)]\tLoss Ss: 0.016284\n","\tEpoch:20 [005/005 (0120/0755)]\tLoss Ss: 0.024227\n","\tEpoch:20 [005/005 (0140/0755)]\tLoss Ss: 0.021732\n","\tEpoch:20 [005/005 (0160/0755)]\tLoss Ss: 0.019003\n","\tEpoch:20 [005/005 (0180/0755)]\tLoss Ss: 0.016540\n","\tEpoch:20 [005/005 (0200/0755)]\tLoss Ss: 0.008449\n","\tEpoch:20 [005/005 (0220/0755)]\tLoss Ss: 0.014114\n","\tEpoch:20 [005/005 (0240/0755)]\tLoss Ss: 0.014617\n","\tEpoch:20 [005/005 (0260/0755)]\tLoss Ss: 0.020900\n","\tEpoch:20 [005/005 (0280/0755)]\tLoss Ss: 0.016622\n","\tEpoch:20 [005/005 (0300/0755)]\tLoss Ss: 0.016902\n","\tEpoch:20 [005/005 (0320/0755)]\tLoss Ss: 0.017354\n","\tEpoch:20 [005/005 (0340/0755)]\tLoss Ss: 0.014172\n","\tEpoch:20 [005/005 (0360/0755)]\tLoss Ss: 0.016435\n","\tEpoch:20 [005/005 (0380/0755)]\tLoss Ss: 0.036711\n","\tEpoch:20 [005/005 (0400/0755)]\tLoss Ss: 0.010964\n","\tEpoch:20 [005/005 (0420/0755)]\tLoss Ss: 0.016527\n","\tEpoch:20 [005/005 (0440/0755)]\tLoss Ss: 0.015884\n","\tEpoch:20 [005/005 (0460/0755)]\tLoss Ss: 0.015386\n","\tEpoch:20 [005/005 (0480/0755)]\tLoss Ss: 0.016234\n","\tEpoch:20 [005/005 (0500/0755)]\tLoss Ss: 0.016966\n","\tEpoch:20 [005/005 (0520/0755)]\tLoss Ss: 0.017051\n","\tEpoch:20 [005/005 (0540/0755)]\tLoss Ss: 0.015135\n","\tEpoch:20 [005/005 (0560/0755)]\tLoss Ss: 0.019111\n","\tEpoch:20 [005/005 (0580/0755)]\tLoss Ss: 0.015672\n","\tEpoch:20 [005/005 (0600/0755)]\tLoss Ss: 0.023597\n","\tEpoch:20 [005/005 (0620/0755)]\tLoss Ss: 0.012485\n","\tEpoch:20 [005/005 (0640/0755)]\tLoss Ss: 0.024331\n","\tEpoch:20 [005/005 (0660/0755)]\tLoss Ss: 0.014740\n","\tEpoch:20 [005/005 (0680/0755)]\tLoss Ss: 0.024477\n","\tEpoch:20 [005/005 (0700/0755)]\tLoss Ss: 0.017243\n","\tEpoch:20 [005/005 (0720/0755)]\tLoss Ss: 0.012241\n","\tEpoch:20 [005/005 (0740/0755)]\tLoss Ss: 0.012267\n","Now train the rotated image\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:20 [000/005 (0000/0755)]\tLoss Ss: 0.228481\n","\tRotated_Epoch:20 [000/005 (0020/0755)]\tLoss Ss: 0.206402\n","\tRotated_Epoch:20 [000/005 (0040/0755)]\tLoss Ss: 0.313518\n","\tRotated_Epoch:20 [000/005 (0060/0755)]\tLoss Ss: 0.202453\n","\tRotated_Epoch:20 [000/005 (0080/0755)]\tLoss Ss: 0.190450\n","\tRotated_Epoch:20 [000/005 (0100/0755)]\tLoss Ss: 0.163814\n","\tRotated_Epoch:20 [000/005 (0120/0755)]\tLoss Ss: 0.177486\n","\tRotated_Epoch:20 [000/005 (0140/0755)]\tLoss Ss: 0.118591\n","\tRotated_Epoch:20 [000/005 (0160/0755)]\tLoss Ss: 0.121553\n","\tRotated_Epoch:20 [000/005 (0180/0755)]\tLoss Ss: 0.107656\n","\tRotated_Epoch:20 [000/005 (0200/0755)]\tLoss Ss: 0.110281\n","\tRotated_Epoch:20 [000/005 (0220/0755)]\tLoss Ss: 0.114740\n","\tRotated_Epoch:20 [000/005 (0240/0755)]\tLoss Ss: 0.093792\n","\tRotated_Epoch:20 [000/005 (0260/0755)]\tLoss Ss: 0.066994\n","\tRotated_Epoch:20 [000/005 (0280/0755)]\tLoss Ss: 0.056246\n","\tRotated_Epoch:20 [000/005 (0300/0755)]\tLoss Ss: 0.057033\n","\tRotated_Epoch:20 [000/005 (0320/0755)]\tLoss Ss: 0.066400\n","\tRotated_Epoch:20 [000/005 (0340/0755)]\tLoss Ss: 0.061526\n","\tRotated_Epoch:20 [000/005 (0360/0755)]\tLoss Ss: 0.061089\n","\tRotated_Epoch:20 [000/005 (0380/0755)]\tLoss Ss: 0.050826\n","\tRotated_Epoch:20 [000/005 (0400/0755)]\tLoss Ss: 0.068401\n","\tRotated_Epoch:20 [000/005 (0420/0755)]\tLoss Ss: 0.057746\n","\tRotated_Epoch:20 [000/005 (0440/0755)]\tLoss Ss: 0.037448\n","\tRotated_Epoch:20 [000/005 (0460/0755)]\tLoss Ss: 0.045215\n","\tRotated_Epoch:20 [000/005 (0480/0755)]\tLoss Ss: 0.055858\n","\tRotated_Epoch:20 [000/005 (0500/0755)]\tLoss Ss: 0.052092\n","\tRotated_Epoch:20 [000/005 (0520/0755)]\tLoss Ss: 0.062049\n","\tRotated_Epoch:20 [000/005 (0540/0755)]\tLoss Ss: 0.062865\n","\tRotated_Epoch:20 [000/005 (0560/0755)]\tLoss Ss: 0.050106\n","\tRotated_Epoch:20 [000/005 (0580/0755)]\tLoss Ss: 0.059140\n","\tRotated_Epoch:20 [000/005 (0600/0755)]\tLoss Ss: 0.041058\n","\tRotated_Epoch:20 [000/005 (0620/0755)]\tLoss Ss: 0.047227\n","\tRotated_Epoch:20 [000/005 (0640/0755)]\tLoss Ss: 0.042997\n","\tRotated_Epoch:20 [000/005 (0660/0755)]\tLoss Ss: 0.053413\n","\tRotated_Epoch:20 [000/005 (0680/0755)]\tLoss Ss: 0.044030\n","\tRotated_Epoch:20 [000/005 (0700/0755)]\tLoss Ss: 0.038694\n","\tRotated_Epoch:20 [000/005 (0720/0755)]\tLoss Ss: 0.043050\n","\tRotated_Epoch:20 [000/005 (0740/0755)]\tLoss Ss: 0.052438\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tRotated_Epoch:20 [001/005 (0000/0693)]\tLoss Ss: 0.137411\n","\tRotated_Epoch:20 [001/005 (0020/0693)]\tLoss Ss: 0.086363\n","\tRotated_Epoch:20 [001/005 (0040/0693)]\tLoss Ss: 0.043676\n","\tRotated_Epoch:20 [001/005 (0060/0693)]\tLoss Ss: 0.049695\n","\tRotated_Epoch:20 [001/005 (0080/0693)]\tLoss Ss: 0.030262\n","\tRotated_Epoch:20 [001/005 (0100/0693)]\tLoss Ss: 0.053309\n","\tRotated_Epoch:20 [001/005 (0120/0693)]\tLoss Ss: 0.025387\n","\tRotated_Epoch:20 [001/005 (0140/0693)]\tLoss Ss: 0.021945\n","\tRotated_Epoch:20 [001/005 (0160/0693)]\tLoss Ss: 0.022547\n","\tRotated_Epoch:20 [001/005 (0180/0693)]\tLoss Ss: 0.022713\n","\tRotated_Epoch:20 [001/005 (0200/0693)]\tLoss Ss: 0.034477\n","\tRotated_Epoch:20 [001/005 (0220/0693)]\tLoss Ss: 0.022892\n","\tRotated_Epoch:20 [001/005 (0240/0693)]\tLoss Ss: 0.024628\n","\tRotated_Epoch:20 [001/005 (0260/0693)]\tLoss Ss: 0.035251\n","\tRotated_Epoch:20 [001/005 (0280/0693)]\tLoss Ss: 0.020560\n","\tRotated_Epoch:20 [001/005 (0300/0693)]\tLoss Ss: 0.018077\n","\tRotated_Epoch:20 [001/005 (0320/0693)]\tLoss Ss: 0.019056\n","\tRotated_Epoch:20 [001/005 (0340/0693)]\tLoss Ss: 0.018724\n","\tRotated_Epoch:20 [001/005 (0360/0693)]\tLoss Ss: 0.023081\n","\tRotated_Epoch:20 [001/005 (0380/0693)]\tLoss Ss: 0.027187\n","\tRotated_Epoch:20 [001/005 (0400/0693)]\tLoss Ss: 0.012347\n","\tRotated_Epoch:20 [001/005 (0420/0693)]\tLoss Ss: 0.022455\n","\tRotated_Epoch:20 [001/005 (0440/0693)]\tLoss Ss: 0.017331\n","\tRotated_Epoch:20 [001/005 (0460/0693)]\tLoss Ss: 0.014568\n","\tRotated_Epoch:20 [001/005 (0480/0693)]\tLoss Ss: 0.022327\n","\tRotated_Epoch:20 [001/005 (0500/0693)]\tLoss Ss: 0.017109\n","\tRotated_Epoch:20 [001/005 (0520/0693)]\tLoss Ss: 0.019612\n","\tRotated_Epoch:20 [001/005 (0540/0693)]\tLoss Ss: 0.016741\n","\tRotated_Epoch:20 [001/005 (0560/0693)]\tLoss Ss: 0.011850\n","\tRotated_Epoch:20 [001/005 (0580/0693)]\tLoss Ss: 0.013702\n","\tRotated_Epoch:20 [001/005 (0600/0693)]\tLoss Ss: 0.015743\n","\tRotated_Epoch:20 [001/005 (0620/0693)]\tLoss Ss: 0.017402\n","\tRotated_Epoch:20 [001/005 (0640/0693)]\tLoss Ss: 0.016798\n","\tRotated_Epoch:20 [001/005 (0660/0693)]\tLoss Ss: 0.015304\n","\tRotated_Epoch:20 [001/005 (0680/0693)]\tLoss Ss: 0.018025\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:20 [002/005 (0000/0755)]\tLoss Ss: 0.083688\n","\tRotated_Epoch:20 [002/005 (0020/0755)]\tLoss Ss: 0.054270\n","\tRotated_Epoch:20 [002/005 (0040/0755)]\tLoss Ss: 0.054414\n","\tRotated_Epoch:20 [002/005 (0060/0755)]\tLoss Ss: 0.053371\n","\tRotated_Epoch:20 [002/005 (0080/0755)]\tLoss Ss: 0.066556\n","\tRotated_Epoch:20 [002/005 (0100/0755)]\tLoss Ss: 0.075944\n","\tRotated_Epoch:20 [002/005 (0120/0755)]\tLoss Ss: 0.046353\n","\tRotated_Epoch:20 [002/005 (0140/0755)]\tLoss Ss: 0.044636\n","\tRotated_Epoch:20 [002/005 (0160/0755)]\tLoss Ss: 0.036580\n","\tRotated_Epoch:20 [002/005 (0180/0755)]\tLoss Ss: 0.039285\n","\tRotated_Epoch:20 [002/005 (0200/0755)]\tLoss Ss: 0.057071\n","\tRotated_Epoch:20 [002/005 (0220/0755)]\tLoss Ss: 0.042124\n","\tRotated_Epoch:20 [002/005 (0240/0755)]\tLoss Ss: 0.038976\n","\tRotated_Epoch:20 [002/005 (0260/0755)]\tLoss Ss: 0.033743\n","\tRotated_Epoch:20 [002/005 (0280/0755)]\tLoss Ss: 0.039287\n","\tRotated_Epoch:20 [002/005 (0300/0755)]\tLoss Ss: 0.033114\n","\tRotated_Epoch:20 [002/005 (0320/0755)]\tLoss Ss: 0.036712\n","\tRotated_Epoch:20 [002/005 (0340/0755)]\tLoss Ss: 0.031653\n","\tRotated_Epoch:20 [002/005 (0360/0755)]\tLoss Ss: 0.016390\n","\tRotated_Epoch:20 [002/005 (0380/0755)]\tLoss Ss: 0.030203\n","\tRotated_Epoch:20 [002/005 (0400/0755)]\tLoss Ss: 0.033395\n","\tRotated_Epoch:20 [002/005 (0420/0755)]\tLoss Ss: 0.017294\n","\tRotated_Epoch:20 [002/005 (0440/0755)]\tLoss Ss: 0.022994\n","\tRotated_Epoch:20 [002/005 (0460/0755)]\tLoss Ss: 0.038239\n","\tRotated_Epoch:20 [002/005 (0480/0755)]\tLoss Ss: 0.041072\n","\tRotated_Epoch:20 [002/005 (0500/0755)]\tLoss Ss: 0.030612\n","\tRotated_Epoch:20 [002/005 (0520/0755)]\tLoss Ss: 0.017374\n","\tRotated_Epoch:20 [002/005 (0540/0755)]\tLoss Ss: 0.014460\n","\tRotated_Epoch:20 [002/005 (0560/0755)]\tLoss Ss: 0.022597\n","\tRotated_Epoch:20 [002/005 (0580/0755)]\tLoss Ss: 0.035687\n","\tRotated_Epoch:20 [002/005 (0600/0755)]\tLoss Ss: 0.028512\n","\tRotated_Epoch:20 [002/005 (0620/0755)]\tLoss Ss: 0.024615\n","\tRotated_Epoch:20 [002/005 (0640/0755)]\tLoss Ss: 0.024536\n","\tRotated_Epoch:20 [002/005 (0660/0755)]\tLoss Ss: 0.026468\n","\tRotated_Epoch:20 [002/005 (0680/0755)]\tLoss Ss: 0.028029\n","\tRotated_Epoch:20 [002/005 (0700/0755)]\tLoss Ss: 0.017585\n","\tRotated_Epoch:20 [002/005 (0720/0755)]\tLoss Ss: 0.021170\n","\tRotated_Epoch:20 [002/005 (0740/0755)]\tLoss Ss: 0.026159\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tRotated_Epoch:20 [003/005 (0000/0588)]\tLoss Ss: 0.148228\n","\tRotated_Epoch:20 [003/005 (0020/0588)]\tLoss Ss: 0.100324\n","\tRotated_Epoch:20 [003/005 (0040/0588)]\tLoss Ss: 0.090025\n","\tRotated_Epoch:20 [003/005 (0060/0588)]\tLoss Ss: 0.063965\n","\tRotated_Epoch:20 [003/005 (0080/0588)]\tLoss Ss: 0.051335\n","\tRotated_Epoch:20 [003/005 (0100/0588)]\tLoss Ss: 0.065212\n","\tRotated_Epoch:20 [003/005 (0120/0588)]\tLoss Ss: 0.039409\n","\tRotated_Epoch:20 [003/005 (0140/0588)]\tLoss Ss: 0.075457\n","\tRotated_Epoch:20 [003/005 (0160/0588)]\tLoss Ss: 0.045269\n","\tRotated_Epoch:20 [003/005 (0180/0588)]\tLoss Ss: 0.079254\n","\tRotated_Epoch:20 [003/005 (0200/0588)]\tLoss Ss: 0.076963\n","\tRotated_Epoch:20 [003/005 (0220/0588)]\tLoss Ss: 0.071184\n","\tRotated_Epoch:20 [003/005 (0240/0588)]\tLoss Ss: 0.054498\n","\tRotated_Epoch:20 [003/005 (0260/0588)]\tLoss Ss: 0.106126\n","\tRotated_Epoch:20 [003/005 (0280/0588)]\tLoss Ss: 0.046650\n","\tRotated_Epoch:20 [003/005 (0300/0588)]\tLoss Ss: 0.082866\n","\tRotated_Epoch:20 [003/005 (0320/0588)]\tLoss Ss: 0.060979\n","\tRotated_Epoch:20 [003/005 (0340/0588)]\tLoss Ss: 0.044138\n","\tRotated_Epoch:20 [003/005 (0360/0588)]\tLoss Ss: 0.067992\n","\tRotated_Epoch:20 [003/005 (0380/0588)]\tLoss Ss: 0.049839\n","\tRotated_Epoch:20 [003/005 (0400/0588)]\tLoss Ss: 0.051179\n","\tRotated_Epoch:20 [003/005 (0420/0588)]\tLoss Ss: 0.064068\n","\tRotated_Epoch:20 [003/005 (0440/0588)]\tLoss Ss: 0.072897\n","\tRotated_Epoch:20 [003/005 (0460/0588)]\tLoss Ss: 0.066169\n","\tRotated_Epoch:20 [003/005 (0480/0588)]\tLoss Ss: 0.045920\n","\tRotated_Epoch:20 [003/005 (0500/0588)]\tLoss Ss: 0.086406\n","\tRotated_Epoch:20 [003/005 (0520/0588)]\tLoss Ss: 0.074407\n","\tRotated_Epoch:20 [003/005 (0540/0588)]\tLoss Ss: 0.052382\n","\tRotated_Epoch:20 [003/005 (0560/0588)]\tLoss Ss: 0.055381\n","\tRotated_Epoch:20 [003/005 (0580/0588)]\tLoss Ss: 0.094651\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tRotated_Epoch:20 [004/005 (0000/0693)]\tLoss Ss: 0.055020\n","\tRotated_Epoch:20 [004/005 (0020/0693)]\tLoss Ss: 0.042819\n","\tRotated_Epoch:20 [004/005 (0040/0693)]\tLoss Ss: 0.020630\n","\tRotated_Epoch:20 [004/005 (0060/0693)]\tLoss Ss: 0.018986\n","\tRotated_Epoch:20 [004/005 (0080/0693)]\tLoss Ss: 0.029423\n","\tRotated_Epoch:20 [004/005 (0100/0693)]\tLoss Ss: 0.017731\n","\tRotated_Epoch:20 [004/005 (0120/0693)]\tLoss Ss: 0.022733\n","\tRotated_Epoch:20 [004/005 (0140/0693)]\tLoss Ss: 0.020985\n","\tRotated_Epoch:20 [004/005 (0160/0693)]\tLoss Ss: 0.020076\n","\tRotated_Epoch:20 [004/005 (0180/0693)]\tLoss Ss: 0.018263\n","\tRotated_Epoch:20 [004/005 (0200/0693)]\tLoss Ss: 0.022655\n","\tRotated_Epoch:20 [004/005 (0220/0693)]\tLoss Ss: 0.019677\n","\tRotated_Epoch:20 [004/005 (0240/0693)]\tLoss Ss: 0.017001\n","\tRotated_Epoch:20 [004/005 (0260/0693)]\tLoss Ss: 0.019624\n","\tRotated_Epoch:20 [004/005 (0280/0693)]\tLoss Ss: 0.018419\n","\tRotated_Epoch:20 [004/005 (0300/0693)]\tLoss Ss: 0.017438\n","\tRotated_Epoch:20 [004/005 (0320/0693)]\tLoss Ss: 0.021052\n","\tRotated_Epoch:20 [004/005 (0340/0693)]\tLoss Ss: 0.020148\n","\tRotated_Epoch:20 [004/005 (0360/0693)]\tLoss Ss: 0.016145\n","\tRotated_Epoch:20 [004/005 (0380/0693)]\tLoss Ss: 0.016902\n","\tRotated_Epoch:20 [004/005 (0400/0693)]\tLoss Ss: 0.022764\n","\tRotated_Epoch:20 [004/005 (0420/0693)]\tLoss Ss: 0.022266\n","\tRotated_Epoch:20 [004/005 (0440/0693)]\tLoss Ss: 0.016582\n","\tRotated_Epoch:20 [004/005 (0460/0693)]\tLoss Ss: 0.012340\n","\tRotated_Epoch:20 [004/005 (0480/0693)]\tLoss Ss: 0.020934\n","\tRotated_Epoch:20 [004/005 (0500/0693)]\tLoss Ss: 0.018765\n","\tRotated_Epoch:20 [004/005 (0520/0693)]\tLoss Ss: 0.017828\n","\tRotated_Epoch:20 [004/005 (0540/0693)]\tLoss Ss: 0.015343\n","\tRotated_Epoch:20 [004/005 (0560/0693)]\tLoss Ss: 0.014158\n","\tRotated_Epoch:20 [004/005 (0580/0693)]\tLoss Ss: 0.017742\n","\tRotated_Epoch:20 [004/005 (0600/0693)]\tLoss Ss: 0.011739\n","\tRotated_Epoch:20 [004/005 (0620/0693)]\tLoss Ss: 0.015224\n","\tRotated_Epoch:20 [004/005 (0640/0693)]\tLoss Ss: 0.012406\n","\tRotated_Epoch:20 [004/005 (0660/0693)]\tLoss Ss: 0.014200\n","\tRotated_Epoch:20 [004/005 (0680/0693)]\tLoss Ss: 0.015584\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tRotated_Epoch:20 [005/005 (0000/0614)]\tLoss Ss: 0.031182\n","\tRotated_Epoch:20 [005/005 (0020/0614)]\tLoss Ss: 0.037302\n","\tRotated_Epoch:20 [005/005 (0040/0614)]\tLoss Ss: 0.028169\n","\tRotated_Epoch:20 [005/005 (0060/0614)]\tLoss Ss: 0.013821\n","\tRotated_Epoch:20 [005/005 (0080/0614)]\tLoss Ss: 0.010091\n","\tRotated_Epoch:20 [005/005 (0100/0614)]\tLoss Ss: 0.012212\n","\tRotated_Epoch:20 [005/005 (0120/0614)]\tLoss Ss: 0.015447\n","\tRotated_Epoch:20 [005/005 (0140/0614)]\tLoss Ss: 0.008796\n","\tRotated_Epoch:20 [005/005 (0160/0614)]\tLoss Ss: 0.006635\n","\tRotated_Epoch:20 [005/005 (0180/0614)]\tLoss Ss: 0.010427\n","\tRotated_Epoch:20 [005/005 (0200/0614)]\tLoss Ss: 0.006727\n","\tRotated_Epoch:20 [005/005 (0220/0614)]\tLoss Ss: 0.010154\n","\tRotated_Epoch:20 [005/005 (0240/0614)]\tLoss Ss: 0.009872\n","\tRotated_Epoch:20 [005/005 (0260/0614)]\tLoss Ss: 0.005114\n","\tRotated_Epoch:20 [005/005 (0280/0614)]\tLoss Ss: 0.007866\n","\tRotated_Epoch:20 [005/005 (0300/0614)]\tLoss Ss: 0.007470\n","\tRotated_Epoch:20 [005/005 (0320/0614)]\tLoss Ss: 0.007633\n","\tRotated_Epoch:20 [005/005 (0340/0614)]\tLoss Ss: 0.006794\n","\tRotated_Epoch:20 [005/005 (0360/0614)]\tLoss Ss: 0.005627\n","\tRotated_Epoch:20 [005/005 (0380/0614)]\tLoss Ss: 0.010414\n","\tRotated_Epoch:20 [005/005 (0400/0614)]\tLoss Ss: 0.005963\n","\tRotated_Epoch:20 [005/005 (0420/0614)]\tLoss Ss: 0.004459\n","\tRotated_Epoch:20 [005/005 (0440/0614)]\tLoss Ss: 0.009793\n","\tRotated_Epoch:20 [005/005 (0460/0614)]\tLoss Ss: 0.008531\n","\tRotated_Epoch:20 [005/005 (0480/0614)]\tLoss Ss: 0.004071\n","\tRotated_Epoch:20 [005/005 (0500/0614)]\tLoss Ss: 0.012337\n","\tRotated_Epoch:20 [005/005 (0520/0614)]\tLoss Ss: 0.007102\n","\tRotated_Epoch:20 [005/005 (0540/0614)]\tLoss Ss: 0.007447\n","\tRotated_Epoch:20 [005/005 (0560/0614)]\tLoss Ss: 0.008668\n","\tRotated_Epoch:20 [005/005 (0580/0614)]\tLoss Ss: 0.007866\n","\tRotated_Epoch:20 [005/005 (0600/0614)]\tLoss Ss: 0.004743\n","new_1_6_d.nii\n","new_1_6_m.nii\n","new_1_7_d.nii\n","new_1_7_m.nii\n","new_2_5_d.nii\n","new_2_5_m.nii\n","new_2_6_d.nii\n","new_2_6_m.nii\n","new_2_7_d.nii\n","new_2_7_m.nii\n","new_3_d (2).nii\n","new_3_m (2).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (3).nii\n","new_3_m (3).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (7).nii\n","new_3_m (7).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (8).nii\n","new_3_m (8).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (9).nii\n","new_3_m (9).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_4_10_d_163526.nii\n","new_4_10_m_163526.nii.gz\n","new_4_11_d_084336.nii\n","new_4_11_m_084336.nii.gz\n","new_4_7_d_184422.nii\n","new_4_7_m_184422.nii.gz\n","new_4_8_d_143412.nii\n","new_4_8_m_143412.nii.gz\n","new_4_9_d_155347.nii\n","new_4_9_m_155347.nii.gz\n","\tEpoch: 20; Dice: 0.9611 +/- 0.0116; Loss: 11.7984\n","Begin Epoch 21\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tEpoch:21 [000/005 (0000/0614)]\tLoss Ss: 0.011939\n","\tEpoch:21 [000/005 (0020/0614)]\tLoss Ss: 0.012202\n","\tEpoch:21 [000/005 (0040/0614)]\tLoss Ss: 0.010432\n","\tEpoch:21 [000/005 (0060/0614)]\tLoss Ss: 0.007956\n","\tEpoch:21 [000/005 (0080/0614)]\tLoss Ss: 0.006999\n","\tEpoch:21 [000/005 (0100/0614)]\tLoss Ss: 0.007483\n","\tEpoch:21 [000/005 (0120/0614)]\tLoss Ss: 0.004858\n","\tEpoch:21 [000/005 (0140/0614)]\tLoss Ss: 0.007106\n","\tEpoch:21 [000/005 (0160/0614)]\tLoss Ss: 0.006827\n","\tEpoch:21 [000/005 (0180/0614)]\tLoss Ss: 0.008928\n","\tEpoch:21 [000/005 (0200/0614)]\tLoss Ss: 0.005562\n","\tEpoch:21 [000/005 (0220/0614)]\tLoss Ss: 0.006643\n","\tEpoch:21 [000/005 (0240/0614)]\tLoss Ss: 0.005175\n","\tEpoch:21 [000/005 (0260/0614)]\tLoss Ss: 0.006312\n","\tEpoch:21 [000/005 (0280/0614)]\tLoss Ss: 0.004892\n","\tEpoch:21 [000/005 (0300/0614)]\tLoss Ss: 0.006288\n","\tEpoch:21 [000/005 (0320/0614)]\tLoss Ss: 0.005835\n","\tEpoch:21 [000/005 (0340/0614)]\tLoss Ss: 0.008167\n","\tEpoch:21 [000/005 (0360/0614)]\tLoss Ss: 0.004764\n","\tEpoch:21 [000/005 (0380/0614)]\tLoss Ss: 0.006271\n","\tEpoch:21 [000/005 (0400/0614)]\tLoss Ss: 0.005940\n","\tEpoch:21 [000/005 (0420/0614)]\tLoss Ss: 0.005553\n","\tEpoch:21 [000/005 (0440/0614)]\tLoss Ss: 0.005651\n","\tEpoch:21 [000/005 (0460/0614)]\tLoss Ss: 0.006299\n","\tEpoch:21 [000/005 (0480/0614)]\tLoss Ss: 0.005859\n","\tEpoch:21 [000/005 (0500/0614)]\tLoss Ss: 0.005202\n","\tEpoch:21 [000/005 (0520/0614)]\tLoss Ss: 0.009897\n","\tEpoch:21 [000/005 (0540/0614)]\tLoss Ss: 0.005213\n","\tEpoch:21 [000/005 (0560/0614)]\tLoss Ss: 0.006439\n","\tEpoch:21 [000/005 (0580/0614)]\tLoss Ss: 0.006892\n","\tEpoch:21 [000/005 (0600/0614)]\tLoss Ss: 0.006971\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:21 [001/005 (0000/0755)]\tLoss Ss: 0.021381\n","\tEpoch:21 [001/005 (0020/0755)]\tLoss Ss: 0.025369\n","\tEpoch:21 [001/005 (0040/0755)]\tLoss Ss: 0.020361\n","\tEpoch:21 [001/005 (0060/0755)]\tLoss Ss: 0.020734\n","\tEpoch:21 [001/005 (0080/0755)]\tLoss Ss: 0.022704\n","\tEpoch:21 [001/005 (0100/0755)]\tLoss Ss: 0.026966\n","\tEpoch:21 [001/005 (0120/0755)]\tLoss Ss: 0.021504\n","\tEpoch:21 [001/005 (0140/0755)]\tLoss Ss: 0.015110\n","\tEpoch:21 [001/005 (0160/0755)]\tLoss Ss: 0.020437\n","\tEpoch:21 [001/005 (0180/0755)]\tLoss Ss: 0.016973\n","\tEpoch:21 [001/005 (0200/0755)]\tLoss Ss: 0.017400\n","\tEpoch:21 [001/005 (0220/0755)]\tLoss Ss: 0.025333\n","\tEpoch:21 [001/005 (0240/0755)]\tLoss Ss: 0.019199\n","\tEpoch:21 [001/005 (0260/0755)]\tLoss Ss: 0.018970\n","\tEpoch:21 [001/005 (0280/0755)]\tLoss Ss: 0.018751\n","\tEpoch:21 [001/005 (0300/0755)]\tLoss Ss: 0.014884\n","\tEpoch:21 [001/005 (0320/0755)]\tLoss Ss: 0.011834\n","\tEpoch:21 [001/005 (0340/0755)]\tLoss Ss: 0.018948\n","\tEpoch:21 [001/005 (0360/0755)]\tLoss Ss: 0.016978\n","\tEpoch:21 [001/005 (0380/0755)]\tLoss Ss: 0.016972\n","\tEpoch:21 [001/005 (0400/0755)]\tLoss Ss: 0.015737\n","\tEpoch:21 [001/005 (0420/0755)]\tLoss Ss: 0.013115\n","\tEpoch:21 [001/005 (0440/0755)]\tLoss Ss: 0.012428\n","\tEpoch:21 [001/005 (0460/0755)]\tLoss Ss: 0.011882\n","\tEpoch:21 [001/005 (0480/0755)]\tLoss Ss: 0.017023\n","\tEpoch:21 [001/005 (0500/0755)]\tLoss Ss: 0.023003\n","\tEpoch:21 [001/005 (0520/0755)]\tLoss Ss: 0.013208\n","\tEpoch:21 [001/005 (0540/0755)]\tLoss Ss: 0.013013\n","\tEpoch:21 [001/005 (0560/0755)]\tLoss Ss: 0.011969\n","\tEpoch:21 [001/005 (0580/0755)]\tLoss Ss: 0.018028\n","\tEpoch:21 [001/005 (0600/0755)]\tLoss Ss: 0.011213\n","\tEpoch:21 [001/005 (0620/0755)]\tLoss Ss: 0.015602\n","\tEpoch:21 [001/005 (0640/0755)]\tLoss Ss: 0.013774\n","\tEpoch:21 [001/005 (0660/0755)]\tLoss Ss: 0.011761\n","\tEpoch:21 [001/005 (0680/0755)]\tLoss Ss: 0.016582\n","\tEpoch:21 [001/005 (0700/0755)]\tLoss Ss: 0.011291\n","\tEpoch:21 [001/005 (0720/0755)]\tLoss Ss: 0.013979\n","\tEpoch:21 [001/005 (0740/0755)]\tLoss Ss: 0.017196\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:21 [002/005 (0000/0755)]\tLoss Ss: 0.027196\n","\tEpoch:21 [002/005 (0020/0755)]\tLoss Ss: 0.022129\n","\tEpoch:21 [002/005 (0040/0755)]\tLoss Ss: 0.022469\n","\tEpoch:21 [002/005 (0060/0755)]\tLoss Ss: 0.017168\n","\tEpoch:21 [002/005 (0080/0755)]\tLoss Ss: 0.021506\n","\tEpoch:21 [002/005 (0100/0755)]\tLoss Ss: 0.022767\n","\tEpoch:21 [002/005 (0120/0755)]\tLoss Ss: 0.021306\n","\tEpoch:21 [002/005 (0140/0755)]\tLoss Ss: 0.017191\n","\tEpoch:21 [002/005 (0160/0755)]\tLoss Ss: 0.016281\n","\tEpoch:21 [002/005 (0180/0755)]\tLoss Ss: 0.021486\n","\tEpoch:21 [002/005 (0200/0755)]\tLoss Ss: 0.010890\n","\tEpoch:21 [002/005 (0220/0755)]\tLoss Ss: 0.020156\n","\tEpoch:21 [002/005 (0240/0755)]\tLoss Ss: 0.014332\n","\tEpoch:21 [002/005 (0260/0755)]\tLoss Ss: 0.022905\n","\tEpoch:21 [002/005 (0280/0755)]\tLoss Ss: 0.032481\n","\tEpoch:21 [002/005 (0300/0755)]\tLoss Ss: 0.015160\n","\tEpoch:21 [002/005 (0320/0755)]\tLoss Ss: 0.021981\n","\tEpoch:21 [002/005 (0340/0755)]\tLoss Ss: 0.012943\n","\tEpoch:21 [002/005 (0360/0755)]\tLoss Ss: 0.016749\n","\tEpoch:21 [002/005 (0380/0755)]\tLoss Ss: 0.013522\n","\tEpoch:21 [002/005 (0400/0755)]\tLoss Ss: 0.012384\n","\tEpoch:21 [002/005 (0420/0755)]\tLoss Ss: 0.012735\n","\tEpoch:21 [002/005 (0440/0755)]\tLoss Ss: 0.015927\n","\tEpoch:21 [002/005 (0460/0755)]\tLoss Ss: 0.020016\n","\tEpoch:21 [002/005 (0480/0755)]\tLoss Ss: 0.016325\n","\tEpoch:21 [002/005 (0500/0755)]\tLoss Ss: 0.018758\n","\tEpoch:21 [002/005 (0520/0755)]\tLoss Ss: 0.016829\n","\tEpoch:21 [002/005 (0540/0755)]\tLoss Ss: 0.016711\n","\tEpoch:21 [002/005 (0560/0755)]\tLoss Ss: 0.016714\n","\tEpoch:21 [002/005 (0580/0755)]\tLoss Ss: 0.015494\n","\tEpoch:21 [002/005 (0600/0755)]\tLoss Ss: 0.012561\n","\tEpoch:21 [002/005 (0620/0755)]\tLoss Ss: 0.015747\n","\tEpoch:21 [002/005 (0640/0755)]\tLoss Ss: 0.014842\n","\tEpoch:21 [002/005 (0660/0755)]\tLoss Ss: 0.012574\n","\tEpoch:21 [002/005 (0680/0755)]\tLoss Ss: 0.018663\n","\tEpoch:21 [002/005 (0700/0755)]\tLoss Ss: 0.009059\n","\tEpoch:21 [002/005 (0720/0755)]\tLoss Ss: 0.023415\n","\tEpoch:21 [002/005 (0740/0755)]\tLoss Ss: 0.011778\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tEpoch:21 [003/005 (0000/0588)]\tLoss Ss: 0.009036\n","\tEpoch:21 [003/005 (0020/0588)]\tLoss Ss: 0.007974\n","\tEpoch:21 [003/005 (0040/0588)]\tLoss Ss: 0.005991\n","\tEpoch:21 [003/005 (0060/0588)]\tLoss Ss: 0.006069\n","\tEpoch:21 [003/005 (0080/0588)]\tLoss Ss: 0.008406\n","\tEpoch:21 [003/005 (0100/0588)]\tLoss Ss: 0.006359\n","\tEpoch:21 [003/005 (0120/0588)]\tLoss Ss: 0.007737\n","\tEpoch:21 [003/005 (0140/0588)]\tLoss Ss: 0.012887\n","\tEpoch:21 [003/005 (0160/0588)]\tLoss Ss: 0.006066\n","\tEpoch:21 [003/005 (0180/0588)]\tLoss Ss: 0.005353\n","\tEpoch:21 [003/005 (0200/0588)]\tLoss Ss: 0.005376\n","\tEpoch:21 [003/005 (0220/0588)]\tLoss Ss: 0.007555\n","\tEpoch:21 [003/005 (0240/0588)]\tLoss Ss: 0.007080\n","\tEpoch:21 [003/005 (0260/0588)]\tLoss Ss: 0.007424\n","\tEpoch:21 [003/005 (0280/0588)]\tLoss Ss: 0.005666\n","\tEpoch:21 [003/005 (0300/0588)]\tLoss Ss: 0.004663\n","\tEpoch:21 [003/005 (0320/0588)]\tLoss Ss: 0.006535\n","\tEpoch:21 [003/005 (0340/0588)]\tLoss Ss: 0.007236\n","\tEpoch:21 [003/005 (0360/0588)]\tLoss Ss: 0.004450\n","\tEpoch:21 [003/005 (0380/0588)]\tLoss Ss: 0.005202\n","\tEpoch:21 [003/005 (0400/0588)]\tLoss Ss: 0.003928\n","\tEpoch:21 [003/005 (0420/0588)]\tLoss Ss: 0.004440\n","\tEpoch:21 [003/005 (0440/0588)]\tLoss Ss: 0.004544\n","\tEpoch:21 [003/005 (0460/0588)]\tLoss Ss: 0.004573\n","\tEpoch:21 [003/005 (0480/0588)]\tLoss Ss: 0.006117\n","\tEpoch:21 [003/005 (0500/0588)]\tLoss Ss: 0.004092\n","\tEpoch:21 [003/005 (0520/0588)]\tLoss Ss: 0.004481\n","\tEpoch:21 [003/005 (0540/0588)]\tLoss Ss: 0.003691\n","\tEpoch:21 [003/005 (0560/0588)]\tLoss Ss: 0.004218\n","\tEpoch:21 [003/005 (0580/0588)]\tLoss Ss: 0.003101\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tEpoch:21 [004/005 (0000/0693)]\tLoss Ss: 0.016616\n","\tEpoch:21 [004/005 (0020/0693)]\tLoss Ss: 0.016124\n","\tEpoch:21 [004/005 (0040/0693)]\tLoss Ss: 0.012343\n","\tEpoch:21 [004/005 (0060/0693)]\tLoss Ss: 0.016096\n","\tEpoch:21 [004/005 (0080/0693)]\tLoss Ss: 0.022178\n","\tEpoch:21 [004/005 (0100/0693)]\tLoss Ss: 0.013239\n","\tEpoch:21 [004/005 (0120/0693)]\tLoss Ss: 0.012080\n","\tEpoch:21 [004/005 (0140/0693)]\tLoss Ss: 0.010541\n","\tEpoch:21 [004/005 (0160/0693)]\tLoss Ss: 0.011601\n","\tEpoch:21 [004/005 (0180/0693)]\tLoss Ss: 0.012191\n","\tEpoch:21 [004/005 (0200/0693)]\tLoss Ss: 0.010516\n","\tEpoch:21 [004/005 (0220/0693)]\tLoss Ss: 0.011134\n","\tEpoch:21 [004/005 (0240/0693)]\tLoss Ss: 0.012270\n","\tEpoch:21 [004/005 (0260/0693)]\tLoss Ss: 0.012990\n","\tEpoch:21 [004/005 (0280/0693)]\tLoss Ss: 0.012077\n","\tEpoch:21 [004/005 (0300/0693)]\tLoss Ss: 0.008028\n","\tEpoch:21 [004/005 (0320/0693)]\tLoss Ss: 0.009408\n","\tEpoch:21 [004/005 (0340/0693)]\tLoss Ss: 0.014296\n","\tEpoch:21 [004/005 (0360/0693)]\tLoss Ss: 0.009800\n","\tEpoch:21 [004/005 (0380/0693)]\tLoss Ss: 0.020246\n","\tEpoch:21 [004/005 (0400/0693)]\tLoss Ss: 0.017930\n","\tEpoch:21 [004/005 (0420/0693)]\tLoss Ss: 0.013412\n","\tEpoch:21 [004/005 (0440/0693)]\tLoss Ss: 0.012004\n","\tEpoch:21 [004/005 (0460/0693)]\tLoss Ss: 0.016682\n","\tEpoch:21 [004/005 (0480/0693)]\tLoss Ss: 0.007623\n","\tEpoch:21 [004/005 (0500/0693)]\tLoss Ss: 0.016686\n","\tEpoch:21 [004/005 (0520/0693)]\tLoss Ss: 0.018907\n","\tEpoch:21 [004/005 (0540/0693)]\tLoss Ss: 0.013144\n","\tEpoch:21 [004/005 (0560/0693)]\tLoss Ss: 0.011358\n","\tEpoch:21 [004/005 (0580/0693)]\tLoss Ss: 0.010378\n","\tEpoch:21 [004/005 (0600/0693)]\tLoss Ss: 0.020065\n","\tEpoch:21 [004/005 (0620/0693)]\tLoss Ss: 0.011423\n","\tEpoch:21 [004/005 (0640/0693)]\tLoss Ss: 0.013367\n","\tEpoch:21 [004/005 (0660/0693)]\tLoss Ss: 0.010711\n","\tEpoch:21 [004/005 (0680/0693)]\tLoss Ss: 0.013540\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tEpoch:21 [005/005 (0000/0693)]\tLoss Ss: 0.014055\n","\tEpoch:21 [005/005 (0020/0693)]\tLoss Ss: 0.010266\n","\tEpoch:21 [005/005 (0040/0693)]\tLoss Ss: 0.015943\n","\tEpoch:21 [005/005 (0060/0693)]\tLoss Ss: 0.015352\n","\tEpoch:21 [005/005 (0080/0693)]\tLoss Ss: 0.018741\n","\tEpoch:21 [005/005 (0100/0693)]\tLoss Ss: 0.015341\n","\tEpoch:21 [005/005 (0120/0693)]\tLoss Ss: 0.012710\n","\tEpoch:21 [005/005 (0140/0693)]\tLoss Ss: 0.009173\n","\tEpoch:21 [005/005 (0160/0693)]\tLoss Ss: 0.010394\n","\tEpoch:21 [005/005 (0180/0693)]\tLoss Ss: 0.013790\n","\tEpoch:21 [005/005 (0200/0693)]\tLoss Ss: 0.015920\n","\tEpoch:21 [005/005 (0220/0693)]\tLoss Ss: 0.014978\n","\tEpoch:21 [005/005 (0240/0693)]\tLoss Ss: 0.013542\n","\tEpoch:21 [005/005 (0260/0693)]\tLoss Ss: 0.015077\n","\tEpoch:21 [005/005 (0280/0693)]\tLoss Ss: 0.017448\n","\tEpoch:21 [005/005 (0300/0693)]\tLoss Ss: 0.015143\n","\tEpoch:21 [005/005 (0320/0693)]\tLoss Ss: 0.013074\n","\tEpoch:21 [005/005 (0340/0693)]\tLoss Ss: 0.016763\n","\tEpoch:21 [005/005 (0360/0693)]\tLoss Ss: 0.015860\n","\tEpoch:21 [005/005 (0380/0693)]\tLoss Ss: 0.017966\n","\tEpoch:21 [005/005 (0400/0693)]\tLoss Ss: 0.014952\n","\tEpoch:21 [005/005 (0420/0693)]\tLoss Ss: 0.017554\n","\tEpoch:21 [005/005 (0440/0693)]\tLoss Ss: 0.013254\n","\tEpoch:21 [005/005 (0460/0693)]\tLoss Ss: 0.011405\n","\tEpoch:21 [005/005 (0480/0693)]\tLoss Ss: 0.016409\n","\tEpoch:21 [005/005 (0500/0693)]\tLoss Ss: 0.017277\n","\tEpoch:21 [005/005 (0520/0693)]\tLoss Ss: 0.017811\n","\tEpoch:21 [005/005 (0540/0693)]\tLoss Ss: 0.011212\n","\tEpoch:21 [005/005 (0560/0693)]\tLoss Ss: 0.018143\n","\tEpoch:21 [005/005 (0580/0693)]\tLoss Ss: 0.021880\n","\tEpoch:21 [005/005 (0600/0693)]\tLoss Ss: 0.013475\n","\tEpoch:21 [005/005 (0620/0693)]\tLoss Ss: 0.012744\n","\tEpoch:21 [005/005 (0640/0693)]\tLoss Ss: 0.009804\n","\tEpoch:21 [005/005 (0660/0693)]\tLoss Ss: 0.019025\n","\tEpoch:21 [005/005 (0680/0693)]\tLoss Ss: 0.014154\n","Now train the rotated image\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tRotated_Epoch:21 [000/005 (0000/0693)]\tLoss Ss: 0.018603\n","\tRotated_Epoch:21 [000/005 (0020/0693)]\tLoss Ss: 0.021466\n","\tRotated_Epoch:21 [000/005 (0040/0693)]\tLoss Ss: 0.018326\n","\tRotated_Epoch:21 [000/005 (0060/0693)]\tLoss Ss: 0.021586\n","\tRotated_Epoch:21 [000/005 (0080/0693)]\tLoss Ss: 0.014689\n","\tRotated_Epoch:21 [000/005 (0100/0693)]\tLoss Ss: 0.018958\n","\tRotated_Epoch:21 [000/005 (0120/0693)]\tLoss Ss: 0.014545\n","\tRotated_Epoch:21 [000/005 (0140/0693)]\tLoss Ss: 0.018498\n","\tRotated_Epoch:21 [000/005 (0160/0693)]\tLoss Ss: 0.016386\n","\tRotated_Epoch:21 [000/005 (0180/0693)]\tLoss Ss: 0.012677\n","\tRotated_Epoch:21 [000/005 (0200/0693)]\tLoss Ss: 0.012790\n","\tRotated_Epoch:21 [000/005 (0220/0693)]\tLoss Ss: 0.013264\n","\tRotated_Epoch:21 [000/005 (0240/0693)]\tLoss Ss: 0.016368\n","\tRotated_Epoch:21 [000/005 (0260/0693)]\tLoss Ss: 0.015331\n","\tRotated_Epoch:21 [000/005 (0280/0693)]\tLoss Ss: 0.012126\n","\tRotated_Epoch:21 [000/005 (0300/0693)]\tLoss Ss: 0.012528\n","\tRotated_Epoch:21 [000/005 (0320/0693)]\tLoss Ss: 0.012592\n","\tRotated_Epoch:21 [000/005 (0340/0693)]\tLoss Ss: 0.013456\n","\tRotated_Epoch:21 [000/005 (0360/0693)]\tLoss Ss: 0.018023\n","\tRotated_Epoch:21 [000/005 (0380/0693)]\tLoss Ss: 0.025283\n","\tRotated_Epoch:21 [000/005 (0400/0693)]\tLoss Ss: 0.015579\n","\tRotated_Epoch:21 [000/005 (0420/0693)]\tLoss Ss: 0.015757\n","\tRotated_Epoch:21 [000/005 (0440/0693)]\tLoss Ss: 0.014035\n","\tRotated_Epoch:21 [000/005 (0460/0693)]\tLoss Ss: 0.015760\n","\tRotated_Epoch:21 [000/005 (0480/0693)]\tLoss Ss: 0.010351\n","\tRotated_Epoch:21 [000/005 (0500/0693)]\tLoss Ss: 0.008707\n","\tRotated_Epoch:21 [000/005 (0520/0693)]\tLoss Ss: 0.014296\n","\tRotated_Epoch:21 [000/005 (0540/0693)]\tLoss Ss: 0.011773\n","\tRotated_Epoch:21 [000/005 (0560/0693)]\tLoss Ss: 0.016091\n","\tRotated_Epoch:21 [000/005 (0580/0693)]\tLoss Ss: 0.015787\n","\tRotated_Epoch:21 [000/005 (0600/0693)]\tLoss Ss: 0.012741\n","\tRotated_Epoch:21 [000/005 (0620/0693)]\tLoss Ss: 0.011244\n","\tRotated_Epoch:21 [000/005 (0640/0693)]\tLoss Ss: 0.017011\n","\tRotated_Epoch:21 [000/005 (0660/0693)]\tLoss Ss: 0.009734\n","\tRotated_Epoch:21 [000/005 (0680/0693)]\tLoss Ss: 0.014928\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tRotated_Epoch:21 [001/005 (0000/0693)]\tLoss Ss: 0.012923\n","\tRotated_Epoch:21 [001/005 (0020/0693)]\tLoss Ss: 0.011573\n","\tRotated_Epoch:21 [001/005 (0040/0693)]\tLoss Ss: 0.023749\n","\tRotated_Epoch:21 [001/005 (0060/0693)]\tLoss Ss: 0.010939\n","\tRotated_Epoch:21 [001/005 (0080/0693)]\tLoss Ss: 0.010458\n","\tRotated_Epoch:21 [001/005 (0100/0693)]\tLoss Ss: 0.014454\n","\tRotated_Epoch:21 [001/005 (0120/0693)]\tLoss Ss: 0.013445\n","\tRotated_Epoch:21 [001/005 (0140/0693)]\tLoss Ss: 0.013832\n","\tRotated_Epoch:21 [001/005 (0160/0693)]\tLoss Ss: 0.016687\n","\tRotated_Epoch:21 [001/005 (0180/0693)]\tLoss Ss: 0.021625\n","\tRotated_Epoch:21 [001/005 (0200/0693)]\tLoss Ss: 0.015761\n","\tRotated_Epoch:21 [001/005 (0220/0693)]\tLoss Ss: 0.014123\n","\tRotated_Epoch:21 [001/005 (0240/0693)]\tLoss Ss: 0.018618\n","\tRotated_Epoch:21 [001/005 (0260/0693)]\tLoss Ss: 0.015102\n","\tRotated_Epoch:21 [001/005 (0280/0693)]\tLoss Ss: 0.017768\n","\tRotated_Epoch:21 [001/005 (0300/0693)]\tLoss Ss: 0.014669\n","\tRotated_Epoch:21 [001/005 (0320/0693)]\tLoss Ss: 0.014324\n","\tRotated_Epoch:21 [001/005 (0340/0693)]\tLoss Ss: 0.020632\n","\tRotated_Epoch:21 [001/005 (0360/0693)]\tLoss Ss: 0.021203\n","\tRotated_Epoch:21 [001/005 (0380/0693)]\tLoss Ss: 0.015516\n","\tRotated_Epoch:21 [001/005 (0400/0693)]\tLoss Ss: 0.013503\n","\tRotated_Epoch:21 [001/005 (0420/0693)]\tLoss Ss: 0.011920\n","\tRotated_Epoch:21 [001/005 (0440/0693)]\tLoss Ss: 0.014480\n","\tRotated_Epoch:21 [001/005 (0460/0693)]\tLoss Ss: 0.010996\n","\tRotated_Epoch:21 [001/005 (0480/0693)]\tLoss Ss: 0.017611\n","\tRotated_Epoch:21 [001/005 (0500/0693)]\tLoss Ss: 0.013923\n","\tRotated_Epoch:21 [001/005 (0520/0693)]\tLoss Ss: 0.016524\n","\tRotated_Epoch:21 [001/005 (0540/0693)]\tLoss Ss: 0.020170\n","\tRotated_Epoch:21 [001/005 (0560/0693)]\tLoss Ss: 0.016677\n","\tRotated_Epoch:21 [001/005 (0580/0693)]\tLoss Ss: 0.021399\n","\tRotated_Epoch:21 [001/005 (0600/0693)]\tLoss Ss: 0.021012\n","\tRotated_Epoch:21 [001/005 (0620/0693)]\tLoss Ss: 0.018292\n","\tRotated_Epoch:21 [001/005 (0640/0693)]\tLoss Ss: 0.012007\n","\tRotated_Epoch:21 [001/005 (0660/0693)]\tLoss Ss: 0.019713\n","\tRotated_Epoch:21 [001/005 (0680/0693)]\tLoss Ss: 0.014012\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:21 [002/005 (0000/0755)]\tLoss Ss: 0.233157\n","\tRotated_Epoch:21 [002/005 (0020/0755)]\tLoss Ss: 0.207389\n","\tRotated_Epoch:21 [002/005 (0040/0755)]\tLoss Ss: 0.145989\n","\tRotated_Epoch:21 [002/005 (0060/0755)]\tLoss Ss: 0.161527\n","\tRotated_Epoch:21 [002/005 (0080/0755)]\tLoss Ss: 0.129033\n","\tRotated_Epoch:21 [002/005 (0100/0755)]\tLoss Ss: 0.057044\n","\tRotated_Epoch:21 [002/005 (0120/0755)]\tLoss Ss: 0.077638\n","\tRotated_Epoch:21 [002/005 (0140/0755)]\tLoss Ss: 0.087686\n","\tRotated_Epoch:21 [002/005 (0160/0755)]\tLoss Ss: 0.174282\n","\tRotated_Epoch:21 [002/005 (0180/0755)]\tLoss Ss: 0.077316\n","\tRotated_Epoch:21 [002/005 (0200/0755)]\tLoss Ss: 0.077533\n","\tRotated_Epoch:21 [002/005 (0220/0755)]\tLoss Ss: 0.088793\n","\tRotated_Epoch:21 [002/005 (0240/0755)]\tLoss Ss: 0.069460\n","\tRotated_Epoch:21 [002/005 (0260/0755)]\tLoss Ss: 0.054874\n","\tRotated_Epoch:21 [002/005 (0280/0755)]\tLoss Ss: 0.067843\n","\tRotated_Epoch:21 [002/005 (0300/0755)]\tLoss Ss: 0.095425\n","\tRotated_Epoch:21 [002/005 (0320/0755)]\tLoss Ss: 0.058048\n","\tRotated_Epoch:21 [002/005 (0340/0755)]\tLoss Ss: 0.056439\n","\tRotated_Epoch:21 [002/005 (0360/0755)]\tLoss Ss: 0.053514\n","\tRotated_Epoch:21 [002/005 (0380/0755)]\tLoss Ss: 0.061628\n","\tRotated_Epoch:21 [002/005 (0400/0755)]\tLoss Ss: 0.072666\n","\tRotated_Epoch:21 [002/005 (0420/0755)]\tLoss Ss: 0.056101\n","\tRotated_Epoch:21 [002/005 (0440/0755)]\tLoss Ss: 0.079354\n","\tRotated_Epoch:21 [002/005 (0460/0755)]\tLoss Ss: 0.065661\n","\tRotated_Epoch:21 [002/005 (0480/0755)]\tLoss Ss: 0.054797\n","\tRotated_Epoch:21 [002/005 (0500/0755)]\tLoss Ss: 0.031554\n","\tRotated_Epoch:21 [002/005 (0520/0755)]\tLoss Ss: 0.049026\n","\tRotated_Epoch:21 [002/005 (0540/0755)]\tLoss Ss: 0.043574\n","\tRotated_Epoch:21 [002/005 (0560/0755)]\tLoss Ss: 0.046422\n","\tRotated_Epoch:21 [002/005 (0580/0755)]\tLoss Ss: 0.045772\n","\tRotated_Epoch:21 [002/005 (0600/0755)]\tLoss Ss: 0.080901\n","\tRotated_Epoch:21 [002/005 (0620/0755)]\tLoss Ss: 0.035513\n","\tRotated_Epoch:21 [002/005 (0640/0755)]\tLoss Ss: 0.040220\n","\tRotated_Epoch:21 [002/005 (0660/0755)]\tLoss Ss: 0.034615\n","\tRotated_Epoch:21 [002/005 (0680/0755)]\tLoss Ss: 0.047716\n","\tRotated_Epoch:21 [002/005 (0700/0755)]\tLoss Ss: 0.044731\n","\tRotated_Epoch:21 [002/005 (0720/0755)]\tLoss Ss: 0.041722\n","\tRotated_Epoch:21 [002/005 (0740/0755)]\tLoss Ss: 0.044792\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tRotated_Epoch:21 [003/005 (0000/0614)]\tLoss Ss: 0.026112\n","\tRotated_Epoch:21 [003/005 (0020/0614)]\tLoss Ss: 0.020675\n","\tRotated_Epoch:21 [003/005 (0040/0614)]\tLoss Ss: 0.016419\n","\tRotated_Epoch:21 [003/005 (0060/0614)]\tLoss Ss: 0.019500\n","\tRotated_Epoch:21 [003/005 (0080/0614)]\tLoss Ss: 0.020080\n","\tRotated_Epoch:21 [003/005 (0100/0614)]\tLoss Ss: 0.012406\n","\tRotated_Epoch:21 [003/005 (0120/0614)]\tLoss Ss: 0.018645\n","\tRotated_Epoch:21 [003/005 (0140/0614)]\tLoss Ss: 0.017416\n","\tRotated_Epoch:21 [003/005 (0160/0614)]\tLoss Ss: 0.008120\n","\tRotated_Epoch:21 [003/005 (0180/0614)]\tLoss Ss: 0.009255\n","\tRotated_Epoch:21 [003/005 (0200/0614)]\tLoss Ss: 0.007790\n","\tRotated_Epoch:21 [003/005 (0220/0614)]\tLoss Ss: 0.009501\n","\tRotated_Epoch:21 [003/005 (0240/0614)]\tLoss Ss: 0.006832\n","\tRotated_Epoch:21 [003/005 (0260/0614)]\tLoss Ss: 0.011395\n","\tRotated_Epoch:21 [003/005 (0280/0614)]\tLoss Ss: 0.007284\n","\tRotated_Epoch:21 [003/005 (0300/0614)]\tLoss Ss: 0.011606\n","\tRotated_Epoch:21 [003/005 (0320/0614)]\tLoss Ss: 0.012444\n","\tRotated_Epoch:21 [003/005 (0340/0614)]\tLoss Ss: 0.018202\n","\tRotated_Epoch:21 [003/005 (0360/0614)]\tLoss Ss: 0.005834\n","\tRotated_Epoch:21 [003/005 (0380/0614)]\tLoss Ss: 0.007124\n","\tRotated_Epoch:21 [003/005 (0400/0614)]\tLoss Ss: 0.004910\n","\tRotated_Epoch:21 [003/005 (0420/0614)]\tLoss Ss: 0.011076\n","\tRotated_Epoch:21 [003/005 (0440/0614)]\tLoss Ss: 0.004719\n","\tRotated_Epoch:21 [003/005 (0460/0614)]\tLoss Ss: 0.007580\n","\tRotated_Epoch:21 [003/005 (0480/0614)]\tLoss Ss: 0.006746\n","\tRotated_Epoch:21 [003/005 (0500/0614)]\tLoss Ss: 0.005940\n","\tRotated_Epoch:21 [003/005 (0520/0614)]\tLoss Ss: 0.008139\n","\tRotated_Epoch:21 [003/005 (0540/0614)]\tLoss Ss: 0.008008\n","\tRotated_Epoch:21 [003/005 (0560/0614)]\tLoss Ss: 0.006105\n","\tRotated_Epoch:21 [003/005 (0580/0614)]\tLoss Ss: 0.004671\n","\tRotated_Epoch:21 [003/005 (0600/0614)]\tLoss Ss: 0.005331\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:21 [004/005 (0000/0755)]\tLoss Ss: 0.118017\n","\tRotated_Epoch:21 [004/005 (0020/0755)]\tLoss Ss: 0.123211\n","\tRotated_Epoch:21 [004/005 (0040/0755)]\tLoss Ss: 0.083400\n","\tRotated_Epoch:21 [004/005 (0060/0755)]\tLoss Ss: 0.049025\n","\tRotated_Epoch:21 [004/005 (0080/0755)]\tLoss Ss: 0.053299\n","\tRotated_Epoch:21 [004/005 (0100/0755)]\tLoss Ss: 0.066196\n","\tRotated_Epoch:21 [004/005 (0120/0755)]\tLoss Ss: 0.048274\n","\tRotated_Epoch:21 [004/005 (0140/0755)]\tLoss Ss: 0.034745\n","\tRotated_Epoch:21 [004/005 (0160/0755)]\tLoss Ss: 0.060563\n","\tRotated_Epoch:21 [004/005 (0180/0755)]\tLoss Ss: 0.041687\n","\tRotated_Epoch:21 [004/005 (0200/0755)]\tLoss Ss: 0.036235\n","\tRotated_Epoch:21 [004/005 (0220/0755)]\tLoss Ss: 0.032565\n","\tRotated_Epoch:21 [004/005 (0240/0755)]\tLoss Ss: 0.038507\n","\tRotated_Epoch:21 [004/005 (0260/0755)]\tLoss Ss: 0.056255\n","\tRotated_Epoch:21 [004/005 (0280/0755)]\tLoss Ss: 0.027741\n","\tRotated_Epoch:21 [004/005 (0300/0755)]\tLoss Ss: 0.029061\n","\tRotated_Epoch:21 [004/005 (0320/0755)]\tLoss Ss: 0.034925\n","\tRotated_Epoch:21 [004/005 (0340/0755)]\tLoss Ss: 0.028002\n","\tRotated_Epoch:21 [004/005 (0360/0755)]\tLoss Ss: 0.032229\n","\tRotated_Epoch:21 [004/005 (0380/0755)]\tLoss Ss: 0.036375\n","\tRotated_Epoch:21 [004/005 (0400/0755)]\tLoss Ss: 0.026604\n","\tRotated_Epoch:21 [004/005 (0420/0755)]\tLoss Ss: 0.019553\n","\tRotated_Epoch:21 [004/005 (0440/0755)]\tLoss Ss: 0.026606\n","\tRotated_Epoch:21 [004/005 (0460/0755)]\tLoss Ss: 0.025472\n","\tRotated_Epoch:21 [004/005 (0480/0755)]\tLoss Ss: 0.021619\n","\tRotated_Epoch:21 [004/005 (0500/0755)]\tLoss Ss: 0.026576\n","\tRotated_Epoch:21 [004/005 (0520/0755)]\tLoss Ss: 0.023506\n","\tRotated_Epoch:21 [004/005 (0540/0755)]\tLoss Ss: 0.032006\n","\tRotated_Epoch:21 [004/005 (0560/0755)]\tLoss Ss: 0.014527\n","\tRotated_Epoch:21 [004/005 (0580/0755)]\tLoss Ss: 0.026500\n","\tRotated_Epoch:21 [004/005 (0600/0755)]\tLoss Ss: 0.014628\n","\tRotated_Epoch:21 [004/005 (0620/0755)]\tLoss Ss: 0.018164\n","\tRotated_Epoch:21 [004/005 (0640/0755)]\tLoss Ss: 0.023772\n","\tRotated_Epoch:21 [004/005 (0660/0755)]\tLoss Ss: 0.016850\n","\tRotated_Epoch:21 [004/005 (0680/0755)]\tLoss Ss: 0.018632\n","\tRotated_Epoch:21 [004/005 (0700/0755)]\tLoss Ss: 0.027203\n","\tRotated_Epoch:21 [004/005 (0720/0755)]\tLoss Ss: 0.026501\n","\tRotated_Epoch:21 [004/005 (0740/0755)]\tLoss Ss: 0.016580\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tRotated_Epoch:21 [005/005 (0000/0588)]\tLoss Ss: 0.287237\n","\tRotated_Epoch:21 [005/005 (0020/0588)]\tLoss Ss: 0.127127\n","\tRotated_Epoch:21 [005/005 (0040/0588)]\tLoss Ss: 0.089031\n","\tRotated_Epoch:21 [005/005 (0060/0588)]\tLoss Ss: 0.057802\n","\tRotated_Epoch:21 [005/005 (0080/0588)]\tLoss Ss: 0.067690\n","\tRotated_Epoch:21 [005/005 (0100/0588)]\tLoss Ss: 0.048939\n","\tRotated_Epoch:21 [005/005 (0120/0588)]\tLoss Ss: 0.064361\n","\tRotated_Epoch:21 [005/005 (0140/0588)]\tLoss Ss: 0.061231\n","\tRotated_Epoch:21 [005/005 (0160/0588)]\tLoss Ss: 0.062359\n","\tRotated_Epoch:21 [005/005 (0180/0588)]\tLoss Ss: 0.064950\n","\tRotated_Epoch:21 [005/005 (0200/0588)]\tLoss Ss: 0.051334\n","\tRotated_Epoch:21 [005/005 (0220/0588)]\tLoss Ss: 0.058791\n","\tRotated_Epoch:21 [005/005 (0240/0588)]\tLoss Ss: 0.046398\n","\tRotated_Epoch:21 [005/005 (0260/0588)]\tLoss Ss: 0.076254\n","\tRotated_Epoch:21 [005/005 (0280/0588)]\tLoss Ss: 0.079543\n","\tRotated_Epoch:21 [005/005 (0300/0588)]\tLoss Ss: 0.053806\n","\tRotated_Epoch:21 [005/005 (0320/0588)]\tLoss Ss: 0.039475\n","\tRotated_Epoch:21 [005/005 (0340/0588)]\tLoss Ss: 0.064215\n","\tRotated_Epoch:21 [005/005 (0360/0588)]\tLoss Ss: 0.070583\n","\tRotated_Epoch:21 [005/005 (0380/0588)]\tLoss Ss: 0.078021\n","\tRotated_Epoch:21 [005/005 (0400/0588)]\tLoss Ss: 0.071788\n","\tRotated_Epoch:21 [005/005 (0420/0588)]\tLoss Ss: 0.046204\n","\tRotated_Epoch:21 [005/005 (0440/0588)]\tLoss Ss: 0.071973\n","\tRotated_Epoch:21 [005/005 (0460/0588)]\tLoss Ss: 0.054090\n","\tRotated_Epoch:21 [005/005 (0480/0588)]\tLoss Ss: 0.047745\n","\tRotated_Epoch:21 [005/005 (0500/0588)]\tLoss Ss: 0.055411\n","\tRotated_Epoch:21 [005/005 (0520/0588)]\tLoss Ss: 0.040537\n","\tRotated_Epoch:21 [005/005 (0540/0588)]\tLoss Ss: 0.066712\n","\tRotated_Epoch:21 [005/005 (0560/0588)]\tLoss Ss: 0.063717\n","\tRotated_Epoch:21 [005/005 (0580/0588)]\tLoss Ss: 0.032793\n","new_1_6_d.nii\n","new_1_6_m.nii\n","new_1_7_d.nii\n","new_1_7_m.nii\n","new_2_5_d.nii\n","new_2_5_m.nii\n","new_2_6_d.nii\n","new_2_6_m.nii\n","new_2_7_d.nii\n","new_2_7_m.nii\n","new_3_d (2).nii\n","new_3_m (2).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (3).nii\n","new_3_m (3).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (7).nii\n","new_3_m (7).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (8).nii\n","new_3_m (8).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (9).nii\n","new_3_m (9).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_4_10_d_163526.nii\n","new_4_10_m_163526.nii.gz\n","new_4_11_d_084336.nii\n","new_4_11_m_084336.nii.gz\n","new_4_7_d_184422.nii\n","new_4_7_m_184422.nii.gz\n","new_4_8_d_143412.nii\n","new_4_8_m_143412.nii.gz\n","new_4_9_d_155347.nii\n","new_4_9_m_155347.nii.gz\n","\tEpoch: 21; Dice: 0.6671 +/- 0.1030; Loss: 10.6255\n","Begin Epoch 22\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tEpoch:22 [000/005 (0000/0693)]\tLoss Ss: 0.132907\n","\tEpoch:22 [000/005 (0020/0693)]\tLoss Ss: 0.137845\n","\tEpoch:22 [000/005 (0040/0693)]\tLoss Ss: 0.080258\n","\tEpoch:22 [000/005 (0060/0693)]\tLoss Ss: 0.037920\n","\tEpoch:22 [000/005 (0080/0693)]\tLoss Ss: 0.031321\n","\tEpoch:22 [000/005 (0100/0693)]\tLoss Ss: 0.022059\n","\tEpoch:22 [000/005 (0120/0693)]\tLoss Ss: 0.029990\n","\tEpoch:22 [000/005 (0140/0693)]\tLoss Ss: 0.033905\n","\tEpoch:22 [000/005 (0160/0693)]\tLoss Ss: 0.027105\n","\tEpoch:22 [000/005 (0180/0693)]\tLoss Ss: 0.028790\n","\tEpoch:22 [000/005 (0200/0693)]\tLoss Ss: 0.026298\n","\tEpoch:22 [000/005 (0220/0693)]\tLoss Ss: 0.025176\n","\tEpoch:22 [000/005 (0240/0693)]\tLoss Ss: 0.026693\n","\tEpoch:22 [000/005 (0260/0693)]\tLoss Ss: 0.029447\n","\tEpoch:22 [000/005 (0280/0693)]\tLoss Ss: 0.019202\n","\tEpoch:22 [000/005 (0300/0693)]\tLoss Ss: 0.021613\n","\tEpoch:22 [000/005 (0320/0693)]\tLoss Ss: 0.020554\n","\tEpoch:22 [000/005 (0340/0693)]\tLoss Ss: 0.016570\n","\tEpoch:22 [000/005 (0360/0693)]\tLoss Ss: 0.014226\n","\tEpoch:22 [000/005 (0380/0693)]\tLoss Ss: 0.019984\n","\tEpoch:22 [000/005 (0400/0693)]\tLoss Ss: 0.014763\n","\tEpoch:22 [000/005 (0420/0693)]\tLoss Ss: 0.021502\n","\tEpoch:22 [000/005 (0440/0693)]\tLoss Ss: 0.019564\n","\tEpoch:22 [000/005 (0460/0693)]\tLoss Ss: 0.013386\n","\tEpoch:22 [000/005 (0480/0693)]\tLoss Ss: 0.013593\n","\tEpoch:22 [000/005 (0500/0693)]\tLoss Ss: 0.018311\n","\tEpoch:22 [000/005 (0520/0693)]\tLoss Ss: 0.015780\n","\tEpoch:22 [000/005 (0540/0693)]\tLoss Ss: 0.017659\n","\tEpoch:22 [000/005 (0560/0693)]\tLoss Ss: 0.015274\n","\tEpoch:22 [000/005 (0580/0693)]\tLoss Ss: 0.021654\n","\tEpoch:22 [000/005 (0600/0693)]\tLoss Ss: 0.013708\n","\tEpoch:22 [000/005 (0620/0693)]\tLoss Ss: 0.013682\n","\tEpoch:22 [000/005 (0640/0693)]\tLoss Ss: 0.012727\n","\tEpoch:22 [000/005 (0660/0693)]\tLoss Ss: 0.018898\n","\tEpoch:22 [000/005 (0680/0693)]\tLoss Ss: 0.011579\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tEpoch:22 [001/005 (0000/0588)]\tLoss Ss: 0.021403\n","\tEpoch:22 [001/005 (0020/0588)]\tLoss Ss: 0.015233\n","\tEpoch:22 [001/005 (0040/0588)]\tLoss Ss: 0.015493\n","\tEpoch:22 [001/005 (0060/0588)]\tLoss Ss: 0.014013\n","\tEpoch:22 [001/005 (0080/0588)]\tLoss Ss: 0.012796\n","\tEpoch:22 [001/005 (0100/0588)]\tLoss Ss: 0.013585\n","\tEpoch:22 [001/005 (0120/0588)]\tLoss Ss: 0.011519\n","\tEpoch:22 [001/005 (0140/0588)]\tLoss Ss: 0.010519\n","\tEpoch:22 [001/005 (0160/0588)]\tLoss Ss: 0.012746\n","\tEpoch:22 [001/005 (0180/0588)]\tLoss Ss: 0.008277\n","\tEpoch:22 [001/005 (0200/0588)]\tLoss Ss: 0.008078\n","\tEpoch:22 [001/005 (0220/0588)]\tLoss Ss: 0.008213\n","\tEpoch:22 [001/005 (0240/0588)]\tLoss Ss: 0.007681\n","\tEpoch:22 [001/005 (0260/0588)]\tLoss Ss: 0.008628\n","\tEpoch:22 [001/005 (0280/0588)]\tLoss Ss: 0.007772\n","\tEpoch:22 [001/005 (0300/0588)]\tLoss Ss: 0.007376\n","\tEpoch:22 [001/005 (0320/0588)]\tLoss Ss: 0.007284\n","\tEpoch:22 [001/005 (0340/0588)]\tLoss Ss: 0.007274\n","\tEpoch:22 [001/005 (0360/0588)]\tLoss Ss: 0.009070\n","\tEpoch:22 [001/005 (0380/0588)]\tLoss Ss: 0.006121\n","\tEpoch:22 [001/005 (0400/0588)]\tLoss Ss: 0.009752\n","\tEpoch:22 [001/005 (0420/0588)]\tLoss Ss: 0.005765\n","\tEpoch:22 [001/005 (0440/0588)]\tLoss Ss: 0.007904\n","\tEpoch:22 [001/005 (0460/0588)]\tLoss Ss: 0.010088\n","\tEpoch:22 [001/005 (0480/0588)]\tLoss Ss: 0.009508\n","\tEpoch:22 [001/005 (0500/0588)]\tLoss Ss: 0.008189\n","\tEpoch:22 [001/005 (0520/0588)]\tLoss Ss: 0.009007\n","\tEpoch:22 [001/005 (0540/0588)]\tLoss Ss: 0.010532\n","\tEpoch:22 [001/005 (0560/0588)]\tLoss Ss: 0.007613\n","\tEpoch:22 [001/005 (0580/0588)]\tLoss Ss: 0.003992\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tEpoch:22 [002/005 (0000/0614)]\tLoss Ss: 0.009485\n","\tEpoch:22 [002/005 (0020/0614)]\tLoss Ss: 0.007352\n","\tEpoch:22 [002/005 (0040/0614)]\tLoss Ss: 0.012193\n","\tEpoch:22 [002/005 (0060/0614)]\tLoss Ss: 0.009003\n","\tEpoch:22 [002/005 (0080/0614)]\tLoss Ss: 0.008767\n","\tEpoch:22 [002/005 (0100/0614)]\tLoss Ss: 0.005400\n","\tEpoch:22 [002/005 (0120/0614)]\tLoss Ss: 0.008289\n","\tEpoch:22 [002/005 (0140/0614)]\tLoss Ss: 0.009357\n","\tEpoch:22 [002/005 (0160/0614)]\tLoss Ss: 0.010718\n","\tEpoch:22 [002/005 (0180/0614)]\tLoss Ss: 0.006616\n","\tEpoch:22 [002/005 (0200/0614)]\tLoss Ss: 0.006647\n","\tEpoch:22 [002/005 (0220/0614)]\tLoss Ss: 0.006306\n","\tEpoch:22 [002/005 (0240/0614)]\tLoss Ss: 0.010830\n","\tEpoch:22 [002/005 (0260/0614)]\tLoss Ss: 0.007265\n","\tEpoch:22 [002/005 (0280/0614)]\tLoss Ss: 0.007636\n","\tEpoch:22 [002/005 (0300/0614)]\tLoss Ss: 0.004344\n","\tEpoch:22 [002/005 (0320/0614)]\tLoss Ss: 0.006039\n","\tEpoch:22 [002/005 (0340/0614)]\tLoss Ss: 0.007016\n","\tEpoch:22 [002/005 (0360/0614)]\tLoss Ss: 0.008018\n","\tEpoch:22 [002/005 (0380/0614)]\tLoss Ss: 0.006329\n","\tEpoch:22 [002/005 (0400/0614)]\tLoss Ss: 0.005158\n","\tEpoch:22 [002/005 (0420/0614)]\tLoss Ss: 0.006674\n","\tEpoch:22 [002/005 (0440/0614)]\tLoss Ss: 0.007080\n","\tEpoch:22 [002/005 (0460/0614)]\tLoss Ss: 0.005060\n","\tEpoch:22 [002/005 (0480/0614)]\tLoss Ss: 0.006293\n","\tEpoch:22 [002/005 (0500/0614)]\tLoss Ss: 0.006722\n","\tEpoch:22 [002/005 (0520/0614)]\tLoss Ss: 0.005521\n","\tEpoch:22 [002/005 (0540/0614)]\tLoss Ss: 0.006394\n","\tEpoch:22 [002/005 (0560/0614)]\tLoss Ss: 0.005839\n","\tEpoch:22 [002/005 (0580/0614)]\tLoss Ss: 0.008031\n","\tEpoch:22 [002/005 (0600/0614)]\tLoss Ss: 0.008810\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:22 [003/005 (0000/0755)]\tLoss Ss: 0.026406\n","\tEpoch:22 [003/005 (0020/0755)]\tLoss Ss: 0.016296\n","\tEpoch:22 [003/005 (0040/0755)]\tLoss Ss: 0.018103\n","\tEpoch:22 [003/005 (0060/0755)]\tLoss Ss: 0.027967\n","\tEpoch:22 [003/005 (0080/0755)]\tLoss Ss: 0.026258\n","\tEpoch:22 [003/005 (0100/0755)]\tLoss Ss: 0.017682\n","\tEpoch:22 [003/005 (0120/0755)]\tLoss Ss: 0.020580\n","\tEpoch:22 [003/005 (0140/0755)]\tLoss Ss: 0.028055\n","\tEpoch:22 [003/005 (0160/0755)]\tLoss Ss: 0.010686\n","\tEpoch:22 [003/005 (0180/0755)]\tLoss Ss: 0.021022\n","\tEpoch:22 [003/005 (0200/0755)]\tLoss Ss: 0.012521\n","\tEpoch:22 [003/005 (0220/0755)]\tLoss Ss: 0.015823\n","\tEpoch:22 [003/005 (0240/0755)]\tLoss Ss: 0.019323\n","\tEpoch:22 [003/005 (0260/0755)]\tLoss Ss: 0.021126\n","\tEpoch:22 [003/005 (0280/0755)]\tLoss Ss: 0.014899\n","\tEpoch:22 [003/005 (0300/0755)]\tLoss Ss: 0.021350\n","\tEpoch:22 [003/005 (0320/0755)]\tLoss Ss: 0.015729\n","\tEpoch:22 [003/005 (0340/0755)]\tLoss Ss: 0.022817\n","\tEpoch:22 [003/005 (0360/0755)]\tLoss Ss: 0.016486\n","\tEpoch:22 [003/005 (0380/0755)]\tLoss Ss: 0.013099\n","\tEpoch:22 [003/005 (0400/0755)]\tLoss Ss: 0.014562\n","\tEpoch:22 [003/005 (0420/0755)]\tLoss Ss: 0.014464\n","\tEpoch:22 [003/005 (0440/0755)]\tLoss Ss: 0.012917\n","\tEpoch:22 [003/005 (0460/0755)]\tLoss Ss: 0.015013\n","\tEpoch:22 [003/005 (0480/0755)]\tLoss Ss: 0.015368\n","\tEpoch:22 [003/005 (0500/0755)]\tLoss Ss: 0.013253\n","\tEpoch:22 [003/005 (0520/0755)]\tLoss Ss: 0.016320\n","\tEpoch:22 [003/005 (0540/0755)]\tLoss Ss: 0.018805\n","\tEpoch:22 [003/005 (0560/0755)]\tLoss Ss: 0.020878\n","\tEpoch:22 [003/005 (0580/0755)]\tLoss Ss: 0.019012\n","\tEpoch:22 [003/005 (0600/0755)]\tLoss Ss: 0.015345\n","\tEpoch:22 [003/005 (0620/0755)]\tLoss Ss: 0.023125\n","\tEpoch:22 [003/005 (0640/0755)]\tLoss Ss: 0.026756\n","\tEpoch:22 [003/005 (0660/0755)]\tLoss Ss: 0.017168\n","\tEpoch:22 [003/005 (0680/0755)]\tLoss Ss: 0.017620\n","\tEpoch:22 [003/005 (0700/0755)]\tLoss Ss: 0.015343\n","\tEpoch:22 [003/005 (0720/0755)]\tLoss Ss: 0.014823\n","\tEpoch:22 [003/005 (0740/0755)]\tLoss Ss: 0.014623\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tEpoch:22 [004/005 (0000/0693)]\tLoss Ss: 0.017815\n","\tEpoch:22 [004/005 (0020/0693)]\tLoss Ss: 0.017073\n","\tEpoch:22 [004/005 (0040/0693)]\tLoss Ss: 0.016567\n","\tEpoch:22 [004/005 (0060/0693)]\tLoss Ss: 0.014674\n","\tEpoch:22 [004/005 (0080/0693)]\tLoss Ss: 0.015714\n","\tEpoch:22 [004/005 (0100/0693)]\tLoss Ss: 0.010049\n","\tEpoch:22 [004/005 (0120/0693)]\tLoss Ss: 0.017517\n","\tEpoch:22 [004/005 (0140/0693)]\tLoss Ss: 0.020180\n","\tEpoch:22 [004/005 (0160/0693)]\tLoss Ss: 0.017881\n","\tEpoch:22 [004/005 (0180/0693)]\tLoss Ss: 0.019942\n","\tEpoch:22 [004/005 (0200/0693)]\tLoss Ss: 0.016021\n","\tEpoch:22 [004/005 (0220/0693)]\tLoss Ss: 0.015972\n","\tEpoch:22 [004/005 (0240/0693)]\tLoss Ss: 0.015095\n","\tEpoch:22 [004/005 (0260/0693)]\tLoss Ss: 0.016335\n","\tEpoch:22 [004/005 (0280/0693)]\tLoss Ss: 0.015365\n","\tEpoch:22 [004/005 (0300/0693)]\tLoss Ss: 0.013164\n","\tEpoch:22 [004/005 (0320/0693)]\tLoss Ss: 0.019629\n","\tEpoch:22 [004/005 (0340/0693)]\tLoss Ss: 0.015733\n","\tEpoch:22 [004/005 (0360/0693)]\tLoss Ss: 0.012451\n","\tEpoch:22 [004/005 (0380/0693)]\tLoss Ss: 0.010127\n","\tEpoch:22 [004/005 (0400/0693)]\tLoss Ss: 0.018372\n","\tEpoch:22 [004/005 (0420/0693)]\tLoss Ss: 0.012062\n","\tEpoch:22 [004/005 (0440/0693)]\tLoss Ss: 0.018292\n","\tEpoch:22 [004/005 (0460/0693)]\tLoss Ss: 0.018783\n","\tEpoch:22 [004/005 (0480/0693)]\tLoss Ss: 0.011818\n","\tEpoch:22 [004/005 (0500/0693)]\tLoss Ss: 0.010585\n","\tEpoch:22 [004/005 (0520/0693)]\tLoss Ss: 0.014764\n","\tEpoch:22 [004/005 (0540/0693)]\tLoss Ss: 0.014693\n","\tEpoch:22 [004/005 (0560/0693)]\tLoss Ss: 0.012828\n","\tEpoch:22 [004/005 (0580/0693)]\tLoss Ss: 0.010298\n","\tEpoch:22 [004/005 (0600/0693)]\tLoss Ss: 0.014163\n","\tEpoch:22 [004/005 (0620/0693)]\tLoss Ss: 0.013660\n","\tEpoch:22 [004/005 (0640/0693)]\tLoss Ss: 0.011128\n","\tEpoch:22 [004/005 (0660/0693)]\tLoss Ss: 0.021504\n","\tEpoch:22 [004/005 (0680/0693)]\tLoss Ss: 0.010864\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:22 [005/005 (0000/0755)]\tLoss Ss: 0.013791\n","\tEpoch:22 [005/005 (0020/0755)]\tLoss Ss: 0.013992\n","\tEpoch:22 [005/005 (0040/0755)]\tLoss Ss: 0.017047\n","\tEpoch:22 [005/005 (0060/0755)]\tLoss Ss: 0.018136\n","\tEpoch:22 [005/005 (0080/0755)]\tLoss Ss: 0.023175\n","\tEpoch:22 [005/005 (0100/0755)]\tLoss Ss: 0.010716\n","\tEpoch:22 [005/005 (0120/0755)]\tLoss Ss: 0.017303\n","\tEpoch:22 [005/005 (0140/0755)]\tLoss Ss: 0.010245\n","\tEpoch:22 [005/005 (0160/0755)]\tLoss Ss: 0.015050\n","\tEpoch:22 [005/005 (0180/0755)]\tLoss Ss: 0.014956\n","\tEpoch:22 [005/005 (0200/0755)]\tLoss Ss: 0.013076\n","\tEpoch:22 [005/005 (0220/0755)]\tLoss Ss: 0.019054\n","\tEpoch:22 [005/005 (0240/0755)]\tLoss Ss: 0.015761\n","\tEpoch:22 [005/005 (0260/0755)]\tLoss Ss: 0.015008\n","\tEpoch:22 [005/005 (0280/0755)]\tLoss Ss: 0.013765\n","\tEpoch:22 [005/005 (0300/0755)]\tLoss Ss: 0.016106\n","\tEpoch:22 [005/005 (0320/0755)]\tLoss Ss: 0.017550\n","\tEpoch:22 [005/005 (0340/0755)]\tLoss Ss: 0.015852\n","\tEpoch:22 [005/005 (0360/0755)]\tLoss Ss: 0.016512\n","\tEpoch:22 [005/005 (0380/0755)]\tLoss Ss: 0.009832\n","\tEpoch:22 [005/005 (0400/0755)]\tLoss Ss: 0.013575\n","\tEpoch:22 [005/005 (0420/0755)]\tLoss Ss: 0.015665\n","\tEpoch:22 [005/005 (0440/0755)]\tLoss Ss: 0.012281\n","\tEpoch:22 [005/005 (0460/0755)]\tLoss Ss: 0.014707\n","\tEpoch:22 [005/005 (0480/0755)]\tLoss Ss: 0.008761\n","\tEpoch:22 [005/005 (0500/0755)]\tLoss Ss: 0.015027\n","\tEpoch:22 [005/005 (0520/0755)]\tLoss Ss: 0.013301\n","\tEpoch:22 [005/005 (0540/0755)]\tLoss Ss: 0.010833\n","\tEpoch:22 [005/005 (0560/0755)]\tLoss Ss: 0.017935\n","\tEpoch:22 [005/005 (0580/0755)]\tLoss Ss: 0.009801\n","\tEpoch:22 [005/005 (0600/0755)]\tLoss Ss: 0.010006\n","\tEpoch:22 [005/005 (0620/0755)]\tLoss Ss: 0.011325\n","\tEpoch:22 [005/005 (0640/0755)]\tLoss Ss: 0.012836\n","\tEpoch:22 [005/005 (0660/0755)]\tLoss Ss: 0.009944\n","\tEpoch:22 [005/005 (0680/0755)]\tLoss Ss: 0.016046\n","\tEpoch:22 [005/005 (0700/0755)]\tLoss Ss: 0.011773\n","\tEpoch:22 [005/005 (0720/0755)]\tLoss Ss: 0.012512\n","\tEpoch:22 [005/005 (0740/0755)]\tLoss Ss: 0.013949\n","Now train the rotated image\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:22 [000/005 (0000/0755)]\tLoss Ss: 0.021792\n","\tRotated_Epoch:22 [000/005 (0020/0755)]\tLoss Ss: 0.017596\n","\tRotated_Epoch:22 [000/005 (0040/0755)]\tLoss Ss: 0.013813\n","\tRotated_Epoch:22 [000/005 (0060/0755)]\tLoss Ss: 0.021883\n","\tRotated_Epoch:22 [000/005 (0080/0755)]\tLoss Ss: 0.017085\n","\tRotated_Epoch:22 [000/005 (0100/0755)]\tLoss Ss: 0.021313\n","\tRotated_Epoch:22 [000/005 (0120/0755)]\tLoss Ss: 0.019620\n","\tRotated_Epoch:22 [000/005 (0140/0755)]\tLoss Ss: 0.016974\n","\tRotated_Epoch:22 [000/005 (0160/0755)]\tLoss Ss: 0.013205\n","\tRotated_Epoch:22 [000/005 (0180/0755)]\tLoss Ss: 0.023466\n","\tRotated_Epoch:22 [000/005 (0200/0755)]\tLoss Ss: 0.020281\n","\tRotated_Epoch:22 [000/005 (0220/0755)]\tLoss Ss: 0.019549\n","\tRotated_Epoch:22 [000/005 (0240/0755)]\tLoss Ss: 0.030289\n","\tRotated_Epoch:22 [000/005 (0260/0755)]\tLoss Ss: 0.017277\n","\tRotated_Epoch:22 [000/005 (0280/0755)]\tLoss Ss: 0.017783\n","\tRotated_Epoch:22 [000/005 (0300/0755)]\tLoss Ss: 0.012070\n","\tRotated_Epoch:22 [000/005 (0320/0755)]\tLoss Ss: 0.016295\n","\tRotated_Epoch:22 [000/005 (0340/0755)]\tLoss Ss: 0.017393\n","\tRotated_Epoch:22 [000/005 (0360/0755)]\tLoss Ss: 0.016840\n","\tRotated_Epoch:22 [000/005 (0380/0755)]\tLoss Ss: 0.018153\n","\tRotated_Epoch:22 [000/005 (0400/0755)]\tLoss Ss: 0.016752\n","\tRotated_Epoch:22 [000/005 (0420/0755)]\tLoss Ss: 0.010227\n","\tRotated_Epoch:22 [000/005 (0440/0755)]\tLoss Ss: 0.028539\n","\tRotated_Epoch:22 [000/005 (0460/0755)]\tLoss Ss: 0.017497\n","\tRotated_Epoch:22 [000/005 (0480/0755)]\tLoss Ss: 0.016123\n","\tRotated_Epoch:22 [000/005 (0500/0755)]\tLoss Ss: 0.017353\n","\tRotated_Epoch:22 [000/005 (0520/0755)]\tLoss Ss: 0.009212\n","\tRotated_Epoch:22 [000/005 (0540/0755)]\tLoss Ss: 0.018962\n","\tRotated_Epoch:22 [000/005 (0560/0755)]\tLoss Ss: 0.016471\n","\tRotated_Epoch:22 [000/005 (0580/0755)]\tLoss Ss: 0.011651\n","\tRotated_Epoch:22 [000/005 (0600/0755)]\tLoss Ss: 0.013034\n","\tRotated_Epoch:22 [000/005 (0620/0755)]\tLoss Ss: 0.014440\n","\tRotated_Epoch:22 [000/005 (0640/0755)]\tLoss Ss: 0.016809\n","\tRotated_Epoch:22 [000/005 (0660/0755)]\tLoss Ss: 0.012271\n","\tRotated_Epoch:22 [000/005 (0680/0755)]\tLoss Ss: 0.013111\n","\tRotated_Epoch:22 [000/005 (0700/0755)]\tLoss Ss: 0.019673\n","\tRotated_Epoch:22 [000/005 (0720/0755)]\tLoss Ss: 0.011021\n","\tRotated_Epoch:22 [000/005 (0740/0755)]\tLoss Ss: 0.016607\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tRotated_Epoch:22 [001/005 (0000/0588)]\tLoss Ss: 0.091482\n","\tRotated_Epoch:22 [001/005 (0020/0588)]\tLoss Ss: 0.103652\n","\tRotated_Epoch:22 [001/005 (0040/0588)]\tLoss Ss: 0.073262\n","\tRotated_Epoch:22 [001/005 (0060/0588)]\tLoss Ss: 0.177855\n","\tRotated_Epoch:22 [001/005 (0080/0588)]\tLoss Ss: 0.049089\n","\tRotated_Epoch:22 [001/005 (0100/0588)]\tLoss Ss: 0.099718\n","\tRotated_Epoch:22 [001/005 (0120/0588)]\tLoss Ss: 0.069273\n","\tRotated_Epoch:22 [001/005 (0140/0588)]\tLoss Ss: 0.077166\n","\tRotated_Epoch:22 [001/005 (0160/0588)]\tLoss Ss: 0.091464\n","\tRotated_Epoch:22 [001/005 (0180/0588)]\tLoss Ss: 0.085307\n","\tRotated_Epoch:22 [001/005 (0200/0588)]\tLoss Ss: 0.066732\n","\tRotated_Epoch:22 [001/005 (0220/0588)]\tLoss Ss: 0.062403\n","\tRotated_Epoch:22 [001/005 (0240/0588)]\tLoss Ss: 0.048930\n","\tRotated_Epoch:22 [001/005 (0260/0588)]\tLoss Ss: 0.031549\n","\tRotated_Epoch:22 [001/005 (0280/0588)]\tLoss Ss: 0.049270\n","\tRotated_Epoch:22 [001/005 (0300/0588)]\tLoss Ss: 0.066862\n","\tRotated_Epoch:22 [001/005 (0320/0588)]\tLoss Ss: 0.047656\n","\tRotated_Epoch:22 [001/005 (0340/0588)]\tLoss Ss: 0.046920\n","\tRotated_Epoch:22 [001/005 (0360/0588)]\tLoss Ss: 0.053887\n","\tRotated_Epoch:22 [001/005 (0380/0588)]\tLoss Ss: 0.052990\n","\tRotated_Epoch:22 [001/005 (0400/0588)]\tLoss Ss: 0.047266\n","\tRotated_Epoch:22 [001/005 (0420/0588)]\tLoss Ss: 0.081165\n","\tRotated_Epoch:22 [001/005 (0440/0588)]\tLoss Ss: 0.059658\n","\tRotated_Epoch:22 [001/005 (0460/0588)]\tLoss Ss: 0.057779\n","\tRotated_Epoch:22 [001/005 (0480/0588)]\tLoss Ss: 0.067325\n","\tRotated_Epoch:22 [001/005 (0500/0588)]\tLoss Ss: 0.059876\n","\tRotated_Epoch:22 [001/005 (0520/0588)]\tLoss Ss: 0.070844\n","\tRotated_Epoch:22 [001/005 (0540/0588)]\tLoss Ss: 0.075998\n","\tRotated_Epoch:22 [001/005 (0560/0588)]\tLoss Ss: 0.051494\n","\tRotated_Epoch:22 [001/005 (0580/0588)]\tLoss Ss: 0.063928\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tRotated_Epoch:22 [002/005 (0000/0693)]\tLoss Ss: 0.073072\n","\tRotated_Epoch:22 [002/005 (0020/0693)]\tLoss Ss: 0.044513\n","\tRotated_Epoch:22 [002/005 (0040/0693)]\tLoss Ss: 0.032927\n","\tRotated_Epoch:22 [002/005 (0060/0693)]\tLoss Ss: 0.020068\n","\tRotated_Epoch:22 [002/005 (0080/0693)]\tLoss Ss: 0.017394\n","\tRotated_Epoch:22 [002/005 (0100/0693)]\tLoss Ss: 0.011946\n","\tRotated_Epoch:22 [002/005 (0120/0693)]\tLoss Ss: 0.021138\n","\tRotated_Epoch:22 [002/005 (0140/0693)]\tLoss Ss: 0.031880\n","\tRotated_Epoch:22 [002/005 (0160/0693)]\tLoss Ss: 0.022795\n","\tRotated_Epoch:22 [002/005 (0180/0693)]\tLoss Ss: 0.016423\n","\tRotated_Epoch:22 [002/005 (0200/0693)]\tLoss Ss: 0.020907\n","\tRotated_Epoch:22 [002/005 (0220/0693)]\tLoss Ss: 0.021298\n","\tRotated_Epoch:22 [002/005 (0240/0693)]\tLoss Ss: 0.021006\n","\tRotated_Epoch:22 [002/005 (0260/0693)]\tLoss Ss: 0.019420\n","\tRotated_Epoch:22 [002/005 (0280/0693)]\tLoss Ss: 0.021266\n","\tRotated_Epoch:22 [002/005 (0300/0693)]\tLoss Ss: 0.014205\n","\tRotated_Epoch:22 [002/005 (0320/0693)]\tLoss Ss: 0.014184\n","\tRotated_Epoch:22 [002/005 (0340/0693)]\tLoss Ss: 0.018791\n","\tRotated_Epoch:22 [002/005 (0360/0693)]\tLoss Ss: 0.017957\n","\tRotated_Epoch:22 [002/005 (0380/0693)]\tLoss Ss: 0.012555\n","\tRotated_Epoch:22 [002/005 (0400/0693)]\tLoss Ss: 0.015001\n","\tRotated_Epoch:22 [002/005 (0420/0693)]\tLoss Ss: 0.011878\n","\tRotated_Epoch:22 [002/005 (0440/0693)]\tLoss Ss: 0.011866\n","\tRotated_Epoch:22 [002/005 (0460/0693)]\tLoss Ss: 0.020246\n","\tRotated_Epoch:22 [002/005 (0480/0693)]\tLoss Ss: 0.012504\n","\tRotated_Epoch:22 [002/005 (0500/0693)]\tLoss Ss: 0.024133\n","\tRotated_Epoch:22 [002/005 (0520/0693)]\tLoss Ss: 0.013037\n","\tRotated_Epoch:22 [002/005 (0540/0693)]\tLoss Ss: 0.015785\n","\tRotated_Epoch:22 [002/005 (0560/0693)]\tLoss Ss: 0.015653\n","\tRotated_Epoch:22 [002/005 (0580/0693)]\tLoss Ss: 0.013233\n","\tRotated_Epoch:22 [002/005 (0600/0693)]\tLoss Ss: 0.017855\n","\tRotated_Epoch:22 [002/005 (0620/0693)]\tLoss Ss: 0.017354\n","\tRotated_Epoch:22 [002/005 (0640/0693)]\tLoss Ss: 0.013348\n","\tRotated_Epoch:22 [002/005 (0660/0693)]\tLoss Ss: 0.016334\n","\tRotated_Epoch:22 [002/005 (0680/0693)]\tLoss Ss: 0.029455\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:22 [003/005 (0000/0755)]\tLoss Ss: 0.212560\n","\tRotated_Epoch:22 [003/005 (0020/0755)]\tLoss Ss: 0.230741\n","\tRotated_Epoch:22 [003/005 (0040/0755)]\tLoss Ss: 0.197714\n","\tRotated_Epoch:22 [003/005 (0060/0755)]\tLoss Ss: 0.236472\n","\tRotated_Epoch:22 [003/005 (0080/0755)]\tLoss Ss: 0.100014\n","\tRotated_Epoch:22 [003/005 (0100/0755)]\tLoss Ss: 0.104800\n","\tRotated_Epoch:22 [003/005 (0120/0755)]\tLoss Ss: 0.088222\n","\tRotated_Epoch:22 [003/005 (0140/0755)]\tLoss Ss: 0.108491\n","\tRotated_Epoch:22 [003/005 (0160/0755)]\tLoss Ss: 0.102730\n","\tRotated_Epoch:22 [003/005 (0180/0755)]\tLoss Ss: 0.109635\n","\tRotated_Epoch:22 [003/005 (0200/0755)]\tLoss Ss: 0.085745\n","\tRotated_Epoch:22 [003/005 (0220/0755)]\tLoss Ss: 0.101374\n","\tRotated_Epoch:22 [003/005 (0240/0755)]\tLoss Ss: 0.061764\n","\tRotated_Epoch:22 [003/005 (0260/0755)]\tLoss Ss: 0.052479\n","\tRotated_Epoch:22 [003/005 (0280/0755)]\tLoss Ss: 0.086475\n","\tRotated_Epoch:22 [003/005 (0300/0755)]\tLoss Ss: 0.087624\n","\tRotated_Epoch:22 [003/005 (0320/0755)]\tLoss Ss: 0.088022\n","\tRotated_Epoch:22 [003/005 (0340/0755)]\tLoss Ss: 0.074722\n","\tRotated_Epoch:22 [003/005 (0360/0755)]\tLoss Ss: 0.092637\n","\tRotated_Epoch:22 [003/005 (0380/0755)]\tLoss Ss: 0.063974\n","\tRotated_Epoch:22 [003/005 (0400/0755)]\tLoss Ss: 0.077314\n","\tRotated_Epoch:22 [003/005 (0420/0755)]\tLoss Ss: 0.065986\n","\tRotated_Epoch:22 [003/005 (0440/0755)]\tLoss Ss: 0.082010\n","\tRotated_Epoch:22 [003/005 (0460/0755)]\tLoss Ss: 0.065091\n","\tRotated_Epoch:22 [003/005 (0480/0755)]\tLoss Ss: 0.075722\n","\tRotated_Epoch:22 [003/005 (0500/0755)]\tLoss Ss: 0.052152\n","\tRotated_Epoch:22 [003/005 (0520/0755)]\tLoss Ss: 0.063776\n","\tRotated_Epoch:22 [003/005 (0540/0755)]\tLoss Ss: 0.057057\n","\tRotated_Epoch:22 [003/005 (0560/0755)]\tLoss Ss: 0.064943\n","\tRotated_Epoch:22 [003/005 (0580/0755)]\tLoss Ss: 0.046038\n","\tRotated_Epoch:22 [003/005 (0600/0755)]\tLoss Ss: 0.055681\n","\tRotated_Epoch:22 [003/005 (0620/0755)]\tLoss Ss: 0.066890\n","\tRotated_Epoch:22 [003/005 (0640/0755)]\tLoss Ss: 0.060473\n","\tRotated_Epoch:22 [003/005 (0660/0755)]\tLoss Ss: 0.048136\n","\tRotated_Epoch:22 [003/005 (0680/0755)]\tLoss Ss: 0.048883\n","\tRotated_Epoch:22 [003/005 (0700/0755)]\tLoss Ss: 0.034496\n","\tRotated_Epoch:22 [003/005 (0720/0755)]\tLoss Ss: 0.045313\n","\tRotated_Epoch:22 [003/005 (0740/0755)]\tLoss Ss: 0.051049\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tRotated_Epoch:22 [004/005 (0000/0693)]\tLoss Ss: 0.059979\n","\tRotated_Epoch:22 [004/005 (0020/0693)]\tLoss Ss: 0.057701\n","\tRotated_Epoch:22 [004/005 (0040/0693)]\tLoss Ss: 0.026691\n","\tRotated_Epoch:22 [004/005 (0060/0693)]\tLoss Ss: 0.026973\n","\tRotated_Epoch:22 [004/005 (0080/0693)]\tLoss Ss: 0.020228\n","\tRotated_Epoch:22 [004/005 (0100/0693)]\tLoss Ss: 0.029319\n","\tRotated_Epoch:22 [004/005 (0120/0693)]\tLoss Ss: 0.027284\n","\tRotated_Epoch:22 [004/005 (0140/0693)]\tLoss Ss: 0.019557\n","\tRotated_Epoch:22 [004/005 (0160/0693)]\tLoss Ss: 0.016025\n","\tRotated_Epoch:22 [004/005 (0180/0693)]\tLoss Ss: 0.022803\n","\tRotated_Epoch:22 [004/005 (0200/0693)]\tLoss Ss: 0.029497\n","\tRotated_Epoch:22 [004/005 (0220/0693)]\tLoss Ss: 0.012733\n","\tRotated_Epoch:22 [004/005 (0240/0693)]\tLoss Ss: 0.021760\n","\tRotated_Epoch:22 [004/005 (0260/0693)]\tLoss Ss: 0.016263\n","\tRotated_Epoch:22 [004/005 (0280/0693)]\tLoss Ss: 0.015094\n","\tRotated_Epoch:22 [004/005 (0300/0693)]\tLoss Ss: 0.015466\n","\tRotated_Epoch:22 [004/005 (0320/0693)]\tLoss Ss: 0.017339\n","\tRotated_Epoch:22 [004/005 (0340/0693)]\tLoss Ss: 0.008597\n","\tRotated_Epoch:22 [004/005 (0360/0693)]\tLoss Ss: 0.013967\n","\tRotated_Epoch:22 [004/005 (0380/0693)]\tLoss Ss: 0.014083\n","\tRotated_Epoch:22 [004/005 (0400/0693)]\tLoss Ss: 0.016687\n","\tRotated_Epoch:22 [004/005 (0420/0693)]\tLoss Ss: 0.017791\n","\tRotated_Epoch:22 [004/005 (0440/0693)]\tLoss Ss: 0.016481\n","\tRotated_Epoch:22 [004/005 (0460/0693)]\tLoss Ss: 0.011763\n","\tRotated_Epoch:22 [004/005 (0480/0693)]\tLoss Ss: 0.012505\n","\tRotated_Epoch:22 [004/005 (0500/0693)]\tLoss Ss: 0.020592\n","\tRotated_Epoch:22 [004/005 (0520/0693)]\tLoss Ss: 0.013745\n","\tRotated_Epoch:22 [004/005 (0540/0693)]\tLoss Ss: 0.011729\n","\tRotated_Epoch:22 [004/005 (0560/0693)]\tLoss Ss: 0.014698\n","\tRotated_Epoch:22 [004/005 (0580/0693)]\tLoss Ss: 0.015626\n","\tRotated_Epoch:22 [004/005 (0600/0693)]\tLoss Ss: 0.014609\n","\tRotated_Epoch:22 [004/005 (0620/0693)]\tLoss Ss: 0.015824\n","\tRotated_Epoch:22 [004/005 (0640/0693)]\tLoss Ss: 0.016462\n","\tRotated_Epoch:22 [004/005 (0660/0693)]\tLoss Ss: 0.017994\n","\tRotated_Epoch:22 [004/005 (0680/0693)]\tLoss Ss: 0.015909\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tRotated_Epoch:22 [005/005 (0000/0614)]\tLoss Ss: 0.017010\n","\tRotated_Epoch:22 [005/005 (0020/0614)]\tLoss Ss: 0.021157\n","\tRotated_Epoch:22 [005/005 (0040/0614)]\tLoss Ss: 0.013106\n","\tRotated_Epoch:22 [005/005 (0060/0614)]\tLoss Ss: 0.016266\n","\tRotated_Epoch:22 [005/005 (0080/0614)]\tLoss Ss: 0.013061\n","\tRotated_Epoch:22 [005/005 (0100/0614)]\tLoss Ss: 0.009780\n","\tRotated_Epoch:22 [005/005 (0120/0614)]\tLoss Ss: 0.016431\n","\tRotated_Epoch:22 [005/005 (0140/0614)]\tLoss Ss: 0.012733\n","\tRotated_Epoch:22 [005/005 (0160/0614)]\tLoss Ss: 0.015995\n","\tRotated_Epoch:22 [005/005 (0180/0614)]\tLoss Ss: 0.006616\n","\tRotated_Epoch:22 [005/005 (0200/0614)]\tLoss Ss: 0.007324\n","\tRotated_Epoch:22 [005/005 (0220/0614)]\tLoss Ss: 0.010191\n","\tRotated_Epoch:22 [005/005 (0240/0614)]\tLoss Ss: 0.006191\n","\tRotated_Epoch:22 [005/005 (0260/0614)]\tLoss Ss: 0.008353\n","\tRotated_Epoch:22 [005/005 (0280/0614)]\tLoss Ss: 0.006653\n","\tRotated_Epoch:22 [005/005 (0300/0614)]\tLoss Ss: 0.005141\n","\tRotated_Epoch:22 [005/005 (0320/0614)]\tLoss Ss: 0.008319\n","\tRotated_Epoch:22 [005/005 (0340/0614)]\tLoss Ss: 0.009507\n","\tRotated_Epoch:22 [005/005 (0360/0614)]\tLoss Ss: 0.006098\n","\tRotated_Epoch:22 [005/005 (0380/0614)]\tLoss Ss: 0.008386\n","\tRotated_Epoch:22 [005/005 (0400/0614)]\tLoss Ss: 0.008116\n","\tRotated_Epoch:22 [005/005 (0420/0614)]\tLoss Ss: 0.007555\n","\tRotated_Epoch:22 [005/005 (0440/0614)]\tLoss Ss: 0.007082\n","\tRotated_Epoch:22 [005/005 (0460/0614)]\tLoss Ss: 0.003482\n","\tRotated_Epoch:22 [005/005 (0480/0614)]\tLoss Ss: 0.004514\n","\tRotated_Epoch:22 [005/005 (0500/0614)]\tLoss Ss: 0.006330\n","\tRotated_Epoch:22 [005/005 (0520/0614)]\tLoss Ss: 0.008984\n","\tRotated_Epoch:22 [005/005 (0540/0614)]\tLoss Ss: 0.006549\n","\tRotated_Epoch:22 [005/005 (0560/0614)]\tLoss Ss: 0.005993\n","\tRotated_Epoch:22 [005/005 (0580/0614)]\tLoss Ss: 0.007369\n","\tRotated_Epoch:22 [005/005 (0600/0614)]\tLoss Ss: 0.007105\n","new_1_6_d.nii\n","new_1_6_m.nii\n","new_1_7_d.nii\n","new_1_7_m.nii\n","new_2_5_d.nii\n","new_2_5_m.nii\n","new_2_6_d.nii\n","new_2_6_m.nii\n","new_2_7_d.nii\n","new_2_7_m.nii\n","new_3_d (2).nii\n","new_3_m (2).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (3).nii\n","new_3_m (3).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (7).nii\n","new_3_m (7).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (8).nii\n","new_3_m (8).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (9).nii\n","new_3_m (9).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_4_10_d_163526.nii\n","new_4_10_m_163526.nii.gz\n","new_4_11_d_084336.nii\n","new_4_11_m_084336.nii.gz\n","new_4_7_d_184422.nii\n","new_4_7_m_184422.nii.gz\n","new_4_8_d_143412.nii\n","new_4_8_m_143412.nii.gz\n","new_4_9_d_155347.nii\n","new_4_9_m_155347.nii.gz\n","\tEpoch: 22; Dice: 0.9583 +/- 0.0111; Loss: 11.1055\n","Begin Epoch 23\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:23 [000/005 (0000/0755)]\tLoss Ss: 0.061467\n","\tEpoch:23 [000/005 (0020/0755)]\tLoss Ss: 0.051127\n","\tEpoch:23 [000/005 (0040/0755)]\tLoss Ss: 0.039398\n","\tEpoch:23 [000/005 (0060/0755)]\tLoss Ss: 0.035314\n","\tEpoch:23 [000/005 (0080/0755)]\tLoss Ss: 0.022497\n","\tEpoch:23 [000/005 (0100/0755)]\tLoss Ss: 0.028532\n","\tEpoch:23 [000/005 (0120/0755)]\tLoss Ss: 0.034133\n","\tEpoch:23 [000/005 (0140/0755)]\tLoss Ss: 0.024230\n","\tEpoch:23 [000/005 (0160/0755)]\tLoss Ss: 0.020421\n","\tEpoch:23 [000/005 (0180/0755)]\tLoss Ss: 0.031795\n","\tEpoch:23 [000/005 (0200/0755)]\tLoss Ss: 0.019896\n","\tEpoch:23 [000/005 (0220/0755)]\tLoss Ss: 0.012489\n","\tEpoch:23 [000/005 (0240/0755)]\tLoss Ss: 0.018439\n","\tEpoch:23 [000/005 (0260/0755)]\tLoss Ss: 0.023681\n","\tEpoch:23 [000/005 (0280/0755)]\tLoss Ss: 0.024877\n","\tEpoch:23 [000/005 (0300/0755)]\tLoss Ss: 0.015004\n","\tEpoch:23 [000/005 (0320/0755)]\tLoss Ss: 0.020812\n","\tEpoch:23 [000/005 (0340/0755)]\tLoss Ss: 0.023127\n","\tEpoch:23 [000/005 (0360/0755)]\tLoss Ss: 0.021602\n","\tEpoch:23 [000/005 (0380/0755)]\tLoss Ss: 0.020689\n","\tEpoch:23 [000/005 (0400/0755)]\tLoss Ss: 0.019304\n","\tEpoch:23 [000/005 (0420/0755)]\tLoss Ss: 0.016965\n","\tEpoch:23 [000/005 (0440/0755)]\tLoss Ss: 0.010486\n","\tEpoch:23 [000/005 (0460/0755)]\tLoss Ss: 0.017854\n","\tEpoch:23 [000/005 (0480/0755)]\tLoss Ss: 0.015863\n","\tEpoch:23 [000/005 (0500/0755)]\tLoss Ss: 0.021576\n","\tEpoch:23 [000/005 (0520/0755)]\tLoss Ss: 0.022368\n","\tEpoch:23 [000/005 (0540/0755)]\tLoss Ss: 0.020585\n","\tEpoch:23 [000/005 (0560/0755)]\tLoss Ss: 0.018399\n","\tEpoch:23 [000/005 (0580/0755)]\tLoss Ss: 0.013316\n","\tEpoch:23 [000/005 (0600/0755)]\tLoss Ss: 0.020754\n","\tEpoch:23 [000/005 (0620/0755)]\tLoss Ss: 0.017075\n","\tEpoch:23 [000/005 (0640/0755)]\tLoss Ss: 0.015452\n","\tEpoch:23 [000/005 (0660/0755)]\tLoss Ss: 0.019756\n","\tEpoch:23 [000/005 (0680/0755)]\tLoss Ss: 0.024359\n","\tEpoch:23 [000/005 (0700/0755)]\tLoss Ss: 0.016128\n","\tEpoch:23 [000/005 (0720/0755)]\tLoss Ss: 0.017518\n","\tEpoch:23 [000/005 (0740/0755)]\tLoss Ss: 0.022869\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tEpoch:23 [001/005 (0000/0693)]\tLoss Ss: 0.018882\n","\tEpoch:23 [001/005 (0020/0693)]\tLoss Ss: 0.021622\n","\tEpoch:23 [001/005 (0040/0693)]\tLoss Ss: 0.015967\n","\tEpoch:23 [001/005 (0060/0693)]\tLoss Ss: 0.018692\n","\tEpoch:23 [001/005 (0080/0693)]\tLoss Ss: 0.017417\n","\tEpoch:23 [001/005 (0100/0693)]\tLoss Ss: 0.015706\n","\tEpoch:23 [001/005 (0120/0693)]\tLoss Ss: 0.017645\n","\tEpoch:23 [001/005 (0140/0693)]\tLoss Ss: 0.009819\n","\tEpoch:23 [001/005 (0160/0693)]\tLoss Ss: 0.015361\n","\tEpoch:23 [001/005 (0180/0693)]\tLoss Ss: 0.015275\n","\tEpoch:23 [001/005 (0200/0693)]\tLoss Ss: 0.012375\n","\tEpoch:23 [001/005 (0220/0693)]\tLoss Ss: 0.012186\n","\tEpoch:23 [001/005 (0240/0693)]\tLoss Ss: 0.015274\n","\tEpoch:23 [001/005 (0260/0693)]\tLoss Ss: 0.013013\n","\tEpoch:23 [001/005 (0280/0693)]\tLoss Ss: 0.011935\n","\tEpoch:23 [001/005 (0300/0693)]\tLoss Ss: 0.013617\n","\tEpoch:23 [001/005 (0320/0693)]\tLoss Ss: 0.010942\n","\tEpoch:23 [001/005 (0340/0693)]\tLoss Ss: 0.019024\n","\tEpoch:23 [001/005 (0360/0693)]\tLoss Ss: 0.013597\n","\tEpoch:23 [001/005 (0380/0693)]\tLoss Ss: 0.013383\n","\tEpoch:23 [001/005 (0400/0693)]\tLoss Ss: 0.012806\n","\tEpoch:23 [001/005 (0420/0693)]\tLoss Ss: 0.010493\n","\tEpoch:23 [001/005 (0440/0693)]\tLoss Ss: 0.011225\n","\tEpoch:23 [001/005 (0460/0693)]\tLoss Ss: 0.008738\n","\tEpoch:23 [001/005 (0480/0693)]\tLoss Ss: 0.009625\n","\tEpoch:23 [001/005 (0500/0693)]\tLoss Ss: 0.012274\n","\tEpoch:23 [001/005 (0520/0693)]\tLoss Ss: 0.011688\n","\tEpoch:23 [001/005 (0540/0693)]\tLoss Ss: 0.010850\n","\tEpoch:23 [001/005 (0560/0693)]\tLoss Ss: 0.013667\n","\tEpoch:23 [001/005 (0580/0693)]\tLoss Ss: 0.014025\n","\tEpoch:23 [001/005 (0600/0693)]\tLoss Ss: 0.012587\n","\tEpoch:23 [001/005 (0620/0693)]\tLoss Ss: 0.012398\n","\tEpoch:23 [001/005 (0640/0693)]\tLoss Ss: 0.019714\n","\tEpoch:23 [001/005 (0660/0693)]\tLoss Ss: 0.009058\n","\tEpoch:23 [001/005 (0680/0693)]\tLoss Ss: 0.016021\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tEpoch:23 [002/005 (0000/0588)]\tLoss Ss: 0.005053\n","\tEpoch:23 [002/005 (0020/0588)]\tLoss Ss: 0.005625\n","\tEpoch:23 [002/005 (0040/0588)]\tLoss Ss: 0.008102\n","\tEpoch:23 [002/005 (0060/0588)]\tLoss Ss: 0.006270\n","\tEpoch:23 [002/005 (0080/0588)]\tLoss Ss: 0.006602\n","\tEpoch:23 [002/005 (0100/0588)]\tLoss Ss: 0.005558\n","\tEpoch:23 [002/005 (0120/0588)]\tLoss Ss: 0.006768\n","\tEpoch:23 [002/005 (0140/0588)]\tLoss Ss: 0.005174\n","\tEpoch:23 [002/005 (0160/0588)]\tLoss Ss: 0.005700\n","\tEpoch:23 [002/005 (0180/0588)]\tLoss Ss: 0.006158\n","\tEpoch:23 [002/005 (0200/0588)]\tLoss Ss: 0.003767\n","\tEpoch:23 [002/005 (0220/0588)]\tLoss Ss: 0.004965\n","\tEpoch:23 [002/005 (0240/0588)]\tLoss Ss: 0.003753\n","\tEpoch:23 [002/005 (0260/0588)]\tLoss Ss: 0.007077\n","\tEpoch:23 [002/005 (0280/0588)]\tLoss Ss: 0.005497\n","\tEpoch:23 [002/005 (0300/0588)]\tLoss Ss: 0.006162\n","\tEpoch:23 [002/005 (0320/0588)]\tLoss Ss: 0.006820\n","\tEpoch:23 [002/005 (0340/0588)]\tLoss Ss: 0.004221\n","\tEpoch:23 [002/005 (0360/0588)]\tLoss Ss: 0.004645\n","\tEpoch:23 [002/005 (0380/0588)]\tLoss Ss: 0.006683\n","\tEpoch:23 [002/005 (0400/0588)]\tLoss Ss: 0.004737\n","\tEpoch:23 [002/005 (0420/0588)]\tLoss Ss: 0.008681\n","\tEpoch:23 [002/005 (0440/0588)]\tLoss Ss: 0.004148\n","\tEpoch:23 [002/005 (0460/0588)]\tLoss Ss: 0.004257\n","\tEpoch:23 [002/005 (0480/0588)]\tLoss Ss: 0.004899\n","\tEpoch:23 [002/005 (0500/0588)]\tLoss Ss: 0.004475\n","\tEpoch:23 [002/005 (0520/0588)]\tLoss Ss: 0.003376\n","\tEpoch:23 [002/005 (0540/0588)]\tLoss Ss: 0.004504\n","\tEpoch:23 [002/005 (0560/0588)]\tLoss Ss: 0.003595\n","\tEpoch:23 [002/005 (0580/0588)]\tLoss Ss: 0.005672\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:23 [003/005 (0000/0755)]\tLoss Ss: 0.016640\n","\tEpoch:23 [003/005 (0020/0755)]\tLoss Ss: 0.007997\n","\tEpoch:23 [003/005 (0040/0755)]\tLoss Ss: 0.017257\n","\tEpoch:23 [003/005 (0060/0755)]\tLoss Ss: 0.019893\n","\tEpoch:23 [003/005 (0080/0755)]\tLoss Ss: 0.010380\n","\tEpoch:23 [003/005 (0100/0755)]\tLoss Ss: 0.009048\n","\tEpoch:23 [003/005 (0120/0755)]\tLoss Ss: 0.022284\n","\tEpoch:23 [003/005 (0140/0755)]\tLoss Ss: 0.016701\n","\tEpoch:23 [003/005 (0160/0755)]\tLoss Ss: 0.018679\n","\tEpoch:23 [003/005 (0180/0755)]\tLoss Ss: 0.014893\n","\tEpoch:23 [003/005 (0200/0755)]\tLoss Ss: 0.011555\n","\tEpoch:23 [003/005 (0220/0755)]\tLoss Ss: 0.017794\n","\tEpoch:23 [003/005 (0240/0755)]\tLoss Ss: 0.015107\n","\tEpoch:23 [003/005 (0260/0755)]\tLoss Ss: 0.012483\n","\tEpoch:23 [003/005 (0280/0755)]\tLoss Ss: 0.016221\n","\tEpoch:23 [003/005 (0300/0755)]\tLoss Ss: 0.013038\n","\tEpoch:23 [003/005 (0320/0755)]\tLoss Ss: 0.015361\n","\tEpoch:23 [003/005 (0340/0755)]\tLoss Ss: 0.010233\n","\tEpoch:23 [003/005 (0360/0755)]\tLoss Ss: 0.006877\n","\tEpoch:23 [003/005 (0380/0755)]\tLoss Ss: 0.014008\n","\tEpoch:23 [003/005 (0400/0755)]\tLoss Ss: 0.021683\n","\tEpoch:23 [003/005 (0420/0755)]\tLoss Ss: 0.013342\n","\tEpoch:23 [003/005 (0440/0755)]\tLoss Ss: 0.013743\n","\tEpoch:23 [003/005 (0460/0755)]\tLoss Ss: 0.015315\n","\tEpoch:23 [003/005 (0480/0755)]\tLoss Ss: 0.012371\n","\tEpoch:23 [003/005 (0500/0755)]\tLoss Ss: 0.009695\n","\tEpoch:23 [003/005 (0520/0755)]\tLoss Ss: 0.012162\n","\tEpoch:23 [003/005 (0540/0755)]\tLoss Ss: 0.009572\n","\tEpoch:23 [003/005 (0560/0755)]\tLoss Ss: 0.015882\n","\tEpoch:23 [003/005 (0580/0755)]\tLoss Ss: 0.009124\n","\tEpoch:23 [003/005 (0600/0755)]\tLoss Ss: 0.012967\n","\tEpoch:23 [003/005 (0620/0755)]\tLoss Ss: 0.013349\n","\tEpoch:23 [003/005 (0640/0755)]\tLoss Ss: 0.009808\n","\tEpoch:23 [003/005 (0660/0755)]\tLoss Ss: 0.013731\n","\tEpoch:23 [003/005 (0680/0755)]\tLoss Ss: 0.015224\n","\tEpoch:23 [003/005 (0700/0755)]\tLoss Ss: 0.016095\n","\tEpoch:23 [003/005 (0720/0755)]\tLoss Ss: 0.011159\n","\tEpoch:23 [003/005 (0740/0755)]\tLoss Ss: 0.013729\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tEpoch:23 [004/005 (0000/0614)]\tLoss Ss: 0.006593\n","\tEpoch:23 [004/005 (0020/0614)]\tLoss Ss: 0.006806\n","\tEpoch:23 [004/005 (0040/0614)]\tLoss Ss: 0.011527\n","\tEpoch:23 [004/005 (0060/0614)]\tLoss Ss: 0.007213\n","\tEpoch:23 [004/005 (0080/0614)]\tLoss Ss: 0.009431\n","\tEpoch:23 [004/005 (0100/0614)]\tLoss Ss: 0.009542\n","\tEpoch:23 [004/005 (0120/0614)]\tLoss Ss: 0.009377\n","\tEpoch:23 [004/005 (0140/0614)]\tLoss Ss: 0.008371\n","\tEpoch:23 [004/005 (0160/0614)]\tLoss Ss: 0.006633\n","\tEpoch:23 [004/005 (0180/0614)]\tLoss Ss: 0.006567\n","\tEpoch:23 [004/005 (0200/0614)]\tLoss Ss: 0.007757\n","\tEpoch:23 [004/005 (0220/0614)]\tLoss Ss: 0.007788\n","\tEpoch:23 [004/005 (0240/0614)]\tLoss Ss: 0.006001\n","\tEpoch:23 [004/005 (0260/0614)]\tLoss Ss: 0.009100\n","\tEpoch:23 [004/005 (0280/0614)]\tLoss Ss: 0.005293\n","\tEpoch:23 [004/005 (0300/0614)]\tLoss Ss: 0.005057\n","\tEpoch:23 [004/005 (0320/0614)]\tLoss Ss: 0.003948\n","\tEpoch:23 [004/005 (0340/0614)]\tLoss Ss: 0.003099\n","\tEpoch:23 [004/005 (0360/0614)]\tLoss Ss: 0.007835\n","\tEpoch:23 [004/005 (0380/0614)]\tLoss Ss: 0.003333\n","\tEpoch:23 [004/005 (0400/0614)]\tLoss Ss: 0.006635\n","\tEpoch:23 [004/005 (0420/0614)]\tLoss Ss: 0.004842\n","\tEpoch:23 [004/005 (0440/0614)]\tLoss Ss: 0.003936\n","\tEpoch:23 [004/005 (0460/0614)]\tLoss Ss: 0.004902\n","\tEpoch:23 [004/005 (0480/0614)]\tLoss Ss: 0.006040\n","\tEpoch:23 [004/005 (0500/0614)]\tLoss Ss: 0.006697\n","\tEpoch:23 [004/005 (0520/0614)]\tLoss Ss: 0.005189\n","\tEpoch:23 [004/005 (0540/0614)]\tLoss Ss: 0.005441\n","\tEpoch:23 [004/005 (0560/0614)]\tLoss Ss: 0.005772\n","\tEpoch:23 [004/005 (0580/0614)]\tLoss Ss: 0.006620\n","\tEpoch:23 [004/005 (0600/0614)]\tLoss Ss: 0.005203\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tEpoch:23 [005/005 (0000/0693)]\tLoss Ss: 0.015025\n","\tEpoch:23 [005/005 (0020/0693)]\tLoss Ss: 0.014516\n","\tEpoch:23 [005/005 (0040/0693)]\tLoss Ss: 0.014171\n","\tEpoch:23 [005/005 (0060/0693)]\tLoss Ss: 0.015536\n","\tEpoch:23 [005/005 (0080/0693)]\tLoss Ss: 0.016941\n","\tEpoch:23 [005/005 (0100/0693)]\tLoss Ss: 0.019476\n","\tEpoch:23 [005/005 (0120/0693)]\tLoss Ss: 0.013270\n","\tEpoch:23 [005/005 (0140/0693)]\tLoss Ss: 0.012897\n","\tEpoch:23 [005/005 (0160/0693)]\tLoss Ss: 0.016165\n","\tEpoch:23 [005/005 (0180/0693)]\tLoss Ss: 0.012143\n","\tEpoch:23 [005/005 (0200/0693)]\tLoss Ss: 0.018696\n","\tEpoch:23 [005/005 (0220/0693)]\tLoss Ss: 0.011498\n","\tEpoch:23 [005/005 (0240/0693)]\tLoss Ss: 0.014939\n","\tEpoch:23 [005/005 (0260/0693)]\tLoss Ss: 0.016264\n","\tEpoch:23 [005/005 (0280/0693)]\tLoss Ss: 0.011250\n","\tEpoch:23 [005/005 (0300/0693)]\tLoss Ss: 0.014741\n","\tEpoch:23 [005/005 (0320/0693)]\tLoss Ss: 0.011397\n","\tEpoch:23 [005/005 (0340/0693)]\tLoss Ss: 0.010162\n","\tEpoch:23 [005/005 (0360/0693)]\tLoss Ss: 0.010171\n","\tEpoch:23 [005/005 (0380/0693)]\tLoss Ss: 0.010092\n","\tEpoch:23 [005/005 (0400/0693)]\tLoss Ss: 0.016604\n","\tEpoch:23 [005/005 (0420/0693)]\tLoss Ss: 0.014250\n","\tEpoch:23 [005/005 (0440/0693)]\tLoss Ss: 0.016545\n","\tEpoch:23 [005/005 (0460/0693)]\tLoss Ss: 0.014775\n","\tEpoch:23 [005/005 (0480/0693)]\tLoss Ss: 0.016090\n","\tEpoch:23 [005/005 (0500/0693)]\tLoss Ss: 0.016009\n","\tEpoch:23 [005/005 (0520/0693)]\tLoss Ss: 0.016553\n","\tEpoch:23 [005/005 (0540/0693)]\tLoss Ss: 0.014452\n","\tEpoch:23 [005/005 (0560/0693)]\tLoss Ss: 0.012125\n","\tEpoch:23 [005/005 (0580/0693)]\tLoss Ss: 0.013879\n","\tEpoch:23 [005/005 (0600/0693)]\tLoss Ss: 0.014164\n","\tEpoch:23 [005/005 (0620/0693)]\tLoss Ss: 0.013812\n","\tEpoch:23 [005/005 (0640/0693)]\tLoss Ss: 0.015490\n","\tEpoch:23 [005/005 (0660/0693)]\tLoss Ss: 0.016580\n","\tEpoch:23 [005/005 (0680/0693)]\tLoss Ss: 0.013699\n","Now train the rotated image\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tRotated_Epoch:23 [000/005 (0000/0693)]\tLoss Ss: 0.015187\n","\tRotated_Epoch:23 [000/005 (0020/0693)]\tLoss Ss: 0.017506\n","\tRotated_Epoch:23 [000/005 (0040/0693)]\tLoss Ss: 0.012284\n","\tRotated_Epoch:23 [000/005 (0060/0693)]\tLoss Ss: 0.018823\n","\tRotated_Epoch:23 [000/005 (0080/0693)]\tLoss Ss: 0.013602\n","\tRotated_Epoch:23 [000/005 (0100/0693)]\tLoss Ss: 0.019269\n","\tRotated_Epoch:23 [000/005 (0120/0693)]\tLoss Ss: 0.014865\n","\tRotated_Epoch:23 [000/005 (0140/0693)]\tLoss Ss: 0.015843\n","\tRotated_Epoch:23 [000/005 (0160/0693)]\tLoss Ss: 0.015033\n","\tRotated_Epoch:23 [000/005 (0180/0693)]\tLoss Ss: 0.015715\n","\tRotated_Epoch:23 [000/005 (0200/0693)]\tLoss Ss: 0.016550\n","\tRotated_Epoch:23 [000/005 (0220/0693)]\tLoss Ss: 0.020234\n","\tRotated_Epoch:23 [000/005 (0240/0693)]\tLoss Ss: 0.017486\n","\tRotated_Epoch:23 [000/005 (0260/0693)]\tLoss Ss: 0.017832\n","\tRotated_Epoch:23 [000/005 (0280/0693)]\tLoss Ss: 0.015257\n","\tRotated_Epoch:23 [000/005 (0300/0693)]\tLoss Ss: 0.016586\n","\tRotated_Epoch:23 [000/005 (0320/0693)]\tLoss Ss: 0.015548\n","\tRotated_Epoch:23 [000/005 (0340/0693)]\tLoss Ss: 0.016010\n","\tRotated_Epoch:23 [000/005 (0360/0693)]\tLoss Ss: 0.016648\n","\tRotated_Epoch:23 [000/005 (0380/0693)]\tLoss Ss: 0.020557\n","\tRotated_Epoch:23 [000/005 (0400/0693)]\tLoss Ss: 0.012229\n","\tRotated_Epoch:23 [000/005 (0420/0693)]\tLoss Ss: 0.025191\n","\tRotated_Epoch:23 [000/005 (0440/0693)]\tLoss Ss: 0.017119\n","\tRotated_Epoch:23 [000/005 (0460/0693)]\tLoss Ss: 0.012949\n","\tRotated_Epoch:23 [000/005 (0480/0693)]\tLoss Ss: 0.014959\n","\tRotated_Epoch:23 [000/005 (0500/0693)]\tLoss Ss: 0.011738\n","\tRotated_Epoch:23 [000/005 (0520/0693)]\tLoss Ss: 0.013619\n","\tRotated_Epoch:23 [000/005 (0540/0693)]\tLoss Ss: 0.014526\n","\tRotated_Epoch:23 [000/005 (0560/0693)]\tLoss Ss: 0.014084\n","\tRotated_Epoch:23 [000/005 (0580/0693)]\tLoss Ss: 0.013675\n","\tRotated_Epoch:23 [000/005 (0600/0693)]\tLoss Ss: 0.015555\n","\tRotated_Epoch:23 [000/005 (0620/0693)]\tLoss Ss: 0.012843\n","\tRotated_Epoch:23 [000/005 (0640/0693)]\tLoss Ss: 0.019181\n","\tRotated_Epoch:23 [000/005 (0660/0693)]\tLoss Ss: 0.010455\n","\tRotated_Epoch:23 [000/005 (0680/0693)]\tLoss Ss: 0.019074\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:23 [001/005 (0000/0755)]\tLoss Ss: 0.138732\n","\tRotated_Epoch:23 [001/005 (0020/0755)]\tLoss Ss: 0.246003\n","\tRotated_Epoch:23 [001/005 (0040/0755)]\tLoss Ss: 0.109762\n","\tRotated_Epoch:23 [001/005 (0060/0755)]\tLoss Ss: 0.085514\n","\tRotated_Epoch:23 [001/005 (0080/0755)]\tLoss Ss: 0.041759\n","\tRotated_Epoch:23 [001/005 (0100/0755)]\tLoss Ss: 0.079853\n","\tRotated_Epoch:23 [001/005 (0120/0755)]\tLoss Ss: 0.056128\n","\tRotated_Epoch:23 [001/005 (0140/0755)]\tLoss Ss: 0.051551\n","\tRotated_Epoch:23 [001/005 (0160/0755)]\tLoss Ss: 0.062546\n","\tRotated_Epoch:23 [001/005 (0180/0755)]\tLoss Ss: 0.043369\n","\tRotated_Epoch:23 [001/005 (0200/0755)]\tLoss Ss: 0.038222\n","\tRotated_Epoch:23 [001/005 (0220/0755)]\tLoss Ss: 0.044558\n","\tRotated_Epoch:23 [001/005 (0240/0755)]\tLoss Ss: 0.035071\n","\tRotated_Epoch:23 [001/005 (0260/0755)]\tLoss Ss: 0.040933\n","\tRotated_Epoch:23 [001/005 (0280/0755)]\tLoss Ss: 0.039279\n","\tRotated_Epoch:23 [001/005 (0300/0755)]\tLoss Ss: 0.033925\n","\tRotated_Epoch:23 [001/005 (0320/0755)]\tLoss Ss: 0.040613\n","\tRotated_Epoch:23 [001/005 (0340/0755)]\tLoss Ss: 0.045145\n","\tRotated_Epoch:23 [001/005 (0360/0755)]\tLoss Ss: 0.045507\n","\tRotated_Epoch:23 [001/005 (0380/0755)]\tLoss Ss: 0.037765\n","\tRotated_Epoch:23 [001/005 (0400/0755)]\tLoss Ss: 0.036146\n","\tRotated_Epoch:23 [001/005 (0420/0755)]\tLoss Ss: 0.050218\n","\tRotated_Epoch:23 [001/005 (0440/0755)]\tLoss Ss: 0.027629\n","\tRotated_Epoch:23 [001/005 (0460/0755)]\tLoss Ss: 0.029118\n","\tRotated_Epoch:23 [001/005 (0480/0755)]\tLoss Ss: 0.030123\n","\tRotated_Epoch:23 [001/005 (0500/0755)]\tLoss Ss: 0.024992\n","\tRotated_Epoch:23 [001/005 (0520/0755)]\tLoss Ss: 0.040414\n","\tRotated_Epoch:23 [001/005 (0540/0755)]\tLoss Ss: 0.033248\n","\tRotated_Epoch:23 [001/005 (0560/0755)]\tLoss Ss: 0.033906\n","\tRotated_Epoch:23 [001/005 (0580/0755)]\tLoss Ss: 0.038177\n","\tRotated_Epoch:23 [001/005 (0600/0755)]\tLoss Ss: 0.032412\n","\tRotated_Epoch:23 [001/005 (0620/0755)]\tLoss Ss: 0.043696\n","\tRotated_Epoch:23 [001/005 (0640/0755)]\tLoss Ss: 0.032531\n","\tRotated_Epoch:23 [001/005 (0660/0755)]\tLoss Ss: 0.046822\n","\tRotated_Epoch:23 [001/005 (0680/0755)]\tLoss Ss: 0.036922\n","\tRotated_Epoch:23 [001/005 (0700/0755)]\tLoss Ss: 0.030215\n","\tRotated_Epoch:23 [001/005 (0720/0755)]\tLoss Ss: 0.026245\n","\tRotated_Epoch:23 [001/005 (0740/0755)]\tLoss Ss: 0.033576\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:23 [002/005 (0000/0755)]\tLoss Ss: 0.226548\n","\tRotated_Epoch:23 [002/005 (0020/0755)]\tLoss Ss: 0.072541\n","\tRotated_Epoch:23 [002/005 (0040/0755)]\tLoss Ss: 0.119679\n","\tRotated_Epoch:23 [002/005 (0060/0755)]\tLoss Ss: 0.101429\n","\tRotated_Epoch:23 [002/005 (0080/0755)]\tLoss Ss: 0.066674\n","\tRotated_Epoch:23 [002/005 (0100/0755)]\tLoss Ss: 0.029979\n","\tRotated_Epoch:23 [002/005 (0120/0755)]\tLoss Ss: 0.058966\n","\tRotated_Epoch:23 [002/005 (0140/0755)]\tLoss Ss: 0.046943\n","\tRotated_Epoch:23 [002/005 (0160/0755)]\tLoss Ss: 0.032320\n","\tRotated_Epoch:23 [002/005 (0180/0755)]\tLoss Ss: 0.070053\n","\tRotated_Epoch:23 [002/005 (0200/0755)]\tLoss Ss: 0.040727\n","\tRotated_Epoch:23 [002/005 (0220/0755)]\tLoss Ss: 0.041529\n","\tRotated_Epoch:23 [002/005 (0240/0755)]\tLoss Ss: 0.031589\n","\tRotated_Epoch:23 [002/005 (0260/0755)]\tLoss Ss: 0.032584\n","\tRotated_Epoch:23 [002/005 (0280/0755)]\tLoss Ss: 0.030225\n","\tRotated_Epoch:23 [002/005 (0300/0755)]\tLoss Ss: 0.033979\n","\tRotated_Epoch:23 [002/005 (0320/0755)]\tLoss Ss: 0.026309\n","\tRotated_Epoch:23 [002/005 (0340/0755)]\tLoss Ss: 0.028832\n","\tRotated_Epoch:23 [002/005 (0360/0755)]\tLoss Ss: 0.027723\n","\tRotated_Epoch:23 [002/005 (0380/0755)]\tLoss Ss: 0.036224\n","\tRotated_Epoch:23 [002/005 (0400/0755)]\tLoss Ss: 0.031216\n","\tRotated_Epoch:23 [002/005 (0420/0755)]\tLoss Ss: 0.031332\n","\tRotated_Epoch:23 [002/005 (0440/0755)]\tLoss Ss: 0.032640\n","\tRotated_Epoch:23 [002/005 (0460/0755)]\tLoss Ss: 0.023270\n","\tRotated_Epoch:23 [002/005 (0480/0755)]\tLoss Ss: 0.043921\n","\tRotated_Epoch:23 [002/005 (0500/0755)]\tLoss Ss: 0.027010\n","\tRotated_Epoch:23 [002/005 (0520/0755)]\tLoss Ss: 0.022222\n","\tRotated_Epoch:23 [002/005 (0540/0755)]\tLoss Ss: 0.015832\n","\tRotated_Epoch:23 [002/005 (0560/0755)]\tLoss Ss: 0.027017\n","\tRotated_Epoch:23 [002/005 (0580/0755)]\tLoss Ss: 0.019473\n","\tRotated_Epoch:23 [002/005 (0600/0755)]\tLoss Ss: 0.020575\n","\tRotated_Epoch:23 [002/005 (0620/0755)]\tLoss Ss: 0.013994\n","\tRotated_Epoch:23 [002/005 (0640/0755)]\tLoss Ss: 0.024170\n","\tRotated_Epoch:23 [002/005 (0660/0755)]\tLoss Ss: 0.019630\n","\tRotated_Epoch:23 [002/005 (0680/0755)]\tLoss Ss: 0.030454\n","\tRotated_Epoch:23 [002/005 (0700/0755)]\tLoss Ss: 0.019230\n","\tRotated_Epoch:23 [002/005 (0720/0755)]\tLoss Ss: 0.020841\n","\tRotated_Epoch:23 [002/005 (0740/0755)]\tLoss Ss: 0.020328\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tRotated_Epoch:23 [003/005 (0000/0588)]\tLoss Ss: 0.072900\n","\tRotated_Epoch:23 [003/005 (0020/0588)]\tLoss Ss: 0.055406\n","\tRotated_Epoch:23 [003/005 (0040/0588)]\tLoss Ss: 0.084758\n","\tRotated_Epoch:23 [003/005 (0060/0588)]\tLoss Ss: 0.044431\n","\tRotated_Epoch:23 [003/005 (0080/0588)]\tLoss Ss: 0.072732\n","\tRotated_Epoch:23 [003/005 (0100/0588)]\tLoss Ss: 0.068094\n","\tRotated_Epoch:23 [003/005 (0120/0588)]\tLoss Ss: 0.071266\n","\tRotated_Epoch:23 [003/005 (0140/0588)]\tLoss Ss: 0.055408\n","\tRotated_Epoch:23 [003/005 (0160/0588)]\tLoss Ss: 0.077056\n","\tRotated_Epoch:23 [003/005 (0180/0588)]\tLoss Ss: 0.068730\n","\tRotated_Epoch:23 [003/005 (0200/0588)]\tLoss Ss: 0.057398\n","\tRotated_Epoch:23 [003/005 (0220/0588)]\tLoss Ss: 0.064090\n","\tRotated_Epoch:23 [003/005 (0240/0588)]\tLoss Ss: 0.043173\n","\tRotated_Epoch:23 [003/005 (0260/0588)]\tLoss Ss: 0.080259\n","\tRotated_Epoch:23 [003/005 (0280/0588)]\tLoss Ss: 0.051586\n","\tRotated_Epoch:23 [003/005 (0300/0588)]\tLoss Ss: 0.038697\n","\tRotated_Epoch:23 [003/005 (0320/0588)]\tLoss Ss: 0.063062\n","\tRotated_Epoch:23 [003/005 (0340/0588)]\tLoss Ss: 0.070937\n","\tRotated_Epoch:23 [003/005 (0360/0588)]\tLoss Ss: 0.055260\n","\tRotated_Epoch:23 [003/005 (0380/0588)]\tLoss Ss: 0.054728\n","\tRotated_Epoch:23 [003/005 (0400/0588)]\tLoss Ss: 0.055692\n","\tRotated_Epoch:23 [003/005 (0420/0588)]\tLoss Ss: 0.053758\n","\tRotated_Epoch:23 [003/005 (0440/0588)]\tLoss Ss: 0.050316\n","\tRotated_Epoch:23 [003/005 (0460/0588)]\tLoss Ss: 0.072494\n","\tRotated_Epoch:23 [003/005 (0480/0588)]\tLoss Ss: 0.041204\n","\tRotated_Epoch:23 [003/005 (0500/0588)]\tLoss Ss: 0.051861\n","\tRotated_Epoch:23 [003/005 (0520/0588)]\tLoss Ss: 0.036424\n","\tRotated_Epoch:23 [003/005 (0540/0588)]\tLoss Ss: 0.051054\n","\tRotated_Epoch:23 [003/005 (0560/0588)]\tLoss Ss: 0.069124\n","\tRotated_Epoch:23 [003/005 (0580/0588)]\tLoss Ss: 0.053492\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tRotated_Epoch:23 [004/005 (0000/0693)]\tLoss Ss: 0.028643\n","\tRotated_Epoch:23 [004/005 (0020/0693)]\tLoss Ss: 0.032597\n","\tRotated_Epoch:23 [004/005 (0040/0693)]\tLoss Ss: 0.023500\n","\tRotated_Epoch:23 [004/005 (0060/0693)]\tLoss Ss: 0.020044\n","\tRotated_Epoch:23 [004/005 (0080/0693)]\tLoss Ss: 0.016441\n","\tRotated_Epoch:23 [004/005 (0100/0693)]\tLoss Ss: 0.027890\n","\tRotated_Epoch:23 [004/005 (0120/0693)]\tLoss Ss: 0.015861\n","\tRotated_Epoch:23 [004/005 (0140/0693)]\tLoss Ss: 0.020597\n","\tRotated_Epoch:23 [004/005 (0160/0693)]\tLoss Ss: 0.025287\n","\tRotated_Epoch:23 [004/005 (0180/0693)]\tLoss Ss: 0.016711\n","\tRotated_Epoch:23 [004/005 (0200/0693)]\tLoss Ss: 0.020520\n","\tRotated_Epoch:23 [004/005 (0220/0693)]\tLoss Ss: 0.012245\n","\tRotated_Epoch:23 [004/005 (0240/0693)]\tLoss Ss: 0.015042\n","\tRotated_Epoch:23 [004/005 (0260/0693)]\tLoss Ss: 0.022145\n","\tRotated_Epoch:23 [004/005 (0280/0693)]\tLoss Ss: 0.020046\n","\tRotated_Epoch:23 [004/005 (0300/0693)]\tLoss Ss: 0.034911\n","\tRotated_Epoch:23 [004/005 (0320/0693)]\tLoss Ss: 0.018620\n","\tRotated_Epoch:23 [004/005 (0340/0693)]\tLoss Ss: 0.016779\n","\tRotated_Epoch:23 [004/005 (0360/0693)]\tLoss Ss: 0.015328\n","\tRotated_Epoch:23 [004/005 (0380/0693)]\tLoss Ss: 0.020010\n","\tRotated_Epoch:23 [004/005 (0400/0693)]\tLoss Ss: 0.014981\n","\tRotated_Epoch:23 [004/005 (0420/0693)]\tLoss Ss: 0.019060\n","\tRotated_Epoch:23 [004/005 (0440/0693)]\tLoss Ss: 0.016787\n","\tRotated_Epoch:23 [004/005 (0460/0693)]\tLoss Ss: 0.017089\n","\tRotated_Epoch:23 [004/005 (0480/0693)]\tLoss Ss: 0.015655\n","\tRotated_Epoch:23 [004/005 (0500/0693)]\tLoss Ss: 0.014785\n","\tRotated_Epoch:23 [004/005 (0520/0693)]\tLoss Ss: 0.011954\n","\tRotated_Epoch:23 [004/005 (0540/0693)]\tLoss Ss: 0.014768\n","\tRotated_Epoch:23 [004/005 (0560/0693)]\tLoss Ss: 0.015605\n","\tRotated_Epoch:23 [004/005 (0580/0693)]\tLoss Ss: 0.015333\n","\tRotated_Epoch:23 [004/005 (0600/0693)]\tLoss Ss: 0.015107\n","\tRotated_Epoch:23 [004/005 (0620/0693)]\tLoss Ss: 0.011037\n","\tRotated_Epoch:23 [004/005 (0640/0693)]\tLoss Ss: 0.021327\n","\tRotated_Epoch:23 [004/005 (0660/0693)]\tLoss Ss: 0.015574\n","\tRotated_Epoch:23 [004/005 (0680/0693)]\tLoss Ss: 0.010462\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tRotated_Epoch:23 [005/005 (0000/0614)]\tLoss Ss: 0.055853\n","\tRotated_Epoch:23 [005/005 (0020/0614)]\tLoss Ss: 0.030663\n","\tRotated_Epoch:23 [005/005 (0040/0614)]\tLoss Ss: 0.034556\n","\tRotated_Epoch:23 [005/005 (0060/0614)]\tLoss Ss: 0.026265\n","\tRotated_Epoch:23 [005/005 (0080/0614)]\tLoss Ss: 0.018445\n","\tRotated_Epoch:23 [005/005 (0100/0614)]\tLoss Ss: 0.012401\n","\tRotated_Epoch:23 [005/005 (0120/0614)]\tLoss Ss: 0.014557\n","\tRotated_Epoch:23 [005/005 (0140/0614)]\tLoss Ss: 0.020049\n","\tRotated_Epoch:23 [005/005 (0160/0614)]\tLoss Ss: 0.010048\n","\tRotated_Epoch:23 [005/005 (0180/0614)]\tLoss Ss: 0.010017\n","\tRotated_Epoch:23 [005/005 (0200/0614)]\tLoss Ss: 0.007172\n","\tRotated_Epoch:23 [005/005 (0220/0614)]\tLoss Ss: 0.010225\n","\tRotated_Epoch:23 [005/005 (0240/0614)]\tLoss Ss: 0.011674\n","\tRotated_Epoch:23 [005/005 (0260/0614)]\tLoss Ss: 0.008166\n","\tRotated_Epoch:23 [005/005 (0280/0614)]\tLoss Ss: 0.007494\n","\tRotated_Epoch:23 [005/005 (0300/0614)]\tLoss Ss: 0.008101\n","\tRotated_Epoch:23 [005/005 (0320/0614)]\tLoss Ss: 0.006953\n","\tRotated_Epoch:23 [005/005 (0340/0614)]\tLoss Ss: 0.006169\n","\tRotated_Epoch:23 [005/005 (0360/0614)]\tLoss Ss: 0.004767\n","\tRotated_Epoch:23 [005/005 (0380/0614)]\tLoss Ss: 0.007977\n","\tRotated_Epoch:23 [005/005 (0400/0614)]\tLoss Ss: 0.008296\n","\tRotated_Epoch:23 [005/005 (0420/0614)]\tLoss Ss: 0.007611\n","\tRotated_Epoch:23 [005/005 (0440/0614)]\tLoss Ss: 0.006913\n","\tRotated_Epoch:23 [005/005 (0460/0614)]\tLoss Ss: 0.007409\n","\tRotated_Epoch:23 [005/005 (0480/0614)]\tLoss Ss: 0.009864\n","\tRotated_Epoch:23 [005/005 (0500/0614)]\tLoss Ss: 0.007436\n","\tRotated_Epoch:23 [005/005 (0520/0614)]\tLoss Ss: 0.005171\n","\tRotated_Epoch:23 [005/005 (0540/0614)]\tLoss Ss: 0.007139\n","\tRotated_Epoch:23 [005/005 (0560/0614)]\tLoss Ss: 0.004425\n","\tRotated_Epoch:23 [005/005 (0580/0614)]\tLoss Ss: 0.005486\n","\tRotated_Epoch:23 [005/005 (0600/0614)]\tLoss Ss: 0.006035\n","new_1_6_d.nii\n","new_1_6_m.nii\n","new_1_7_d.nii\n","new_1_7_m.nii\n","new_2_5_d.nii\n","new_2_5_m.nii\n","new_2_6_d.nii\n","new_2_6_m.nii\n","new_2_7_d.nii\n","new_2_7_m.nii\n","new_3_d (2).nii\n","new_3_m (2).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (3).nii\n","new_3_m (3).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (7).nii\n","new_3_m (7).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (8).nii\n","new_3_m (8).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (9).nii\n","new_3_m (9).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_4_10_d_163526.nii\n","new_4_10_m_163526.nii.gz\n","new_4_11_d_084336.nii\n","new_4_11_m_084336.nii.gz\n","new_4_7_d_184422.nii\n","new_4_7_m_184422.nii.gz\n","new_4_8_d_143412.nii\n","new_4_8_m_143412.nii.gz\n","new_4_9_d_155347.nii\n","new_4_9_m_155347.nii.gz\n","\tEpoch: 23; Dice: 0.9637 +/- 0.0060; Loss: 9.6865\n","Begin Epoch 24\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tEpoch:24 [000/005 (0000/0693)]\tLoss Ss: 0.028209\n","\tEpoch:24 [000/005 (0020/0693)]\tLoss Ss: 0.018909\n","\tEpoch:24 [000/005 (0040/0693)]\tLoss Ss: 0.028663\n","\tEpoch:24 [000/005 (0060/0693)]\tLoss Ss: 0.018796\n","\tEpoch:24 [000/005 (0080/0693)]\tLoss Ss: 0.014387\n","\tEpoch:24 [000/005 (0100/0693)]\tLoss Ss: 0.015757\n","\tEpoch:24 [000/005 (0120/0693)]\tLoss Ss: 0.018279\n","\tEpoch:24 [000/005 (0140/0693)]\tLoss Ss: 0.018870\n","\tEpoch:24 [000/005 (0160/0693)]\tLoss Ss: 0.010995\n","\tEpoch:24 [000/005 (0180/0693)]\tLoss Ss: 0.015164\n","\tEpoch:24 [000/005 (0200/0693)]\tLoss Ss: 0.014645\n","\tEpoch:24 [000/005 (0220/0693)]\tLoss Ss: 0.016967\n","\tEpoch:24 [000/005 (0240/0693)]\tLoss Ss: 0.010991\n","\tEpoch:24 [000/005 (0260/0693)]\tLoss Ss: 0.015348\n","\tEpoch:24 [000/005 (0280/0693)]\tLoss Ss: 0.015783\n","\tEpoch:24 [000/005 (0300/0693)]\tLoss Ss: 0.010254\n","\tEpoch:24 [000/005 (0320/0693)]\tLoss Ss: 0.012005\n","\tEpoch:24 [000/005 (0340/0693)]\tLoss Ss: 0.015518\n","\tEpoch:24 [000/005 (0360/0693)]\tLoss Ss: 0.016918\n","\tEpoch:24 [000/005 (0380/0693)]\tLoss Ss: 0.011825\n","\tEpoch:24 [000/005 (0400/0693)]\tLoss Ss: 0.012613\n","\tEpoch:24 [000/005 (0420/0693)]\tLoss Ss: 0.014710\n","\tEpoch:24 [000/005 (0440/0693)]\tLoss Ss: 0.017232\n","\tEpoch:24 [000/005 (0460/0693)]\tLoss Ss: 0.019623\n","\tEpoch:24 [000/005 (0480/0693)]\tLoss Ss: 0.020905\n","\tEpoch:24 [000/005 (0500/0693)]\tLoss Ss: 0.015549\n","\tEpoch:24 [000/005 (0520/0693)]\tLoss Ss: 0.009568\n","\tEpoch:24 [000/005 (0540/0693)]\tLoss Ss: 0.016014\n","\tEpoch:24 [000/005 (0560/0693)]\tLoss Ss: 0.017991\n","\tEpoch:24 [000/005 (0580/0693)]\tLoss Ss: 0.013925\n","\tEpoch:24 [000/005 (0600/0693)]\tLoss Ss: 0.012836\n","\tEpoch:24 [000/005 (0620/0693)]\tLoss Ss: 0.014189\n","\tEpoch:24 [000/005 (0640/0693)]\tLoss Ss: 0.014988\n","\tEpoch:24 [000/005 (0660/0693)]\tLoss Ss: 0.021346\n","\tEpoch:24 [000/005 (0680/0693)]\tLoss Ss: 0.014336\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tEpoch:24 [001/005 (0000/0693)]\tLoss Ss: 0.024905\n","\tEpoch:24 [001/005 (0020/0693)]\tLoss Ss: 0.015532\n","\tEpoch:24 [001/005 (0040/0693)]\tLoss Ss: 0.010493\n","\tEpoch:24 [001/005 (0060/0693)]\tLoss Ss: 0.015035\n","\tEpoch:24 [001/005 (0080/0693)]\tLoss Ss: 0.012001\n","\tEpoch:24 [001/005 (0100/0693)]\tLoss Ss: 0.011665\n","\tEpoch:24 [001/005 (0120/0693)]\tLoss Ss: 0.012542\n","\tEpoch:24 [001/005 (0140/0693)]\tLoss Ss: 0.012337\n","\tEpoch:24 [001/005 (0160/0693)]\tLoss Ss: 0.013894\n","\tEpoch:24 [001/005 (0180/0693)]\tLoss Ss: 0.009304\n","\tEpoch:24 [001/005 (0200/0693)]\tLoss Ss: 0.016231\n","\tEpoch:24 [001/005 (0220/0693)]\tLoss Ss: 0.010044\n","\tEpoch:24 [001/005 (0240/0693)]\tLoss Ss: 0.012701\n","\tEpoch:24 [001/005 (0260/0693)]\tLoss Ss: 0.011775\n","\tEpoch:24 [001/005 (0280/0693)]\tLoss Ss: 0.012687\n","\tEpoch:24 [001/005 (0300/0693)]\tLoss Ss: 0.009189\n","\tEpoch:24 [001/005 (0320/0693)]\tLoss Ss: 0.011951\n","\tEpoch:24 [001/005 (0340/0693)]\tLoss Ss: 0.016887\n","\tEpoch:24 [001/005 (0360/0693)]\tLoss Ss: 0.018120\n","\tEpoch:24 [001/005 (0380/0693)]\tLoss Ss: 0.011337\n","\tEpoch:24 [001/005 (0400/0693)]\tLoss Ss: 0.011420\n","\tEpoch:24 [001/005 (0420/0693)]\tLoss Ss: 0.009514\n","\tEpoch:24 [001/005 (0440/0693)]\tLoss Ss: 0.012274\n","\tEpoch:24 [001/005 (0460/0693)]\tLoss Ss: 0.008098\n","\tEpoch:24 [001/005 (0480/0693)]\tLoss Ss: 0.011484\n","\tEpoch:24 [001/005 (0500/0693)]\tLoss Ss: 0.013085\n","\tEpoch:24 [001/005 (0520/0693)]\tLoss Ss: 0.012895\n","\tEpoch:24 [001/005 (0540/0693)]\tLoss Ss: 0.011132\n","\tEpoch:24 [001/005 (0560/0693)]\tLoss Ss: 0.011425\n","\tEpoch:24 [001/005 (0580/0693)]\tLoss Ss: 0.015507\n","\tEpoch:24 [001/005 (0600/0693)]\tLoss Ss: 0.013411\n","\tEpoch:24 [001/005 (0620/0693)]\tLoss Ss: 0.013373\n","\tEpoch:24 [001/005 (0640/0693)]\tLoss Ss: 0.012609\n","\tEpoch:24 [001/005 (0660/0693)]\tLoss Ss: 0.015463\n","\tEpoch:24 [001/005 (0680/0693)]\tLoss Ss: 0.011085\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tEpoch:24 [002/005 (0000/0614)]\tLoss Ss: 0.004676\n","\tEpoch:24 [002/005 (0020/0614)]\tLoss Ss: 0.010732\n","\tEpoch:24 [002/005 (0040/0614)]\tLoss Ss: 0.008184\n","\tEpoch:24 [002/005 (0060/0614)]\tLoss Ss: 0.005634\n","\tEpoch:24 [002/005 (0080/0614)]\tLoss Ss: 0.006272\n","\tEpoch:24 [002/005 (0100/0614)]\tLoss Ss: 0.005874\n","\tEpoch:24 [002/005 (0120/0614)]\tLoss Ss: 0.007910\n","\tEpoch:24 [002/005 (0140/0614)]\tLoss Ss: 0.007120\n","\tEpoch:24 [002/005 (0160/0614)]\tLoss Ss: 0.008557\n","\tEpoch:24 [002/005 (0180/0614)]\tLoss Ss: 0.007577\n","\tEpoch:24 [002/005 (0200/0614)]\tLoss Ss: 0.006715\n","\tEpoch:24 [002/005 (0220/0614)]\tLoss Ss: 0.006034\n","\tEpoch:24 [002/005 (0240/0614)]\tLoss Ss: 0.005483\n","\tEpoch:24 [002/005 (0260/0614)]\tLoss Ss: 0.007843\n","\tEpoch:24 [002/005 (0280/0614)]\tLoss Ss: 0.005218\n","\tEpoch:24 [002/005 (0300/0614)]\tLoss Ss: 0.004254\n","\tEpoch:24 [002/005 (0320/0614)]\tLoss Ss: 0.008720\n","\tEpoch:24 [002/005 (0340/0614)]\tLoss Ss: 0.004208\n","\tEpoch:24 [002/005 (0360/0614)]\tLoss Ss: 0.006474\n","\tEpoch:24 [002/005 (0380/0614)]\tLoss Ss: 0.006168\n","\tEpoch:24 [002/005 (0400/0614)]\tLoss Ss: 0.009238\n","\tEpoch:24 [002/005 (0420/0614)]\tLoss Ss: 0.006872\n","\tEpoch:24 [002/005 (0440/0614)]\tLoss Ss: 0.005125\n","\tEpoch:24 [002/005 (0460/0614)]\tLoss Ss: 0.005688\n","\tEpoch:24 [002/005 (0480/0614)]\tLoss Ss: 0.003953\n","\tEpoch:24 [002/005 (0500/0614)]\tLoss Ss: 0.005438\n","\tEpoch:24 [002/005 (0520/0614)]\tLoss Ss: 0.004507\n","\tEpoch:24 [002/005 (0540/0614)]\tLoss Ss: 0.004693\n","\tEpoch:24 [002/005 (0560/0614)]\tLoss Ss: 0.005228\n","\tEpoch:24 [002/005 (0580/0614)]\tLoss Ss: 0.005471\n","\tEpoch:24 [002/005 (0600/0614)]\tLoss Ss: 0.008437\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tEpoch:24 [003/005 (0000/0588)]\tLoss Ss: 0.005980\n","\tEpoch:24 [003/005 (0020/0588)]\tLoss Ss: 0.007273\n","\tEpoch:24 [003/005 (0040/0588)]\tLoss Ss: 0.005758\n","\tEpoch:24 [003/005 (0060/0588)]\tLoss Ss: 0.004952\n","\tEpoch:24 [003/005 (0080/0588)]\tLoss Ss: 0.004560\n","\tEpoch:24 [003/005 (0100/0588)]\tLoss Ss: 0.007129\n","\tEpoch:24 [003/005 (0120/0588)]\tLoss Ss: 0.007367\n","\tEpoch:24 [003/005 (0140/0588)]\tLoss Ss: 0.006181\n","\tEpoch:24 [003/005 (0160/0588)]\tLoss Ss: 0.005919\n","\tEpoch:24 [003/005 (0180/0588)]\tLoss Ss: 0.005435\n","\tEpoch:24 [003/005 (0200/0588)]\tLoss Ss: 0.003617\n","\tEpoch:24 [003/005 (0220/0588)]\tLoss Ss: 0.005110\n","\tEpoch:24 [003/005 (0240/0588)]\tLoss Ss: 0.005429\n","\tEpoch:24 [003/005 (0260/0588)]\tLoss Ss: 0.004818\n","\tEpoch:24 [003/005 (0280/0588)]\tLoss Ss: 0.005475\n","\tEpoch:24 [003/005 (0300/0588)]\tLoss Ss: 0.005790\n","\tEpoch:24 [003/005 (0320/0588)]\tLoss Ss: 0.002914\n","\tEpoch:24 [003/005 (0340/0588)]\tLoss Ss: 0.006206\n","\tEpoch:24 [003/005 (0360/0588)]\tLoss Ss: 0.006346\n","\tEpoch:24 [003/005 (0380/0588)]\tLoss Ss: 0.003135\n","\tEpoch:24 [003/005 (0400/0588)]\tLoss Ss: 0.004887\n","\tEpoch:24 [003/005 (0420/0588)]\tLoss Ss: 0.004108\n","\tEpoch:24 [003/005 (0440/0588)]\tLoss Ss: 0.006160\n","\tEpoch:24 [003/005 (0460/0588)]\tLoss Ss: 0.003346\n","\tEpoch:24 [003/005 (0480/0588)]\tLoss Ss: 0.006491\n","\tEpoch:24 [003/005 (0500/0588)]\tLoss Ss: 0.003171\n","\tEpoch:24 [003/005 (0520/0588)]\tLoss Ss: 0.006336\n","\tEpoch:24 [003/005 (0540/0588)]\tLoss Ss: 0.006022\n","\tEpoch:24 [003/005 (0560/0588)]\tLoss Ss: 0.003057\n","\tEpoch:24 [003/005 (0580/0588)]\tLoss Ss: 0.002126\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:24 [004/005 (0000/0755)]\tLoss Ss: 0.016079\n","\tEpoch:24 [004/005 (0020/0755)]\tLoss Ss: 0.022419\n","\tEpoch:24 [004/005 (0040/0755)]\tLoss Ss: 0.026784\n","\tEpoch:24 [004/005 (0060/0755)]\tLoss Ss: 0.021832\n","\tEpoch:24 [004/005 (0080/0755)]\tLoss Ss: 0.021541\n","\tEpoch:24 [004/005 (0100/0755)]\tLoss Ss: 0.016726\n","\tEpoch:24 [004/005 (0120/0755)]\tLoss Ss: 0.016401\n","\tEpoch:24 [004/005 (0140/0755)]\tLoss Ss: 0.022794\n","\tEpoch:24 [004/005 (0160/0755)]\tLoss Ss: 0.018597\n","\tEpoch:24 [004/005 (0180/0755)]\tLoss Ss: 0.010127\n","\tEpoch:24 [004/005 (0200/0755)]\tLoss Ss: 0.012946\n","\tEpoch:24 [004/005 (0220/0755)]\tLoss Ss: 0.013370\n","\tEpoch:24 [004/005 (0240/0755)]\tLoss Ss: 0.021724\n","\tEpoch:24 [004/005 (0260/0755)]\tLoss Ss: 0.020369\n","\tEpoch:24 [004/005 (0280/0755)]\tLoss Ss: 0.025700\n","\tEpoch:24 [004/005 (0300/0755)]\tLoss Ss: 0.020010\n","\tEpoch:24 [004/005 (0320/0755)]\tLoss Ss: 0.021002\n","\tEpoch:24 [004/005 (0340/0755)]\tLoss Ss: 0.013245\n","\tEpoch:24 [004/005 (0360/0755)]\tLoss Ss: 0.013572\n","\tEpoch:24 [004/005 (0380/0755)]\tLoss Ss: 0.016702\n","\tEpoch:24 [004/005 (0400/0755)]\tLoss Ss: 0.019468\n","\tEpoch:24 [004/005 (0420/0755)]\tLoss Ss: 0.018377\n","\tEpoch:24 [004/005 (0440/0755)]\tLoss Ss: 0.017436\n","\tEpoch:24 [004/005 (0460/0755)]\tLoss Ss: 0.018937\n","\tEpoch:24 [004/005 (0480/0755)]\tLoss Ss: 0.013788\n","\tEpoch:24 [004/005 (0500/0755)]\tLoss Ss: 0.014115\n","\tEpoch:24 [004/005 (0520/0755)]\tLoss Ss: 0.020011\n","\tEpoch:24 [004/005 (0540/0755)]\tLoss Ss: 0.012681\n","\tEpoch:24 [004/005 (0560/0755)]\tLoss Ss: 0.017859\n","\tEpoch:24 [004/005 (0580/0755)]\tLoss Ss: 0.017497\n","\tEpoch:24 [004/005 (0600/0755)]\tLoss Ss: 0.015938\n","\tEpoch:24 [004/005 (0620/0755)]\tLoss Ss: 0.013394\n","\tEpoch:24 [004/005 (0640/0755)]\tLoss Ss: 0.021608\n","\tEpoch:24 [004/005 (0660/0755)]\tLoss Ss: 0.022160\n","\tEpoch:24 [004/005 (0680/0755)]\tLoss Ss: 0.010245\n","\tEpoch:24 [004/005 (0700/0755)]\tLoss Ss: 0.020563\n","\tEpoch:24 [004/005 (0720/0755)]\tLoss Ss: 0.020031\n","\tEpoch:24 [004/005 (0740/0755)]\tLoss Ss: 0.010245\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:24 [005/005 (0000/0755)]\tLoss Ss: 0.017435\n","\tEpoch:24 [005/005 (0020/0755)]\tLoss Ss: 0.021773\n","\tEpoch:24 [005/005 (0040/0755)]\tLoss Ss: 0.019475\n","\tEpoch:24 [005/005 (0060/0755)]\tLoss Ss: 0.014658\n","\tEpoch:24 [005/005 (0080/0755)]\tLoss Ss: 0.013991\n","\tEpoch:24 [005/005 (0100/0755)]\tLoss Ss: 0.015608\n","\tEpoch:24 [005/005 (0120/0755)]\tLoss Ss: 0.009619\n","\tEpoch:24 [005/005 (0140/0755)]\tLoss Ss: 0.018301\n","\tEpoch:24 [005/005 (0160/0755)]\tLoss Ss: 0.016321\n","\tEpoch:24 [005/005 (0180/0755)]\tLoss Ss: 0.008344\n","\tEpoch:24 [005/005 (0200/0755)]\tLoss Ss: 0.016370\n","\tEpoch:24 [005/005 (0220/0755)]\tLoss Ss: 0.010465\n","\tEpoch:24 [005/005 (0240/0755)]\tLoss Ss: 0.008801\n","\tEpoch:24 [005/005 (0260/0755)]\tLoss Ss: 0.012763\n","\tEpoch:24 [005/005 (0280/0755)]\tLoss Ss: 0.012880\n","\tEpoch:24 [005/005 (0300/0755)]\tLoss Ss: 0.013225\n","\tEpoch:24 [005/005 (0320/0755)]\tLoss Ss: 0.010229\n","\tEpoch:24 [005/005 (0340/0755)]\tLoss Ss: 0.012671\n","\tEpoch:24 [005/005 (0360/0755)]\tLoss Ss: 0.012784\n","\tEpoch:24 [005/005 (0380/0755)]\tLoss Ss: 0.017417\n","\tEpoch:24 [005/005 (0400/0755)]\tLoss Ss: 0.015141\n","\tEpoch:24 [005/005 (0420/0755)]\tLoss Ss: 0.008886\n","\tEpoch:24 [005/005 (0440/0755)]\tLoss Ss: 0.015964\n","\tEpoch:24 [005/005 (0460/0755)]\tLoss Ss: 0.011988\n","\tEpoch:24 [005/005 (0480/0755)]\tLoss Ss: 0.017146\n","\tEpoch:24 [005/005 (0500/0755)]\tLoss Ss: 0.008797\n","\tEpoch:24 [005/005 (0520/0755)]\tLoss Ss: 0.018543\n","\tEpoch:24 [005/005 (0540/0755)]\tLoss Ss: 0.014832\n","\tEpoch:24 [005/005 (0560/0755)]\tLoss Ss: 0.013487\n","\tEpoch:24 [005/005 (0580/0755)]\tLoss Ss: 0.011852\n","\tEpoch:24 [005/005 (0600/0755)]\tLoss Ss: 0.012659\n","\tEpoch:24 [005/005 (0620/0755)]\tLoss Ss: 0.013832\n","\tEpoch:24 [005/005 (0640/0755)]\tLoss Ss: 0.014750\n","\tEpoch:24 [005/005 (0660/0755)]\tLoss Ss: 0.013581\n","\tEpoch:24 [005/005 (0680/0755)]\tLoss Ss: 0.013541\n","\tEpoch:24 [005/005 (0700/0755)]\tLoss Ss: 0.009897\n","\tEpoch:24 [005/005 (0720/0755)]\tLoss Ss: 0.013302\n","\tEpoch:24 [005/005 (0740/0755)]\tLoss Ss: 0.015642\n","Now train the rotated image\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tRotated_Epoch:24 [000/005 (0000/0693)]\tLoss Ss: 0.016408\n","\tRotated_Epoch:24 [000/005 (0020/0693)]\tLoss Ss: 0.017126\n","\tRotated_Epoch:24 [000/005 (0040/0693)]\tLoss Ss: 0.034161\n","\tRotated_Epoch:24 [000/005 (0060/0693)]\tLoss Ss: 0.016049\n","\tRotated_Epoch:24 [000/005 (0080/0693)]\tLoss Ss: 0.013639\n","\tRotated_Epoch:24 [000/005 (0100/0693)]\tLoss Ss: 0.013331\n","\tRotated_Epoch:24 [000/005 (0120/0693)]\tLoss Ss: 0.021786\n","\tRotated_Epoch:24 [000/005 (0140/0693)]\tLoss Ss: 0.015751\n","\tRotated_Epoch:24 [000/005 (0160/0693)]\tLoss Ss: 0.021755\n","\tRotated_Epoch:24 [000/005 (0180/0693)]\tLoss Ss: 0.019028\n","\tRotated_Epoch:24 [000/005 (0200/0693)]\tLoss Ss: 0.018621\n","\tRotated_Epoch:24 [000/005 (0220/0693)]\tLoss Ss: 0.011514\n","\tRotated_Epoch:24 [000/005 (0240/0693)]\tLoss Ss: 0.013832\n","\tRotated_Epoch:24 [000/005 (0260/0693)]\tLoss Ss: 0.018624\n","\tRotated_Epoch:24 [000/005 (0280/0693)]\tLoss Ss: 0.015286\n","\tRotated_Epoch:24 [000/005 (0300/0693)]\tLoss Ss: 0.018697\n","\tRotated_Epoch:24 [000/005 (0320/0693)]\tLoss Ss: 0.020049\n","\tRotated_Epoch:24 [000/005 (0340/0693)]\tLoss Ss: 0.017111\n","\tRotated_Epoch:24 [000/005 (0360/0693)]\tLoss Ss: 0.012861\n","\tRotated_Epoch:24 [000/005 (0380/0693)]\tLoss Ss: 0.012834\n","\tRotated_Epoch:24 [000/005 (0400/0693)]\tLoss Ss: 0.012223\n","\tRotated_Epoch:24 [000/005 (0420/0693)]\tLoss Ss: 0.012824\n","\tRotated_Epoch:24 [000/005 (0440/0693)]\tLoss Ss: 0.013090\n","\tRotated_Epoch:24 [000/005 (0460/0693)]\tLoss Ss: 0.016054\n","\tRotated_Epoch:24 [000/005 (0480/0693)]\tLoss Ss: 0.018308\n","\tRotated_Epoch:24 [000/005 (0500/0693)]\tLoss Ss: 0.017264\n","\tRotated_Epoch:24 [000/005 (0520/0693)]\tLoss Ss: 0.018139\n","\tRotated_Epoch:24 [000/005 (0540/0693)]\tLoss Ss: 0.015199\n","\tRotated_Epoch:24 [000/005 (0560/0693)]\tLoss Ss: 0.018591\n","\tRotated_Epoch:24 [000/005 (0580/0693)]\tLoss Ss: 0.013865\n","\tRotated_Epoch:24 [000/005 (0600/0693)]\tLoss Ss: 0.010429\n","\tRotated_Epoch:24 [000/005 (0620/0693)]\tLoss Ss: 0.015587\n","\tRotated_Epoch:24 [000/005 (0640/0693)]\tLoss Ss: 0.016132\n","\tRotated_Epoch:24 [000/005 (0660/0693)]\tLoss Ss: 0.010487\n","\tRotated_Epoch:24 [000/005 (0680/0693)]\tLoss Ss: 0.013927\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tRotated_Epoch:24 [001/005 (0000/0693)]\tLoss Ss: 0.015453\n","\tRotated_Epoch:24 [001/005 (0020/0693)]\tLoss Ss: 0.016334\n","\tRotated_Epoch:24 [001/005 (0040/0693)]\tLoss Ss: 0.017156\n","\tRotated_Epoch:24 [001/005 (0060/0693)]\tLoss Ss: 0.017250\n","\tRotated_Epoch:24 [001/005 (0080/0693)]\tLoss Ss: 0.012474\n","\tRotated_Epoch:24 [001/005 (0100/0693)]\tLoss Ss: 0.015221\n","\tRotated_Epoch:24 [001/005 (0120/0693)]\tLoss Ss: 0.016526\n","\tRotated_Epoch:24 [001/005 (0140/0693)]\tLoss Ss: 0.015364\n","\tRotated_Epoch:24 [001/005 (0160/0693)]\tLoss Ss: 0.010853\n","\tRotated_Epoch:24 [001/005 (0180/0693)]\tLoss Ss: 0.015148\n","\tRotated_Epoch:24 [001/005 (0200/0693)]\tLoss Ss: 0.015450\n","\tRotated_Epoch:24 [001/005 (0220/0693)]\tLoss Ss: 0.015321\n","\tRotated_Epoch:24 [001/005 (0240/0693)]\tLoss Ss: 0.018202\n","\tRotated_Epoch:24 [001/005 (0260/0693)]\tLoss Ss: 0.013142\n","\tRotated_Epoch:24 [001/005 (0280/0693)]\tLoss Ss: 0.012895\n","\tRotated_Epoch:24 [001/005 (0300/0693)]\tLoss Ss: 0.016026\n","\tRotated_Epoch:24 [001/005 (0320/0693)]\tLoss Ss: 0.016258\n","\tRotated_Epoch:24 [001/005 (0340/0693)]\tLoss Ss: 0.011078\n","\tRotated_Epoch:24 [001/005 (0360/0693)]\tLoss Ss: 0.011617\n","\tRotated_Epoch:24 [001/005 (0380/0693)]\tLoss Ss: 0.016276\n","\tRotated_Epoch:24 [001/005 (0400/0693)]\tLoss Ss: 0.015945\n","\tRotated_Epoch:24 [001/005 (0420/0693)]\tLoss Ss: 0.012208\n","\tRotated_Epoch:24 [001/005 (0440/0693)]\tLoss Ss: 0.017437\n","\tRotated_Epoch:24 [001/005 (0460/0693)]\tLoss Ss: 0.008850\n","\tRotated_Epoch:24 [001/005 (0480/0693)]\tLoss Ss: 0.013579\n","\tRotated_Epoch:24 [001/005 (0500/0693)]\tLoss Ss: 0.014511\n","\tRotated_Epoch:24 [001/005 (0520/0693)]\tLoss Ss: 0.008986\n","\tRotated_Epoch:24 [001/005 (0540/0693)]\tLoss Ss: 0.016482\n","\tRotated_Epoch:24 [001/005 (0560/0693)]\tLoss Ss: 0.012009\n","\tRotated_Epoch:24 [001/005 (0580/0693)]\tLoss Ss: 0.012488\n","\tRotated_Epoch:24 [001/005 (0600/0693)]\tLoss Ss: 0.012520\n","\tRotated_Epoch:24 [001/005 (0620/0693)]\tLoss Ss: 0.012320\n","\tRotated_Epoch:24 [001/005 (0640/0693)]\tLoss Ss: 0.009917\n","\tRotated_Epoch:24 [001/005 (0660/0693)]\tLoss Ss: 0.012803\n","\tRotated_Epoch:24 [001/005 (0680/0693)]\tLoss Ss: 0.015778\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:24 [002/005 (0000/0755)]\tLoss Ss: 0.384387\n","\tRotated_Epoch:24 [002/005 (0020/0755)]\tLoss Ss: 0.346501\n","\tRotated_Epoch:24 [002/005 (0040/0755)]\tLoss Ss: 0.300297\n","\tRotated_Epoch:24 [002/005 (0060/0755)]\tLoss Ss: 0.180693\n","\tRotated_Epoch:24 [002/005 (0080/0755)]\tLoss Ss: 0.154638\n","\tRotated_Epoch:24 [002/005 (0100/0755)]\tLoss Ss: 0.168916\n","\tRotated_Epoch:24 [002/005 (0120/0755)]\tLoss Ss: 0.099089\n","\tRotated_Epoch:24 [002/005 (0140/0755)]\tLoss Ss: 0.145984\n","\tRotated_Epoch:24 [002/005 (0160/0755)]\tLoss Ss: 0.110840\n","\tRotated_Epoch:24 [002/005 (0180/0755)]\tLoss Ss: 0.065704\n","\tRotated_Epoch:24 [002/005 (0200/0755)]\tLoss Ss: 0.100535\n","\tRotated_Epoch:24 [002/005 (0220/0755)]\tLoss Ss: 0.071649\n","\tRotated_Epoch:24 [002/005 (0240/0755)]\tLoss Ss: 0.051808\n","\tRotated_Epoch:24 [002/005 (0260/0755)]\tLoss Ss: 0.086643\n","\tRotated_Epoch:24 [002/005 (0280/0755)]\tLoss Ss: 0.064684\n","\tRotated_Epoch:24 [002/005 (0300/0755)]\tLoss Ss: 0.079413\n","\tRotated_Epoch:24 [002/005 (0320/0755)]\tLoss Ss: 0.105184\n","\tRotated_Epoch:24 [002/005 (0340/0755)]\tLoss Ss: 0.072398\n","\tRotated_Epoch:24 [002/005 (0360/0755)]\tLoss Ss: 0.081142\n","\tRotated_Epoch:24 [002/005 (0380/0755)]\tLoss Ss: 0.071722\n","\tRotated_Epoch:24 [002/005 (0400/0755)]\tLoss Ss: 0.061490\n","\tRotated_Epoch:24 [002/005 (0420/0755)]\tLoss Ss: 0.071718\n","\tRotated_Epoch:24 [002/005 (0440/0755)]\tLoss Ss: 0.065129\n","\tRotated_Epoch:24 [002/005 (0460/0755)]\tLoss Ss: 0.064009\n","\tRotated_Epoch:24 [002/005 (0480/0755)]\tLoss Ss: 0.053270\n","\tRotated_Epoch:24 [002/005 (0500/0755)]\tLoss Ss: 0.087411\n","\tRotated_Epoch:24 [002/005 (0520/0755)]\tLoss Ss: 0.034365\n","\tRotated_Epoch:24 [002/005 (0540/0755)]\tLoss Ss: 0.053344\n","\tRotated_Epoch:24 [002/005 (0560/0755)]\tLoss Ss: 0.062708\n","\tRotated_Epoch:24 [002/005 (0580/0755)]\tLoss Ss: 0.068234\n","\tRotated_Epoch:24 [002/005 (0600/0755)]\tLoss Ss: 0.103001\n","\tRotated_Epoch:24 [002/005 (0620/0755)]\tLoss Ss: 0.053114\n","\tRotated_Epoch:24 [002/005 (0640/0755)]\tLoss Ss: 0.047918\n","\tRotated_Epoch:24 [002/005 (0660/0755)]\tLoss Ss: 0.046845\n","\tRotated_Epoch:24 [002/005 (0680/0755)]\tLoss Ss: 0.056072\n","\tRotated_Epoch:24 [002/005 (0700/0755)]\tLoss Ss: 0.038604\n","\tRotated_Epoch:24 [002/005 (0720/0755)]\tLoss Ss: 0.049632\n","\tRotated_Epoch:24 [002/005 (0740/0755)]\tLoss Ss: 0.061332\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:24 [003/005 (0000/0755)]\tLoss Ss: 0.179813\n","\tRotated_Epoch:24 [003/005 (0020/0755)]\tLoss Ss: 0.235018\n","\tRotated_Epoch:24 [003/005 (0040/0755)]\tLoss Ss: 0.173457\n","\tRotated_Epoch:24 [003/005 (0060/0755)]\tLoss Ss: 0.086373\n","\tRotated_Epoch:24 [003/005 (0080/0755)]\tLoss Ss: 0.070044\n","\tRotated_Epoch:24 [003/005 (0100/0755)]\tLoss Ss: 0.099303\n","\tRotated_Epoch:24 [003/005 (0120/0755)]\tLoss Ss: 0.075369\n","\tRotated_Epoch:24 [003/005 (0140/0755)]\tLoss Ss: 0.052926\n","\tRotated_Epoch:24 [003/005 (0160/0755)]\tLoss Ss: 0.053734\n","\tRotated_Epoch:24 [003/005 (0180/0755)]\tLoss Ss: 0.049855\n","\tRotated_Epoch:24 [003/005 (0200/0755)]\tLoss Ss: 0.056385\n","\tRotated_Epoch:24 [003/005 (0220/0755)]\tLoss Ss: 0.044392\n","\tRotated_Epoch:24 [003/005 (0240/0755)]\tLoss Ss: 0.054203\n","\tRotated_Epoch:24 [003/005 (0260/0755)]\tLoss Ss: 0.044302\n","\tRotated_Epoch:24 [003/005 (0280/0755)]\tLoss Ss: 0.043337\n","\tRotated_Epoch:24 [003/005 (0300/0755)]\tLoss Ss: 0.030018\n","\tRotated_Epoch:24 [003/005 (0320/0755)]\tLoss Ss: 0.045500\n","\tRotated_Epoch:24 [003/005 (0340/0755)]\tLoss Ss: 0.034317\n","\tRotated_Epoch:24 [003/005 (0360/0755)]\tLoss Ss: 0.022473\n","\tRotated_Epoch:24 [003/005 (0380/0755)]\tLoss Ss: 0.040940\n","\tRotated_Epoch:24 [003/005 (0400/0755)]\tLoss Ss: 0.042533\n","\tRotated_Epoch:24 [003/005 (0420/0755)]\tLoss Ss: 0.030221\n","\tRotated_Epoch:24 [003/005 (0440/0755)]\tLoss Ss: 0.030671\n","\tRotated_Epoch:24 [003/005 (0460/0755)]\tLoss Ss: 0.031569\n","\tRotated_Epoch:24 [003/005 (0480/0755)]\tLoss Ss: 0.040145\n","\tRotated_Epoch:24 [003/005 (0500/0755)]\tLoss Ss: 0.035152\n","\tRotated_Epoch:24 [003/005 (0520/0755)]\tLoss Ss: 0.025495\n","\tRotated_Epoch:24 [003/005 (0540/0755)]\tLoss Ss: 0.048466\n","\tRotated_Epoch:24 [003/005 (0560/0755)]\tLoss Ss: 0.026873\n","\tRotated_Epoch:24 [003/005 (0580/0755)]\tLoss Ss: 0.032029\n","\tRotated_Epoch:24 [003/005 (0600/0755)]\tLoss Ss: 0.032406\n","\tRotated_Epoch:24 [003/005 (0620/0755)]\tLoss Ss: 0.029100\n","\tRotated_Epoch:24 [003/005 (0640/0755)]\tLoss Ss: 0.033935\n","\tRotated_Epoch:24 [003/005 (0660/0755)]\tLoss Ss: 0.021794\n","\tRotated_Epoch:24 [003/005 (0680/0755)]\tLoss Ss: 0.023041\n","\tRotated_Epoch:24 [003/005 (0700/0755)]\tLoss Ss: 0.029977\n","\tRotated_Epoch:24 [003/005 (0720/0755)]\tLoss Ss: 0.022038\n","\tRotated_Epoch:24 [003/005 (0740/0755)]\tLoss Ss: 0.024902\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tRotated_Epoch:24 [004/005 (0000/0588)]\tLoss Ss: 0.176888\n","\tRotated_Epoch:24 [004/005 (0020/0588)]\tLoss Ss: 0.149019\n","\tRotated_Epoch:24 [004/005 (0040/0588)]\tLoss Ss: 0.118410\n","\tRotated_Epoch:24 [004/005 (0060/0588)]\tLoss Ss: 0.087652\n","\tRotated_Epoch:24 [004/005 (0080/0588)]\tLoss Ss: 0.061501\n","\tRotated_Epoch:24 [004/005 (0100/0588)]\tLoss Ss: 0.083427\n","\tRotated_Epoch:24 [004/005 (0120/0588)]\tLoss Ss: 0.071442\n","\tRotated_Epoch:24 [004/005 (0140/0588)]\tLoss Ss: 0.062732\n","\tRotated_Epoch:24 [004/005 (0160/0588)]\tLoss Ss: 0.058093\n","\tRotated_Epoch:24 [004/005 (0180/0588)]\tLoss Ss: 0.057431\n","\tRotated_Epoch:24 [004/005 (0200/0588)]\tLoss Ss: 0.053832\n","\tRotated_Epoch:24 [004/005 (0220/0588)]\tLoss Ss: 0.053946\n","\tRotated_Epoch:24 [004/005 (0240/0588)]\tLoss Ss: 0.058726\n","\tRotated_Epoch:24 [004/005 (0260/0588)]\tLoss Ss: 0.041343\n","\tRotated_Epoch:24 [004/005 (0280/0588)]\tLoss Ss: 0.064041\n","\tRotated_Epoch:24 [004/005 (0300/0588)]\tLoss Ss: 0.056729\n","\tRotated_Epoch:24 [004/005 (0320/0588)]\tLoss Ss: 0.078011\n","\tRotated_Epoch:24 [004/005 (0340/0588)]\tLoss Ss: 0.057949\n","\tRotated_Epoch:24 [004/005 (0360/0588)]\tLoss Ss: 0.053580\n","\tRotated_Epoch:24 [004/005 (0380/0588)]\tLoss Ss: 0.070373\n","\tRotated_Epoch:24 [004/005 (0400/0588)]\tLoss Ss: 0.061944\n","\tRotated_Epoch:24 [004/005 (0420/0588)]\tLoss Ss: 0.058985\n","\tRotated_Epoch:24 [004/005 (0440/0588)]\tLoss Ss: 0.077940\n","\tRotated_Epoch:24 [004/005 (0460/0588)]\tLoss Ss: 0.049988\n","\tRotated_Epoch:24 [004/005 (0480/0588)]\tLoss Ss: 0.064667\n","\tRotated_Epoch:24 [004/005 (0500/0588)]\tLoss Ss: 0.054744\n","\tRotated_Epoch:24 [004/005 (0520/0588)]\tLoss Ss: 0.050036\n","\tRotated_Epoch:24 [004/005 (0540/0588)]\tLoss Ss: 0.059550\n","\tRotated_Epoch:24 [004/005 (0560/0588)]\tLoss Ss: 0.055185\n","\tRotated_Epoch:24 [004/005 (0580/0588)]\tLoss Ss: 0.044516\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tRotated_Epoch:24 [005/005 (0000/0614)]\tLoss Ss: 0.120385\n","\tRotated_Epoch:24 [005/005 (0020/0614)]\tLoss Ss: 0.043054\n","\tRotated_Epoch:24 [005/005 (0040/0614)]\tLoss Ss: 0.052407\n","\tRotated_Epoch:24 [005/005 (0060/0614)]\tLoss Ss: 0.082727\n","\tRotated_Epoch:24 [005/005 (0080/0614)]\tLoss Ss: 0.061139\n","\tRotated_Epoch:24 [005/005 (0100/0614)]\tLoss Ss: 0.048006\n","\tRotated_Epoch:24 [005/005 (0120/0614)]\tLoss Ss: 0.030172\n","\tRotated_Epoch:24 [005/005 (0140/0614)]\tLoss Ss: 0.042975\n","\tRotated_Epoch:24 [005/005 (0160/0614)]\tLoss Ss: 0.024137\n","\tRotated_Epoch:24 [005/005 (0180/0614)]\tLoss Ss: 0.028494\n","\tRotated_Epoch:24 [005/005 (0200/0614)]\tLoss Ss: 0.025187\n","\tRotated_Epoch:24 [005/005 (0220/0614)]\tLoss Ss: 0.023605\n","\tRotated_Epoch:24 [005/005 (0240/0614)]\tLoss Ss: 0.037427\n","\tRotated_Epoch:24 [005/005 (0260/0614)]\tLoss Ss: 0.023188\n","\tRotated_Epoch:24 [005/005 (0280/0614)]\tLoss Ss: 0.022334\n","\tRotated_Epoch:24 [005/005 (0300/0614)]\tLoss Ss: 0.019818\n","\tRotated_Epoch:24 [005/005 (0320/0614)]\tLoss Ss: 0.025236\n","\tRotated_Epoch:24 [005/005 (0340/0614)]\tLoss Ss: 0.024537\n","\tRotated_Epoch:24 [005/005 (0360/0614)]\tLoss Ss: 0.021968\n","\tRotated_Epoch:24 [005/005 (0380/0614)]\tLoss Ss: 0.021553\n","\tRotated_Epoch:24 [005/005 (0400/0614)]\tLoss Ss: 0.015868\n","\tRotated_Epoch:24 [005/005 (0420/0614)]\tLoss Ss: 0.017518\n","\tRotated_Epoch:24 [005/005 (0440/0614)]\tLoss Ss: 0.018280\n","\tRotated_Epoch:24 [005/005 (0460/0614)]\tLoss Ss: 0.019264\n","\tRotated_Epoch:24 [005/005 (0480/0614)]\tLoss Ss: 0.012815\n","\tRotated_Epoch:24 [005/005 (0500/0614)]\tLoss Ss: 0.018462\n","\tRotated_Epoch:24 [005/005 (0520/0614)]\tLoss Ss: 0.015165\n","\tRotated_Epoch:24 [005/005 (0540/0614)]\tLoss Ss: 0.012995\n","\tRotated_Epoch:24 [005/005 (0560/0614)]\tLoss Ss: 0.016830\n","\tRotated_Epoch:24 [005/005 (0580/0614)]\tLoss Ss: 0.015740\n","\tRotated_Epoch:24 [005/005 (0600/0614)]\tLoss Ss: 0.013954\n","new_1_6_d.nii\n","new_1_6_m.nii\n","new_1_7_d.nii\n","new_1_7_m.nii\n","new_2_5_d.nii\n","new_2_5_m.nii\n","new_2_6_d.nii\n","new_2_6_m.nii\n","new_2_7_d.nii\n","new_2_7_m.nii\n","new_3_d (2).nii\n","new_3_m (2).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (3).nii\n","new_3_m (3).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (7).nii\n","new_3_m (7).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (8).nii\n","new_3_m (8).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (9).nii\n","new_3_m (9).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_4_10_d_163526.nii\n","new_4_10_m_163526.nii.gz\n","new_4_11_d_084336.nii\n","new_4_11_m_084336.nii.gz\n","new_4_7_d_184422.nii\n","new_4_7_m_184422.nii.gz\n","new_4_8_d_143412.nii\n","new_4_8_m_143412.nii.gz\n","new_4_9_d_155347.nii\n","new_4_9_m_155347.nii.gz\n","\tEpoch: 24; Dice: 0.9606 +/- 0.0103; Loss: 12.5571\n","Begin Epoch 25\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tEpoch:25 [000/005 (0000/0614)]\tLoss Ss: 0.016128\n","\tEpoch:25 [000/005 (0020/0614)]\tLoss Ss: 0.012214\n","\tEpoch:25 [000/005 (0040/0614)]\tLoss Ss: 0.011429\n","\tEpoch:25 [000/005 (0060/0614)]\tLoss Ss: 0.013450\n","\tEpoch:25 [000/005 (0080/0614)]\tLoss Ss: 0.010259\n","\tEpoch:25 [000/005 (0100/0614)]\tLoss Ss: 0.009014\n","\tEpoch:25 [000/005 (0120/0614)]\tLoss Ss: 0.011270\n","\tEpoch:25 [000/005 (0140/0614)]\tLoss Ss: 0.012497\n","\tEpoch:25 [000/005 (0160/0614)]\tLoss Ss: 0.014054\n","\tEpoch:25 [000/005 (0180/0614)]\tLoss Ss: 0.008837\n","\tEpoch:25 [000/005 (0200/0614)]\tLoss Ss: 0.009963\n","\tEpoch:25 [000/005 (0220/0614)]\tLoss Ss: 0.008659\n","\tEpoch:25 [000/005 (0240/0614)]\tLoss Ss: 0.007432\n","\tEpoch:25 [000/005 (0260/0614)]\tLoss Ss: 0.008321\n","\tEpoch:25 [000/005 (0280/0614)]\tLoss Ss: 0.008203\n","\tEpoch:25 [000/005 (0300/0614)]\tLoss Ss: 0.010203\n","\tEpoch:25 [000/005 (0320/0614)]\tLoss Ss: 0.006652\n","\tEpoch:25 [000/005 (0340/0614)]\tLoss Ss: 0.008321\n","\tEpoch:25 [000/005 (0360/0614)]\tLoss Ss: 0.006862\n","\tEpoch:25 [000/005 (0380/0614)]\tLoss Ss: 0.007832\n","\tEpoch:25 [000/005 (0400/0614)]\tLoss Ss: 0.007234\n","\tEpoch:25 [000/005 (0420/0614)]\tLoss Ss: 0.006143\n","\tEpoch:25 [000/005 (0440/0614)]\tLoss Ss: 0.011841\n","\tEpoch:25 [000/005 (0460/0614)]\tLoss Ss: 0.009749\n","\tEpoch:25 [000/005 (0480/0614)]\tLoss Ss: 0.008229\n","\tEpoch:25 [000/005 (0500/0614)]\tLoss Ss: 0.005501\n","\tEpoch:25 [000/005 (0520/0614)]\tLoss Ss: 0.005893\n","\tEpoch:25 [000/005 (0540/0614)]\tLoss Ss: 0.006456\n","\tEpoch:25 [000/005 (0560/0614)]\tLoss Ss: 0.007522\n","\tEpoch:25 [000/005 (0580/0614)]\tLoss Ss: 0.009146\n","\tEpoch:25 [000/005 (0600/0614)]\tLoss Ss: 0.006828\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tEpoch:25 [001/005 (0000/0693)]\tLoss Ss: 0.018249\n","\tEpoch:25 [001/005 (0020/0693)]\tLoss Ss: 0.020028\n","\tEpoch:25 [001/005 (0040/0693)]\tLoss Ss: 0.020579\n","\tEpoch:25 [001/005 (0060/0693)]\tLoss Ss: 0.022532\n","\tEpoch:25 [001/005 (0080/0693)]\tLoss Ss: 0.018977\n","\tEpoch:25 [001/005 (0100/0693)]\tLoss Ss: 0.022767\n","\tEpoch:25 [001/005 (0120/0693)]\tLoss Ss: 0.013944\n","\tEpoch:25 [001/005 (0140/0693)]\tLoss Ss: 0.011066\n","\tEpoch:25 [001/005 (0160/0693)]\tLoss Ss: 0.013624\n","\tEpoch:25 [001/005 (0180/0693)]\tLoss Ss: 0.010638\n","\tEpoch:25 [001/005 (0200/0693)]\tLoss Ss: 0.014810\n","\tEpoch:25 [001/005 (0220/0693)]\tLoss Ss: 0.013221\n","\tEpoch:25 [001/005 (0240/0693)]\tLoss Ss: 0.015327\n","\tEpoch:25 [001/005 (0260/0693)]\tLoss Ss: 0.016980\n","\tEpoch:25 [001/005 (0280/0693)]\tLoss Ss: 0.012853\n","\tEpoch:25 [001/005 (0300/0693)]\tLoss Ss: 0.016567\n","\tEpoch:25 [001/005 (0320/0693)]\tLoss Ss: 0.012214\n","\tEpoch:25 [001/005 (0340/0693)]\tLoss Ss: 0.015225\n","\tEpoch:25 [001/005 (0360/0693)]\tLoss Ss: 0.013186\n","\tEpoch:25 [001/005 (0380/0693)]\tLoss Ss: 0.012301\n","\tEpoch:25 [001/005 (0400/0693)]\tLoss Ss: 0.019686\n","\tEpoch:25 [001/005 (0420/0693)]\tLoss Ss: 0.011608\n","\tEpoch:25 [001/005 (0440/0693)]\tLoss Ss: 0.009515\n","\tEpoch:25 [001/005 (0460/0693)]\tLoss Ss: 0.009242\n","\tEpoch:25 [001/005 (0480/0693)]\tLoss Ss: 0.018841\n","\tEpoch:25 [001/005 (0500/0693)]\tLoss Ss: 0.011727\n","\tEpoch:25 [001/005 (0520/0693)]\tLoss Ss: 0.015145\n","\tEpoch:25 [001/005 (0540/0693)]\tLoss Ss: 0.014016\n","\tEpoch:25 [001/005 (0560/0693)]\tLoss Ss: 0.013596\n","\tEpoch:25 [001/005 (0580/0693)]\tLoss Ss: 0.017656\n","\tEpoch:25 [001/005 (0600/0693)]\tLoss Ss: 0.014616\n","\tEpoch:25 [001/005 (0620/0693)]\tLoss Ss: 0.011991\n","\tEpoch:25 [001/005 (0640/0693)]\tLoss Ss: 0.015655\n","\tEpoch:25 [001/005 (0660/0693)]\tLoss Ss: 0.011377\n","\tEpoch:25 [001/005 (0680/0693)]\tLoss Ss: 0.015457\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:25 [002/005 (0000/0755)]\tLoss Ss: 0.030178\n","\tEpoch:25 [002/005 (0020/0755)]\tLoss Ss: 0.028594\n","\tEpoch:25 [002/005 (0040/0755)]\tLoss Ss: 0.026644\n","\tEpoch:25 [002/005 (0060/0755)]\tLoss Ss: 0.025114\n","\tEpoch:25 [002/005 (0080/0755)]\tLoss Ss: 0.027153\n","\tEpoch:25 [002/005 (0100/0755)]\tLoss Ss: 0.028813\n","\tEpoch:25 [002/005 (0120/0755)]\tLoss Ss: 0.016770\n","\tEpoch:25 [002/005 (0140/0755)]\tLoss Ss: 0.016146\n","\tEpoch:25 [002/005 (0160/0755)]\tLoss Ss: 0.022156\n","\tEpoch:25 [002/005 (0180/0755)]\tLoss Ss: 0.024664\n","\tEpoch:25 [002/005 (0200/0755)]\tLoss Ss: 0.017440\n","\tEpoch:25 [002/005 (0220/0755)]\tLoss Ss: 0.014975\n","\tEpoch:25 [002/005 (0240/0755)]\tLoss Ss: 0.021684\n","\tEpoch:25 [002/005 (0260/0755)]\tLoss Ss: 0.016500\n","\tEpoch:25 [002/005 (0280/0755)]\tLoss Ss: 0.024014\n","\tEpoch:25 [002/005 (0300/0755)]\tLoss Ss: 0.023823\n","\tEpoch:25 [002/005 (0320/0755)]\tLoss Ss: 0.019871\n","\tEpoch:25 [002/005 (0340/0755)]\tLoss Ss: 0.018415\n","\tEpoch:25 [002/005 (0360/0755)]\tLoss Ss: 0.017504\n","\tEpoch:25 [002/005 (0380/0755)]\tLoss Ss: 0.019580\n","\tEpoch:25 [002/005 (0400/0755)]\tLoss Ss: 0.017988\n","\tEpoch:25 [002/005 (0420/0755)]\tLoss Ss: 0.017796\n","\tEpoch:25 [002/005 (0440/0755)]\tLoss Ss: 0.015621\n","\tEpoch:25 [002/005 (0460/0755)]\tLoss Ss: 0.011770\n","\tEpoch:25 [002/005 (0480/0755)]\tLoss Ss: 0.016902\n","\tEpoch:25 [002/005 (0500/0755)]\tLoss Ss: 0.015849\n","\tEpoch:25 [002/005 (0520/0755)]\tLoss Ss: 0.014933\n","\tEpoch:25 [002/005 (0540/0755)]\tLoss Ss: 0.019276\n","\tEpoch:25 [002/005 (0560/0755)]\tLoss Ss: 0.013829\n","\tEpoch:25 [002/005 (0580/0755)]\tLoss Ss: 0.020314\n","\tEpoch:25 [002/005 (0600/0755)]\tLoss Ss: 0.021337\n","\tEpoch:25 [002/005 (0620/0755)]\tLoss Ss: 0.013516\n","\tEpoch:25 [002/005 (0640/0755)]\tLoss Ss: 0.019538\n","\tEpoch:25 [002/005 (0660/0755)]\tLoss Ss: 0.011363\n","\tEpoch:25 [002/005 (0680/0755)]\tLoss Ss: 0.011859\n","\tEpoch:25 [002/005 (0700/0755)]\tLoss Ss: 0.010097\n","\tEpoch:25 [002/005 (0720/0755)]\tLoss Ss: 0.018698\n","\tEpoch:25 [002/005 (0740/0755)]\tLoss Ss: 0.016355\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tEpoch:25 [003/005 (0000/0588)]\tLoss Ss: 0.008435\n","\tEpoch:25 [003/005 (0020/0588)]\tLoss Ss: 0.011987\n","\tEpoch:25 [003/005 (0040/0588)]\tLoss Ss: 0.005334\n","\tEpoch:25 [003/005 (0060/0588)]\tLoss Ss: 0.005129\n","\tEpoch:25 [003/005 (0080/0588)]\tLoss Ss: 0.005289\n","\tEpoch:25 [003/005 (0100/0588)]\tLoss Ss: 0.005289\n","\tEpoch:25 [003/005 (0120/0588)]\tLoss Ss: 0.006731\n","\tEpoch:25 [003/005 (0140/0588)]\tLoss Ss: 0.005995\n","\tEpoch:25 [003/005 (0160/0588)]\tLoss Ss: 0.004055\n","\tEpoch:25 [003/005 (0180/0588)]\tLoss Ss: 0.004730\n","\tEpoch:25 [003/005 (0200/0588)]\tLoss Ss: 0.005115\n","\tEpoch:25 [003/005 (0220/0588)]\tLoss Ss: 0.004001\n","\tEpoch:25 [003/005 (0240/0588)]\tLoss Ss: 0.006045\n","\tEpoch:25 [003/005 (0260/0588)]\tLoss Ss: 0.006593\n","\tEpoch:25 [003/005 (0280/0588)]\tLoss Ss: 0.005656\n","\tEpoch:25 [003/005 (0300/0588)]\tLoss Ss: 0.006548\n","\tEpoch:25 [003/005 (0320/0588)]\tLoss Ss: 0.004644\n","\tEpoch:25 [003/005 (0340/0588)]\tLoss Ss: 0.008076\n","\tEpoch:25 [003/005 (0360/0588)]\tLoss Ss: 0.005594\n","\tEpoch:25 [003/005 (0380/0588)]\tLoss Ss: 0.005797\n","\tEpoch:25 [003/005 (0400/0588)]\tLoss Ss: 0.004981\n","\tEpoch:25 [003/005 (0420/0588)]\tLoss Ss: 0.005441\n","\tEpoch:25 [003/005 (0440/0588)]\tLoss Ss: 0.002421\n","\tEpoch:25 [003/005 (0460/0588)]\tLoss Ss: 0.005764\n","\tEpoch:25 [003/005 (0480/0588)]\tLoss Ss: 0.004058\n","\tEpoch:25 [003/005 (0500/0588)]\tLoss Ss: 0.004063\n","\tEpoch:25 [003/005 (0520/0588)]\tLoss Ss: 0.003542\n","\tEpoch:25 [003/005 (0540/0588)]\tLoss Ss: 0.004946\n","\tEpoch:25 [003/005 (0560/0588)]\tLoss Ss: 0.003571\n","\tEpoch:25 [003/005 (0580/0588)]\tLoss Ss: 0.004498\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:25 [004/005 (0000/0755)]\tLoss Ss: 0.012522\n","\tEpoch:25 [004/005 (0020/0755)]\tLoss Ss: 0.013233\n","\tEpoch:25 [004/005 (0040/0755)]\tLoss Ss: 0.016391\n","\tEpoch:25 [004/005 (0060/0755)]\tLoss Ss: 0.017631\n","\tEpoch:25 [004/005 (0080/0755)]\tLoss Ss: 0.013883\n","\tEpoch:25 [004/005 (0100/0755)]\tLoss Ss: 0.017552\n","\tEpoch:25 [004/005 (0120/0755)]\tLoss Ss: 0.010565\n","\tEpoch:25 [004/005 (0140/0755)]\tLoss Ss: 0.019146\n","\tEpoch:25 [004/005 (0160/0755)]\tLoss Ss: 0.017724\n","\tEpoch:25 [004/005 (0180/0755)]\tLoss Ss: 0.009060\n","\tEpoch:25 [004/005 (0200/0755)]\tLoss Ss: 0.010037\n","\tEpoch:25 [004/005 (0220/0755)]\tLoss Ss: 0.016634\n","\tEpoch:25 [004/005 (0240/0755)]\tLoss Ss: 0.012606\n","\tEpoch:25 [004/005 (0260/0755)]\tLoss Ss: 0.009268\n","\tEpoch:25 [004/005 (0280/0755)]\tLoss Ss: 0.020195\n","\tEpoch:25 [004/005 (0300/0755)]\tLoss Ss: 0.015017\n","\tEpoch:25 [004/005 (0320/0755)]\tLoss Ss: 0.012678\n","\tEpoch:25 [004/005 (0340/0755)]\tLoss Ss: 0.019254\n","\tEpoch:25 [004/005 (0360/0755)]\tLoss Ss: 0.013347\n","\tEpoch:25 [004/005 (0380/0755)]\tLoss Ss: 0.010839\n","\tEpoch:25 [004/005 (0400/0755)]\tLoss Ss: 0.010338\n","\tEpoch:25 [004/005 (0420/0755)]\tLoss Ss: 0.018713\n","\tEpoch:25 [004/005 (0440/0755)]\tLoss Ss: 0.015003\n","\tEpoch:25 [004/005 (0460/0755)]\tLoss Ss: 0.012310\n","\tEpoch:25 [004/005 (0480/0755)]\tLoss Ss: 0.013085\n","\tEpoch:25 [004/005 (0500/0755)]\tLoss Ss: 0.011554\n","\tEpoch:25 [004/005 (0520/0755)]\tLoss Ss: 0.010324\n","\tEpoch:25 [004/005 (0540/0755)]\tLoss Ss: 0.014563\n","\tEpoch:25 [004/005 (0560/0755)]\tLoss Ss: 0.010624\n","\tEpoch:25 [004/005 (0580/0755)]\tLoss Ss: 0.015236\n","\tEpoch:25 [004/005 (0600/0755)]\tLoss Ss: 0.010299\n","\tEpoch:25 [004/005 (0620/0755)]\tLoss Ss: 0.012320\n","\tEpoch:25 [004/005 (0640/0755)]\tLoss Ss: 0.011745\n","\tEpoch:25 [004/005 (0660/0755)]\tLoss Ss: 0.013132\n","\tEpoch:25 [004/005 (0680/0755)]\tLoss Ss: 0.012174\n","\tEpoch:25 [004/005 (0700/0755)]\tLoss Ss: 0.014163\n","\tEpoch:25 [004/005 (0720/0755)]\tLoss Ss: 0.009939\n","\tEpoch:25 [004/005 (0740/0755)]\tLoss Ss: 0.022296\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tEpoch:25 [005/005 (0000/0693)]\tLoss Ss: 0.012551\n","\tEpoch:25 [005/005 (0020/0693)]\tLoss Ss: 0.022776\n","\tEpoch:25 [005/005 (0040/0693)]\tLoss Ss: 0.013009\n","\tEpoch:25 [005/005 (0060/0693)]\tLoss Ss: 0.015694\n","\tEpoch:25 [005/005 (0080/0693)]\tLoss Ss: 0.016516\n","\tEpoch:25 [005/005 (0100/0693)]\tLoss Ss: 0.014328\n","\tEpoch:25 [005/005 (0120/0693)]\tLoss Ss: 0.010373\n","\tEpoch:25 [005/005 (0140/0693)]\tLoss Ss: 0.016644\n","\tEpoch:25 [005/005 (0160/0693)]\tLoss Ss: 0.024432\n","\tEpoch:25 [005/005 (0180/0693)]\tLoss Ss: 0.017738\n","\tEpoch:25 [005/005 (0200/0693)]\tLoss Ss: 0.012602\n","\tEpoch:25 [005/005 (0220/0693)]\tLoss Ss: 0.012744\n","\tEpoch:25 [005/005 (0240/0693)]\tLoss Ss: 0.012871\n","\tEpoch:25 [005/005 (0260/0693)]\tLoss Ss: 0.022945\n","\tEpoch:25 [005/005 (0280/0693)]\tLoss Ss: 0.017628\n","\tEpoch:25 [005/005 (0300/0693)]\tLoss Ss: 0.012073\n","\tEpoch:25 [005/005 (0320/0693)]\tLoss Ss: 0.018187\n","\tEpoch:25 [005/005 (0340/0693)]\tLoss Ss: 0.013490\n","\tEpoch:25 [005/005 (0360/0693)]\tLoss Ss: 0.009182\n","\tEpoch:25 [005/005 (0380/0693)]\tLoss Ss: 0.015630\n","\tEpoch:25 [005/005 (0400/0693)]\tLoss Ss: 0.014147\n","\tEpoch:25 [005/005 (0420/0693)]\tLoss Ss: 0.009562\n","\tEpoch:25 [005/005 (0440/0693)]\tLoss Ss: 0.011710\n","\tEpoch:25 [005/005 (0460/0693)]\tLoss Ss: 0.011345\n","\tEpoch:25 [005/005 (0480/0693)]\tLoss Ss: 0.016924\n","\tEpoch:25 [005/005 (0500/0693)]\tLoss Ss: 0.014822\n","\tEpoch:25 [005/005 (0520/0693)]\tLoss Ss: 0.012387\n","\tEpoch:25 [005/005 (0540/0693)]\tLoss Ss: 0.019137\n","\tEpoch:25 [005/005 (0560/0693)]\tLoss Ss: 0.013030\n","\tEpoch:25 [005/005 (0580/0693)]\tLoss Ss: 0.012530\n","\tEpoch:25 [005/005 (0600/0693)]\tLoss Ss: 0.018153\n","\tEpoch:25 [005/005 (0620/0693)]\tLoss Ss: 0.010638\n","\tEpoch:25 [005/005 (0640/0693)]\tLoss Ss: 0.013507\n","\tEpoch:25 [005/005 (0660/0693)]\tLoss Ss: 0.016548\n","\tEpoch:25 [005/005 (0680/0693)]\tLoss Ss: 0.008748\n","Now train the rotated image\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:25 [000/005 (0000/0755)]\tLoss Ss: 0.206032\n","\tRotated_Epoch:25 [000/005 (0020/0755)]\tLoss Ss: 0.180334\n","\tRotated_Epoch:25 [000/005 (0040/0755)]\tLoss Ss: 0.123281\n","\tRotated_Epoch:25 [000/005 (0060/0755)]\tLoss Ss: 0.191924\n","\tRotated_Epoch:25 [000/005 (0080/0755)]\tLoss Ss: 0.138954\n","\tRotated_Epoch:25 [000/005 (0100/0755)]\tLoss Ss: 0.095396\n","\tRotated_Epoch:25 [000/005 (0120/0755)]\tLoss Ss: 0.073104\n","\tRotated_Epoch:25 [000/005 (0140/0755)]\tLoss Ss: 0.066243\n","\tRotated_Epoch:25 [000/005 (0160/0755)]\tLoss Ss: 0.074884\n","\tRotated_Epoch:25 [000/005 (0180/0755)]\tLoss Ss: 0.073958\n","\tRotated_Epoch:25 [000/005 (0200/0755)]\tLoss Ss: 0.046520\n","\tRotated_Epoch:25 [000/005 (0220/0755)]\tLoss Ss: 0.051659\n","\tRotated_Epoch:25 [000/005 (0240/0755)]\tLoss Ss: 0.076642\n","\tRotated_Epoch:25 [000/005 (0260/0755)]\tLoss Ss: 0.057545\n","\tRotated_Epoch:25 [000/005 (0280/0755)]\tLoss Ss: 0.052211\n","\tRotated_Epoch:25 [000/005 (0300/0755)]\tLoss Ss: 0.064210\n","\tRotated_Epoch:25 [000/005 (0320/0755)]\tLoss Ss: 0.084686\n","\tRotated_Epoch:25 [000/005 (0340/0755)]\tLoss Ss: 0.039641\n","\tRotated_Epoch:25 [000/005 (0360/0755)]\tLoss Ss: 0.045478\n","\tRotated_Epoch:25 [000/005 (0380/0755)]\tLoss Ss: 0.048852\n","\tRotated_Epoch:25 [000/005 (0400/0755)]\tLoss Ss: 0.040141\n","\tRotated_Epoch:25 [000/005 (0420/0755)]\tLoss Ss: 0.059157\n","\tRotated_Epoch:25 [000/005 (0440/0755)]\tLoss Ss: 0.065852\n","\tRotated_Epoch:25 [000/005 (0460/0755)]\tLoss Ss: 0.075341\n","\tRotated_Epoch:25 [000/005 (0480/0755)]\tLoss Ss: 0.049817\n","\tRotated_Epoch:25 [000/005 (0500/0755)]\tLoss Ss: 0.051293\n","\tRotated_Epoch:25 [000/005 (0520/0755)]\tLoss Ss: 0.039985\n","\tRotated_Epoch:25 [000/005 (0540/0755)]\tLoss Ss: 0.042625\n","\tRotated_Epoch:25 [000/005 (0560/0755)]\tLoss Ss: 0.045558\n","\tRotated_Epoch:25 [000/005 (0580/0755)]\tLoss Ss: 0.037859\n","\tRotated_Epoch:25 [000/005 (0600/0755)]\tLoss Ss: 0.050800\n","\tRotated_Epoch:25 [000/005 (0620/0755)]\tLoss Ss: 0.054178\n","\tRotated_Epoch:25 [000/005 (0640/0755)]\tLoss Ss: 0.031622\n","\tRotated_Epoch:25 [000/005 (0660/0755)]\tLoss Ss: 0.058704\n","\tRotated_Epoch:25 [000/005 (0680/0755)]\tLoss Ss: 0.038672\n","\tRotated_Epoch:25 [000/005 (0700/0755)]\tLoss Ss: 0.048475\n","\tRotated_Epoch:25 [000/005 (0720/0755)]\tLoss Ss: 0.036695\n","\tRotated_Epoch:25 [000/005 (0740/0755)]\tLoss Ss: 0.046745\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tRotated_Epoch:25 [001/005 (0000/0693)]\tLoss Ss: 0.053481\n","\tRotated_Epoch:25 [001/005 (0020/0693)]\tLoss Ss: 0.041924\n","\tRotated_Epoch:25 [001/005 (0040/0693)]\tLoss Ss: 0.029548\n","\tRotated_Epoch:25 [001/005 (0060/0693)]\tLoss Ss: 0.042049\n","\tRotated_Epoch:25 [001/005 (0080/0693)]\tLoss Ss: 0.035260\n","\tRotated_Epoch:25 [001/005 (0100/0693)]\tLoss Ss: 0.024259\n","\tRotated_Epoch:25 [001/005 (0120/0693)]\tLoss Ss: 0.030589\n","\tRotated_Epoch:25 [001/005 (0140/0693)]\tLoss Ss: 0.027348\n","\tRotated_Epoch:25 [001/005 (0160/0693)]\tLoss Ss: 0.024827\n","\tRotated_Epoch:25 [001/005 (0180/0693)]\tLoss Ss: 0.016753\n","\tRotated_Epoch:25 [001/005 (0200/0693)]\tLoss Ss: 0.018030\n","\tRotated_Epoch:25 [001/005 (0220/0693)]\tLoss Ss: 0.018841\n","\tRotated_Epoch:25 [001/005 (0240/0693)]\tLoss Ss: 0.023589\n","\tRotated_Epoch:25 [001/005 (0260/0693)]\tLoss Ss: 0.018248\n","\tRotated_Epoch:25 [001/005 (0280/0693)]\tLoss Ss: 0.021572\n","\tRotated_Epoch:25 [001/005 (0300/0693)]\tLoss Ss: 0.018023\n","\tRotated_Epoch:25 [001/005 (0320/0693)]\tLoss Ss: 0.021196\n","\tRotated_Epoch:25 [001/005 (0340/0693)]\tLoss Ss: 0.012223\n","\tRotated_Epoch:25 [001/005 (0360/0693)]\tLoss Ss: 0.016793\n","\tRotated_Epoch:25 [001/005 (0380/0693)]\tLoss Ss: 0.013865\n","\tRotated_Epoch:25 [001/005 (0400/0693)]\tLoss Ss: 0.017659\n","\tRotated_Epoch:25 [001/005 (0420/0693)]\tLoss Ss: 0.012817\n","\tRotated_Epoch:25 [001/005 (0440/0693)]\tLoss Ss: 0.019759\n","\tRotated_Epoch:25 [001/005 (0460/0693)]\tLoss Ss: 0.016725\n","\tRotated_Epoch:25 [001/005 (0480/0693)]\tLoss Ss: 0.011398\n","\tRotated_Epoch:25 [001/005 (0500/0693)]\tLoss Ss: 0.015600\n","\tRotated_Epoch:25 [001/005 (0520/0693)]\tLoss Ss: 0.014565\n","\tRotated_Epoch:25 [001/005 (0540/0693)]\tLoss Ss: 0.013366\n","\tRotated_Epoch:25 [001/005 (0560/0693)]\tLoss Ss: 0.017582\n","\tRotated_Epoch:25 [001/005 (0580/0693)]\tLoss Ss: 0.017654\n","\tRotated_Epoch:25 [001/005 (0600/0693)]\tLoss Ss: 0.016833\n","\tRotated_Epoch:25 [001/005 (0620/0693)]\tLoss Ss: 0.014846\n","\tRotated_Epoch:25 [001/005 (0640/0693)]\tLoss Ss: 0.013225\n","\tRotated_Epoch:25 [001/005 (0660/0693)]\tLoss Ss: 0.021176\n","\tRotated_Epoch:25 [001/005 (0680/0693)]\tLoss Ss: 0.011341\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:25 [002/005 (0000/0755)]\tLoss Ss: 0.059895\n","\tRotated_Epoch:25 [002/005 (0020/0755)]\tLoss Ss: 0.057593\n","\tRotated_Epoch:25 [002/005 (0040/0755)]\tLoss Ss: 0.070862\n","\tRotated_Epoch:25 [002/005 (0060/0755)]\tLoss Ss: 0.062205\n","\tRotated_Epoch:25 [002/005 (0080/0755)]\tLoss Ss: 0.057396\n","\tRotated_Epoch:25 [002/005 (0100/0755)]\tLoss Ss: 0.034743\n","\tRotated_Epoch:25 [002/005 (0120/0755)]\tLoss Ss: 0.047516\n","\tRotated_Epoch:25 [002/005 (0140/0755)]\tLoss Ss: 0.046394\n","\tRotated_Epoch:25 [002/005 (0160/0755)]\tLoss Ss: 0.019933\n","\tRotated_Epoch:25 [002/005 (0180/0755)]\tLoss Ss: 0.028439\n","\tRotated_Epoch:25 [002/005 (0200/0755)]\tLoss Ss: 0.034220\n","\tRotated_Epoch:25 [002/005 (0220/0755)]\tLoss Ss: 0.023675\n","\tRotated_Epoch:25 [002/005 (0240/0755)]\tLoss Ss: 0.031430\n","\tRotated_Epoch:25 [002/005 (0260/0755)]\tLoss Ss: 0.030397\n","\tRotated_Epoch:25 [002/005 (0280/0755)]\tLoss Ss: 0.028555\n","\tRotated_Epoch:25 [002/005 (0300/0755)]\tLoss Ss: 0.024824\n","\tRotated_Epoch:25 [002/005 (0320/0755)]\tLoss Ss: 0.025972\n","\tRotated_Epoch:25 [002/005 (0340/0755)]\tLoss Ss: 0.024755\n","\tRotated_Epoch:25 [002/005 (0360/0755)]\tLoss Ss: 0.024290\n","\tRotated_Epoch:25 [002/005 (0380/0755)]\tLoss Ss: 0.025570\n","\tRotated_Epoch:25 [002/005 (0400/0755)]\tLoss Ss: 0.014997\n","\tRotated_Epoch:25 [002/005 (0420/0755)]\tLoss Ss: 0.017153\n","\tRotated_Epoch:25 [002/005 (0440/0755)]\tLoss Ss: 0.015206\n","\tRotated_Epoch:25 [002/005 (0460/0755)]\tLoss Ss: 0.020182\n","\tRotated_Epoch:25 [002/005 (0480/0755)]\tLoss Ss: 0.016534\n","\tRotated_Epoch:25 [002/005 (0500/0755)]\tLoss Ss: 0.022363\n","\tRotated_Epoch:25 [002/005 (0520/0755)]\tLoss Ss: 0.023499\n","\tRotated_Epoch:25 [002/005 (0540/0755)]\tLoss Ss: 0.014311\n","\tRotated_Epoch:25 [002/005 (0560/0755)]\tLoss Ss: 0.020417\n","\tRotated_Epoch:25 [002/005 (0580/0755)]\tLoss Ss: 0.019730\n","\tRotated_Epoch:25 [002/005 (0600/0755)]\tLoss Ss: 0.025123\n","\tRotated_Epoch:25 [002/005 (0620/0755)]\tLoss Ss: 0.025514\n","\tRotated_Epoch:25 [002/005 (0640/0755)]\tLoss Ss: 0.030232\n","\tRotated_Epoch:25 [002/005 (0660/0755)]\tLoss Ss: 0.018858\n","\tRotated_Epoch:25 [002/005 (0680/0755)]\tLoss Ss: 0.011297\n","\tRotated_Epoch:25 [002/005 (0700/0755)]\tLoss Ss: 0.014725\n","\tRotated_Epoch:25 [002/005 (0720/0755)]\tLoss Ss: 0.015987\n","\tRotated_Epoch:25 [002/005 (0740/0755)]\tLoss Ss: 0.015376\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tRotated_Epoch:25 [003/005 (0000/0693)]\tLoss Ss: 0.022625\n","\tRotated_Epoch:25 [003/005 (0020/0693)]\tLoss Ss: 0.016267\n","\tRotated_Epoch:25 [003/005 (0040/0693)]\tLoss Ss: 0.015712\n","\tRotated_Epoch:25 [003/005 (0060/0693)]\tLoss Ss: 0.014533\n","\tRotated_Epoch:25 [003/005 (0080/0693)]\tLoss Ss: 0.019613\n","\tRotated_Epoch:25 [003/005 (0100/0693)]\tLoss Ss: 0.015227\n","\tRotated_Epoch:25 [003/005 (0120/0693)]\tLoss Ss: 0.013352\n","\tRotated_Epoch:25 [003/005 (0140/0693)]\tLoss Ss: 0.014131\n","\tRotated_Epoch:25 [003/005 (0160/0693)]\tLoss Ss: 0.014322\n","\tRotated_Epoch:25 [003/005 (0180/0693)]\tLoss Ss: 0.017643\n","\tRotated_Epoch:25 [003/005 (0200/0693)]\tLoss Ss: 0.019236\n","\tRotated_Epoch:25 [003/005 (0220/0693)]\tLoss Ss: 0.016418\n","\tRotated_Epoch:25 [003/005 (0240/0693)]\tLoss Ss: 0.016527\n","\tRotated_Epoch:25 [003/005 (0260/0693)]\tLoss Ss: 0.012053\n","\tRotated_Epoch:25 [003/005 (0280/0693)]\tLoss Ss: 0.019697\n","\tRotated_Epoch:25 [003/005 (0300/0693)]\tLoss Ss: 0.013233\n","\tRotated_Epoch:25 [003/005 (0320/0693)]\tLoss Ss: 0.017853\n","\tRotated_Epoch:25 [003/005 (0340/0693)]\tLoss Ss: 0.014344\n","\tRotated_Epoch:25 [003/005 (0360/0693)]\tLoss Ss: 0.013966\n","\tRotated_Epoch:25 [003/005 (0380/0693)]\tLoss Ss: 0.013753\n","\tRotated_Epoch:25 [003/005 (0400/0693)]\tLoss Ss: 0.013681\n","\tRotated_Epoch:25 [003/005 (0420/0693)]\tLoss Ss: 0.009213\n","\tRotated_Epoch:25 [003/005 (0440/0693)]\tLoss Ss: 0.015351\n","\tRotated_Epoch:25 [003/005 (0460/0693)]\tLoss Ss: 0.015935\n","\tRotated_Epoch:25 [003/005 (0480/0693)]\tLoss Ss: 0.012946\n","\tRotated_Epoch:25 [003/005 (0500/0693)]\tLoss Ss: 0.013984\n","\tRotated_Epoch:25 [003/005 (0520/0693)]\tLoss Ss: 0.014908\n","\tRotated_Epoch:25 [003/005 (0540/0693)]\tLoss Ss: 0.014679\n","\tRotated_Epoch:25 [003/005 (0560/0693)]\tLoss Ss: 0.016688\n","\tRotated_Epoch:25 [003/005 (0580/0693)]\tLoss Ss: 0.018629\n","\tRotated_Epoch:25 [003/005 (0600/0693)]\tLoss Ss: 0.017987\n","\tRotated_Epoch:25 [003/005 (0620/0693)]\tLoss Ss: 0.016640\n","\tRotated_Epoch:25 [003/005 (0640/0693)]\tLoss Ss: 0.014983\n","\tRotated_Epoch:25 [003/005 (0660/0693)]\tLoss Ss: 0.016542\n","\tRotated_Epoch:25 [003/005 (0680/0693)]\tLoss Ss: 0.012870\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tRotated_Epoch:25 [004/005 (0000/0588)]\tLoss Ss: 0.143202\n","\tRotated_Epoch:25 [004/005 (0020/0588)]\tLoss Ss: 0.114977\n","\tRotated_Epoch:25 [004/005 (0040/0588)]\tLoss Ss: 0.104556\n","\tRotated_Epoch:25 [004/005 (0060/0588)]\tLoss Ss: 0.080236\n","\tRotated_Epoch:25 [004/005 (0080/0588)]\tLoss Ss: 0.098163\n","\tRotated_Epoch:25 [004/005 (0100/0588)]\tLoss Ss: 0.080164\n","\tRotated_Epoch:25 [004/005 (0120/0588)]\tLoss Ss: 0.053137\n","\tRotated_Epoch:25 [004/005 (0140/0588)]\tLoss Ss: 0.098427\n","\tRotated_Epoch:25 [004/005 (0160/0588)]\tLoss Ss: 0.078413\n","\tRotated_Epoch:25 [004/005 (0180/0588)]\tLoss Ss: 0.074630\n","\tRotated_Epoch:25 [004/005 (0200/0588)]\tLoss Ss: 0.092192\n","\tRotated_Epoch:25 [004/005 (0220/0588)]\tLoss Ss: 0.068093\n","\tRotated_Epoch:25 [004/005 (0240/0588)]\tLoss Ss: 0.088157\n","\tRotated_Epoch:25 [004/005 (0260/0588)]\tLoss Ss: 0.081817\n","\tRotated_Epoch:25 [004/005 (0280/0588)]\tLoss Ss: 0.075205\n","\tRotated_Epoch:25 [004/005 (0300/0588)]\tLoss Ss: 0.057119\n","\tRotated_Epoch:25 [004/005 (0320/0588)]\tLoss Ss: 0.065514\n","\tRotated_Epoch:25 [004/005 (0340/0588)]\tLoss Ss: 0.065331\n","\tRotated_Epoch:25 [004/005 (0360/0588)]\tLoss Ss: 0.070924\n","\tRotated_Epoch:25 [004/005 (0380/0588)]\tLoss Ss: 0.036094\n","\tRotated_Epoch:25 [004/005 (0400/0588)]\tLoss Ss: 0.051271\n","\tRotated_Epoch:25 [004/005 (0420/0588)]\tLoss Ss: 0.073652\n","\tRotated_Epoch:25 [004/005 (0440/0588)]\tLoss Ss: 0.057041\n","\tRotated_Epoch:25 [004/005 (0460/0588)]\tLoss Ss: 0.080539\n","\tRotated_Epoch:25 [004/005 (0480/0588)]\tLoss Ss: 0.073420\n","\tRotated_Epoch:25 [004/005 (0500/0588)]\tLoss Ss: 0.056990\n","\tRotated_Epoch:25 [004/005 (0520/0588)]\tLoss Ss: 0.041845\n","\tRotated_Epoch:25 [004/005 (0540/0588)]\tLoss Ss: 0.048025\n","\tRotated_Epoch:25 [004/005 (0560/0588)]\tLoss Ss: 0.037968\n","\tRotated_Epoch:25 [004/005 (0580/0588)]\tLoss Ss: 0.053315\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tRotated_Epoch:25 [005/005 (0000/0614)]\tLoss Ss: 0.093959\n","\tRotated_Epoch:25 [005/005 (0020/0614)]\tLoss Ss: 0.064455\n","\tRotated_Epoch:25 [005/005 (0040/0614)]\tLoss Ss: 0.043786\n","\tRotated_Epoch:25 [005/005 (0060/0614)]\tLoss Ss: 0.023825\n","\tRotated_Epoch:25 [005/005 (0080/0614)]\tLoss Ss: 0.023230\n","\tRotated_Epoch:25 [005/005 (0100/0614)]\tLoss Ss: 0.026765\n","\tRotated_Epoch:25 [005/005 (0120/0614)]\tLoss Ss: 0.018375\n","\tRotated_Epoch:25 [005/005 (0140/0614)]\tLoss Ss: 0.018235\n","\tRotated_Epoch:25 [005/005 (0160/0614)]\tLoss Ss: 0.014972\n","\tRotated_Epoch:25 [005/005 (0180/0614)]\tLoss Ss: 0.010431\n","\tRotated_Epoch:25 [005/005 (0200/0614)]\tLoss Ss: 0.011793\n","\tRotated_Epoch:25 [005/005 (0220/0614)]\tLoss Ss: 0.011864\n","\tRotated_Epoch:25 [005/005 (0240/0614)]\tLoss Ss: 0.012399\n","\tRotated_Epoch:25 [005/005 (0260/0614)]\tLoss Ss: 0.011556\n","\tRotated_Epoch:25 [005/005 (0280/0614)]\tLoss Ss: 0.013700\n","\tRotated_Epoch:25 [005/005 (0300/0614)]\tLoss Ss: 0.009983\n","\tRotated_Epoch:25 [005/005 (0320/0614)]\tLoss Ss: 0.011144\n","\tRotated_Epoch:25 [005/005 (0340/0614)]\tLoss Ss: 0.011413\n","\tRotated_Epoch:25 [005/005 (0360/0614)]\tLoss Ss: 0.008183\n","\tRotated_Epoch:25 [005/005 (0380/0614)]\tLoss Ss: 0.007003\n","\tRotated_Epoch:25 [005/005 (0400/0614)]\tLoss Ss: 0.009770\n","\tRotated_Epoch:25 [005/005 (0420/0614)]\tLoss Ss: 0.007777\n","\tRotated_Epoch:25 [005/005 (0440/0614)]\tLoss Ss: 0.012879\n","\tRotated_Epoch:25 [005/005 (0460/0614)]\tLoss Ss: 0.007885\n","\tRotated_Epoch:25 [005/005 (0480/0614)]\tLoss Ss: 0.011470\n","\tRotated_Epoch:25 [005/005 (0500/0614)]\tLoss Ss: 0.008673\n","\tRotated_Epoch:25 [005/005 (0520/0614)]\tLoss Ss: 0.006835\n","\tRotated_Epoch:25 [005/005 (0540/0614)]\tLoss Ss: 0.007097\n","\tRotated_Epoch:25 [005/005 (0560/0614)]\tLoss Ss: 0.009100\n","\tRotated_Epoch:25 [005/005 (0580/0614)]\tLoss Ss: 0.006625\n","\tRotated_Epoch:25 [005/005 (0600/0614)]\tLoss Ss: 0.007351\n","new_1_6_d.nii\n","new_1_6_m.nii\n","new_1_7_d.nii\n","new_1_7_m.nii\n","new_2_5_d.nii\n","new_2_5_m.nii\n","new_2_6_d.nii\n","new_2_6_m.nii\n","new_2_7_d.nii\n","new_2_7_m.nii\n","new_3_d (2).nii\n","new_3_m (2).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (3).nii\n","new_3_m (3).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (7).nii\n","new_3_m (7).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (8).nii\n","new_3_m (8).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (9).nii\n","new_3_m (9).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_4_10_d_163526.nii\n","new_4_10_m_163526.nii.gz\n","new_4_11_d_084336.nii\n","new_4_11_m_084336.nii.gz\n","new_4_7_d_184422.nii\n","new_4_7_m_184422.nii.gz\n","new_4_8_d_143412.nii\n","new_4_8_m_143412.nii.gz\n","new_4_9_d_155347.nii\n","new_4_9_m_155347.nii.gz\n","\tEpoch: 25; Dice: 0.9644 +/- 0.0057; Loss: 10.5395\n","Begin Epoch 26\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:26 [000/005 (0000/0755)]\tLoss Ss: 0.022418\n","\tEpoch:26 [000/005 (0020/0755)]\tLoss Ss: 0.024549\n","\tEpoch:26 [000/005 (0040/0755)]\tLoss Ss: 0.030320\n","\tEpoch:26 [000/005 (0060/0755)]\tLoss Ss: 0.017233\n","\tEpoch:26 [000/005 (0080/0755)]\tLoss Ss: 0.020473\n","\tEpoch:26 [000/005 (0100/0755)]\tLoss Ss: 0.013705\n","\tEpoch:26 [000/005 (0120/0755)]\tLoss Ss: 0.016635\n","\tEpoch:26 [000/005 (0140/0755)]\tLoss Ss: 0.019482\n","\tEpoch:26 [000/005 (0160/0755)]\tLoss Ss: 0.022689\n","\tEpoch:26 [000/005 (0180/0755)]\tLoss Ss: 0.016153\n","\tEpoch:26 [000/005 (0200/0755)]\tLoss Ss: 0.016537\n","\tEpoch:26 [000/005 (0220/0755)]\tLoss Ss: 0.021590\n","\tEpoch:26 [000/005 (0240/0755)]\tLoss Ss: 0.017255\n","\tEpoch:26 [000/005 (0260/0755)]\tLoss Ss: 0.016569\n","\tEpoch:26 [000/005 (0280/0755)]\tLoss Ss: 0.014849\n","\tEpoch:26 [000/005 (0300/0755)]\tLoss Ss: 0.023095\n","\tEpoch:26 [000/005 (0320/0755)]\tLoss Ss: 0.012910\n","\tEpoch:26 [000/005 (0340/0755)]\tLoss Ss: 0.014727\n","\tEpoch:26 [000/005 (0360/0755)]\tLoss Ss: 0.018620\n","\tEpoch:26 [000/005 (0380/0755)]\tLoss Ss: 0.012948\n","\tEpoch:26 [000/005 (0400/0755)]\tLoss Ss: 0.009246\n","\tEpoch:26 [000/005 (0420/0755)]\tLoss Ss: 0.016108\n","\tEpoch:26 [000/005 (0440/0755)]\tLoss Ss: 0.015111\n","\tEpoch:26 [000/005 (0460/0755)]\tLoss Ss: 0.010870\n","\tEpoch:26 [000/005 (0480/0755)]\tLoss Ss: 0.015256\n","\tEpoch:26 [000/005 (0500/0755)]\tLoss Ss: 0.012546\n","\tEpoch:26 [000/005 (0520/0755)]\tLoss Ss: 0.010409\n","\tEpoch:26 [000/005 (0540/0755)]\tLoss Ss: 0.012217\n","\tEpoch:26 [000/005 (0560/0755)]\tLoss Ss: 0.020546\n","\tEpoch:26 [000/005 (0580/0755)]\tLoss Ss: 0.014302\n","\tEpoch:26 [000/005 (0600/0755)]\tLoss Ss: 0.013790\n","\tEpoch:26 [000/005 (0620/0755)]\tLoss Ss: 0.017529\n","\tEpoch:26 [000/005 (0640/0755)]\tLoss Ss: 0.009156\n","\tEpoch:26 [000/005 (0660/0755)]\tLoss Ss: 0.014122\n","\tEpoch:26 [000/005 (0680/0755)]\tLoss Ss: 0.013989\n","\tEpoch:26 [000/005 (0700/0755)]\tLoss Ss: 0.017563\n","\tEpoch:26 [000/005 (0720/0755)]\tLoss Ss: 0.016065\n","\tEpoch:26 [000/005 (0740/0755)]\tLoss Ss: 0.013467\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tEpoch:26 [001/005 (0000/0693)]\tLoss Ss: 0.012966\n","\tEpoch:26 [001/005 (0020/0693)]\tLoss Ss: 0.020255\n","\tEpoch:26 [001/005 (0040/0693)]\tLoss Ss: 0.025880\n","\tEpoch:26 [001/005 (0060/0693)]\tLoss Ss: 0.014097\n","\tEpoch:26 [001/005 (0080/0693)]\tLoss Ss: 0.016134\n","\tEpoch:26 [001/005 (0100/0693)]\tLoss Ss: 0.016436\n","\tEpoch:26 [001/005 (0120/0693)]\tLoss Ss: 0.019049\n","\tEpoch:26 [001/005 (0140/0693)]\tLoss Ss: 0.017332\n","\tEpoch:26 [001/005 (0160/0693)]\tLoss Ss: 0.013749\n","\tEpoch:26 [001/005 (0180/0693)]\tLoss Ss: 0.020643\n","\tEpoch:26 [001/005 (0200/0693)]\tLoss Ss: 0.009230\n","\tEpoch:26 [001/005 (0220/0693)]\tLoss Ss: 0.016028\n","\tEpoch:26 [001/005 (0240/0693)]\tLoss Ss: 0.013014\n","\tEpoch:26 [001/005 (0260/0693)]\tLoss Ss: 0.013334\n","\tEpoch:26 [001/005 (0280/0693)]\tLoss Ss: 0.017341\n","\tEpoch:26 [001/005 (0300/0693)]\tLoss Ss: 0.013918\n","\tEpoch:26 [001/005 (0320/0693)]\tLoss Ss: 0.013122\n","\tEpoch:26 [001/005 (0340/0693)]\tLoss Ss: 0.019230\n","\tEpoch:26 [001/005 (0360/0693)]\tLoss Ss: 0.012901\n","\tEpoch:26 [001/005 (0380/0693)]\tLoss Ss: 0.015556\n","\tEpoch:26 [001/005 (0400/0693)]\tLoss Ss: 0.008364\n","\tEpoch:26 [001/005 (0420/0693)]\tLoss Ss: 0.014578\n","\tEpoch:26 [001/005 (0440/0693)]\tLoss Ss: 0.006603\n","\tEpoch:26 [001/005 (0460/0693)]\tLoss Ss: 0.014035\n","\tEpoch:26 [001/005 (0480/0693)]\tLoss Ss: 0.016812\n","\tEpoch:26 [001/005 (0500/0693)]\tLoss Ss: 0.013997\n","\tEpoch:26 [001/005 (0520/0693)]\tLoss Ss: 0.012705\n","\tEpoch:26 [001/005 (0540/0693)]\tLoss Ss: 0.013853\n","\tEpoch:26 [001/005 (0560/0693)]\tLoss Ss: 0.016378\n","\tEpoch:26 [001/005 (0580/0693)]\tLoss Ss: 0.013378\n","\tEpoch:26 [001/005 (0600/0693)]\tLoss Ss: 0.016372\n","\tEpoch:26 [001/005 (0620/0693)]\tLoss Ss: 0.013074\n","\tEpoch:26 [001/005 (0640/0693)]\tLoss Ss: 0.012869\n","\tEpoch:26 [001/005 (0660/0693)]\tLoss Ss: 0.008869\n","\tEpoch:26 [001/005 (0680/0693)]\tLoss Ss: 0.009324\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tEpoch:26 [002/005 (0000/0693)]\tLoss Ss: 0.012775\n","\tEpoch:26 [002/005 (0020/0693)]\tLoss Ss: 0.012046\n","\tEpoch:26 [002/005 (0040/0693)]\tLoss Ss: 0.011499\n","\tEpoch:26 [002/005 (0060/0693)]\tLoss Ss: 0.009335\n","\tEpoch:26 [002/005 (0080/0693)]\tLoss Ss: 0.014862\n","\tEpoch:26 [002/005 (0100/0693)]\tLoss Ss: 0.013007\n","\tEpoch:26 [002/005 (0120/0693)]\tLoss Ss: 0.012151\n","\tEpoch:26 [002/005 (0140/0693)]\tLoss Ss: 0.010243\n","\tEpoch:26 [002/005 (0160/0693)]\tLoss Ss: 0.012884\n","\tEpoch:26 [002/005 (0180/0693)]\tLoss Ss: 0.012913\n","\tEpoch:26 [002/005 (0200/0693)]\tLoss Ss: 0.009783\n","\tEpoch:26 [002/005 (0220/0693)]\tLoss Ss: 0.014065\n","\tEpoch:26 [002/005 (0240/0693)]\tLoss Ss: 0.012374\n","\tEpoch:26 [002/005 (0260/0693)]\tLoss Ss: 0.011456\n","\tEpoch:26 [002/005 (0280/0693)]\tLoss Ss: 0.012824\n","\tEpoch:26 [002/005 (0300/0693)]\tLoss Ss: 0.010935\n","\tEpoch:26 [002/005 (0320/0693)]\tLoss Ss: 0.012598\n","\tEpoch:26 [002/005 (0340/0693)]\tLoss Ss: 0.011413\n","\tEpoch:26 [002/005 (0360/0693)]\tLoss Ss: 0.011709\n","\tEpoch:26 [002/005 (0380/0693)]\tLoss Ss: 0.015842\n","\tEpoch:26 [002/005 (0400/0693)]\tLoss Ss: 0.010645\n","\tEpoch:26 [002/005 (0420/0693)]\tLoss Ss: 0.009268\n","\tEpoch:26 [002/005 (0440/0693)]\tLoss Ss: 0.011812\n","\tEpoch:26 [002/005 (0460/0693)]\tLoss Ss: 0.011271\n","\tEpoch:26 [002/005 (0480/0693)]\tLoss Ss: 0.018931\n","\tEpoch:26 [002/005 (0500/0693)]\tLoss Ss: 0.013978\n","\tEpoch:26 [002/005 (0520/0693)]\tLoss Ss: 0.013890\n","\tEpoch:26 [002/005 (0540/0693)]\tLoss Ss: 0.013553\n","\tEpoch:26 [002/005 (0560/0693)]\tLoss Ss: 0.007793\n","\tEpoch:26 [002/005 (0580/0693)]\tLoss Ss: 0.019061\n","\tEpoch:26 [002/005 (0600/0693)]\tLoss Ss: 0.011542\n","\tEpoch:26 [002/005 (0620/0693)]\tLoss Ss: 0.013367\n","\tEpoch:26 [002/005 (0640/0693)]\tLoss Ss: 0.018276\n","\tEpoch:26 [002/005 (0660/0693)]\tLoss Ss: 0.007903\n","\tEpoch:26 [002/005 (0680/0693)]\tLoss Ss: 0.012715\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tEpoch:26 [003/005 (0000/0614)]\tLoss Ss: 0.005565\n","\tEpoch:26 [003/005 (0020/0614)]\tLoss Ss: 0.005078\n","\tEpoch:26 [003/005 (0040/0614)]\tLoss Ss: 0.006116\n","\tEpoch:26 [003/005 (0060/0614)]\tLoss Ss: 0.007107\n","\tEpoch:26 [003/005 (0080/0614)]\tLoss Ss: 0.009119\n","\tEpoch:26 [003/005 (0100/0614)]\tLoss Ss: 0.005837\n","\tEpoch:26 [003/005 (0120/0614)]\tLoss Ss: 0.004435\n","\tEpoch:26 [003/005 (0140/0614)]\tLoss Ss: 0.005172\n","\tEpoch:26 [003/005 (0160/0614)]\tLoss Ss: 0.006713\n","\tEpoch:26 [003/005 (0180/0614)]\tLoss Ss: 0.008318\n","\tEpoch:26 [003/005 (0200/0614)]\tLoss Ss: 0.006597\n","\tEpoch:26 [003/005 (0220/0614)]\tLoss Ss: 0.006173\n","\tEpoch:26 [003/005 (0240/0614)]\tLoss Ss: 0.005061\n","\tEpoch:26 [003/005 (0260/0614)]\tLoss Ss: 0.006381\n","\tEpoch:26 [003/005 (0280/0614)]\tLoss Ss: 0.008345\n","\tEpoch:26 [003/005 (0300/0614)]\tLoss Ss: 0.006566\n","\tEpoch:26 [003/005 (0320/0614)]\tLoss Ss: 0.007076\n","\tEpoch:26 [003/005 (0340/0614)]\tLoss Ss: 0.006547\n","\tEpoch:26 [003/005 (0360/0614)]\tLoss Ss: 0.005006\n","\tEpoch:26 [003/005 (0380/0614)]\tLoss Ss: 0.005020\n","\tEpoch:26 [003/005 (0400/0614)]\tLoss Ss: 0.004847\n","\tEpoch:26 [003/005 (0420/0614)]\tLoss Ss: 0.008167\n","\tEpoch:26 [003/005 (0440/0614)]\tLoss Ss: 0.005792\n","\tEpoch:26 [003/005 (0460/0614)]\tLoss Ss: 0.006710\n","\tEpoch:26 [003/005 (0480/0614)]\tLoss Ss: 0.004822\n","\tEpoch:26 [003/005 (0500/0614)]\tLoss Ss: 0.005308\n","\tEpoch:26 [003/005 (0520/0614)]\tLoss Ss: 0.007166\n","\tEpoch:26 [003/005 (0540/0614)]\tLoss Ss: 0.005046\n","\tEpoch:26 [003/005 (0560/0614)]\tLoss Ss: 0.004274\n","\tEpoch:26 [003/005 (0580/0614)]\tLoss Ss: 0.004505\n","\tEpoch:26 [003/005 (0600/0614)]\tLoss Ss: 0.002848\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:26 [004/005 (0000/0755)]\tLoss Ss: 0.015933\n","\tEpoch:26 [004/005 (0020/0755)]\tLoss Ss: 0.012578\n","\tEpoch:26 [004/005 (0040/0755)]\tLoss Ss: 0.023239\n","\tEpoch:26 [004/005 (0060/0755)]\tLoss Ss: 0.021231\n","\tEpoch:26 [004/005 (0080/0755)]\tLoss Ss: 0.016195\n","\tEpoch:26 [004/005 (0100/0755)]\tLoss Ss: 0.016539\n","\tEpoch:26 [004/005 (0120/0755)]\tLoss Ss: 0.018575\n","\tEpoch:26 [004/005 (0140/0755)]\tLoss Ss: 0.015383\n","\tEpoch:26 [004/005 (0160/0755)]\tLoss Ss: 0.035471\n","\tEpoch:26 [004/005 (0180/0755)]\tLoss Ss: 0.016835\n","\tEpoch:26 [004/005 (0200/0755)]\tLoss Ss: 0.016830\n","\tEpoch:26 [004/005 (0220/0755)]\tLoss Ss: 0.024525\n","\tEpoch:26 [004/005 (0240/0755)]\tLoss Ss: 0.010588\n","\tEpoch:26 [004/005 (0260/0755)]\tLoss Ss: 0.017106\n","\tEpoch:26 [004/005 (0280/0755)]\tLoss Ss: 0.018229\n","\tEpoch:26 [004/005 (0300/0755)]\tLoss Ss: 0.014518\n","\tEpoch:26 [004/005 (0320/0755)]\tLoss Ss: 0.017867\n","\tEpoch:26 [004/005 (0340/0755)]\tLoss Ss: 0.016486\n","\tEpoch:26 [004/005 (0360/0755)]\tLoss Ss: 0.012745\n","\tEpoch:26 [004/005 (0380/0755)]\tLoss Ss: 0.015318\n","\tEpoch:26 [004/005 (0400/0755)]\tLoss Ss: 0.018313\n","\tEpoch:26 [004/005 (0420/0755)]\tLoss Ss: 0.019800\n","\tEpoch:26 [004/005 (0440/0755)]\tLoss Ss: 0.011631\n","\tEpoch:26 [004/005 (0460/0755)]\tLoss Ss: 0.011048\n","\tEpoch:26 [004/005 (0480/0755)]\tLoss Ss: 0.008511\n","\tEpoch:26 [004/005 (0500/0755)]\tLoss Ss: 0.015617\n","\tEpoch:26 [004/005 (0520/0755)]\tLoss Ss: 0.015322\n","\tEpoch:26 [004/005 (0540/0755)]\tLoss Ss: 0.013093\n","\tEpoch:26 [004/005 (0560/0755)]\tLoss Ss: 0.015986\n","\tEpoch:26 [004/005 (0580/0755)]\tLoss Ss: 0.013310\n","\tEpoch:26 [004/005 (0600/0755)]\tLoss Ss: 0.015804\n","\tEpoch:26 [004/005 (0620/0755)]\tLoss Ss: 0.015576\n","\tEpoch:26 [004/005 (0640/0755)]\tLoss Ss: 0.013807\n","\tEpoch:26 [004/005 (0660/0755)]\tLoss Ss: 0.014812\n","\tEpoch:26 [004/005 (0680/0755)]\tLoss Ss: 0.018951\n","\tEpoch:26 [004/005 (0700/0755)]\tLoss Ss: 0.017552\n","\tEpoch:26 [004/005 (0720/0755)]\tLoss Ss: 0.019977\n","\tEpoch:26 [004/005 (0740/0755)]\tLoss Ss: 0.019395\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tEpoch:26 [005/005 (0000/0588)]\tLoss Ss: 0.006468\n","\tEpoch:26 [005/005 (0020/0588)]\tLoss Ss: 0.008908\n","\tEpoch:26 [005/005 (0040/0588)]\tLoss Ss: 0.004176\n","\tEpoch:26 [005/005 (0060/0588)]\tLoss Ss: 0.004341\n","\tEpoch:26 [005/005 (0080/0588)]\tLoss Ss: 0.006546\n","\tEpoch:26 [005/005 (0100/0588)]\tLoss Ss: 0.005389\n","\tEpoch:26 [005/005 (0120/0588)]\tLoss Ss: 0.005522\n","\tEpoch:26 [005/005 (0140/0588)]\tLoss Ss: 0.004656\n","\tEpoch:26 [005/005 (0160/0588)]\tLoss Ss: 0.006405\n","\tEpoch:26 [005/005 (0180/0588)]\tLoss Ss: 0.005822\n","\tEpoch:26 [005/005 (0200/0588)]\tLoss Ss: 0.004999\n","\tEpoch:26 [005/005 (0220/0588)]\tLoss Ss: 0.004162\n","\tEpoch:26 [005/005 (0240/0588)]\tLoss Ss: 0.004872\n","\tEpoch:26 [005/005 (0260/0588)]\tLoss Ss: 0.004936\n","\tEpoch:26 [005/005 (0280/0588)]\tLoss Ss: 0.004639\n","\tEpoch:26 [005/005 (0300/0588)]\tLoss Ss: 0.006703\n","\tEpoch:26 [005/005 (0320/0588)]\tLoss Ss: 0.004558\n","\tEpoch:26 [005/005 (0340/0588)]\tLoss Ss: 0.003759\n","\tEpoch:26 [005/005 (0360/0588)]\tLoss Ss: 0.005229\n","\tEpoch:26 [005/005 (0380/0588)]\tLoss Ss: 0.004218\n","\tEpoch:26 [005/005 (0400/0588)]\tLoss Ss: 0.004116\n","\tEpoch:26 [005/005 (0420/0588)]\tLoss Ss: 0.004020\n","\tEpoch:26 [005/005 (0440/0588)]\tLoss Ss: 0.004674\n","\tEpoch:26 [005/005 (0460/0588)]\tLoss Ss: 0.004258\n","\tEpoch:26 [005/005 (0480/0588)]\tLoss Ss: 0.003996\n","\tEpoch:26 [005/005 (0500/0588)]\tLoss Ss: 0.004263\n","\tEpoch:26 [005/005 (0520/0588)]\tLoss Ss: 0.004782\n","\tEpoch:26 [005/005 (0540/0588)]\tLoss Ss: 0.005038\n","\tEpoch:26 [005/005 (0560/0588)]\tLoss Ss: 0.004118\n","\tEpoch:26 [005/005 (0580/0588)]\tLoss Ss: 0.003821\n","Now train the rotated image\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tRotated_Epoch:26 [000/005 (0000/0614)]\tLoss Ss: 0.006262\n","\tRotated_Epoch:26 [000/005 (0020/0614)]\tLoss Ss: 0.005470\n","\tRotated_Epoch:26 [000/005 (0040/0614)]\tLoss Ss: 0.011682\n","\tRotated_Epoch:26 [000/005 (0060/0614)]\tLoss Ss: 0.014183\n","\tRotated_Epoch:26 [000/005 (0080/0614)]\tLoss Ss: 0.006733\n","\tRotated_Epoch:26 [000/005 (0100/0614)]\tLoss Ss: 0.004157\n","\tRotated_Epoch:26 [000/005 (0120/0614)]\tLoss Ss: 0.009790\n","\tRotated_Epoch:26 [000/005 (0140/0614)]\tLoss Ss: 0.007132\n","\tRotated_Epoch:26 [000/005 (0160/0614)]\tLoss Ss: 0.003462\n","\tRotated_Epoch:26 [000/005 (0180/0614)]\tLoss Ss: 0.005253\n","\tRotated_Epoch:26 [000/005 (0200/0614)]\tLoss Ss: 0.006082\n","\tRotated_Epoch:26 [000/005 (0220/0614)]\tLoss Ss: 0.005905\n","\tRotated_Epoch:26 [000/005 (0240/0614)]\tLoss Ss: 0.009982\n","\tRotated_Epoch:26 [000/005 (0260/0614)]\tLoss Ss: 0.004419\n","\tRotated_Epoch:26 [000/005 (0280/0614)]\tLoss Ss: 0.004357\n","\tRotated_Epoch:26 [000/005 (0300/0614)]\tLoss Ss: 0.006773\n","\tRotated_Epoch:26 [000/005 (0320/0614)]\tLoss Ss: 0.007456\n","\tRotated_Epoch:26 [000/005 (0340/0614)]\tLoss Ss: 0.006117\n","\tRotated_Epoch:26 [000/005 (0360/0614)]\tLoss Ss: 0.005097\n","\tRotated_Epoch:26 [000/005 (0380/0614)]\tLoss Ss: 0.006048\n","\tRotated_Epoch:26 [000/005 (0400/0614)]\tLoss Ss: 0.008975\n","\tRotated_Epoch:26 [000/005 (0420/0614)]\tLoss Ss: 0.007289\n","\tRotated_Epoch:26 [000/005 (0440/0614)]\tLoss Ss: 0.006475\n","\tRotated_Epoch:26 [000/005 (0460/0614)]\tLoss Ss: 0.005614\n","\tRotated_Epoch:26 [000/005 (0480/0614)]\tLoss Ss: 0.006025\n","\tRotated_Epoch:26 [000/005 (0500/0614)]\tLoss Ss: 0.005233\n","\tRotated_Epoch:26 [000/005 (0520/0614)]\tLoss Ss: 0.005019\n","\tRotated_Epoch:26 [000/005 (0540/0614)]\tLoss Ss: 0.005503\n","\tRotated_Epoch:26 [000/005 (0560/0614)]\tLoss Ss: 0.003723\n","\tRotated_Epoch:26 [000/005 (0580/0614)]\tLoss Ss: 0.006531\n","\tRotated_Epoch:26 [000/005 (0600/0614)]\tLoss Ss: 0.004855\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tRotated_Epoch:26 [001/005 (0000/0588)]\tLoss Ss: 0.175018\n","\tRotated_Epoch:26 [001/005 (0020/0588)]\tLoss Ss: 0.155751\n","\tRotated_Epoch:26 [001/005 (0040/0588)]\tLoss Ss: 0.199186\n","\tRotated_Epoch:26 [001/005 (0060/0588)]\tLoss Ss: 0.116838\n","\tRotated_Epoch:26 [001/005 (0080/0588)]\tLoss Ss: 0.132521\n","\tRotated_Epoch:26 [001/005 (0100/0588)]\tLoss Ss: 0.072902\n","\tRotated_Epoch:26 [001/005 (0120/0588)]\tLoss Ss: 0.071427\n","\tRotated_Epoch:26 [001/005 (0140/0588)]\tLoss Ss: 0.079253\n","\tRotated_Epoch:26 [001/005 (0160/0588)]\tLoss Ss: 0.069281\n","\tRotated_Epoch:26 [001/005 (0180/0588)]\tLoss Ss: 0.054667\n","\tRotated_Epoch:26 [001/005 (0200/0588)]\tLoss Ss: 0.072902\n","\tRotated_Epoch:26 [001/005 (0220/0588)]\tLoss Ss: 0.058294\n","\tRotated_Epoch:26 [001/005 (0240/0588)]\tLoss Ss: 0.054611\n","\tRotated_Epoch:26 [001/005 (0260/0588)]\tLoss Ss: 0.058452\n","\tRotated_Epoch:26 [001/005 (0280/0588)]\tLoss Ss: 0.091826\n","\tRotated_Epoch:26 [001/005 (0300/0588)]\tLoss Ss: 0.060446\n","\tRotated_Epoch:26 [001/005 (0320/0588)]\tLoss Ss: 0.052158\n","\tRotated_Epoch:26 [001/005 (0340/0588)]\tLoss Ss: 0.076100\n","\tRotated_Epoch:26 [001/005 (0360/0588)]\tLoss Ss: 0.058645\n","\tRotated_Epoch:26 [001/005 (0380/0588)]\tLoss Ss: 0.051847\n","\tRotated_Epoch:26 [001/005 (0400/0588)]\tLoss Ss: 0.039358\n","\tRotated_Epoch:26 [001/005 (0420/0588)]\tLoss Ss: 0.055538\n","\tRotated_Epoch:26 [001/005 (0440/0588)]\tLoss Ss: 0.073406\n","\tRotated_Epoch:26 [001/005 (0460/0588)]\tLoss Ss: 0.060396\n","\tRotated_Epoch:26 [001/005 (0480/0588)]\tLoss Ss: 0.042730\n","\tRotated_Epoch:26 [001/005 (0500/0588)]\tLoss Ss: 0.058441\n","\tRotated_Epoch:26 [001/005 (0520/0588)]\tLoss Ss: 0.060155\n","\tRotated_Epoch:26 [001/005 (0540/0588)]\tLoss Ss: 0.046130\n","\tRotated_Epoch:26 [001/005 (0560/0588)]\tLoss Ss: 0.044996\n","\tRotated_Epoch:26 [001/005 (0580/0588)]\tLoss Ss: 0.053463\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tRotated_Epoch:26 [002/005 (0000/0693)]\tLoss Ss: 0.072640\n","\tRotated_Epoch:26 [002/005 (0020/0693)]\tLoss Ss: 0.073177\n","\tRotated_Epoch:26 [002/005 (0040/0693)]\tLoss Ss: 0.040225\n","\tRotated_Epoch:26 [002/005 (0060/0693)]\tLoss Ss: 0.031490\n","\tRotated_Epoch:26 [002/005 (0080/0693)]\tLoss Ss: 0.020902\n","\tRotated_Epoch:26 [002/005 (0100/0693)]\tLoss Ss: 0.015438\n","\tRotated_Epoch:26 [002/005 (0120/0693)]\tLoss Ss: 0.021463\n","\tRotated_Epoch:26 [002/005 (0140/0693)]\tLoss Ss: 0.021968\n","\tRotated_Epoch:26 [002/005 (0160/0693)]\tLoss Ss: 0.015164\n","\tRotated_Epoch:26 [002/005 (0180/0693)]\tLoss Ss: 0.020920\n","\tRotated_Epoch:26 [002/005 (0200/0693)]\tLoss Ss: 0.019469\n","\tRotated_Epoch:26 [002/005 (0220/0693)]\tLoss Ss: 0.016611\n","\tRotated_Epoch:26 [002/005 (0240/0693)]\tLoss Ss: 0.014026\n","\tRotated_Epoch:26 [002/005 (0260/0693)]\tLoss Ss: 0.015969\n","\tRotated_Epoch:26 [002/005 (0280/0693)]\tLoss Ss: 0.016507\n","\tRotated_Epoch:26 [002/005 (0300/0693)]\tLoss Ss: 0.021122\n","\tRotated_Epoch:26 [002/005 (0320/0693)]\tLoss Ss: 0.021252\n","\tRotated_Epoch:26 [002/005 (0340/0693)]\tLoss Ss: 0.019295\n","\tRotated_Epoch:26 [002/005 (0360/0693)]\tLoss Ss: 0.019699\n","\tRotated_Epoch:26 [002/005 (0380/0693)]\tLoss Ss: 0.021688\n","\tRotated_Epoch:26 [002/005 (0400/0693)]\tLoss Ss: 0.018957\n","\tRotated_Epoch:26 [002/005 (0420/0693)]\tLoss Ss: 0.012767\n","\tRotated_Epoch:26 [002/005 (0440/0693)]\tLoss Ss: 0.019587\n","\tRotated_Epoch:26 [002/005 (0460/0693)]\tLoss Ss: 0.014989\n","\tRotated_Epoch:26 [002/005 (0480/0693)]\tLoss Ss: 0.015782\n","\tRotated_Epoch:26 [002/005 (0500/0693)]\tLoss Ss: 0.017276\n","\tRotated_Epoch:26 [002/005 (0520/0693)]\tLoss Ss: 0.012839\n","\tRotated_Epoch:26 [002/005 (0540/0693)]\tLoss Ss: 0.017140\n","\tRotated_Epoch:26 [002/005 (0560/0693)]\tLoss Ss: 0.020069\n","\tRotated_Epoch:26 [002/005 (0580/0693)]\tLoss Ss: 0.015476\n","\tRotated_Epoch:26 [002/005 (0600/0693)]\tLoss Ss: 0.017233\n","\tRotated_Epoch:26 [002/005 (0620/0693)]\tLoss Ss: 0.014096\n","\tRotated_Epoch:26 [002/005 (0640/0693)]\tLoss Ss: 0.013156\n","\tRotated_Epoch:26 [002/005 (0660/0693)]\tLoss Ss: 0.011321\n","\tRotated_Epoch:26 [002/005 (0680/0693)]\tLoss Ss: 0.013866\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:26 [003/005 (0000/0755)]\tLoss Ss: 0.344284\n","\tRotated_Epoch:26 [003/005 (0020/0755)]\tLoss Ss: 0.199098\n","\tRotated_Epoch:26 [003/005 (0040/0755)]\tLoss Ss: 0.264583\n","\tRotated_Epoch:26 [003/005 (0060/0755)]\tLoss Ss: 0.345585\n","\tRotated_Epoch:26 [003/005 (0080/0755)]\tLoss Ss: 0.198179\n","\tRotated_Epoch:26 [003/005 (0100/0755)]\tLoss Ss: 0.218569\n","\tRotated_Epoch:26 [003/005 (0120/0755)]\tLoss Ss: 0.161889\n","\tRotated_Epoch:26 [003/005 (0140/0755)]\tLoss Ss: 0.122449\n","\tRotated_Epoch:26 [003/005 (0160/0755)]\tLoss Ss: 0.153870\n","\tRotated_Epoch:26 [003/005 (0180/0755)]\tLoss Ss: 0.128566\n","\tRotated_Epoch:26 [003/005 (0200/0755)]\tLoss Ss: 0.119759\n","\tRotated_Epoch:26 [003/005 (0220/0755)]\tLoss Ss: 0.081227\n","\tRotated_Epoch:26 [003/005 (0240/0755)]\tLoss Ss: 0.087987\n","\tRotated_Epoch:26 [003/005 (0260/0755)]\tLoss Ss: 0.115922\n","\tRotated_Epoch:26 [003/005 (0280/0755)]\tLoss Ss: 0.085061\n","\tRotated_Epoch:26 [003/005 (0300/0755)]\tLoss Ss: 0.123142\n","\tRotated_Epoch:26 [003/005 (0320/0755)]\tLoss Ss: 0.085360\n","\tRotated_Epoch:26 [003/005 (0340/0755)]\tLoss Ss: 0.117653\n","\tRotated_Epoch:26 [003/005 (0360/0755)]\tLoss Ss: 0.100241\n","\tRotated_Epoch:26 [003/005 (0380/0755)]\tLoss Ss: 0.095549\n","\tRotated_Epoch:26 [003/005 (0400/0755)]\tLoss Ss: 0.095324\n","\tRotated_Epoch:26 [003/005 (0420/0755)]\tLoss Ss: 0.108243\n","\tRotated_Epoch:26 [003/005 (0440/0755)]\tLoss Ss: 0.079087\n","\tRotated_Epoch:26 [003/005 (0460/0755)]\tLoss Ss: 0.088770\n","\tRotated_Epoch:26 [003/005 (0480/0755)]\tLoss Ss: 0.107199\n","\tRotated_Epoch:26 [003/005 (0500/0755)]\tLoss Ss: 0.085151\n","\tRotated_Epoch:26 [003/005 (0520/0755)]\tLoss Ss: 0.095723\n","\tRotated_Epoch:26 [003/005 (0540/0755)]\tLoss Ss: 0.082003\n","\tRotated_Epoch:26 [003/005 (0560/0755)]\tLoss Ss: 0.055958\n","\tRotated_Epoch:26 [003/005 (0580/0755)]\tLoss Ss: 0.086922\n","\tRotated_Epoch:26 [003/005 (0600/0755)]\tLoss Ss: 0.072290\n","\tRotated_Epoch:26 [003/005 (0620/0755)]\tLoss Ss: 0.038173\n","\tRotated_Epoch:26 [003/005 (0640/0755)]\tLoss Ss: 0.067281\n","\tRotated_Epoch:26 [003/005 (0660/0755)]\tLoss Ss: 0.050084\n","\tRotated_Epoch:26 [003/005 (0680/0755)]\tLoss Ss: 0.068403\n","\tRotated_Epoch:26 [003/005 (0700/0755)]\tLoss Ss: 0.058346\n","\tRotated_Epoch:26 [003/005 (0720/0755)]\tLoss Ss: 0.056464\n","\tRotated_Epoch:26 [003/005 (0740/0755)]\tLoss Ss: 0.057057\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:26 [004/005 (0000/0755)]\tLoss Ss: 0.181420\n","\tRotated_Epoch:26 [004/005 (0020/0755)]\tLoss Ss: 0.187158\n","\tRotated_Epoch:26 [004/005 (0040/0755)]\tLoss Ss: 0.139671\n","\tRotated_Epoch:26 [004/005 (0060/0755)]\tLoss Ss: 0.166413\n","\tRotated_Epoch:26 [004/005 (0080/0755)]\tLoss Ss: 0.139111\n","\tRotated_Epoch:26 [004/005 (0100/0755)]\tLoss Ss: 0.101677\n","\tRotated_Epoch:26 [004/005 (0120/0755)]\tLoss Ss: 0.038699\n","\tRotated_Epoch:26 [004/005 (0140/0755)]\tLoss Ss: 0.045351\n","\tRotated_Epoch:26 [004/005 (0160/0755)]\tLoss Ss: 0.051424\n","\tRotated_Epoch:26 [004/005 (0180/0755)]\tLoss Ss: 0.055611\n","\tRotated_Epoch:26 [004/005 (0200/0755)]\tLoss Ss: 0.051564\n","\tRotated_Epoch:26 [004/005 (0220/0755)]\tLoss Ss: 0.041223\n","\tRotated_Epoch:26 [004/005 (0240/0755)]\tLoss Ss: 0.047550\n","\tRotated_Epoch:26 [004/005 (0260/0755)]\tLoss Ss: 0.036122\n","\tRotated_Epoch:26 [004/005 (0280/0755)]\tLoss Ss: 0.035414\n","\tRotated_Epoch:26 [004/005 (0300/0755)]\tLoss Ss: 0.042312\n","\tRotated_Epoch:26 [004/005 (0320/0755)]\tLoss Ss: 0.044503\n","\tRotated_Epoch:26 [004/005 (0340/0755)]\tLoss Ss: 0.033754\n","\tRotated_Epoch:26 [004/005 (0360/0755)]\tLoss Ss: 0.026826\n","\tRotated_Epoch:26 [004/005 (0380/0755)]\tLoss Ss: 0.036418\n","\tRotated_Epoch:26 [004/005 (0400/0755)]\tLoss Ss: 0.029449\n","\tRotated_Epoch:26 [004/005 (0420/0755)]\tLoss Ss: 0.040426\n","\tRotated_Epoch:26 [004/005 (0440/0755)]\tLoss Ss: 0.035337\n","\tRotated_Epoch:26 [004/005 (0460/0755)]\tLoss Ss: 0.039268\n","\tRotated_Epoch:26 [004/005 (0480/0755)]\tLoss Ss: 0.043560\n","\tRotated_Epoch:26 [004/005 (0500/0755)]\tLoss Ss: 0.048411\n","\tRotated_Epoch:26 [004/005 (0520/0755)]\tLoss Ss: 0.029617\n","\tRotated_Epoch:26 [004/005 (0540/0755)]\tLoss Ss: 0.028960\n","\tRotated_Epoch:26 [004/005 (0560/0755)]\tLoss Ss: 0.023168\n","\tRotated_Epoch:26 [004/005 (0580/0755)]\tLoss Ss: 0.021036\n","\tRotated_Epoch:26 [004/005 (0600/0755)]\tLoss Ss: 0.030973\n","\tRotated_Epoch:26 [004/005 (0620/0755)]\tLoss Ss: 0.032948\n","\tRotated_Epoch:26 [004/005 (0640/0755)]\tLoss Ss: 0.028835\n","\tRotated_Epoch:26 [004/005 (0660/0755)]\tLoss Ss: 0.031356\n","\tRotated_Epoch:26 [004/005 (0680/0755)]\tLoss Ss: 0.034077\n","\tRotated_Epoch:26 [004/005 (0700/0755)]\tLoss Ss: 0.023998\n","\tRotated_Epoch:26 [004/005 (0720/0755)]\tLoss Ss: 0.032389\n","\tRotated_Epoch:26 [004/005 (0740/0755)]\tLoss Ss: 0.026260\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tRotated_Epoch:26 [005/005 (0000/0693)]\tLoss Ss: 0.031091\n","\tRotated_Epoch:26 [005/005 (0020/0693)]\tLoss Ss: 0.026916\n","\tRotated_Epoch:26 [005/005 (0040/0693)]\tLoss Ss: 0.022790\n","\tRotated_Epoch:26 [005/005 (0060/0693)]\tLoss Ss: 0.033995\n","\tRotated_Epoch:26 [005/005 (0080/0693)]\tLoss Ss: 0.036888\n","\tRotated_Epoch:26 [005/005 (0100/0693)]\tLoss Ss: 0.038659\n","\tRotated_Epoch:26 [005/005 (0120/0693)]\tLoss Ss: 0.021982\n","\tRotated_Epoch:26 [005/005 (0140/0693)]\tLoss Ss: 0.026697\n","\tRotated_Epoch:26 [005/005 (0160/0693)]\tLoss Ss: 0.024291\n","\tRotated_Epoch:26 [005/005 (0180/0693)]\tLoss Ss: 0.023647\n","\tRotated_Epoch:26 [005/005 (0200/0693)]\tLoss Ss: 0.020925\n","\tRotated_Epoch:26 [005/005 (0220/0693)]\tLoss Ss: 0.018792\n","\tRotated_Epoch:26 [005/005 (0240/0693)]\tLoss Ss: 0.022347\n","\tRotated_Epoch:26 [005/005 (0260/0693)]\tLoss Ss: 0.026026\n","\tRotated_Epoch:26 [005/005 (0280/0693)]\tLoss Ss: 0.022254\n","\tRotated_Epoch:26 [005/005 (0300/0693)]\tLoss Ss: 0.019374\n","\tRotated_Epoch:26 [005/005 (0320/0693)]\tLoss Ss: 0.024634\n","\tRotated_Epoch:26 [005/005 (0340/0693)]\tLoss Ss: 0.015615\n","\tRotated_Epoch:26 [005/005 (0360/0693)]\tLoss Ss: 0.022298\n","\tRotated_Epoch:26 [005/005 (0380/0693)]\tLoss Ss: 0.020594\n","\tRotated_Epoch:26 [005/005 (0400/0693)]\tLoss Ss: 0.027137\n","\tRotated_Epoch:26 [005/005 (0420/0693)]\tLoss Ss: 0.016438\n","\tRotated_Epoch:26 [005/005 (0440/0693)]\tLoss Ss: 0.017550\n","\tRotated_Epoch:26 [005/005 (0460/0693)]\tLoss Ss: 0.018900\n","\tRotated_Epoch:26 [005/005 (0480/0693)]\tLoss Ss: 0.020252\n","\tRotated_Epoch:26 [005/005 (0500/0693)]\tLoss Ss: 0.018198\n","\tRotated_Epoch:26 [005/005 (0520/0693)]\tLoss Ss: 0.013619\n","\tRotated_Epoch:26 [005/005 (0540/0693)]\tLoss Ss: 0.024401\n","\tRotated_Epoch:26 [005/005 (0560/0693)]\tLoss Ss: 0.017137\n","\tRotated_Epoch:26 [005/005 (0580/0693)]\tLoss Ss: 0.022395\n","\tRotated_Epoch:26 [005/005 (0600/0693)]\tLoss Ss: 0.016514\n","\tRotated_Epoch:26 [005/005 (0620/0693)]\tLoss Ss: 0.016773\n","\tRotated_Epoch:26 [005/005 (0640/0693)]\tLoss Ss: 0.021224\n","\tRotated_Epoch:26 [005/005 (0660/0693)]\tLoss Ss: 0.016257\n","\tRotated_Epoch:26 [005/005 (0680/0693)]\tLoss Ss: 0.017310\n","new_1_6_d.nii\n","new_1_6_m.nii\n","new_1_7_d.nii\n","new_1_7_m.nii\n","new_2_5_d.nii\n","new_2_5_m.nii\n","new_2_6_d.nii\n","new_2_6_m.nii\n","new_2_7_d.nii\n","new_2_7_m.nii\n","new_3_d (2).nii\n","new_3_m (2).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (3).nii\n","new_3_m (3).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (7).nii\n","new_3_m (7).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (8).nii\n","new_3_m (8).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (9).nii\n","new_3_m (9).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_4_10_d_163526.nii\n","new_4_10_m_163526.nii.gz\n","new_4_11_d_084336.nii\n","new_4_11_m_084336.nii.gz\n","new_4_7_d_184422.nii\n","new_4_7_m_184422.nii.gz\n","new_4_8_d_143412.nii\n","new_4_8_m_143412.nii.gz\n","new_4_9_d_155347.nii\n","new_4_9_m_155347.nii.gz\n","\tEpoch: 26; Dice: 0.9610 +/- 0.0053; Loss: 13.1646\n","Begin Epoch 27\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tEpoch:27 [000/005 (0000/0588)]\tLoss Ss: 0.023806\n","\tEpoch:27 [000/005 (0020/0588)]\tLoss Ss: 0.023016\n","\tEpoch:27 [000/005 (0040/0588)]\tLoss Ss: 0.022896\n","\tEpoch:27 [000/005 (0060/0588)]\tLoss Ss: 0.015081\n","\tEpoch:27 [000/005 (0080/0588)]\tLoss Ss: 0.011715\n","\tEpoch:27 [000/005 (0100/0588)]\tLoss Ss: 0.007578\n","\tEpoch:27 [000/005 (0120/0588)]\tLoss Ss: 0.008367\n","\tEpoch:27 [000/005 (0140/0588)]\tLoss Ss: 0.008443\n","\tEpoch:27 [000/005 (0160/0588)]\tLoss Ss: 0.011519\n","\tEpoch:27 [000/005 (0180/0588)]\tLoss Ss: 0.009328\n","\tEpoch:27 [000/005 (0200/0588)]\tLoss Ss: 0.012306\n","\tEpoch:27 [000/005 (0220/0588)]\tLoss Ss: 0.013837\n","\tEpoch:27 [000/005 (0240/0588)]\tLoss Ss: 0.009857\n","\tEpoch:27 [000/005 (0260/0588)]\tLoss Ss: 0.009158\n","\tEpoch:27 [000/005 (0280/0588)]\tLoss Ss: 0.008463\n","\tEpoch:27 [000/005 (0300/0588)]\tLoss Ss: 0.006309\n","\tEpoch:27 [000/005 (0320/0588)]\tLoss Ss: 0.009386\n","\tEpoch:27 [000/005 (0340/0588)]\tLoss Ss: 0.008839\n","\tEpoch:27 [000/005 (0360/0588)]\tLoss Ss: 0.007754\n","\tEpoch:27 [000/005 (0380/0588)]\tLoss Ss: 0.006882\n","\tEpoch:27 [000/005 (0400/0588)]\tLoss Ss: 0.007531\n","\tEpoch:27 [000/005 (0420/0588)]\tLoss Ss: 0.010398\n","\tEpoch:27 [000/005 (0440/0588)]\tLoss Ss: 0.004559\n","\tEpoch:27 [000/005 (0460/0588)]\tLoss Ss: 0.007594\n","\tEpoch:27 [000/005 (0480/0588)]\tLoss Ss: 0.011709\n","\tEpoch:27 [000/005 (0500/0588)]\tLoss Ss: 0.006817\n","\tEpoch:27 [000/005 (0520/0588)]\tLoss Ss: 0.006641\n","\tEpoch:27 [000/005 (0540/0588)]\tLoss Ss: 0.005419\n","\tEpoch:27 [000/005 (0560/0588)]\tLoss Ss: 0.008771\n","\tEpoch:27 [000/005 (0580/0588)]\tLoss Ss: 0.006058\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:27 [001/005 (0000/0755)]\tLoss Ss: 0.024937\n","\tEpoch:27 [001/005 (0020/0755)]\tLoss Ss: 0.020174\n","\tEpoch:27 [001/005 (0040/0755)]\tLoss Ss: 0.028477\n","\tEpoch:27 [001/005 (0060/0755)]\tLoss Ss: 0.019464\n","\tEpoch:27 [001/005 (0080/0755)]\tLoss Ss: 0.025856\n","\tEpoch:27 [001/005 (0100/0755)]\tLoss Ss: 0.014843\n","\tEpoch:27 [001/005 (0120/0755)]\tLoss Ss: 0.018628\n","\tEpoch:27 [001/005 (0140/0755)]\tLoss Ss: 0.020658\n","\tEpoch:27 [001/005 (0160/0755)]\tLoss Ss: 0.019851\n","\tEpoch:27 [001/005 (0180/0755)]\tLoss Ss: 0.027606\n","\tEpoch:27 [001/005 (0200/0755)]\tLoss Ss: 0.017619\n","\tEpoch:27 [001/005 (0220/0755)]\tLoss Ss: 0.016880\n","\tEpoch:27 [001/005 (0240/0755)]\tLoss Ss: 0.015892\n","\tEpoch:27 [001/005 (0260/0755)]\tLoss Ss: 0.028564\n","\tEpoch:27 [001/005 (0280/0755)]\tLoss Ss: 0.019868\n","\tEpoch:27 [001/005 (0300/0755)]\tLoss Ss: 0.011576\n","\tEpoch:27 [001/005 (0320/0755)]\tLoss Ss: 0.015702\n","\tEpoch:27 [001/005 (0340/0755)]\tLoss Ss: 0.018408\n","\tEpoch:27 [001/005 (0360/0755)]\tLoss Ss: 0.011812\n","\tEpoch:27 [001/005 (0380/0755)]\tLoss Ss: 0.019642\n","\tEpoch:27 [001/005 (0400/0755)]\tLoss Ss: 0.012875\n","\tEpoch:27 [001/005 (0420/0755)]\tLoss Ss: 0.011860\n","\tEpoch:27 [001/005 (0440/0755)]\tLoss Ss: 0.014146\n","\tEpoch:27 [001/005 (0460/0755)]\tLoss Ss: 0.017202\n","\tEpoch:27 [001/005 (0480/0755)]\tLoss Ss: 0.014348\n","\tEpoch:27 [001/005 (0500/0755)]\tLoss Ss: 0.015506\n","\tEpoch:27 [001/005 (0520/0755)]\tLoss Ss: 0.013390\n","\tEpoch:27 [001/005 (0540/0755)]\tLoss Ss: 0.014676\n","\tEpoch:27 [001/005 (0560/0755)]\tLoss Ss: 0.011933\n","\tEpoch:27 [001/005 (0580/0755)]\tLoss Ss: 0.014758\n","\tEpoch:27 [001/005 (0600/0755)]\tLoss Ss: 0.018082\n","\tEpoch:27 [001/005 (0620/0755)]\tLoss Ss: 0.013924\n","\tEpoch:27 [001/005 (0640/0755)]\tLoss Ss: 0.014437\n","\tEpoch:27 [001/005 (0660/0755)]\tLoss Ss: 0.014731\n","\tEpoch:27 [001/005 (0680/0755)]\tLoss Ss: 0.015350\n","\tEpoch:27 [001/005 (0700/0755)]\tLoss Ss: 0.014464\n","\tEpoch:27 [001/005 (0720/0755)]\tLoss Ss: 0.012921\n","\tEpoch:27 [001/005 (0740/0755)]\tLoss Ss: 0.015197\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tEpoch:27 [002/005 (0000/0614)]\tLoss Ss: 0.007083\n","\tEpoch:27 [002/005 (0020/0614)]\tLoss Ss: 0.009050\n","\tEpoch:27 [002/005 (0040/0614)]\tLoss Ss: 0.012573\n","\tEpoch:27 [002/005 (0060/0614)]\tLoss Ss: 0.009353\n","\tEpoch:27 [002/005 (0080/0614)]\tLoss Ss: 0.006772\n","\tEpoch:27 [002/005 (0100/0614)]\tLoss Ss: 0.007015\n","\tEpoch:27 [002/005 (0120/0614)]\tLoss Ss: 0.007802\n","\tEpoch:27 [002/005 (0140/0614)]\tLoss Ss: 0.007372\n","\tEpoch:27 [002/005 (0160/0614)]\tLoss Ss: 0.007369\n","\tEpoch:27 [002/005 (0180/0614)]\tLoss Ss: 0.007948\n","\tEpoch:27 [002/005 (0200/0614)]\tLoss Ss: 0.008963\n","\tEpoch:27 [002/005 (0220/0614)]\tLoss Ss: 0.008546\n","\tEpoch:27 [002/005 (0240/0614)]\tLoss Ss: 0.006063\n","\tEpoch:27 [002/005 (0260/0614)]\tLoss Ss: 0.005963\n","\tEpoch:27 [002/005 (0280/0614)]\tLoss Ss: 0.005847\n","\tEpoch:27 [002/005 (0300/0614)]\tLoss Ss: 0.006189\n","\tEpoch:27 [002/005 (0320/0614)]\tLoss Ss: 0.007898\n","\tEpoch:27 [002/005 (0340/0614)]\tLoss Ss: 0.005357\n","\tEpoch:27 [002/005 (0360/0614)]\tLoss Ss: 0.005989\n","\tEpoch:27 [002/005 (0380/0614)]\tLoss Ss: 0.007819\n","\tEpoch:27 [002/005 (0400/0614)]\tLoss Ss: 0.008138\n","\tEpoch:27 [002/005 (0420/0614)]\tLoss Ss: 0.006316\n","\tEpoch:27 [002/005 (0440/0614)]\tLoss Ss: 0.003976\n","\tEpoch:27 [002/005 (0460/0614)]\tLoss Ss: 0.003977\n","\tEpoch:27 [002/005 (0480/0614)]\tLoss Ss: 0.005234\n","\tEpoch:27 [002/005 (0500/0614)]\tLoss Ss: 0.008901\n","\tEpoch:27 [002/005 (0520/0614)]\tLoss Ss: 0.006375\n","\tEpoch:27 [002/005 (0540/0614)]\tLoss Ss: 0.009028\n","\tEpoch:27 [002/005 (0560/0614)]\tLoss Ss: 0.005511\n","\tEpoch:27 [002/005 (0580/0614)]\tLoss Ss: 0.006103\n","\tEpoch:27 [002/005 (0600/0614)]\tLoss Ss: 0.005742\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tEpoch:27 [003/005 (0000/0693)]\tLoss Ss: 0.013634\n","\tEpoch:27 [003/005 (0020/0693)]\tLoss Ss: 0.010619\n","\tEpoch:27 [003/005 (0040/0693)]\tLoss Ss: 0.037748\n","\tEpoch:27 [003/005 (0060/0693)]\tLoss Ss: 0.013493\n","\tEpoch:27 [003/005 (0080/0693)]\tLoss Ss: 0.010760\n","\tEpoch:27 [003/005 (0100/0693)]\tLoss Ss: 0.010362\n","\tEpoch:27 [003/005 (0120/0693)]\tLoss Ss: 0.013441\n","\tEpoch:27 [003/005 (0140/0693)]\tLoss Ss: 0.020646\n","\tEpoch:27 [003/005 (0160/0693)]\tLoss Ss: 0.013197\n","\tEpoch:27 [003/005 (0180/0693)]\tLoss Ss: 0.017508\n","\tEpoch:27 [003/005 (0200/0693)]\tLoss Ss: 0.015156\n","\tEpoch:27 [003/005 (0220/0693)]\tLoss Ss: 0.017116\n","\tEpoch:27 [003/005 (0240/0693)]\tLoss Ss: 0.007269\n","\tEpoch:27 [003/005 (0260/0693)]\tLoss Ss: 0.016615\n","\tEpoch:27 [003/005 (0280/0693)]\tLoss Ss: 0.015965\n","\tEpoch:27 [003/005 (0300/0693)]\tLoss Ss: 0.016130\n","\tEpoch:27 [003/005 (0320/0693)]\tLoss Ss: 0.010955\n","\tEpoch:27 [003/005 (0340/0693)]\tLoss Ss: 0.018847\n","\tEpoch:27 [003/005 (0360/0693)]\tLoss Ss: 0.020472\n","\tEpoch:27 [003/005 (0380/0693)]\tLoss Ss: 0.014484\n","\tEpoch:27 [003/005 (0400/0693)]\tLoss Ss: 0.016977\n","\tEpoch:27 [003/005 (0420/0693)]\tLoss Ss: 0.013157\n","\tEpoch:27 [003/005 (0440/0693)]\tLoss Ss: 0.015600\n","\tEpoch:27 [003/005 (0460/0693)]\tLoss Ss: 0.016994\n","\tEpoch:27 [003/005 (0480/0693)]\tLoss Ss: 0.015171\n","\tEpoch:27 [003/005 (0500/0693)]\tLoss Ss: 0.009007\n","\tEpoch:27 [003/005 (0520/0693)]\tLoss Ss: 0.012866\n","\tEpoch:27 [003/005 (0540/0693)]\tLoss Ss: 0.020591\n","\tEpoch:27 [003/005 (0560/0693)]\tLoss Ss: 0.011435\n","\tEpoch:27 [003/005 (0580/0693)]\tLoss Ss: 0.012368\n","\tEpoch:27 [003/005 (0600/0693)]\tLoss Ss: 0.012706\n","\tEpoch:27 [003/005 (0620/0693)]\tLoss Ss: 0.013531\n","\tEpoch:27 [003/005 (0640/0693)]\tLoss Ss: 0.018112\n","\tEpoch:27 [003/005 (0660/0693)]\tLoss Ss: 0.015303\n","\tEpoch:27 [003/005 (0680/0693)]\tLoss Ss: 0.018864\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tEpoch:27 [004/005 (0000/0693)]\tLoss Ss: 0.020145\n","\tEpoch:27 [004/005 (0020/0693)]\tLoss Ss: 0.013228\n","\tEpoch:27 [004/005 (0040/0693)]\tLoss Ss: 0.015469\n","\tEpoch:27 [004/005 (0060/0693)]\tLoss Ss: 0.012358\n","\tEpoch:27 [004/005 (0080/0693)]\tLoss Ss: 0.012779\n","\tEpoch:27 [004/005 (0100/0693)]\tLoss Ss: 0.016954\n","\tEpoch:27 [004/005 (0120/0693)]\tLoss Ss: 0.017615\n","\tEpoch:27 [004/005 (0140/0693)]\tLoss Ss: 0.011976\n","\tEpoch:27 [004/005 (0160/0693)]\tLoss Ss: 0.016103\n","\tEpoch:27 [004/005 (0180/0693)]\tLoss Ss: 0.012533\n","\tEpoch:27 [004/005 (0200/0693)]\tLoss Ss: 0.013800\n","\tEpoch:27 [004/005 (0220/0693)]\tLoss Ss: 0.014608\n","\tEpoch:27 [004/005 (0240/0693)]\tLoss Ss: 0.013772\n","\tEpoch:27 [004/005 (0260/0693)]\tLoss Ss: 0.013079\n","\tEpoch:27 [004/005 (0280/0693)]\tLoss Ss: 0.011421\n","\tEpoch:27 [004/005 (0300/0693)]\tLoss Ss: 0.012729\n","\tEpoch:27 [004/005 (0320/0693)]\tLoss Ss: 0.013707\n","\tEpoch:27 [004/005 (0340/0693)]\tLoss Ss: 0.006186\n","\tEpoch:27 [004/005 (0360/0693)]\tLoss Ss: 0.013180\n","\tEpoch:27 [004/005 (0380/0693)]\tLoss Ss: 0.010932\n","\tEpoch:27 [004/005 (0400/0693)]\tLoss Ss: 0.013837\n","\tEpoch:27 [004/005 (0420/0693)]\tLoss Ss: 0.012098\n","\tEpoch:27 [004/005 (0440/0693)]\tLoss Ss: 0.011280\n","\tEpoch:27 [004/005 (0460/0693)]\tLoss Ss: 0.016271\n","\tEpoch:27 [004/005 (0480/0693)]\tLoss Ss: 0.010843\n","\tEpoch:27 [004/005 (0500/0693)]\tLoss Ss: 0.020523\n","\tEpoch:27 [004/005 (0520/0693)]\tLoss Ss: 0.015517\n","\tEpoch:27 [004/005 (0540/0693)]\tLoss Ss: 0.014086\n","\tEpoch:27 [004/005 (0560/0693)]\tLoss Ss: 0.010417\n","\tEpoch:27 [004/005 (0580/0693)]\tLoss Ss: 0.008968\n","\tEpoch:27 [004/005 (0600/0693)]\tLoss Ss: 0.010506\n","\tEpoch:27 [004/005 (0620/0693)]\tLoss Ss: 0.008344\n","\tEpoch:27 [004/005 (0640/0693)]\tLoss Ss: 0.012105\n","\tEpoch:27 [004/005 (0660/0693)]\tLoss Ss: 0.011437\n","\tEpoch:27 [004/005 (0680/0693)]\tLoss Ss: 0.007467\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:27 [005/005 (0000/0755)]\tLoss Ss: 0.024619\n","\tEpoch:27 [005/005 (0020/0755)]\tLoss Ss: 0.024955\n","\tEpoch:27 [005/005 (0040/0755)]\tLoss Ss: 0.022341\n","\tEpoch:27 [005/005 (0060/0755)]\tLoss Ss: 0.019604\n","\tEpoch:27 [005/005 (0080/0755)]\tLoss Ss: 0.015451\n","\tEpoch:27 [005/005 (0100/0755)]\tLoss Ss: 0.021028\n","\tEpoch:27 [005/005 (0120/0755)]\tLoss Ss: 0.018634\n","\tEpoch:27 [005/005 (0140/0755)]\tLoss Ss: 0.021488\n","\tEpoch:27 [005/005 (0160/0755)]\tLoss Ss: 0.023128\n","\tEpoch:27 [005/005 (0180/0755)]\tLoss Ss: 0.017636\n","\tEpoch:27 [005/005 (0200/0755)]\tLoss Ss: 0.015567\n","\tEpoch:27 [005/005 (0220/0755)]\tLoss Ss: 0.015623\n","\tEpoch:27 [005/005 (0240/0755)]\tLoss Ss: 0.018379\n","\tEpoch:27 [005/005 (0260/0755)]\tLoss Ss: 0.013287\n","\tEpoch:27 [005/005 (0280/0755)]\tLoss Ss: 0.019175\n","\tEpoch:27 [005/005 (0300/0755)]\tLoss Ss: 0.013112\n","\tEpoch:27 [005/005 (0320/0755)]\tLoss Ss: 0.023543\n","\tEpoch:27 [005/005 (0340/0755)]\tLoss Ss: 0.015622\n","\tEpoch:27 [005/005 (0360/0755)]\tLoss Ss: 0.030247\n","\tEpoch:27 [005/005 (0380/0755)]\tLoss Ss: 0.014246\n","\tEpoch:27 [005/005 (0400/0755)]\tLoss Ss: 0.016916\n","\tEpoch:27 [005/005 (0420/0755)]\tLoss Ss: 0.015071\n","\tEpoch:27 [005/005 (0440/0755)]\tLoss Ss: 0.013737\n","\tEpoch:27 [005/005 (0460/0755)]\tLoss Ss: 0.018154\n","\tEpoch:27 [005/005 (0480/0755)]\tLoss Ss: 0.015812\n","\tEpoch:27 [005/005 (0500/0755)]\tLoss Ss: 0.015297\n","\tEpoch:27 [005/005 (0520/0755)]\tLoss Ss: 0.011353\n","\tEpoch:27 [005/005 (0540/0755)]\tLoss Ss: 0.017797\n","\tEpoch:27 [005/005 (0560/0755)]\tLoss Ss: 0.013143\n","\tEpoch:27 [005/005 (0580/0755)]\tLoss Ss: 0.014687\n","\tEpoch:27 [005/005 (0600/0755)]\tLoss Ss: 0.015025\n","\tEpoch:27 [005/005 (0620/0755)]\tLoss Ss: 0.016345\n","\tEpoch:27 [005/005 (0640/0755)]\tLoss Ss: 0.021213\n","\tEpoch:27 [005/005 (0660/0755)]\tLoss Ss: 0.013220\n","\tEpoch:27 [005/005 (0680/0755)]\tLoss Ss: 0.013387\n","\tEpoch:27 [005/005 (0700/0755)]\tLoss Ss: 0.016726\n","\tEpoch:27 [005/005 (0720/0755)]\tLoss Ss: 0.012903\n","\tEpoch:27 [005/005 (0740/0755)]\tLoss Ss: 0.009303\n","Now train the rotated image\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tRotated_Epoch:27 [000/005 (0000/0693)]\tLoss Ss: 0.024953\n","\tRotated_Epoch:27 [000/005 (0020/0693)]\tLoss Ss: 0.020580\n","\tRotated_Epoch:27 [000/005 (0040/0693)]\tLoss Ss: 0.019041\n","\tRotated_Epoch:27 [000/005 (0060/0693)]\tLoss Ss: 0.031259\n","\tRotated_Epoch:27 [000/005 (0080/0693)]\tLoss Ss: 0.026269\n","\tRotated_Epoch:27 [000/005 (0100/0693)]\tLoss Ss: 0.026379\n","\tRotated_Epoch:27 [000/005 (0120/0693)]\tLoss Ss: 0.013841\n","\tRotated_Epoch:27 [000/005 (0140/0693)]\tLoss Ss: 0.028198\n","\tRotated_Epoch:27 [000/005 (0160/0693)]\tLoss Ss: 0.015799\n","\tRotated_Epoch:27 [000/005 (0180/0693)]\tLoss Ss: 0.020649\n","\tRotated_Epoch:27 [000/005 (0200/0693)]\tLoss Ss: 0.014636\n","\tRotated_Epoch:27 [000/005 (0220/0693)]\tLoss Ss: 0.021583\n","\tRotated_Epoch:27 [000/005 (0240/0693)]\tLoss Ss: 0.017555\n","\tRotated_Epoch:27 [000/005 (0260/0693)]\tLoss Ss: 0.017304\n","\tRotated_Epoch:27 [000/005 (0280/0693)]\tLoss Ss: 0.018959\n","\tRotated_Epoch:27 [000/005 (0300/0693)]\tLoss Ss: 0.010557\n","\tRotated_Epoch:27 [000/005 (0320/0693)]\tLoss Ss: 0.012799\n","\tRotated_Epoch:27 [000/005 (0340/0693)]\tLoss Ss: 0.010626\n","\tRotated_Epoch:27 [000/005 (0360/0693)]\tLoss Ss: 0.016375\n","\tRotated_Epoch:27 [000/005 (0380/0693)]\tLoss Ss: 0.018294\n","\tRotated_Epoch:27 [000/005 (0400/0693)]\tLoss Ss: 0.019771\n","\tRotated_Epoch:27 [000/005 (0420/0693)]\tLoss Ss: 0.014394\n","\tRotated_Epoch:27 [000/005 (0440/0693)]\tLoss Ss: 0.018236\n","\tRotated_Epoch:27 [000/005 (0460/0693)]\tLoss Ss: 0.013010\n","\tRotated_Epoch:27 [000/005 (0480/0693)]\tLoss Ss: 0.017592\n","\tRotated_Epoch:27 [000/005 (0500/0693)]\tLoss Ss: 0.019048\n","\tRotated_Epoch:27 [000/005 (0520/0693)]\tLoss Ss: 0.015503\n","\tRotated_Epoch:27 [000/005 (0540/0693)]\tLoss Ss: 0.013608\n","\tRotated_Epoch:27 [000/005 (0560/0693)]\tLoss Ss: 0.014202\n","\tRotated_Epoch:27 [000/005 (0580/0693)]\tLoss Ss: 0.015086\n","\tRotated_Epoch:27 [000/005 (0600/0693)]\tLoss Ss: 0.012551\n","\tRotated_Epoch:27 [000/005 (0620/0693)]\tLoss Ss: 0.011985\n","\tRotated_Epoch:27 [000/005 (0640/0693)]\tLoss Ss: 0.017872\n","\tRotated_Epoch:27 [000/005 (0660/0693)]\tLoss Ss: 0.018289\n","\tRotated_Epoch:27 [000/005 (0680/0693)]\tLoss Ss: 0.013681\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tRotated_Epoch:27 [001/005 (0000/0614)]\tLoss Ss: 0.008014\n","\tRotated_Epoch:27 [001/005 (0020/0614)]\tLoss Ss: 0.011705\n","\tRotated_Epoch:27 [001/005 (0040/0614)]\tLoss Ss: 0.007536\n","\tRotated_Epoch:27 [001/005 (0060/0614)]\tLoss Ss: 0.006129\n","\tRotated_Epoch:27 [001/005 (0080/0614)]\tLoss Ss: 0.008534\n","\tRotated_Epoch:27 [001/005 (0100/0614)]\tLoss Ss: 0.009150\n","\tRotated_Epoch:27 [001/005 (0120/0614)]\tLoss Ss: 0.007188\n","\tRotated_Epoch:27 [001/005 (0140/0614)]\tLoss Ss: 0.005686\n","\tRotated_Epoch:27 [001/005 (0160/0614)]\tLoss Ss: 0.012257\n","\tRotated_Epoch:27 [001/005 (0180/0614)]\tLoss Ss: 0.008731\n","\tRotated_Epoch:27 [001/005 (0200/0614)]\tLoss Ss: 0.007077\n","\tRotated_Epoch:27 [001/005 (0220/0614)]\tLoss Ss: 0.008901\n","\tRotated_Epoch:27 [001/005 (0240/0614)]\tLoss Ss: 0.012244\n","\tRotated_Epoch:27 [001/005 (0260/0614)]\tLoss Ss: 0.009478\n","\tRotated_Epoch:27 [001/005 (0280/0614)]\tLoss Ss: 0.007803\n","\tRotated_Epoch:27 [001/005 (0300/0614)]\tLoss Ss: 0.004281\n","\tRotated_Epoch:27 [001/005 (0320/0614)]\tLoss Ss: 0.004659\n","\tRotated_Epoch:27 [001/005 (0340/0614)]\tLoss Ss: 0.005305\n","\tRotated_Epoch:27 [001/005 (0360/0614)]\tLoss Ss: 0.004827\n","\tRotated_Epoch:27 [001/005 (0380/0614)]\tLoss Ss: 0.007089\n","\tRotated_Epoch:27 [001/005 (0400/0614)]\tLoss Ss: 0.006097\n","\tRotated_Epoch:27 [001/005 (0420/0614)]\tLoss Ss: 0.007662\n","\tRotated_Epoch:27 [001/005 (0440/0614)]\tLoss Ss: 0.005599\n","\tRotated_Epoch:27 [001/005 (0460/0614)]\tLoss Ss: 0.007888\n","\tRotated_Epoch:27 [001/005 (0480/0614)]\tLoss Ss: 0.005924\n","\tRotated_Epoch:27 [001/005 (0500/0614)]\tLoss Ss: 0.006281\n","\tRotated_Epoch:27 [001/005 (0520/0614)]\tLoss Ss: 0.006109\n","\tRotated_Epoch:27 [001/005 (0540/0614)]\tLoss Ss: 0.008018\n","\tRotated_Epoch:27 [001/005 (0560/0614)]\tLoss Ss: 0.007328\n","\tRotated_Epoch:27 [001/005 (0580/0614)]\tLoss Ss: 0.004672\n","\tRotated_Epoch:27 [001/005 (0600/0614)]\tLoss Ss: 0.007132\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tRotated_Epoch:27 [002/005 (0000/0693)]\tLoss Ss: 0.010337\n","\tRotated_Epoch:27 [002/005 (0020/0693)]\tLoss Ss: 0.012676\n","\tRotated_Epoch:27 [002/005 (0040/0693)]\tLoss Ss: 0.013885\n","\tRotated_Epoch:27 [002/005 (0060/0693)]\tLoss Ss: 0.010357\n","\tRotated_Epoch:27 [002/005 (0080/0693)]\tLoss Ss: 0.015932\n","\tRotated_Epoch:27 [002/005 (0100/0693)]\tLoss Ss: 0.015191\n","\tRotated_Epoch:27 [002/005 (0120/0693)]\tLoss Ss: 0.012062\n","\tRotated_Epoch:27 [002/005 (0140/0693)]\tLoss Ss: 0.015045\n","\tRotated_Epoch:27 [002/005 (0160/0693)]\tLoss Ss: 0.017319\n","\tRotated_Epoch:27 [002/005 (0180/0693)]\tLoss Ss: 0.011360\n","\tRotated_Epoch:27 [002/005 (0200/0693)]\tLoss Ss: 0.014508\n","\tRotated_Epoch:27 [002/005 (0220/0693)]\tLoss Ss: 0.016234\n","\tRotated_Epoch:27 [002/005 (0240/0693)]\tLoss Ss: 0.013516\n","\tRotated_Epoch:27 [002/005 (0260/0693)]\tLoss Ss: 0.011781\n","\tRotated_Epoch:27 [002/005 (0280/0693)]\tLoss Ss: 0.012315\n","\tRotated_Epoch:27 [002/005 (0300/0693)]\tLoss Ss: 0.012235\n","\tRotated_Epoch:27 [002/005 (0320/0693)]\tLoss Ss: 0.016947\n","\tRotated_Epoch:27 [002/005 (0340/0693)]\tLoss Ss: 0.012750\n","\tRotated_Epoch:27 [002/005 (0360/0693)]\tLoss Ss: 0.018993\n","\tRotated_Epoch:27 [002/005 (0380/0693)]\tLoss Ss: 0.010385\n","\tRotated_Epoch:27 [002/005 (0400/0693)]\tLoss Ss: 0.016385\n","\tRotated_Epoch:27 [002/005 (0420/0693)]\tLoss Ss: 0.014764\n","\tRotated_Epoch:27 [002/005 (0440/0693)]\tLoss Ss: 0.010733\n","\tRotated_Epoch:27 [002/005 (0460/0693)]\tLoss Ss: 0.016717\n","\tRotated_Epoch:27 [002/005 (0480/0693)]\tLoss Ss: 0.015453\n","\tRotated_Epoch:27 [002/005 (0500/0693)]\tLoss Ss: 0.010676\n","\tRotated_Epoch:27 [002/005 (0520/0693)]\tLoss Ss: 0.012973\n","\tRotated_Epoch:27 [002/005 (0540/0693)]\tLoss Ss: 0.014357\n","\tRotated_Epoch:27 [002/005 (0560/0693)]\tLoss Ss: 0.012461\n","\tRotated_Epoch:27 [002/005 (0580/0693)]\tLoss Ss: 0.013778\n","\tRotated_Epoch:27 [002/005 (0600/0693)]\tLoss Ss: 0.016892\n","\tRotated_Epoch:27 [002/005 (0620/0693)]\tLoss Ss: 0.013184\n","\tRotated_Epoch:27 [002/005 (0640/0693)]\tLoss Ss: 0.014727\n","\tRotated_Epoch:27 [002/005 (0660/0693)]\tLoss Ss: 0.012715\n","\tRotated_Epoch:27 [002/005 (0680/0693)]\tLoss Ss: 0.011386\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tRotated_Epoch:27 [003/005 (0000/0588)]\tLoss Ss: 0.150938\n","\tRotated_Epoch:27 [003/005 (0020/0588)]\tLoss Ss: 0.171465\n","\tRotated_Epoch:27 [003/005 (0040/0588)]\tLoss Ss: 0.090824\n","\tRotated_Epoch:27 [003/005 (0060/0588)]\tLoss Ss: 0.099696\n","\tRotated_Epoch:27 [003/005 (0080/0588)]\tLoss Ss: 0.084252\n","\tRotated_Epoch:27 [003/005 (0100/0588)]\tLoss Ss: 0.063964\n","\tRotated_Epoch:27 [003/005 (0120/0588)]\tLoss Ss: 0.090758\n","\tRotated_Epoch:27 [003/005 (0140/0588)]\tLoss Ss: 0.071020\n","\tRotated_Epoch:27 [003/005 (0160/0588)]\tLoss Ss: 0.042196\n","\tRotated_Epoch:27 [003/005 (0180/0588)]\tLoss Ss: 0.044542\n","\tRotated_Epoch:27 [003/005 (0200/0588)]\tLoss Ss: 0.081076\n","\tRotated_Epoch:27 [003/005 (0220/0588)]\tLoss Ss: 0.063611\n","\tRotated_Epoch:27 [003/005 (0240/0588)]\tLoss Ss: 0.057224\n","\tRotated_Epoch:27 [003/005 (0260/0588)]\tLoss Ss: 0.078185\n","\tRotated_Epoch:27 [003/005 (0280/0588)]\tLoss Ss: 0.079826\n","\tRotated_Epoch:27 [003/005 (0300/0588)]\tLoss Ss: 0.063309\n","\tRotated_Epoch:27 [003/005 (0320/0588)]\tLoss Ss: 0.061275\n","\tRotated_Epoch:27 [003/005 (0340/0588)]\tLoss Ss: 0.063044\n","\tRotated_Epoch:27 [003/005 (0360/0588)]\tLoss Ss: 0.052648\n","\tRotated_Epoch:27 [003/005 (0380/0588)]\tLoss Ss: 0.041601\n","\tRotated_Epoch:27 [003/005 (0400/0588)]\tLoss Ss: 0.080106\n","\tRotated_Epoch:27 [003/005 (0420/0588)]\tLoss Ss: 0.064832\n","\tRotated_Epoch:27 [003/005 (0440/0588)]\tLoss Ss: 0.051688\n","\tRotated_Epoch:27 [003/005 (0460/0588)]\tLoss Ss: 0.035166\n","\tRotated_Epoch:27 [003/005 (0480/0588)]\tLoss Ss: 0.039341\n","\tRotated_Epoch:27 [003/005 (0500/0588)]\tLoss Ss: 0.060214\n","\tRotated_Epoch:27 [003/005 (0520/0588)]\tLoss Ss: 0.055410\n","\tRotated_Epoch:27 [003/005 (0540/0588)]\tLoss Ss: 0.050567\n","\tRotated_Epoch:27 [003/005 (0560/0588)]\tLoss Ss: 0.063512\n","\tRotated_Epoch:27 [003/005 (0580/0588)]\tLoss Ss: 0.029281\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:27 [004/005 (0000/0755)]\tLoss Ss: 0.229938\n","\tRotated_Epoch:27 [004/005 (0020/0755)]\tLoss Ss: 0.193147\n","\tRotated_Epoch:27 [004/005 (0040/0755)]\tLoss Ss: 0.173822\n","\tRotated_Epoch:27 [004/005 (0060/0755)]\tLoss Ss: 0.132829\n","\tRotated_Epoch:27 [004/005 (0080/0755)]\tLoss Ss: 0.096464\n","\tRotated_Epoch:27 [004/005 (0100/0755)]\tLoss Ss: 0.084239\n","\tRotated_Epoch:27 [004/005 (0120/0755)]\tLoss Ss: 0.078590\n","\tRotated_Epoch:27 [004/005 (0140/0755)]\tLoss Ss: 0.075111\n","\tRotated_Epoch:27 [004/005 (0160/0755)]\tLoss Ss: 0.074085\n","\tRotated_Epoch:27 [004/005 (0180/0755)]\tLoss Ss: 0.065914\n","\tRotated_Epoch:27 [004/005 (0200/0755)]\tLoss Ss: 0.098705\n","\tRotated_Epoch:27 [004/005 (0220/0755)]\tLoss Ss: 0.080776\n","\tRotated_Epoch:27 [004/005 (0240/0755)]\tLoss Ss: 0.064706\n","\tRotated_Epoch:27 [004/005 (0260/0755)]\tLoss Ss: 0.078260\n","\tRotated_Epoch:27 [004/005 (0280/0755)]\tLoss Ss: 0.065181\n","\tRotated_Epoch:27 [004/005 (0300/0755)]\tLoss Ss: 0.049771\n","\tRotated_Epoch:27 [004/005 (0320/0755)]\tLoss Ss: 0.058026\n","\tRotated_Epoch:27 [004/005 (0340/0755)]\tLoss Ss: 0.049960\n","\tRotated_Epoch:27 [004/005 (0360/0755)]\tLoss Ss: 0.068383\n","\tRotated_Epoch:27 [004/005 (0380/0755)]\tLoss Ss: 0.062555\n","\tRotated_Epoch:27 [004/005 (0400/0755)]\tLoss Ss: 0.058510\n","\tRotated_Epoch:27 [004/005 (0420/0755)]\tLoss Ss: 0.059786\n","\tRotated_Epoch:27 [004/005 (0440/0755)]\tLoss Ss: 0.039442\n","\tRotated_Epoch:27 [004/005 (0460/0755)]\tLoss Ss: 0.056195\n","\tRotated_Epoch:27 [004/005 (0480/0755)]\tLoss Ss: 0.042312\n","\tRotated_Epoch:27 [004/005 (0500/0755)]\tLoss Ss: 0.045999\n","\tRotated_Epoch:27 [004/005 (0520/0755)]\tLoss Ss: 0.069895\n","\tRotated_Epoch:27 [004/005 (0540/0755)]\tLoss Ss: 0.053298\n","\tRotated_Epoch:27 [004/005 (0560/0755)]\tLoss Ss: 0.051396\n","\tRotated_Epoch:27 [004/005 (0580/0755)]\tLoss Ss: 0.044668\n","\tRotated_Epoch:27 [004/005 (0600/0755)]\tLoss Ss: 0.046286\n","\tRotated_Epoch:27 [004/005 (0620/0755)]\tLoss Ss: 0.039281\n","\tRotated_Epoch:27 [004/005 (0640/0755)]\tLoss Ss: 0.040121\n","\tRotated_Epoch:27 [004/005 (0660/0755)]\tLoss Ss: 0.036665\n","\tRotated_Epoch:27 [004/005 (0680/0755)]\tLoss Ss: 0.056576\n","\tRotated_Epoch:27 [004/005 (0700/0755)]\tLoss Ss: 0.034649\n","\tRotated_Epoch:27 [004/005 (0720/0755)]\tLoss Ss: 0.049560\n","\tRotated_Epoch:27 [004/005 (0740/0755)]\tLoss Ss: 0.030483\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:27 [005/005 (0000/0755)]\tLoss Ss: 0.157113\n","\tRotated_Epoch:27 [005/005 (0020/0755)]\tLoss Ss: 0.233210\n","\tRotated_Epoch:27 [005/005 (0040/0755)]\tLoss Ss: 0.097078\n","\tRotated_Epoch:27 [005/005 (0060/0755)]\tLoss Ss: 0.101085\n","\tRotated_Epoch:27 [005/005 (0080/0755)]\tLoss Ss: 0.096494\n","\tRotated_Epoch:27 [005/005 (0100/0755)]\tLoss Ss: 0.058010\n","\tRotated_Epoch:27 [005/005 (0120/0755)]\tLoss Ss: 0.083052\n","\tRotated_Epoch:27 [005/005 (0140/0755)]\tLoss Ss: 0.041373\n","\tRotated_Epoch:27 [005/005 (0160/0755)]\tLoss Ss: 0.051763\n","\tRotated_Epoch:27 [005/005 (0180/0755)]\tLoss Ss: 0.073332\n","\tRotated_Epoch:27 [005/005 (0200/0755)]\tLoss Ss: 0.043806\n","\tRotated_Epoch:27 [005/005 (0220/0755)]\tLoss Ss: 0.023448\n","\tRotated_Epoch:27 [005/005 (0240/0755)]\tLoss Ss: 0.032742\n","\tRotated_Epoch:27 [005/005 (0260/0755)]\tLoss Ss: 0.041307\n","\tRotated_Epoch:27 [005/005 (0280/0755)]\tLoss Ss: 0.032699\n","\tRotated_Epoch:27 [005/005 (0300/0755)]\tLoss Ss: 0.032697\n","\tRotated_Epoch:27 [005/005 (0320/0755)]\tLoss Ss: 0.031465\n","\tRotated_Epoch:27 [005/005 (0340/0755)]\tLoss Ss: 0.035445\n","\tRotated_Epoch:27 [005/005 (0360/0755)]\tLoss Ss: 0.035859\n","\tRotated_Epoch:27 [005/005 (0380/0755)]\tLoss Ss: 0.029462\n","\tRotated_Epoch:27 [005/005 (0400/0755)]\tLoss Ss: 0.034683\n","\tRotated_Epoch:27 [005/005 (0420/0755)]\tLoss Ss: 0.038137\n","\tRotated_Epoch:27 [005/005 (0440/0755)]\tLoss Ss: 0.028405\n","\tRotated_Epoch:27 [005/005 (0460/0755)]\tLoss Ss: 0.025779\n","\tRotated_Epoch:27 [005/005 (0480/0755)]\tLoss Ss: 0.026756\n","\tRotated_Epoch:27 [005/005 (0500/0755)]\tLoss Ss: 0.026982\n","\tRotated_Epoch:27 [005/005 (0520/0755)]\tLoss Ss: 0.022188\n","\tRotated_Epoch:27 [005/005 (0540/0755)]\tLoss Ss: 0.030054\n","\tRotated_Epoch:27 [005/005 (0560/0755)]\tLoss Ss: 0.030190\n","\tRotated_Epoch:27 [005/005 (0580/0755)]\tLoss Ss: 0.025771\n","\tRotated_Epoch:27 [005/005 (0600/0755)]\tLoss Ss: 0.023490\n","\tRotated_Epoch:27 [005/005 (0620/0755)]\tLoss Ss: 0.027129\n","\tRotated_Epoch:27 [005/005 (0640/0755)]\tLoss Ss: 0.021485\n","\tRotated_Epoch:27 [005/005 (0660/0755)]\tLoss Ss: 0.017700\n","\tRotated_Epoch:27 [005/005 (0680/0755)]\tLoss Ss: 0.024104\n","\tRotated_Epoch:27 [005/005 (0700/0755)]\tLoss Ss: 0.024647\n","\tRotated_Epoch:27 [005/005 (0720/0755)]\tLoss Ss: 0.022145\n","\tRotated_Epoch:27 [005/005 (0740/0755)]\tLoss Ss: 0.030969\n","new_1_6_d.nii\n","new_1_6_m.nii\n","new_1_7_d.nii\n","new_1_7_m.nii\n","new_2_5_d.nii\n","new_2_5_m.nii\n","new_2_6_d.nii\n","new_2_6_m.nii\n","new_2_7_d.nii\n","new_2_7_m.nii\n","new_3_d (2).nii\n","new_3_m (2).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (3).nii\n","new_3_m (3).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (7).nii\n","new_3_m (7).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (8).nii\n","new_3_m (8).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (9).nii\n","new_3_m (9).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_4_10_d_163526.nii\n","new_4_10_m_163526.nii.gz\n","new_4_11_d_084336.nii\n","new_4_11_m_084336.nii.gz\n","new_4_7_d_184422.nii\n","new_4_7_m_184422.nii.gz\n","new_4_8_d_143412.nii\n","new_4_8_m_143412.nii.gz\n","new_4_9_d_155347.nii\n","new_4_9_m_155347.nii.gz\n","\tEpoch: 27; Dice: 0.9518 +/- 0.0127; Loss: 10.7977\n","Begin Epoch 28\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tEpoch:28 [000/005 (0000/0693)]\tLoss Ss: 0.019597\n","\tEpoch:28 [000/005 (0020/0693)]\tLoss Ss: 0.014061\n","\tEpoch:28 [000/005 (0040/0693)]\tLoss Ss: 0.020678\n","\tEpoch:28 [000/005 (0060/0693)]\tLoss Ss: 0.014931\n","\tEpoch:28 [000/005 (0080/0693)]\tLoss Ss: 0.015436\n","\tEpoch:28 [000/005 (0100/0693)]\tLoss Ss: 0.024674\n","\tEpoch:28 [000/005 (0120/0693)]\tLoss Ss: 0.015540\n","\tEpoch:28 [000/005 (0140/0693)]\tLoss Ss: 0.018409\n","\tEpoch:28 [000/005 (0160/0693)]\tLoss Ss: 0.018792\n","\tEpoch:28 [000/005 (0180/0693)]\tLoss Ss: 0.011931\n","\tEpoch:28 [000/005 (0200/0693)]\tLoss Ss: 0.019365\n","\tEpoch:28 [000/005 (0220/0693)]\tLoss Ss: 0.016984\n","\tEpoch:28 [000/005 (0240/0693)]\tLoss Ss: 0.016920\n","\tEpoch:28 [000/005 (0260/0693)]\tLoss Ss: 0.015280\n","\tEpoch:28 [000/005 (0280/0693)]\tLoss Ss: 0.014121\n","\tEpoch:28 [000/005 (0300/0693)]\tLoss Ss: 0.019444\n","\tEpoch:28 [000/005 (0320/0693)]\tLoss Ss: 0.009083\n","\tEpoch:28 [000/005 (0340/0693)]\tLoss Ss: 0.010697\n","\tEpoch:28 [000/005 (0360/0693)]\tLoss Ss: 0.014703\n","\tEpoch:28 [000/005 (0380/0693)]\tLoss Ss: 0.012937\n","\tEpoch:28 [000/005 (0400/0693)]\tLoss Ss: 0.012951\n","\tEpoch:28 [000/005 (0420/0693)]\tLoss Ss: 0.014305\n","\tEpoch:28 [000/005 (0440/0693)]\tLoss Ss: 0.012740\n","\tEpoch:28 [000/005 (0460/0693)]\tLoss Ss: 0.012301\n","\tEpoch:28 [000/005 (0480/0693)]\tLoss Ss: 0.014652\n","\tEpoch:28 [000/005 (0500/0693)]\tLoss Ss: 0.013098\n","\tEpoch:28 [000/005 (0520/0693)]\tLoss Ss: 0.011432\n","\tEpoch:28 [000/005 (0540/0693)]\tLoss Ss: 0.015319\n","\tEpoch:28 [000/005 (0560/0693)]\tLoss Ss: 0.013110\n","\tEpoch:28 [000/005 (0580/0693)]\tLoss Ss: 0.010316\n","\tEpoch:28 [000/005 (0600/0693)]\tLoss Ss: 0.013476\n","\tEpoch:28 [000/005 (0620/0693)]\tLoss Ss: 0.012910\n","\tEpoch:28 [000/005 (0640/0693)]\tLoss Ss: 0.012850\n","\tEpoch:28 [000/005 (0660/0693)]\tLoss Ss: 0.015363\n","\tEpoch:28 [000/005 (0680/0693)]\tLoss Ss: 0.007713\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:28 [001/005 (0000/0755)]\tLoss Ss: 0.023464\n","\tEpoch:28 [001/005 (0020/0755)]\tLoss Ss: 0.016838\n","\tEpoch:28 [001/005 (0040/0755)]\tLoss Ss: 0.032735\n","\tEpoch:28 [001/005 (0060/0755)]\tLoss Ss: 0.028883\n","\tEpoch:28 [001/005 (0080/0755)]\tLoss Ss: 0.018498\n","\tEpoch:28 [001/005 (0100/0755)]\tLoss Ss: 0.022559\n","\tEpoch:28 [001/005 (0120/0755)]\tLoss Ss: 0.024774\n","\tEpoch:28 [001/005 (0140/0755)]\tLoss Ss: 0.016098\n","\tEpoch:28 [001/005 (0160/0755)]\tLoss Ss: 0.022798\n","\tEpoch:28 [001/005 (0180/0755)]\tLoss Ss: 0.020372\n","\tEpoch:28 [001/005 (0200/0755)]\tLoss Ss: 0.019894\n","\tEpoch:28 [001/005 (0220/0755)]\tLoss Ss: 0.013619\n","\tEpoch:28 [001/005 (0240/0755)]\tLoss Ss: 0.017492\n","\tEpoch:28 [001/005 (0260/0755)]\tLoss Ss: 0.030400\n","\tEpoch:28 [001/005 (0280/0755)]\tLoss Ss: 0.022205\n","\tEpoch:28 [001/005 (0300/0755)]\tLoss Ss: 0.025969\n","\tEpoch:28 [001/005 (0320/0755)]\tLoss Ss: 0.019103\n","\tEpoch:28 [001/005 (0340/0755)]\tLoss Ss: 0.016533\n","\tEpoch:28 [001/005 (0360/0755)]\tLoss Ss: 0.017745\n","\tEpoch:28 [001/005 (0380/0755)]\tLoss Ss: 0.013337\n","\tEpoch:28 [001/005 (0400/0755)]\tLoss Ss: 0.019583\n","\tEpoch:28 [001/005 (0420/0755)]\tLoss Ss: 0.015085\n","\tEpoch:28 [001/005 (0440/0755)]\tLoss Ss: 0.014867\n","\tEpoch:28 [001/005 (0460/0755)]\tLoss Ss: 0.010842\n","\tEpoch:28 [001/005 (0480/0755)]\tLoss Ss: 0.023592\n","\tEpoch:28 [001/005 (0500/0755)]\tLoss Ss: 0.014090\n","\tEpoch:28 [001/005 (0520/0755)]\tLoss Ss: 0.012687\n","\tEpoch:28 [001/005 (0540/0755)]\tLoss Ss: 0.023895\n","\tEpoch:28 [001/005 (0560/0755)]\tLoss Ss: 0.022286\n","\tEpoch:28 [001/005 (0580/0755)]\tLoss Ss: 0.015463\n","\tEpoch:28 [001/005 (0600/0755)]\tLoss Ss: 0.010284\n","\tEpoch:28 [001/005 (0620/0755)]\tLoss Ss: 0.015174\n","\tEpoch:28 [001/005 (0640/0755)]\tLoss Ss: 0.016086\n","\tEpoch:28 [001/005 (0660/0755)]\tLoss Ss: 0.020239\n","\tEpoch:28 [001/005 (0680/0755)]\tLoss Ss: 0.014943\n","\tEpoch:28 [001/005 (0700/0755)]\tLoss Ss: 0.018941\n","\tEpoch:28 [001/005 (0720/0755)]\tLoss Ss: 0.014333\n","\tEpoch:28 [001/005 (0740/0755)]\tLoss Ss: 0.012471\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:28 [002/005 (0000/0755)]\tLoss Ss: 0.015100\n","\tEpoch:28 [002/005 (0020/0755)]\tLoss Ss: 0.022254\n","\tEpoch:28 [002/005 (0040/0755)]\tLoss Ss: 0.010609\n","\tEpoch:28 [002/005 (0060/0755)]\tLoss Ss: 0.025626\n","\tEpoch:28 [002/005 (0080/0755)]\tLoss Ss: 0.016569\n","\tEpoch:28 [002/005 (0100/0755)]\tLoss Ss: 0.018849\n","\tEpoch:28 [002/005 (0120/0755)]\tLoss Ss: 0.020522\n","\tEpoch:28 [002/005 (0140/0755)]\tLoss Ss: 0.010869\n","\tEpoch:28 [002/005 (0160/0755)]\tLoss Ss: 0.014934\n","\tEpoch:28 [002/005 (0180/0755)]\tLoss Ss: 0.017230\n","\tEpoch:28 [002/005 (0200/0755)]\tLoss Ss: 0.014937\n","\tEpoch:28 [002/005 (0220/0755)]\tLoss Ss: 0.011370\n","\tEpoch:28 [002/005 (0240/0755)]\tLoss Ss: 0.012210\n","\tEpoch:28 [002/005 (0260/0755)]\tLoss Ss: 0.009176\n","\tEpoch:28 [002/005 (0280/0755)]\tLoss Ss: 0.008364\n","\tEpoch:28 [002/005 (0300/0755)]\tLoss Ss: 0.011376\n","\tEpoch:28 [002/005 (0320/0755)]\tLoss Ss: 0.015443\n","\tEpoch:28 [002/005 (0340/0755)]\tLoss Ss: 0.018203\n","\tEpoch:28 [002/005 (0360/0755)]\tLoss Ss: 0.020557\n","\tEpoch:28 [002/005 (0380/0755)]\tLoss Ss: 0.017070\n","\tEpoch:28 [002/005 (0400/0755)]\tLoss Ss: 0.017362\n","\tEpoch:28 [002/005 (0420/0755)]\tLoss Ss: 0.015289\n","\tEpoch:28 [002/005 (0440/0755)]\tLoss Ss: 0.015067\n","\tEpoch:28 [002/005 (0460/0755)]\tLoss Ss: 0.020886\n","\tEpoch:28 [002/005 (0480/0755)]\tLoss Ss: 0.010102\n","\tEpoch:28 [002/005 (0500/0755)]\tLoss Ss: 0.011174\n","\tEpoch:28 [002/005 (0520/0755)]\tLoss Ss: 0.012397\n","\tEpoch:28 [002/005 (0540/0755)]\tLoss Ss: 0.009616\n","\tEpoch:28 [002/005 (0560/0755)]\tLoss Ss: 0.011151\n","\tEpoch:28 [002/005 (0580/0755)]\tLoss Ss: 0.008841\n","\tEpoch:28 [002/005 (0600/0755)]\tLoss Ss: 0.012486\n","\tEpoch:28 [002/005 (0620/0755)]\tLoss Ss: 0.012726\n","\tEpoch:28 [002/005 (0640/0755)]\tLoss Ss: 0.011947\n","\tEpoch:28 [002/005 (0660/0755)]\tLoss Ss: 0.018327\n","\tEpoch:28 [002/005 (0680/0755)]\tLoss Ss: 0.016040\n","\tEpoch:28 [002/005 (0700/0755)]\tLoss Ss: 0.017384\n","\tEpoch:28 [002/005 (0720/0755)]\tLoss Ss: 0.013688\n","\tEpoch:28 [002/005 (0740/0755)]\tLoss Ss: 0.010355\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tEpoch:28 [003/005 (0000/0588)]\tLoss Ss: 0.013630\n","\tEpoch:28 [003/005 (0020/0588)]\tLoss Ss: 0.017097\n","\tEpoch:28 [003/005 (0040/0588)]\tLoss Ss: 0.016856\n","\tEpoch:28 [003/005 (0060/0588)]\tLoss Ss: 0.008959\n","\tEpoch:28 [003/005 (0080/0588)]\tLoss Ss: 0.008174\n","\tEpoch:28 [003/005 (0100/0588)]\tLoss Ss: 0.005881\n","\tEpoch:28 [003/005 (0120/0588)]\tLoss Ss: 0.006652\n","\tEpoch:28 [003/005 (0140/0588)]\tLoss Ss: 0.006502\n","\tEpoch:28 [003/005 (0160/0588)]\tLoss Ss: 0.007595\n","\tEpoch:28 [003/005 (0180/0588)]\tLoss Ss: 0.005510\n","\tEpoch:28 [003/005 (0200/0588)]\tLoss Ss: 0.007759\n","\tEpoch:28 [003/005 (0220/0588)]\tLoss Ss: 0.011294\n","\tEpoch:28 [003/005 (0240/0588)]\tLoss Ss: 0.007062\n","\tEpoch:28 [003/005 (0260/0588)]\tLoss Ss: 0.004335\n","\tEpoch:28 [003/005 (0280/0588)]\tLoss Ss: 0.004657\n","\tEpoch:28 [003/005 (0300/0588)]\tLoss Ss: 0.005331\n","\tEpoch:28 [003/005 (0320/0588)]\tLoss Ss: 0.005314\n","\tEpoch:28 [003/005 (0340/0588)]\tLoss Ss: 0.004913\n","\tEpoch:28 [003/005 (0360/0588)]\tLoss Ss: 0.006815\n","\tEpoch:28 [003/005 (0380/0588)]\tLoss Ss: 0.005564\n","\tEpoch:28 [003/005 (0400/0588)]\tLoss Ss: 0.005381\n","\tEpoch:28 [003/005 (0420/0588)]\tLoss Ss: 0.008040\n","\tEpoch:28 [003/005 (0440/0588)]\tLoss Ss: 0.006311\n","\tEpoch:28 [003/005 (0460/0588)]\tLoss Ss: 0.012085\n","\tEpoch:28 [003/005 (0480/0588)]\tLoss Ss: 0.003962\n","\tEpoch:28 [003/005 (0500/0588)]\tLoss Ss: 0.005128\n","\tEpoch:28 [003/005 (0520/0588)]\tLoss Ss: 0.005525\n","\tEpoch:28 [003/005 (0540/0588)]\tLoss Ss: 0.004369\n","\tEpoch:28 [003/005 (0560/0588)]\tLoss Ss: 0.008051\n","\tEpoch:28 [003/005 (0580/0588)]\tLoss Ss: 0.003682\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tEpoch:28 [004/005 (0000/0614)]\tLoss Ss: 0.006505\n","\tEpoch:28 [004/005 (0020/0614)]\tLoss Ss: 0.009272\n","\tEpoch:28 [004/005 (0040/0614)]\tLoss Ss: 0.009546\n","\tEpoch:28 [004/005 (0060/0614)]\tLoss Ss: 0.008566\n","\tEpoch:28 [004/005 (0080/0614)]\tLoss Ss: 0.005718\n","\tEpoch:28 [004/005 (0100/0614)]\tLoss Ss: 0.004983\n","\tEpoch:28 [004/005 (0120/0614)]\tLoss Ss: 0.005905\n","\tEpoch:28 [004/005 (0140/0614)]\tLoss Ss: 0.006146\n","\tEpoch:28 [004/005 (0160/0614)]\tLoss Ss: 0.004198\n","\tEpoch:28 [004/005 (0180/0614)]\tLoss Ss: 0.006824\n","\tEpoch:28 [004/005 (0200/0614)]\tLoss Ss: 0.007447\n","\tEpoch:28 [004/005 (0220/0614)]\tLoss Ss: 0.005381\n","\tEpoch:28 [004/005 (0240/0614)]\tLoss Ss: 0.005341\n","\tEpoch:28 [004/005 (0260/0614)]\tLoss Ss: 0.005423\n","\tEpoch:28 [004/005 (0280/0614)]\tLoss Ss: 0.003458\n","\tEpoch:28 [004/005 (0300/0614)]\tLoss Ss: 0.006416\n","\tEpoch:28 [004/005 (0320/0614)]\tLoss Ss: 0.007647\n","\tEpoch:28 [004/005 (0340/0614)]\tLoss Ss: 0.006776\n","\tEpoch:28 [004/005 (0360/0614)]\tLoss Ss: 0.003555\n","\tEpoch:28 [004/005 (0380/0614)]\tLoss Ss: 0.008227\n","\tEpoch:28 [004/005 (0400/0614)]\tLoss Ss: 0.005826\n","\tEpoch:28 [004/005 (0420/0614)]\tLoss Ss: 0.006143\n","\tEpoch:28 [004/005 (0440/0614)]\tLoss Ss: 0.004602\n","\tEpoch:28 [004/005 (0460/0614)]\tLoss Ss: 0.006080\n","\tEpoch:28 [004/005 (0480/0614)]\tLoss Ss: 0.007217\n","\tEpoch:28 [004/005 (0500/0614)]\tLoss Ss: 0.007125\n","\tEpoch:28 [004/005 (0520/0614)]\tLoss Ss: 0.003594\n","\tEpoch:28 [004/005 (0540/0614)]\tLoss Ss: 0.006885\n","\tEpoch:28 [004/005 (0560/0614)]\tLoss Ss: 0.006306\n","\tEpoch:28 [004/005 (0580/0614)]\tLoss Ss: 0.006280\n","\tEpoch:28 [004/005 (0600/0614)]\tLoss Ss: 0.003373\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tEpoch:28 [005/005 (0000/0693)]\tLoss Ss: 0.018651\n","\tEpoch:28 [005/005 (0020/0693)]\tLoss Ss: 0.014991\n","\tEpoch:28 [005/005 (0040/0693)]\tLoss Ss: 0.018289\n","\tEpoch:28 [005/005 (0060/0693)]\tLoss Ss: 0.015214\n","\tEpoch:28 [005/005 (0080/0693)]\tLoss Ss: 0.010856\n","\tEpoch:28 [005/005 (0100/0693)]\tLoss Ss: 0.015764\n","\tEpoch:28 [005/005 (0120/0693)]\tLoss Ss: 0.018576\n","\tEpoch:28 [005/005 (0140/0693)]\tLoss Ss: 0.011843\n","\tEpoch:28 [005/005 (0160/0693)]\tLoss Ss: 0.012705\n","\tEpoch:28 [005/005 (0180/0693)]\tLoss Ss: 0.014582\n","\tEpoch:28 [005/005 (0200/0693)]\tLoss Ss: 0.012217\n","\tEpoch:28 [005/005 (0220/0693)]\tLoss Ss: 0.013438\n","\tEpoch:28 [005/005 (0240/0693)]\tLoss Ss: 0.010914\n","\tEpoch:28 [005/005 (0260/0693)]\tLoss Ss: 0.012198\n","\tEpoch:28 [005/005 (0280/0693)]\tLoss Ss: 0.012250\n","\tEpoch:28 [005/005 (0300/0693)]\tLoss Ss: 0.016165\n","\tEpoch:28 [005/005 (0320/0693)]\tLoss Ss: 0.012568\n","\tEpoch:28 [005/005 (0340/0693)]\tLoss Ss: 0.012162\n","\tEpoch:28 [005/005 (0360/0693)]\tLoss Ss: 0.021366\n","\tEpoch:28 [005/005 (0380/0693)]\tLoss Ss: 0.015223\n","\tEpoch:28 [005/005 (0400/0693)]\tLoss Ss: 0.018803\n","\tEpoch:28 [005/005 (0420/0693)]\tLoss Ss: 0.010892\n","\tEpoch:28 [005/005 (0440/0693)]\tLoss Ss: 0.013958\n","\tEpoch:28 [005/005 (0460/0693)]\tLoss Ss: 0.021966\n","\tEpoch:28 [005/005 (0480/0693)]\tLoss Ss: 0.011239\n","\tEpoch:28 [005/005 (0500/0693)]\tLoss Ss: 0.017293\n","\tEpoch:28 [005/005 (0520/0693)]\tLoss Ss: 0.011273\n","\tEpoch:28 [005/005 (0540/0693)]\tLoss Ss: 0.017052\n","\tEpoch:28 [005/005 (0560/0693)]\tLoss Ss: 0.013462\n","\tEpoch:28 [005/005 (0580/0693)]\tLoss Ss: 0.013946\n","\tEpoch:28 [005/005 (0600/0693)]\tLoss Ss: 0.016935\n","\tEpoch:28 [005/005 (0620/0693)]\tLoss Ss: 0.013752\n","\tEpoch:28 [005/005 (0640/0693)]\tLoss Ss: 0.010114\n","\tEpoch:28 [005/005 (0660/0693)]\tLoss Ss: 0.012835\n","\tEpoch:28 [005/005 (0680/0693)]\tLoss Ss: 0.015292\n","Now train the rotated image\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:28 [000/005 (0000/0755)]\tLoss Ss: 0.158980\n","\tRotated_Epoch:28 [000/005 (0020/0755)]\tLoss Ss: 0.274347\n","\tRotated_Epoch:28 [000/005 (0040/0755)]\tLoss Ss: 0.199601\n","\tRotated_Epoch:28 [000/005 (0060/0755)]\tLoss Ss: 0.156517\n","\tRotated_Epoch:28 [000/005 (0080/0755)]\tLoss Ss: 0.154691\n","\tRotated_Epoch:28 [000/005 (0100/0755)]\tLoss Ss: 0.134242\n","\tRotated_Epoch:28 [000/005 (0120/0755)]\tLoss Ss: 0.082808\n","\tRotated_Epoch:28 [000/005 (0140/0755)]\tLoss Ss: 0.086606\n","\tRotated_Epoch:28 [000/005 (0160/0755)]\tLoss Ss: 0.110512\n","\tRotated_Epoch:28 [000/005 (0180/0755)]\tLoss Ss: 0.076878\n","\tRotated_Epoch:28 [000/005 (0200/0755)]\tLoss Ss: 0.093332\n","\tRotated_Epoch:28 [000/005 (0220/0755)]\tLoss Ss: 0.072916\n","\tRotated_Epoch:28 [000/005 (0240/0755)]\tLoss Ss: 0.067920\n","\tRotated_Epoch:28 [000/005 (0260/0755)]\tLoss Ss: 0.077181\n","\tRotated_Epoch:28 [000/005 (0280/0755)]\tLoss Ss: 0.082461\n","\tRotated_Epoch:28 [000/005 (0300/0755)]\tLoss Ss: 0.075396\n","\tRotated_Epoch:28 [000/005 (0320/0755)]\tLoss Ss: 0.051432\n","\tRotated_Epoch:28 [000/005 (0340/0755)]\tLoss Ss: 0.046297\n","\tRotated_Epoch:28 [000/005 (0360/0755)]\tLoss Ss: 0.070242\n","\tRotated_Epoch:28 [000/005 (0380/0755)]\tLoss Ss: 0.060784\n","\tRotated_Epoch:28 [000/005 (0400/0755)]\tLoss Ss: 0.048575\n","\tRotated_Epoch:28 [000/005 (0420/0755)]\tLoss Ss: 0.068830\n","\tRotated_Epoch:28 [000/005 (0440/0755)]\tLoss Ss: 0.065882\n","\tRotated_Epoch:28 [000/005 (0460/0755)]\tLoss Ss: 0.050647\n","\tRotated_Epoch:28 [000/005 (0480/0755)]\tLoss Ss: 0.055670\n","\tRotated_Epoch:28 [000/005 (0500/0755)]\tLoss Ss: 0.041962\n","\tRotated_Epoch:28 [000/005 (0520/0755)]\tLoss Ss: 0.064701\n","\tRotated_Epoch:28 [000/005 (0540/0755)]\tLoss Ss: 0.039616\n","\tRotated_Epoch:28 [000/005 (0560/0755)]\tLoss Ss: 0.031606\n","\tRotated_Epoch:28 [000/005 (0580/0755)]\tLoss Ss: 0.054084\n","\tRotated_Epoch:28 [000/005 (0600/0755)]\tLoss Ss: 0.062476\n","\tRotated_Epoch:28 [000/005 (0620/0755)]\tLoss Ss: 0.048190\n","\tRotated_Epoch:28 [000/005 (0640/0755)]\tLoss Ss: 0.033408\n","\tRotated_Epoch:28 [000/005 (0660/0755)]\tLoss Ss: 0.033042\n","\tRotated_Epoch:28 [000/005 (0680/0755)]\tLoss Ss: 0.046042\n","\tRotated_Epoch:28 [000/005 (0700/0755)]\tLoss Ss: 0.034729\n","\tRotated_Epoch:28 [000/005 (0720/0755)]\tLoss Ss: 0.060608\n","\tRotated_Epoch:28 [000/005 (0740/0755)]\tLoss Ss: 0.038791\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tRotated_Epoch:28 [001/005 (0000/0693)]\tLoss Ss: 0.056972\n","\tRotated_Epoch:28 [001/005 (0020/0693)]\tLoss Ss: 0.037206\n","\tRotated_Epoch:28 [001/005 (0040/0693)]\tLoss Ss: 0.035128\n","\tRotated_Epoch:28 [001/005 (0060/0693)]\tLoss Ss: 0.018012\n","\tRotated_Epoch:28 [001/005 (0080/0693)]\tLoss Ss: 0.028228\n","\tRotated_Epoch:28 [001/005 (0100/0693)]\tLoss Ss: 0.030902\n","\tRotated_Epoch:28 [001/005 (0120/0693)]\tLoss Ss: 0.031153\n","\tRotated_Epoch:28 [001/005 (0140/0693)]\tLoss Ss: 0.024242\n","\tRotated_Epoch:28 [001/005 (0160/0693)]\tLoss Ss: 0.016406\n","\tRotated_Epoch:28 [001/005 (0180/0693)]\tLoss Ss: 0.015183\n","\tRotated_Epoch:28 [001/005 (0200/0693)]\tLoss Ss: 0.018891\n","\tRotated_Epoch:28 [001/005 (0220/0693)]\tLoss Ss: 0.018060\n","\tRotated_Epoch:28 [001/005 (0240/0693)]\tLoss Ss: 0.019067\n","\tRotated_Epoch:28 [001/005 (0260/0693)]\tLoss Ss: 0.021130\n","\tRotated_Epoch:28 [001/005 (0280/0693)]\tLoss Ss: 0.015334\n","\tRotated_Epoch:28 [001/005 (0300/0693)]\tLoss Ss: 0.023686\n","\tRotated_Epoch:28 [001/005 (0320/0693)]\tLoss Ss: 0.016860\n","\tRotated_Epoch:28 [001/005 (0340/0693)]\tLoss Ss: 0.015499\n","\tRotated_Epoch:28 [001/005 (0360/0693)]\tLoss Ss: 0.017050\n","\tRotated_Epoch:28 [001/005 (0380/0693)]\tLoss Ss: 0.012509\n","\tRotated_Epoch:28 [001/005 (0400/0693)]\tLoss Ss: 0.021602\n","\tRotated_Epoch:28 [001/005 (0420/0693)]\tLoss Ss: 0.016695\n","\tRotated_Epoch:28 [001/005 (0440/0693)]\tLoss Ss: 0.018257\n","\tRotated_Epoch:28 [001/005 (0460/0693)]\tLoss Ss: 0.016836\n","\tRotated_Epoch:28 [001/005 (0480/0693)]\tLoss Ss: 0.020165\n","\tRotated_Epoch:28 [001/005 (0500/0693)]\tLoss Ss: 0.009369\n","\tRotated_Epoch:28 [001/005 (0520/0693)]\tLoss Ss: 0.021182\n","\tRotated_Epoch:28 [001/005 (0540/0693)]\tLoss Ss: 0.014156\n","\tRotated_Epoch:28 [001/005 (0560/0693)]\tLoss Ss: 0.015681\n","\tRotated_Epoch:28 [001/005 (0580/0693)]\tLoss Ss: 0.011819\n","\tRotated_Epoch:28 [001/005 (0600/0693)]\tLoss Ss: 0.016506\n","\tRotated_Epoch:28 [001/005 (0620/0693)]\tLoss Ss: 0.014648\n","\tRotated_Epoch:28 [001/005 (0640/0693)]\tLoss Ss: 0.016186\n","\tRotated_Epoch:28 [001/005 (0660/0693)]\tLoss Ss: 0.017280\n","\tRotated_Epoch:28 [001/005 (0680/0693)]\tLoss Ss: 0.015012\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tRotated_Epoch:28 [002/005 (0000/0614)]\tLoss Ss: 0.036460\n","\tRotated_Epoch:28 [002/005 (0020/0614)]\tLoss Ss: 0.035685\n","\tRotated_Epoch:28 [002/005 (0040/0614)]\tLoss Ss: 0.021205\n","\tRotated_Epoch:28 [002/005 (0060/0614)]\tLoss Ss: 0.018971\n","\tRotated_Epoch:28 [002/005 (0080/0614)]\tLoss Ss: 0.011118\n","\tRotated_Epoch:28 [002/005 (0100/0614)]\tLoss Ss: 0.014237\n","\tRotated_Epoch:28 [002/005 (0120/0614)]\tLoss Ss: 0.010935\n","\tRotated_Epoch:28 [002/005 (0140/0614)]\tLoss Ss: 0.005765\n","\tRotated_Epoch:28 [002/005 (0160/0614)]\tLoss Ss: 0.010458\n","\tRotated_Epoch:28 [002/005 (0180/0614)]\tLoss Ss: 0.013664\n","\tRotated_Epoch:28 [002/005 (0200/0614)]\tLoss Ss: 0.008834\n","\tRotated_Epoch:28 [002/005 (0220/0614)]\tLoss Ss: 0.008578\n","\tRotated_Epoch:28 [002/005 (0240/0614)]\tLoss Ss: 0.007005\n","\tRotated_Epoch:28 [002/005 (0260/0614)]\tLoss Ss: 0.008737\n","\tRotated_Epoch:28 [002/005 (0280/0614)]\tLoss Ss: 0.013571\n","\tRotated_Epoch:28 [002/005 (0300/0614)]\tLoss Ss: 0.005626\n","\tRotated_Epoch:28 [002/005 (0320/0614)]\tLoss Ss: 0.010481\n","\tRotated_Epoch:28 [002/005 (0340/0614)]\tLoss Ss: 0.008953\n","\tRotated_Epoch:28 [002/005 (0360/0614)]\tLoss Ss: 0.007730\n","\tRotated_Epoch:28 [002/005 (0380/0614)]\tLoss Ss: 0.006417\n","\tRotated_Epoch:28 [002/005 (0400/0614)]\tLoss Ss: 0.006602\n","\tRotated_Epoch:28 [002/005 (0420/0614)]\tLoss Ss: 0.004223\n","\tRotated_Epoch:28 [002/005 (0440/0614)]\tLoss Ss: 0.005184\n","\tRotated_Epoch:28 [002/005 (0460/0614)]\tLoss Ss: 0.008149\n","\tRotated_Epoch:28 [002/005 (0480/0614)]\tLoss Ss: 0.009810\n","\tRotated_Epoch:28 [002/005 (0500/0614)]\tLoss Ss: 0.006263\n","\tRotated_Epoch:28 [002/005 (0520/0614)]\tLoss Ss: 0.006563\n","\tRotated_Epoch:28 [002/005 (0540/0614)]\tLoss Ss: 0.005629\n","\tRotated_Epoch:28 [002/005 (0560/0614)]\tLoss Ss: 0.006438\n","\tRotated_Epoch:28 [002/005 (0580/0614)]\tLoss Ss: 0.007261\n","\tRotated_Epoch:28 [002/005 (0600/0614)]\tLoss Ss: 0.007816\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:28 [003/005 (0000/0755)]\tLoss Ss: 0.077830\n","\tRotated_Epoch:28 [003/005 (0020/0755)]\tLoss Ss: 0.069770\n","\tRotated_Epoch:28 [003/005 (0040/0755)]\tLoss Ss: 0.044750\n","\tRotated_Epoch:28 [003/005 (0060/0755)]\tLoss Ss: 0.067091\n","\tRotated_Epoch:28 [003/005 (0080/0755)]\tLoss Ss: 0.067491\n","\tRotated_Epoch:28 [003/005 (0100/0755)]\tLoss Ss: 0.051391\n","\tRotated_Epoch:28 [003/005 (0120/0755)]\tLoss Ss: 0.031471\n","\tRotated_Epoch:28 [003/005 (0140/0755)]\tLoss Ss: 0.030086\n","\tRotated_Epoch:28 [003/005 (0160/0755)]\tLoss Ss: 0.032926\n","\tRotated_Epoch:28 [003/005 (0180/0755)]\tLoss Ss: 0.027732\n","\tRotated_Epoch:28 [003/005 (0200/0755)]\tLoss Ss: 0.023580\n","\tRotated_Epoch:28 [003/005 (0220/0755)]\tLoss Ss: 0.020694\n","\tRotated_Epoch:28 [003/005 (0240/0755)]\tLoss Ss: 0.028698\n","\tRotated_Epoch:28 [003/005 (0260/0755)]\tLoss Ss: 0.030259\n","\tRotated_Epoch:28 [003/005 (0280/0755)]\tLoss Ss: 0.016439\n","\tRotated_Epoch:28 [003/005 (0300/0755)]\tLoss Ss: 0.020555\n","\tRotated_Epoch:28 [003/005 (0320/0755)]\tLoss Ss: 0.022046\n","\tRotated_Epoch:28 [003/005 (0340/0755)]\tLoss Ss: 0.020737\n","\tRotated_Epoch:28 [003/005 (0360/0755)]\tLoss Ss: 0.028444\n","\tRotated_Epoch:28 [003/005 (0380/0755)]\tLoss Ss: 0.016419\n","\tRotated_Epoch:28 [003/005 (0400/0755)]\tLoss Ss: 0.023632\n","\tRotated_Epoch:28 [003/005 (0420/0755)]\tLoss Ss: 0.021549\n","\tRotated_Epoch:28 [003/005 (0440/0755)]\tLoss Ss: 0.018261\n","\tRotated_Epoch:28 [003/005 (0460/0755)]\tLoss Ss: 0.024621\n","\tRotated_Epoch:28 [003/005 (0480/0755)]\tLoss Ss: 0.016299\n","\tRotated_Epoch:28 [003/005 (0500/0755)]\tLoss Ss: 0.028039\n","\tRotated_Epoch:28 [003/005 (0520/0755)]\tLoss Ss: 0.014822\n","\tRotated_Epoch:28 [003/005 (0540/0755)]\tLoss Ss: 0.021102\n","\tRotated_Epoch:28 [003/005 (0560/0755)]\tLoss Ss: 0.013316\n","\tRotated_Epoch:28 [003/005 (0580/0755)]\tLoss Ss: 0.015674\n","\tRotated_Epoch:28 [003/005 (0600/0755)]\tLoss Ss: 0.027482\n","\tRotated_Epoch:28 [003/005 (0620/0755)]\tLoss Ss: 0.016911\n","\tRotated_Epoch:28 [003/005 (0640/0755)]\tLoss Ss: 0.023971\n","\tRotated_Epoch:28 [003/005 (0660/0755)]\tLoss Ss: 0.022555\n","\tRotated_Epoch:28 [003/005 (0680/0755)]\tLoss Ss: 0.017291\n","\tRotated_Epoch:28 [003/005 (0700/0755)]\tLoss Ss: 0.011574\n","\tRotated_Epoch:28 [003/005 (0720/0755)]\tLoss Ss: 0.016125\n","\tRotated_Epoch:28 [003/005 (0740/0755)]\tLoss Ss: 0.024162\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tRotated_Epoch:28 [004/005 (0000/0588)]\tLoss Ss: 0.445261\n","\tRotated_Epoch:28 [004/005 (0020/0588)]\tLoss Ss: 0.088036\n","\tRotated_Epoch:28 [004/005 (0040/0588)]\tLoss Ss: 0.114846\n","\tRotated_Epoch:28 [004/005 (0060/0588)]\tLoss Ss: 0.105765\n","\tRotated_Epoch:28 [004/005 (0080/0588)]\tLoss Ss: 0.077714\n","\tRotated_Epoch:28 [004/005 (0100/0588)]\tLoss Ss: 0.061230\n","\tRotated_Epoch:28 [004/005 (0120/0588)]\tLoss Ss: 0.059000\n","\tRotated_Epoch:28 [004/005 (0140/0588)]\tLoss Ss: 0.057322\n","\tRotated_Epoch:28 [004/005 (0160/0588)]\tLoss Ss: 0.053162\n","\tRotated_Epoch:28 [004/005 (0180/0588)]\tLoss Ss: 0.055274\n","\tRotated_Epoch:28 [004/005 (0200/0588)]\tLoss Ss: 0.044394\n","\tRotated_Epoch:28 [004/005 (0220/0588)]\tLoss Ss: 0.050549\n","\tRotated_Epoch:28 [004/005 (0240/0588)]\tLoss Ss: 0.038984\n","\tRotated_Epoch:28 [004/005 (0260/0588)]\tLoss Ss: 0.054729\n","\tRotated_Epoch:28 [004/005 (0280/0588)]\tLoss Ss: 0.052739\n","\tRotated_Epoch:28 [004/005 (0300/0588)]\tLoss Ss: 0.070383\n","\tRotated_Epoch:28 [004/005 (0320/0588)]\tLoss Ss: 0.076138\n","\tRotated_Epoch:28 [004/005 (0340/0588)]\tLoss Ss: 0.036042\n","\tRotated_Epoch:28 [004/005 (0360/0588)]\tLoss Ss: 0.052038\n","\tRotated_Epoch:28 [004/005 (0380/0588)]\tLoss Ss: 0.069851\n","\tRotated_Epoch:28 [004/005 (0400/0588)]\tLoss Ss: 0.051968\n","\tRotated_Epoch:28 [004/005 (0420/0588)]\tLoss Ss: 0.076539\n","\tRotated_Epoch:28 [004/005 (0440/0588)]\tLoss Ss: 0.044764\n","\tRotated_Epoch:28 [004/005 (0460/0588)]\tLoss Ss: 0.073357\n","\tRotated_Epoch:28 [004/005 (0480/0588)]\tLoss Ss: 0.051155\n","\tRotated_Epoch:28 [004/005 (0500/0588)]\tLoss Ss: 0.062300\n","\tRotated_Epoch:28 [004/005 (0520/0588)]\tLoss Ss: 0.062529\n","\tRotated_Epoch:28 [004/005 (0540/0588)]\tLoss Ss: 0.043369\n","\tRotated_Epoch:28 [004/005 (0560/0588)]\tLoss Ss: 0.068774\n","\tRotated_Epoch:28 [004/005 (0580/0588)]\tLoss Ss: 0.075434\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tRotated_Epoch:28 [005/005 (0000/0693)]\tLoss Ss: 0.129575\n","\tRotated_Epoch:28 [005/005 (0020/0693)]\tLoss Ss: 0.067265\n","\tRotated_Epoch:28 [005/005 (0040/0693)]\tLoss Ss: 0.024577\n","\tRotated_Epoch:28 [005/005 (0060/0693)]\tLoss Ss: 0.021863\n","\tRotated_Epoch:28 [005/005 (0080/0693)]\tLoss Ss: 0.019932\n","\tRotated_Epoch:28 [005/005 (0100/0693)]\tLoss Ss: 0.028043\n","\tRotated_Epoch:28 [005/005 (0120/0693)]\tLoss Ss: 0.031205\n","\tRotated_Epoch:28 [005/005 (0140/0693)]\tLoss Ss: 0.026715\n","\tRotated_Epoch:28 [005/005 (0160/0693)]\tLoss Ss: 0.019481\n","\tRotated_Epoch:28 [005/005 (0180/0693)]\tLoss Ss: 0.020124\n","\tRotated_Epoch:28 [005/005 (0200/0693)]\tLoss Ss: 0.018574\n","\tRotated_Epoch:28 [005/005 (0220/0693)]\tLoss Ss: 0.018218\n","\tRotated_Epoch:28 [005/005 (0240/0693)]\tLoss Ss: 0.030813\n","\tRotated_Epoch:28 [005/005 (0260/0693)]\tLoss Ss: 0.016072\n","\tRotated_Epoch:28 [005/005 (0280/0693)]\tLoss Ss: 0.020170\n","\tRotated_Epoch:28 [005/005 (0300/0693)]\tLoss Ss: 0.020302\n","\tRotated_Epoch:28 [005/005 (0320/0693)]\tLoss Ss: 0.012584\n","\tRotated_Epoch:28 [005/005 (0340/0693)]\tLoss Ss: 0.012753\n","\tRotated_Epoch:28 [005/005 (0360/0693)]\tLoss Ss: 0.020101\n","\tRotated_Epoch:28 [005/005 (0380/0693)]\tLoss Ss: 0.017641\n","\tRotated_Epoch:28 [005/005 (0400/0693)]\tLoss Ss: 0.014836\n","\tRotated_Epoch:28 [005/005 (0420/0693)]\tLoss Ss: 0.015902\n","\tRotated_Epoch:28 [005/005 (0440/0693)]\tLoss Ss: 0.018577\n","\tRotated_Epoch:28 [005/005 (0460/0693)]\tLoss Ss: 0.019231\n","\tRotated_Epoch:28 [005/005 (0480/0693)]\tLoss Ss: 0.014529\n","\tRotated_Epoch:28 [005/005 (0500/0693)]\tLoss Ss: 0.019254\n","\tRotated_Epoch:28 [005/005 (0520/0693)]\tLoss Ss: 0.015894\n","\tRotated_Epoch:28 [005/005 (0540/0693)]\tLoss Ss: 0.017129\n","\tRotated_Epoch:28 [005/005 (0560/0693)]\tLoss Ss: 0.011615\n","\tRotated_Epoch:28 [005/005 (0580/0693)]\tLoss Ss: 0.012330\n","\tRotated_Epoch:28 [005/005 (0600/0693)]\tLoss Ss: 0.016174\n","\tRotated_Epoch:28 [005/005 (0620/0693)]\tLoss Ss: 0.014292\n","\tRotated_Epoch:28 [005/005 (0640/0693)]\tLoss Ss: 0.020610\n","\tRotated_Epoch:28 [005/005 (0660/0693)]\tLoss Ss: 0.012707\n","\tRotated_Epoch:28 [005/005 (0680/0693)]\tLoss Ss: 0.016892\n","new_1_6_d.nii\n","new_1_6_m.nii\n","new_1_7_d.nii\n","new_1_7_m.nii\n","new_2_5_d.nii\n","new_2_5_m.nii\n","new_2_6_d.nii\n","new_2_6_m.nii\n","new_2_7_d.nii\n","new_2_7_m.nii\n","new_3_d (2).nii\n","new_3_m (2).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (3).nii\n","new_3_m (3).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (7).nii\n","new_3_m (7).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (8).nii\n","new_3_m (8).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (9).nii\n","new_3_m (9).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_4_10_d_163526.nii\n","new_4_10_m_163526.nii.gz\n","new_4_11_d_084336.nii\n","new_4_11_m_084336.nii.gz\n","new_4_7_d_184422.nii\n","new_4_7_m_184422.nii.gz\n","new_4_8_d_143412.nii\n","new_4_8_m_143412.nii.gz\n","new_4_9_d_155347.nii\n","new_4_9_m_155347.nii.gz\n","\tEpoch: 28; Dice: 0.9553 +/- 0.0117; Loss: 10.9551\n","Begin Epoch 29\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tEpoch:29 [000/005 (0000/0588)]\tLoss Ss: 0.022427\n","\tEpoch:29 [000/005 (0020/0588)]\tLoss Ss: 0.021341\n","\tEpoch:29 [000/005 (0040/0588)]\tLoss Ss: 0.021383\n","\tEpoch:29 [000/005 (0060/0588)]\tLoss Ss: 0.013451\n","\tEpoch:29 [000/005 (0080/0588)]\tLoss Ss: 0.021308\n","\tEpoch:29 [000/005 (0100/0588)]\tLoss Ss: 0.011289\n","\tEpoch:29 [000/005 (0120/0588)]\tLoss Ss: 0.015679\n","\tEpoch:29 [000/005 (0140/0588)]\tLoss Ss: 0.009027\n","\tEpoch:29 [000/005 (0160/0588)]\tLoss Ss: 0.011688\n","\tEpoch:29 [000/005 (0180/0588)]\tLoss Ss: 0.011133\n","\tEpoch:29 [000/005 (0200/0588)]\tLoss Ss: 0.007266\n","\tEpoch:29 [000/005 (0220/0588)]\tLoss Ss: 0.009558\n","\tEpoch:29 [000/005 (0240/0588)]\tLoss Ss: 0.007641\n","\tEpoch:29 [000/005 (0260/0588)]\tLoss Ss: 0.012396\n","\tEpoch:29 [000/005 (0280/0588)]\tLoss Ss: 0.007542\n","\tEpoch:29 [000/005 (0300/0588)]\tLoss Ss: 0.006791\n","\tEpoch:29 [000/005 (0320/0588)]\tLoss Ss: 0.010393\n","\tEpoch:29 [000/005 (0340/0588)]\tLoss Ss: 0.006236\n","\tEpoch:29 [000/005 (0360/0588)]\tLoss Ss: 0.010096\n","\tEpoch:29 [000/005 (0380/0588)]\tLoss Ss: 0.006598\n","\tEpoch:29 [000/005 (0400/0588)]\tLoss Ss: 0.008922\n","\tEpoch:29 [000/005 (0420/0588)]\tLoss Ss: 0.004574\n","\tEpoch:29 [000/005 (0440/0588)]\tLoss Ss: 0.007736\n","\tEpoch:29 [000/005 (0460/0588)]\tLoss Ss: 0.007545\n","\tEpoch:29 [000/005 (0480/0588)]\tLoss Ss: 0.007136\n","\tEpoch:29 [000/005 (0500/0588)]\tLoss Ss: 0.004946\n","\tEpoch:29 [000/005 (0520/0588)]\tLoss Ss: 0.006403\n","\tEpoch:29 [000/005 (0540/0588)]\tLoss Ss: 0.004707\n","\tEpoch:29 [000/005 (0560/0588)]\tLoss Ss: 0.008150\n","\tEpoch:29 [000/005 (0580/0588)]\tLoss Ss: 0.003896\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:29 [001/005 (0000/0755)]\tLoss Ss: 0.016974\n","\tEpoch:29 [001/005 (0020/0755)]\tLoss Ss: 0.037560\n","\tEpoch:29 [001/005 (0040/0755)]\tLoss Ss: 0.020319\n","\tEpoch:29 [001/005 (0060/0755)]\tLoss Ss: 0.015531\n","\tEpoch:29 [001/005 (0080/0755)]\tLoss Ss: 0.023532\n","\tEpoch:29 [001/005 (0100/0755)]\tLoss Ss: 0.021036\n","\tEpoch:29 [001/005 (0120/0755)]\tLoss Ss: 0.023598\n","\tEpoch:29 [001/005 (0140/0755)]\tLoss Ss: 0.022762\n","\tEpoch:29 [001/005 (0160/0755)]\tLoss Ss: 0.014483\n","\tEpoch:29 [001/005 (0180/0755)]\tLoss Ss: 0.022910\n","\tEpoch:29 [001/005 (0200/0755)]\tLoss Ss: 0.020342\n","\tEpoch:29 [001/005 (0220/0755)]\tLoss Ss: 0.019890\n","\tEpoch:29 [001/005 (0240/0755)]\tLoss Ss: 0.012730\n","\tEpoch:29 [001/005 (0260/0755)]\tLoss Ss: 0.024426\n","\tEpoch:29 [001/005 (0280/0755)]\tLoss Ss: 0.020424\n","\tEpoch:29 [001/005 (0300/0755)]\tLoss Ss: 0.015880\n","\tEpoch:29 [001/005 (0320/0755)]\tLoss Ss: 0.021584\n","\tEpoch:29 [001/005 (0340/0755)]\tLoss Ss: 0.013722\n","\tEpoch:29 [001/005 (0360/0755)]\tLoss Ss: 0.017619\n","\tEpoch:29 [001/005 (0380/0755)]\tLoss Ss: 0.014317\n","\tEpoch:29 [001/005 (0400/0755)]\tLoss Ss: 0.010636\n","\tEpoch:29 [001/005 (0420/0755)]\tLoss Ss: 0.013688\n","\tEpoch:29 [001/005 (0440/0755)]\tLoss Ss: 0.018166\n","\tEpoch:29 [001/005 (0460/0755)]\tLoss Ss: 0.012799\n","\tEpoch:29 [001/005 (0480/0755)]\tLoss Ss: 0.022037\n","\tEpoch:29 [001/005 (0500/0755)]\tLoss Ss: 0.024429\n","\tEpoch:29 [001/005 (0520/0755)]\tLoss Ss: 0.011199\n","\tEpoch:29 [001/005 (0540/0755)]\tLoss Ss: 0.016406\n","\tEpoch:29 [001/005 (0560/0755)]\tLoss Ss: 0.010079\n","\tEpoch:29 [001/005 (0580/0755)]\tLoss Ss: 0.019381\n","\tEpoch:29 [001/005 (0600/0755)]\tLoss Ss: 0.017818\n","\tEpoch:29 [001/005 (0620/0755)]\tLoss Ss: 0.013810\n","\tEpoch:29 [001/005 (0640/0755)]\tLoss Ss: 0.012387\n","\tEpoch:29 [001/005 (0660/0755)]\tLoss Ss: 0.013993\n","\tEpoch:29 [001/005 (0680/0755)]\tLoss Ss: 0.015591\n","\tEpoch:29 [001/005 (0700/0755)]\tLoss Ss: 0.020892\n","\tEpoch:29 [001/005 (0720/0755)]\tLoss Ss: 0.022702\n","\tEpoch:29 [001/005 (0740/0755)]\tLoss Ss: 0.018602\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tEpoch:29 [002/005 (0000/0693)]\tLoss Ss: 0.028616\n","\tEpoch:29 [002/005 (0020/0693)]\tLoss Ss: 0.019723\n","\tEpoch:29 [002/005 (0040/0693)]\tLoss Ss: 0.014902\n","\tEpoch:29 [002/005 (0060/0693)]\tLoss Ss: 0.015398\n","\tEpoch:29 [002/005 (0080/0693)]\tLoss Ss: 0.007103\n","\tEpoch:29 [002/005 (0100/0693)]\tLoss Ss: 0.012457\n","\tEpoch:29 [002/005 (0120/0693)]\tLoss Ss: 0.011582\n","\tEpoch:29 [002/005 (0140/0693)]\tLoss Ss: 0.012905\n","\tEpoch:29 [002/005 (0160/0693)]\tLoss Ss: 0.014617\n","\tEpoch:29 [002/005 (0180/0693)]\tLoss Ss: 0.009674\n","\tEpoch:29 [002/005 (0200/0693)]\tLoss Ss: 0.013081\n","\tEpoch:29 [002/005 (0220/0693)]\tLoss Ss: 0.010418\n","\tEpoch:29 [002/005 (0240/0693)]\tLoss Ss: 0.009243\n","\tEpoch:29 [002/005 (0260/0693)]\tLoss Ss: 0.010944\n","\tEpoch:29 [002/005 (0280/0693)]\tLoss Ss: 0.007814\n","\tEpoch:29 [002/005 (0300/0693)]\tLoss Ss: 0.013725\n","\tEpoch:29 [002/005 (0320/0693)]\tLoss Ss: 0.015186\n","\tEpoch:29 [002/005 (0340/0693)]\tLoss Ss: 0.011551\n","\tEpoch:29 [002/005 (0360/0693)]\tLoss Ss: 0.011793\n","\tEpoch:29 [002/005 (0380/0693)]\tLoss Ss: 0.013333\n","\tEpoch:29 [002/005 (0400/0693)]\tLoss Ss: 0.014377\n","\tEpoch:29 [002/005 (0420/0693)]\tLoss Ss: 0.015179\n","\tEpoch:29 [002/005 (0440/0693)]\tLoss Ss: 0.014931\n","\tEpoch:29 [002/005 (0460/0693)]\tLoss Ss: 0.013922\n","\tEpoch:29 [002/005 (0480/0693)]\tLoss Ss: 0.016584\n","\tEpoch:29 [002/005 (0500/0693)]\tLoss Ss: 0.016365\n","\tEpoch:29 [002/005 (0520/0693)]\tLoss Ss: 0.015339\n","\tEpoch:29 [002/005 (0540/0693)]\tLoss Ss: 0.015093\n","\tEpoch:29 [002/005 (0560/0693)]\tLoss Ss: 0.016346\n","\tEpoch:29 [002/005 (0580/0693)]\tLoss Ss: 0.016332\n","\tEpoch:29 [002/005 (0600/0693)]\tLoss Ss: 0.010902\n","\tEpoch:29 [002/005 (0620/0693)]\tLoss Ss: 0.010622\n","\tEpoch:29 [002/005 (0640/0693)]\tLoss Ss: 0.013675\n","\tEpoch:29 [002/005 (0660/0693)]\tLoss Ss: 0.011555\n","\tEpoch:29 [002/005 (0680/0693)]\tLoss Ss: 0.009561\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tEpoch:29 [003/005 (0000/0693)]\tLoss Ss: 0.016910\n","\tEpoch:29 [003/005 (0020/0693)]\tLoss Ss: 0.013105\n","\tEpoch:29 [003/005 (0040/0693)]\tLoss Ss: 0.013071\n","\tEpoch:29 [003/005 (0060/0693)]\tLoss Ss: 0.019550\n","\tEpoch:29 [003/005 (0080/0693)]\tLoss Ss: 0.008708\n","\tEpoch:29 [003/005 (0100/0693)]\tLoss Ss: 0.015820\n","\tEpoch:29 [003/005 (0120/0693)]\tLoss Ss: 0.017237\n","\tEpoch:29 [003/005 (0140/0693)]\tLoss Ss: 0.018401\n","\tEpoch:29 [003/005 (0160/0693)]\tLoss Ss: 0.015115\n","\tEpoch:29 [003/005 (0180/0693)]\tLoss Ss: 0.017780\n","\tEpoch:29 [003/005 (0200/0693)]\tLoss Ss: 0.014123\n","\tEpoch:29 [003/005 (0220/0693)]\tLoss Ss: 0.019209\n","\tEpoch:29 [003/005 (0240/0693)]\tLoss Ss: 0.016180\n","\tEpoch:29 [003/005 (0260/0693)]\tLoss Ss: 0.015086\n","\tEpoch:29 [003/005 (0280/0693)]\tLoss Ss: 0.012991\n","\tEpoch:29 [003/005 (0300/0693)]\tLoss Ss: 0.015405\n","\tEpoch:29 [003/005 (0320/0693)]\tLoss Ss: 0.014004\n","\tEpoch:29 [003/005 (0340/0693)]\tLoss Ss: 0.009934\n","\tEpoch:29 [003/005 (0360/0693)]\tLoss Ss: 0.014813\n","\tEpoch:29 [003/005 (0380/0693)]\tLoss Ss: 0.010245\n","\tEpoch:29 [003/005 (0400/0693)]\tLoss Ss: 0.018197\n","\tEpoch:29 [003/005 (0420/0693)]\tLoss Ss: 0.009936\n","\tEpoch:29 [003/005 (0440/0693)]\tLoss Ss: 0.009682\n","\tEpoch:29 [003/005 (0460/0693)]\tLoss Ss: 0.013214\n","\tEpoch:29 [003/005 (0480/0693)]\tLoss Ss: 0.014248\n","\tEpoch:29 [003/005 (0500/0693)]\tLoss Ss: 0.010698\n","\tEpoch:29 [003/005 (0520/0693)]\tLoss Ss: 0.011824\n","\tEpoch:29 [003/005 (0540/0693)]\tLoss Ss: 0.018697\n","\tEpoch:29 [003/005 (0560/0693)]\tLoss Ss: 0.016286\n","\tEpoch:29 [003/005 (0580/0693)]\tLoss Ss: 0.008889\n","\tEpoch:29 [003/005 (0600/0693)]\tLoss Ss: 0.012658\n","\tEpoch:29 [003/005 (0620/0693)]\tLoss Ss: 0.017988\n","\tEpoch:29 [003/005 (0640/0693)]\tLoss Ss: 0.014826\n","\tEpoch:29 [003/005 (0660/0693)]\tLoss Ss: 0.011097\n","\tEpoch:29 [003/005 (0680/0693)]\tLoss Ss: 0.010642\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tEpoch:29 [004/005 (0000/0614)]\tLoss Ss: 0.008088\n","\tEpoch:29 [004/005 (0020/0614)]\tLoss Ss: 0.006988\n","\tEpoch:29 [004/005 (0040/0614)]\tLoss Ss: 0.005751\n","\tEpoch:29 [004/005 (0060/0614)]\tLoss Ss: 0.006441\n","\tEpoch:29 [004/005 (0080/0614)]\tLoss Ss: 0.007577\n","\tEpoch:29 [004/005 (0100/0614)]\tLoss Ss: 0.005812\n","\tEpoch:29 [004/005 (0120/0614)]\tLoss Ss: 0.007043\n","\tEpoch:29 [004/005 (0140/0614)]\tLoss Ss: 0.008194\n","\tEpoch:29 [004/005 (0160/0614)]\tLoss Ss: 0.005884\n","\tEpoch:29 [004/005 (0180/0614)]\tLoss Ss: 0.009279\n","\tEpoch:29 [004/005 (0200/0614)]\tLoss Ss: 0.007045\n","\tEpoch:29 [004/005 (0220/0614)]\tLoss Ss: 0.005920\n","\tEpoch:29 [004/005 (0240/0614)]\tLoss Ss: 0.004249\n","\tEpoch:29 [004/005 (0260/0614)]\tLoss Ss: 0.006624\n","\tEpoch:29 [004/005 (0280/0614)]\tLoss Ss: 0.006491\n","\tEpoch:29 [004/005 (0300/0614)]\tLoss Ss: 0.005909\n","\tEpoch:29 [004/005 (0320/0614)]\tLoss Ss: 0.006263\n","\tEpoch:29 [004/005 (0340/0614)]\tLoss Ss: 0.006095\n","\tEpoch:29 [004/005 (0360/0614)]\tLoss Ss: 0.006353\n","\tEpoch:29 [004/005 (0380/0614)]\tLoss Ss: 0.009686\n","\tEpoch:29 [004/005 (0400/0614)]\tLoss Ss: 0.006567\n","\tEpoch:29 [004/005 (0420/0614)]\tLoss Ss: 0.005974\n","\tEpoch:29 [004/005 (0440/0614)]\tLoss Ss: 0.005349\n","\tEpoch:29 [004/005 (0460/0614)]\tLoss Ss: 0.004782\n","\tEpoch:29 [004/005 (0480/0614)]\tLoss Ss: 0.002555\n","\tEpoch:29 [004/005 (0500/0614)]\tLoss Ss: 0.003691\n","\tEpoch:29 [004/005 (0520/0614)]\tLoss Ss: 0.003929\n","\tEpoch:29 [004/005 (0540/0614)]\tLoss Ss: 0.004019\n","\tEpoch:29 [004/005 (0560/0614)]\tLoss Ss: 0.003411\n","\tEpoch:29 [004/005 (0580/0614)]\tLoss Ss: 0.004781\n","\tEpoch:29 [004/005 (0600/0614)]\tLoss Ss: 0.005673\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:29 [005/005 (0000/0755)]\tLoss Ss: 0.012452\n","\tEpoch:29 [005/005 (0020/0755)]\tLoss Ss: 0.016497\n","\tEpoch:29 [005/005 (0040/0755)]\tLoss Ss: 0.009949\n","\tEpoch:29 [005/005 (0060/0755)]\tLoss Ss: 0.013264\n","\tEpoch:29 [005/005 (0080/0755)]\tLoss Ss: 0.013427\n","\tEpoch:29 [005/005 (0100/0755)]\tLoss Ss: 0.011600\n","\tEpoch:29 [005/005 (0120/0755)]\tLoss Ss: 0.013894\n","\tEpoch:29 [005/005 (0140/0755)]\tLoss Ss: 0.015140\n","\tEpoch:29 [005/005 (0160/0755)]\tLoss Ss: 0.010737\n","\tEpoch:29 [005/005 (0180/0755)]\tLoss Ss: 0.012728\n","\tEpoch:29 [005/005 (0200/0755)]\tLoss Ss: 0.014326\n","\tEpoch:29 [005/005 (0220/0755)]\tLoss Ss: 0.012458\n","\tEpoch:29 [005/005 (0240/0755)]\tLoss Ss: 0.010175\n","\tEpoch:29 [005/005 (0260/0755)]\tLoss Ss: 0.012399\n","\tEpoch:29 [005/005 (0280/0755)]\tLoss Ss: 0.011181\n","\tEpoch:29 [005/005 (0300/0755)]\tLoss Ss: 0.017500\n","\tEpoch:29 [005/005 (0320/0755)]\tLoss Ss: 0.008438\n","\tEpoch:29 [005/005 (0340/0755)]\tLoss Ss: 0.013218\n","\tEpoch:29 [005/005 (0360/0755)]\tLoss Ss: 0.013361\n","\tEpoch:29 [005/005 (0380/0755)]\tLoss Ss: 0.014746\n","\tEpoch:29 [005/005 (0400/0755)]\tLoss Ss: 0.018297\n","\tEpoch:29 [005/005 (0420/0755)]\tLoss Ss: 0.012375\n","\tEpoch:29 [005/005 (0440/0755)]\tLoss Ss: 0.014297\n","\tEpoch:29 [005/005 (0460/0755)]\tLoss Ss: 0.013401\n","\tEpoch:29 [005/005 (0480/0755)]\tLoss Ss: 0.010793\n","\tEpoch:29 [005/005 (0500/0755)]\tLoss Ss: 0.012877\n","\tEpoch:29 [005/005 (0520/0755)]\tLoss Ss: 0.014918\n","\tEpoch:29 [005/005 (0540/0755)]\tLoss Ss: 0.009724\n","\tEpoch:29 [005/005 (0560/0755)]\tLoss Ss: 0.014201\n","\tEpoch:29 [005/005 (0580/0755)]\tLoss Ss: 0.014103\n","\tEpoch:29 [005/005 (0600/0755)]\tLoss Ss: 0.010413\n","\tEpoch:29 [005/005 (0620/0755)]\tLoss Ss: 0.015825\n","\tEpoch:29 [005/005 (0640/0755)]\tLoss Ss: 0.012614\n","\tEpoch:29 [005/005 (0660/0755)]\tLoss Ss: 0.016167\n","\tEpoch:29 [005/005 (0680/0755)]\tLoss Ss: 0.017842\n","\tEpoch:29 [005/005 (0700/0755)]\tLoss Ss: 0.011954\n","\tEpoch:29 [005/005 (0720/0755)]\tLoss Ss: 0.017036\n","\tEpoch:29 [005/005 (0740/0755)]\tLoss Ss: 0.010964\n","Now train the rotated image\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:29 [000/005 (0000/0755)]\tLoss Ss: 0.014983\n","\tRotated_Epoch:29 [000/005 (0020/0755)]\tLoss Ss: 0.023298\n","\tRotated_Epoch:29 [000/005 (0040/0755)]\tLoss Ss: 0.021147\n","\tRotated_Epoch:29 [000/005 (0060/0755)]\tLoss Ss: 0.012805\n","\tRotated_Epoch:29 [000/005 (0080/0755)]\tLoss Ss: 0.022764\n","\tRotated_Epoch:29 [000/005 (0100/0755)]\tLoss Ss: 0.021453\n","\tRotated_Epoch:29 [000/005 (0120/0755)]\tLoss Ss: 0.011227\n","\tRotated_Epoch:29 [000/005 (0140/0755)]\tLoss Ss: 0.013129\n","\tRotated_Epoch:29 [000/005 (0160/0755)]\tLoss Ss: 0.007880\n","\tRotated_Epoch:29 [000/005 (0180/0755)]\tLoss Ss: 0.022258\n","\tRotated_Epoch:29 [000/005 (0200/0755)]\tLoss Ss: 0.014348\n","\tRotated_Epoch:29 [000/005 (0220/0755)]\tLoss Ss: 0.014191\n","\tRotated_Epoch:29 [000/005 (0240/0755)]\tLoss Ss: 0.018823\n","\tRotated_Epoch:29 [000/005 (0260/0755)]\tLoss Ss: 0.014138\n","\tRotated_Epoch:29 [000/005 (0280/0755)]\tLoss Ss: 0.016931\n","\tRotated_Epoch:29 [000/005 (0300/0755)]\tLoss Ss: 0.010321\n","\tRotated_Epoch:29 [000/005 (0320/0755)]\tLoss Ss: 0.012207\n","\tRotated_Epoch:29 [000/005 (0340/0755)]\tLoss Ss: 0.012618\n","\tRotated_Epoch:29 [000/005 (0360/0755)]\tLoss Ss: 0.017231\n","\tRotated_Epoch:29 [000/005 (0380/0755)]\tLoss Ss: 0.013314\n","\tRotated_Epoch:29 [000/005 (0400/0755)]\tLoss Ss: 0.019507\n","\tRotated_Epoch:29 [000/005 (0420/0755)]\tLoss Ss: 0.021867\n","\tRotated_Epoch:29 [000/005 (0440/0755)]\tLoss Ss: 0.018885\n","\tRotated_Epoch:29 [000/005 (0460/0755)]\tLoss Ss: 0.012813\n","\tRotated_Epoch:29 [000/005 (0480/0755)]\tLoss Ss: 0.023238\n","\tRotated_Epoch:29 [000/005 (0500/0755)]\tLoss Ss: 0.022754\n","\tRotated_Epoch:29 [000/005 (0520/0755)]\tLoss Ss: 0.012636\n","\tRotated_Epoch:29 [000/005 (0540/0755)]\tLoss Ss: 0.015884\n","\tRotated_Epoch:29 [000/005 (0560/0755)]\tLoss Ss: 0.016485\n","\tRotated_Epoch:29 [000/005 (0580/0755)]\tLoss Ss: 0.013430\n","\tRotated_Epoch:29 [000/005 (0600/0755)]\tLoss Ss: 0.020405\n","\tRotated_Epoch:29 [000/005 (0620/0755)]\tLoss Ss: 0.011878\n","\tRotated_Epoch:29 [000/005 (0640/0755)]\tLoss Ss: 0.011856\n","\tRotated_Epoch:29 [000/005 (0660/0755)]\tLoss Ss: 0.015401\n","\tRotated_Epoch:29 [000/005 (0680/0755)]\tLoss Ss: 0.012895\n","\tRotated_Epoch:29 [000/005 (0700/0755)]\tLoss Ss: 0.025041\n","\tRotated_Epoch:29 [000/005 (0720/0755)]\tLoss Ss: 0.014318\n","\tRotated_Epoch:29 [000/005 (0740/0755)]\tLoss Ss: 0.019146\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tRotated_Epoch:29 [001/005 (0000/0614)]\tLoss Ss: 0.011557\n","\tRotated_Epoch:29 [001/005 (0020/0614)]\tLoss Ss: 0.011672\n","\tRotated_Epoch:29 [001/005 (0040/0614)]\tLoss Ss: 0.012326\n","\tRotated_Epoch:29 [001/005 (0060/0614)]\tLoss Ss: 0.010029\n","\tRotated_Epoch:29 [001/005 (0080/0614)]\tLoss Ss: 0.009109\n","\tRotated_Epoch:29 [001/005 (0100/0614)]\tLoss Ss: 0.005359\n","\tRotated_Epoch:29 [001/005 (0120/0614)]\tLoss Ss: 0.010375\n","\tRotated_Epoch:29 [001/005 (0140/0614)]\tLoss Ss: 0.007589\n","\tRotated_Epoch:29 [001/005 (0160/0614)]\tLoss Ss: 0.006757\n","\tRotated_Epoch:29 [001/005 (0180/0614)]\tLoss Ss: 0.004952\n","\tRotated_Epoch:29 [001/005 (0200/0614)]\tLoss Ss: 0.006979\n","\tRotated_Epoch:29 [001/005 (0220/0614)]\tLoss Ss: 0.007713\n","\tRotated_Epoch:29 [001/005 (0240/0614)]\tLoss Ss: 0.004683\n","\tRotated_Epoch:29 [001/005 (0260/0614)]\tLoss Ss: 0.003893\n","\tRotated_Epoch:29 [001/005 (0280/0614)]\tLoss Ss: 0.005660\n","\tRotated_Epoch:29 [001/005 (0300/0614)]\tLoss Ss: 0.006281\n","\tRotated_Epoch:29 [001/005 (0320/0614)]\tLoss Ss: 0.006247\n","\tRotated_Epoch:29 [001/005 (0340/0614)]\tLoss Ss: 0.006245\n","\tRotated_Epoch:29 [001/005 (0360/0614)]\tLoss Ss: 0.007855\n","\tRotated_Epoch:29 [001/005 (0380/0614)]\tLoss Ss: 0.005258\n","\tRotated_Epoch:29 [001/005 (0400/0614)]\tLoss Ss: 0.005213\n","\tRotated_Epoch:29 [001/005 (0420/0614)]\tLoss Ss: 0.009735\n","\tRotated_Epoch:29 [001/005 (0440/0614)]\tLoss Ss: 0.007389\n","\tRotated_Epoch:29 [001/005 (0460/0614)]\tLoss Ss: 0.006162\n","\tRotated_Epoch:29 [001/005 (0480/0614)]\tLoss Ss: 0.006682\n","\tRotated_Epoch:29 [001/005 (0500/0614)]\tLoss Ss: 0.007045\n","\tRotated_Epoch:29 [001/005 (0520/0614)]\tLoss Ss: 0.005759\n","\tRotated_Epoch:29 [001/005 (0540/0614)]\tLoss Ss: 0.006912\n","\tRotated_Epoch:29 [001/005 (0560/0614)]\tLoss Ss: 0.008353\n","\tRotated_Epoch:29 [001/005 (0580/0614)]\tLoss Ss: 0.005965\n","\tRotated_Epoch:29 [001/005 (0600/0614)]\tLoss Ss: 0.007615\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tRotated_Epoch:29 [002/005 (0000/0693)]\tLoss Ss: 0.023719\n","\tRotated_Epoch:29 [002/005 (0020/0693)]\tLoss Ss: 0.021011\n","\tRotated_Epoch:29 [002/005 (0040/0693)]\tLoss Ss: 0.019241\n","\tRotated_Epoch:29 [002/005 (0060/0693)]\tLoss Ss: 0.015437\n","\tRotated_Epoch:29 [002/005 (0080/0693)]\tLoss Ss: 0.014336\n","\tRotated_Epoch:29 [002/005 (0100/0693)]\tLoss Ss: 0.015576\n","\tRotated_Epoch:29 [002/005 (0120/0693)]\tLoss Ss: 0.012008\n","\tRotated_Epoch:29 [002/005 (0140/0693)]\tLoss Ss: 0.014987\n","\tRotated_Epoch:29 [002/005 (0160/0693)]\tLoss Ss: 0.019381\n","\tRotated_Epoch:29 [002/005 (0180/0693)]\tLoss Ss: 0.017665\n","\tRotated_Epoch:29 [002/005 (0200/0693)]\tLoss Ss: 0.014906\n","\tRotated_Epoch:29 [002/005 (0220/0693)]\tLoss Ss: 0.019818\n","\tRotated_Epoch:29 [002/005 (0240/0693)]\tLoss Ss: 0.024499\n","\tRotated_Epoch:29 [002/005 (0260/0693)]\tLoss Ss: 0.010504\n","\tRotated_Epoch:29 [002/005 (0280/0693)]\tLoss Ss: 0.018693\n","\tRotated_Epoch:29 [002/005 (0300/0693)]\tLoss Ss: 0.021073\n","\tRotated_Epoch:29 [002/005 (0320/0693)]\tLoss Ss: 0.013407\n","\tRotated_Epoch:29 [002/005 (0340/0693)]\tLoss Ss: 0.014581\n","\tRotated_Epoch:29 [002/005 (0360/0693)]\tLoss Ss: 0.016890\n","\tRotated_Epoch:29 [002/005 (0380/0693)]\tLoss Ss: 0.014013\n","\tRotated_Epoch:29 [002/005 (0400/0693)]\tLoss Ss: 0.019692\n","\tRotated_Epoch:29 [002/005 (0420/0693)]\tLoss Ss: 0.017398\n","\tRotated_Epoch:29 [002/005 (0440/0693)]\tLoss Ss: 0.012881\n","\tRotated_Epoch:29 [002/005 (0460/0693)]\tLoss Ss: 0.011946\n","\tRotated_Epoch:29 [002/005 (0480/0693)]\tLoss Ss: 0.024696\n","\tRotated_Epoch:29 [002/005 (0500/0693)]\tLoss Ss: 0.011470\n","\tRotated_Epoch:29 [002/005 (0520/0693)]\tLoss Ss: 0.016683\n","\tRotated_Epoch:29 [002/005 (0540/0693)]\tLoss Ss: 0.013474\n","\tRotated_Epoch:29 [002/005 (0560/0693)]\tLoss Ss: 0.007746\n","\tRotated_Epoch:29 [002/005 (0580/0693)]\tLoss Ss: 0.017033\n","\tRotated_Epoch:29 [002/005 (0600/0693)]\tLoss Ss: 0.015662\n","\tRotated_Epoch:29 [002/005 (0620/0693)]\tLoss Ss: 0.011391\n","\tRotated_Epoch:29 [002/005 (0640/0693)]\tLoss Ss: 0.012179\n","\tRotated_Epoch:29 [002/005 (0660/0693)]\tLoss Ss: 0.015491\n","\tRotated_Epoch:29 [002/005 (0680/0693)]\tLoss Ss: 0.011108\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:29 [003/005 (0000/0755)]\tLoss Ss: 0.263693\n","\tRotated_Epoch:29 [003/005 (0020/0755)]\tLoss Ss: 0.194895\n","\tRotated_Epoch:29 [003/005 (0040/0755)]\tLoss Ss: 0.203742\n","\tRotated_Epoch:29 [003/005 (0060/0755)]\tLoss Ss: 0.192200\n","\tRotated_Epoch:29 [003/005 (0080/0755)]\tLoss Ss: 0.133859\n","\tRotated_Epoch:29 [003/005 (0100/0755)]\tLoss Ss: 0.152429\n","\tRotated_Epoch:29 [003/005 (0120/0755)]\tLoss Ss: 0.142546\n","\tRotated_Epoch:29 [003/005 (0140/0755)]\tLoss Ss: 0.067693\n","\tRotated_Epoch:29 [003/005 (0160/0755)]\tLoss Ss: 0.131187\n","\tRotated_Epoch:29 [003/005 (0180/0755)]\tLoss Ss: 0.102585\n","\tRotated_Epoch:29 [003/005 (0200/0755)]\tLoss Ss: 0.056462\n","\tRotated_Epoch:29 [003/005 (0220/0755)]\tLoss Ss: 0.052319\n","\tRotated_Epoch:29 [003/005 (0240/0755)]\tLoss Ss: 0.072165\n","\tRotated_Epoch:29 [003/005 (0260/0755)]\tLoss Ss: 0.080144\n","\tRotated_Epoch:29 [003/005 (0280/0755)]\tLoss Ss: 0.081439\n","\tRotated_Epoch:29 [003/005 (0300/0755)]\tLoss Ss: 0.064847\n","\tRotated_Epoch:29 [003/005 (0320/0755)]\tLoss Ss: 0.065565\n","\tRotated_Epoch:29 [003/005 (0340/0755)]\tLoss Ss: 0.066690\n","\tRotated_Epoch:29 [003/005 (0360/0755)]\tLoss Ss: 0.063162\n","\tRotated_Epoch:29 [003/005 (0380/0755)]\tLoss Ss: 0.055334\n","\tRotated_Epoch:29 [003/005 (0400/0755)]\tLoss Ss: 0.077602\n","\tRotated_Epoch:29 [003/005 (0420/0755)]\tLoss Ss: 0.053119\n","\tRotated_Epoch:29 [003/005 (0440/0755)]\tLoss Ss: 0.055791\n","\tRotated_Epoch:29 [003/005 (0460/0755)]\tLoss Ss: 0.070924\n","\tRotated_Epoch:29 [003/005 (0480/0755)]\tLoss Ss: 0.056318\n","\tRotated_Epoch:29 [003/005 (0500/0755)]\tLoss Ss: 0.046206\n","\tRotated_Epoch:29 [003/005 (0520/0755)]\tLoss Ss: 0.036649\n","\tRotated_Epoch:29 [003/005 (0540/0755)]\tLoss Ss: 0.027658\n","\tRotated_Epoch:29 [003/005 (0560/0755)]\tLoss Ss: 0.048143\n","\tRotated_Epoch:29 [003/005 (0580/0755)]\tLoss Ss: 0.067642\n","\tRotated_Epoch:29 [003/005 (0600/0755)]\tLoss Ss: 0.056937\n","\tRotated_Epoch:29 [003/005 (0620/0755)]\tLoss Ss: 0.039242\n","\tRotated_Epoch:29 [003/005 (0640/0755)]\tLoss Ss: 0.031008\n","\tRotated_Epoch:29 [003/005 (0660/0755)]\tLoss Ss: 0.051989\n","\tRotated_Epoch:29 [003/005 (0680/0755)]\tLoss Ss: 0.040895\n","\tRotated_Epoch:29 [003/005 (0700/0755)]\tLoss Ss: 0.049602\n","\tRotated_Epoch:29 [003/005 (0720/0755)]\tLoss Ss: 0.041865\n","\tRotated_Epoch:29 [003/005 (0740/0755)]\tLoss Ss: 0.051860\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tRotated_Epoch:29 [004/005 (0000/0588)]\tLoss Ss: 0.071704\n","\tRotated_Epoch:29 [004/005 (0020/0588)]\tLoss Ss: 0.059475\n","\tRotated_Epoch:29 [004/005 (0040/0588)]\tLoss Ss: 0.099158\n","\tRotated_Epoch:29 [004/005 (0060/0588)]\tLoss Ss: 0.049794\n","\tRotated_Epoch:29 [004/005 (0080/0588)]\tLoss Ss: 0.050337\n","\tRotated_Epoch:29 [004/005 (0100/0588)]\tLoss Ss: 0.079220\n","\tRotated_Epoch:29 [004/005 (0120/0588)]\tLoss Ss: 0.053864\n","\tRotated_Epoch:29 [004/005 (0140/0588)]\tLoss Ss: 0.060897\n","\tRotated_Epoch:29 [004/005 (0160/0588)]\tLoss Ss: 0.045811\n","\tRotated_Epoch:29 [004/005 (0180/0588)]\tLoss Ss: 0.050669\n","\tRotated_Epoch:29 [004/005 (0200/0588)]\tLoss Ss: 0.058349\n","\tRotated_Epoch:29 [004/005 (0220/0588)]\tLoss Ss: 0.067539\n","\tRotated_Epoch:29 [004/005 (0240/0588)]\tLoss Ss: 0.077622\n","\tRotated_Epoch:29 [004/005 (0260/0588)]\tLoss Ss: 0.052606\n","\tRotated_Epoch:29 [004/005 (0280/0588)]\tLoss Ss: 0.048817\n","\tRotated_Epoch:29 [004/005 (0300/0588)]\tLoss Ss: 0.053265\n","\tRotated_Epoch:29 [004/005 (0320/0588)]\tLoss Ss: 0.046273\n","\tRotated_Epoch:29 [004/005 (0340/0588)]\tLoss Ss: 0.046306\n","\tRotated_Epoch:29 [004/005 (0360/0588)]\tLoss Ss: 0.043991\n","\tRotated_Epoch:29 [004/005 (0380/0588)]\tLoss Ss: 0.063532\n","\tRotated_Epoch:29 [004/005 (0400/0588)]\tLoss Ss: 0.057978\n","\tRotated_Epoch:29 [004/005 (0420/0588)]\tLoss Ss: 0.060203\n","\tRotated_Epoch:29 [004/005 (0440/0588)]\tLoss Ss: 0.053218\n","\tRotated_Epoch:29 [004/005 (0460/0588)]\tLoss Ss: 0.053060\n","\tRotated_Epoch:29 [004/005 (0480/0588)]\tLoss Ss: 0.046869\n","\tRotated_Epoch:29 [004/005 (0500/0588)]\tLoss Ss: 0.045905\n","\tRotated_Epoch:29 [004/005 (0520/0588)]\tLoss Ss: 0.063315\n","\tRotated_Epoch:29 [004/005 (0540/0588)]\tLoss Ss: 0.045774\n","\tRotated_Epoch:29 [004/005 (0560/0588)]\tLoss Ss: 0.051703\n","\tRotated_Epoch:29 [004/005 (0580/0588)]\tLoss Ss: 0.049792\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tRotated_Epoch:29 [005/005 (0000/0693)]\tLoss Ss: 0.055929\n","\tRotated_Epoch:29 [005/005 (0020/0693)]\tLoss Ss: 0.042295\n","\tRotated_Epoch:29 [005/005 (0040/0693)]\tLoss Ss: 0.025115\n","\tRotated_Epoch:29 [005/005 (0060/0693)]\tLoss Ss: 0.027932\n","\tRotated_Epoch:29 [005/005 (0080/0693)]\tLoss Ss: 0.029216\n","\tRotated_Epoch:29 [005/005 (0100/0693)]\tLoss Ss: 0.027840\n","\tRotated_Epoch:29 [005/005 (0120/0693)]\tLoss Ss: 0.019381\n","\tRotated_Epoch:29 [005/005 (0140/0693)]\tLoss Ss: 0.018396\n","\tRotated_Epoch:29 [005/005 (0160/0693)]\tLoss Ss: 0.012973\n","\tRotated_Epoch:29 [005/005 (0180/0693)]\tLoss Ss: 0.021549\n","\tRotated_Epoch:29 [005/005 (0200/0693)]\tLoss Ss: 0.025873\n","\tRotated_Epoch:29 [005/005 (0220/0693)]\tLoss Ss: 0.023922\n","\tRotated_Epoch:29 [005/005 (0240/0693)]\tLoss Ss: 0.022758\n","\tRotated_Epoch:29 [005/005 (0260/0693)]\tLoss Ss: 0.022759\n","\tRotated_Epoch:29 [005/005 (0280/0693)]\tLoss Ss: 0.016300\n","\tRotated_Epoch:29 [005/005 (0300/0693)]\tLoss Ss: 0.015066\n","\tRotated_Epoch:29 [005/005 (0320/0693)]\tLoss Ss: 0.021855\n","\tRotated_Epoch:29 [005/005 (0340/0693)]\tLoss Ss: 0.017633\n","\tRotated_Epoch:29 [005/005 (0360/0693)]\tLoss Ss: 0.015917\n","\tRotated_Epoch:29 [005/005 (0380/0693)]\tLoss Ss: 0.015366\n","\tRotated_Epoch:29 [005/005 (0400/0693)]\tLoss Ss: 0.012067\n","\tRotated_Epoch:29 [005/005 (0420/0693)]\tLoss Ss: 0.014688\n","\tRotated_Epoch:29 [005/005 (0440/0693)]\tLoss Ss: 0.022407\n","\tRotated_Epoch:29 [005/005 (0460/0693)]\tLoss Ss: 0.016004\n","\tRotated_Epoch:29 [005/005 (0480/0693)]\tLoss Ss: 0.018121\n","\tRotated_Epoch:29 [005/005 (0500/0693)]\tLoss Ss: 0.015988\n","\tRotated_Epoch:29 [005/005 (0520/0693)]\tLoss Ss: 0.014015\n","\tRotated_Epoch:29 [005/005 (0540/0693)]\tLoss Ss: 0.015842\n","\tRotated_Epoch:29 [005/005 (0560/0693)]\tLoss Ss: 0.014141\n","\tRotated_Epoch:29 [005/005 (0580/0693)]\tLoss Ss: 0.013583\n","\tRotated_Epoch:29 [005/005 (0600/0693)]\tLoss Ss: 0.013706\n","\tRotated_Epoch:29 [005/005 (0620/0693)]\tLoss Ss: 0.013702\n","\tRotated_Epoch:29 [005/005 (0640/0693)]\tLoss Ss: 0.015624\n","\tRotated_Epoch:29 [005/005 (0660/0693)]\tLoss Ss: 0.019362\n","\tRotated_Epoch:29 [005/005 (0680/0693)]\tLoss Ss: 0.016622\n","new_1_6_d.nii\n","new_1_6_m.nii\n","new_1_7_d.nii\n","new_1_7_m.nii\n","new_2_5_d.nii\n","new_2_5_m.nii\n","new_2_6_d.nii\n","new_2_6_m.nii\n","new_2_7_d.nii\n","new_2_7_m.nii\n","new_3_d (2).nii\n","new_3_m (2).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (3).nii\n","new_3_m (3).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (7).nii\n","new_3_m (7).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (8).nii\n","new_3_m (8).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (9).nii\n","new_3_m (9).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_4_10_d_163526.nii\n","new_4_10_m_163526.nii.gz\n","new_4_11_d_084336.nii\n","new_4_11_m_084336.nii.gz\n","new_4_7_d_184422.nii\n","new_4_7_m_184422.nii.gz\n","new_4_8_d_143412.nii\n","new_4_8_m_143412.nii.gz\n","new_4_9_d_155347.nii\n","new_4_9_m_155347.nii.gz\n","\tEpoch: 29; Dice: 0.9622 +/- 0.0041; Loss: 9.6435\n","Begin Epoch 30\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tEpoch:30 [000/005 (0000/0693)]\tLoss Ss: 0.016616\n","\tEpoch:30 [000/005 (0020/0693)]\tLoss Ss: 0.010485\n","\tEpoch:30 [000/005 (0040/0693)]\tLoss Ss: 0.017851\n","\tEpoch:30 [000/005 (0060/0693)]\tLoss Ss: 0.017485\n","\tEpoch:30 [000/005 (0080/0693)]\tLoss Ss: 0.019014\n","\tEpoch:30 [000/005 (0100/0693)]\tLoss Ss: 0.025551\n","\tEpoch:30 [000/005 (0120/0693)]\tLoss Ss: 0.016583\n","\tEpoch:30 [000/005 (0140/0693)]\tLoss Ss: 0.012497\n","\tEpoch:30 [000/005 (0160/0693)]\tLoss Ss: 0.013252\n","\tEpoch:30 [000/005 (0180/0693)]\tLoss Ss: 0.010063\n","\tEpoch:30 [000/005 (0200/0693)]\tLoss Ss: 0.018537\n","\tEpoch:30 [000/005 (0220/0693)]\tLoss Ss: 0.012565\n","\tEpoch:30 [000/005 (0240/0693)]\tLoss Ss: 0.014750\n","\tEpoch:30 [000/005 (0260/0693)]\tLoss Ss: 0.017345\n","\tEpoch:30 [000/005 (0280/0693)]\tLoss Ss: 0.014625\n","\tEpoch:30 [000/005 (0300/0693)]\tLoss Ss: 0.016541\n","\tEpoch:30 [000/005 (0320/0693)]\tLoss Ss: 0.015484\n","\tEpoch:30 [000/005 (0340/0693)]\tLoss Ss: 0.010323\n","\tEpoch:30 [000/005 (0360/0693)]\tLoss Ss: 0.018153\n","\tEpoch:30 [000/005 (0380/0693)]\tLoss Ss: 0.016000\n","\tEpoch:30 [000/005 (0400/0693)]\tLoss Ss: 0.018160\n","\tEpoch:30 [000/005 (0420/0693)]\tLoss Ss: 0.015431\n","\tEpoch:30 [000/005 (0440/0693)]\tLoss Ss: 0.012321\n","\tEpoch:30 [000/005 (0460/0693)]\tLoss Ss: 0.009774\n","\tEpoch:30 [000/005 (0480/0693)]\tLoss Ss: 0.014505\n","\tEpoch:30 [000/005 (0500/0693)]\tLoss Ss: 0.018101\n","\tEpoch:30 [000/005 (0520/0693)]\tLoss Ss: 0.012506\n","\tEpoch:30 [000/005 (0540/0693)]\tLoss Ss: 0.011146\n","\tEpoch:30 [000/005 (0560/0693)]\tLoss Ss: 0.014299\n","\tEpoch:30 [000/005 (0580/0693)]\tLoss Ss: 0.016086\n","\tEpoch:30 [000/005 (0600/0693)]\tLoss Ss: 0.009311\n","\tEpoch:30 [000/005 (0620/0693)]\tLoss Ss: 0.010772\n","\tEpoch:30 [000/005 (0640/0693)]\tLoss Ss: 0.019148\n","\tEpoch:30 [000/005 (0660/0693)]\tLoss Ss: 0.013606\n","\tEpoch:30 [000/005 (0680/0693)]\tLoss Ss: 0.013951\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tEpoch:30 [001/005 (0000/0588)]\tLoss Ss: 0.006337\n","\tEpoch:30 [001/005 (0020/0588)]\tLoss Ss: 0.010137\n","\tEpoch:30 [001/005 (0040/0588)]\tLoss Ss: 0.008459\n","\tEpoch:30 [001/005 (0060/0588)]\tLoss Ss: 0.006050\n","\tEpoch:30 [001/005 (0080/0588)]\tLoss Ss: 0.011505\n","\tEpoch:30 [001/005 (0100/0588)]\tLoss Ss: 0.006411\n","\tEpoch:30 [001/005 (0120/0588)]\tLoss Ss: 0.005388\n","\tEpoch:30 [001/005 (0140/0588)]\tLoss Ss: 0.009200\n","\tEpoch:30 [001/005 (0160/0588)]\tLoss Ss: 0.006032\n","\tEpoch:30 [001/005 (0180/0588)]\tLoss Ss: 0.007020\n","\tEpoch:30 [001/005 (0200/0588)]\tLoss Ss: 0.007660\n","\tEpoch:30 [001/005 (0220/0588)]\tLoss Ss: 0.003623\n","\tEpoch:30 [001/005 (0240/0588)]\tLoss Ss: 0.005568\n","\tEpoch:30 [001/005 (0260/0588)]\tLoss Ss: 0.006065\n","\tEpoch:30 [001/005 (0280/0588)]\tLoss Ss: 0.005489\n","\tEpoch:30 [001/005 (0300/0588)]\tLoss Ss: 0.005634\n","\tEpoch:30 [001/005 (0320/0588)]\tLoss Ss: 0.009429\n","\tEpoch:30 [001/005 (0340/0588)]\tLoss Ss: 0.009748\n","\tEpoch:30 [001/005 (0360/0588)]\tLoss Ss: 0.006430\n","\tEpoch:30 [001/005 (0380/0588)]\tLoss Ss: 0.004731\n","\tEpoch:30 [001/005 (0400/0588)]\tLoss Ss: 0.004371\n","\tEpoch:30 [001/005 (0420/0588)]\tLoss Ss: 0.008702\n","\tEpoch:30 [001/005 (0440/0588)]\tLoss Ss: 0.004430\n","\tEpoch:30 [001/005 (0460/0588)]\tLoss Ss: 0.005586\n","\tEpoch:30 [001/005 (0480/0588)]\tLoss Ss: 0.004586\n","\tEpoch:30 [001/005 (0500/0588)]\tLoss Ss: 0.007958\n","\tEpoch:30 [001/005 (0520/0588)]\tLoss Ss: 0.005086\n","\tEpoch:30 [001/005 (0540/0588)]\tLoss Ss: 0.005283\n","\tEpoch:30 [001/005 (0560/0588)]\tLoss Ss: 0.004626\n","\tEpoch:30 [001/005 (0580/0588)]\tLoss Ss: 0.004472\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:30 [002/005 (0000/0755)]\tLoss Ss: 0.027331\n","\tEpoch:30 [002/005 (0020/0755)]\tLoss Ss: 0.030576\n","\tEpoch:30 [002/005 (0040/0755)]\tLoss Ss: 0.024075\n","\tEpoch:30 [002/005 (0060/0755)]\tLoss Ss: 0.023557\n","\tEpoch:30 [002/005 (0080/0755)]\tLoss Ss: 0.024917\n","\tEpoch:30 [002/005 (0100/0755)]\tLoss Ss: 0.016020\n","\tEpoch:30 [002/005 (0120/0755)]\tLoss Ss: 0.023151\n","\tEpoch:30 [002/005 (0140/0755)]\tLoss Ss: 0.021501\n","\tEpoch:30 [002/005 (0160/0755)]\tLoss Ss: 0.021021\n","\tEpoch:30 [002/005 (0180/0755)]\tLoss Ss: 0.018881\n","\tEpoch:30 [002/005 (0200/0755)]\tLoss Ss: 0.023935\n","\tEpoch:30 [002/005 (0220/0755)]\tLoss Ss: 0.017639\n","\tEpoch:30 [002/005 (0240/0755)]\tLoss Ss: 0.017267\n","\tEpoch:30 [002/005 (0260/0755)]\tLoss Ss: 0.026595\n","\tEpoch:30 [002/005 (0280/0755)]\tLoss Ss: 0.014364\n","\tEpoch:30 [002/005 (0300/0755)]\tLoss Ss: 0.018153\n","\tEpoch:30 [002/005 (0320/0755)]\tLoss Ss: 0.014779\n","\tEpoch:30 [002/005 (0340/0755)]\tLoss Ss: 0.014988\n","\tEpoch:30 [002/005 (0360/0755)]\tLoss Ss: 0.013712\n","\tEpoch:30 [002/005 (0380/0755)]\tLoss Ss: 0.017618\n","\tEpoch:30 [002/005 (0400/0755)]\tLoss Ss: 0.018962\n","\tEpoch:30 [002/005 (0420/0755)]\tLoss Ss: 0.020854\n","\tEpoch:30 [002/005 (0440/0755)]\tLoss Ss: 0.011598\n","\tEpoch:30 [002/005 (0460/0755)]\tLoss Ss: 0.019209\n","\tEpoch:30 [002/005 (0480/0755)]\tLoss Ss: 0.022186\n","\tEpoch:30 [002/005 (0500/0755)]\tLoss Ss: 0.014369\n","\tEpoch:30 [002/005 (0520/0755)]\tLoss Ss: 0.019983\n","\tEpoch:30 [002/005 (0540/0755)]\tLoss Ss: 0.015170\n","\tEpoch:30 [002/005 (0560/0755)]\tLoss Ss: 0.016163\n","\tEpoch:30 [002/005 (0580/0755)]\tLoss Ss: 0.012126\n","\tEpoch:30 [002/005 (0600/0755)]\tLoss Ss: 0.015094\n","\tEpoch:30 [002/005 (0620/0755)]\tLoss Ss: 0.014895\n","\tEpoch:30 [002/005 (0640/0755)]\tLoss Ss: 0.015368\n","\tEpoch:30 [002/005 (0660/0755)]\tLoss Ss: 0.018241\n","\tEpoch:30 [002/005 (0680/0755)]\tLoss Ss: 0.013111\n","\tEpoch:30 [002/005 (0700/0755)]\tLoss Ss: 0.019422\n","\tEpoch:30 [002/005 (0720/0755)]\tLoss Ss: 0.010776\n","\tEpoch:30 [002/005 (0740/0755)]\tLoss Ss: 0.018185\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tEpoch:30 [003/005 (0000/0693)]\tLoss Ss: 0.018616\n","\tEpoch:30 [003/005 (0020/0693)]\tLoss Ss: 0.015025\n","\tEpoch:30 [003/005 (0040/0693)]\tLoss Ss: 0.017458\n","\tEpoch:30 [003/005 (0060/0693)]\tLoss Ss: 0.018297\n","\tEpoch:30 [003/005 (0080/0693)]\tLoss Ss: 0.016018\n","\tEpoch:30 [003/005 (0100/0693)]\tLoss Ss: 0.015913\n","\tEpoch:30 [003/005 (0120/0693)]\tLoss Ss: 0.010546\n","\tEpoch:30 [003/005 (0140/0693)]\tLoss Ss: 0.016457\n","\tEpoch:30 [003/005 (0160/0693)]\tLoss Ss: 0.008505\n","\tEpoch:30 [003/005 (0180/0693)]\tLoss Ss: 0.011242\n","\tEpoch:30 [003/005 (0200/0693)]\tLoss Ss: 0.015061\n","\tEpoch:30 [003/005 (0220/0693)]\tLoss Ss: 0.010292\n","\tEpoch:30 [003/005 (0240/0693)]\tLoss Ss: 0.010439\n","\tEpoch:30 [003/005 (0260/0693)]\tLoss Ss: 0.010490\n","\tEpoch:30 [003/005 (0280/0693)]\tLoss Ss: 0.016371\n","\tEpoch:30 [003/005 (0300/0693)]\tLoss Ss: 0.010636\n","\tEpoch:30 [003/005 (0320/0693)]\tLoss Ss: 0.007721\n","\tEpoch:30 [003/005 (0340/0693)]\tLoss Ss: 0.019862\n","\tEpoch:30 [003/005 (0360/0693)]\tLoss Ss: 0.008926\n","\tEpoch:30 [003/005 (0380/0693)]\tLoss Ss: 0.011525\n","\tEpoch:30 [003/005 (0400/0693)]\tLoss Ss: 0.010248\n","\tEpoch:30 [003/005 (0420/0693)]\tLoss Ss: 0.009374\n","\tEpoch:30 [003/005 (0440/0693)]\tLoss Ss: 0.015406\n","\tEpoch:30 [003/005 (0460/0693)]\tLoss Ss: 0.015119\n","\tEpoch:30 [003/005 (0480/0693)]\tLoss Ss: 0.013090\n","\tEpoch:30 [003/005 (0500/0693)]\tLoss Ss: 0.012571\n","\tEpoch:30 [003/005 (0520/0693)]\tLoss Ss: 0.013182\n","\tEpoch:30 [003/005 (0540/0693)]\tLoss Ss: 0.014872\n","\tEpoch:30 [003/005 (0560/0693)]\tLoss Ss: 0.010031\n","\tEpoch:30 [003/005 (0580/0693)]\tLoss Ss: 0.011345\n","\tEpoch:30 [003/005 (0600/0693)]\tLoss Ss: 0.008627\n","\tEpoch:30 [003/005 (0620/0693)]\tLoss Ss: 0.012711\n","\tEpoch:30 [003/005 (0640/0693)]\tLoss Ss: 0.013864\n","\tEpoch:30 [003/005 (0660/0693)]\tLoss Ss: 0.012734\n","\tEpoch:30 [003/005 (0680/0693)]\tLoss Ss: 0.010248\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tEpoch:30 [004/005 (0000/0614)]\tLoss Ss: 0.006790\n","\tEpoch:30 [004/005 (0020/0614)]\tLoss Ss: 0.006347\n","\tEpoch:30 [004/005 (0040/0614)]\tLoss Ss: 0.005205\n","\tEpoch:30 [004/005 (0060/0614)]\tLoss Ss: 0.004002\n","\tEpoch:30 [004/005 (0080/0614)]\tLoss Ss: 0.006314\n","\tEpoch:30 [004/005 (0100/0614)]\tLoss Ss: 0.004345\n","\tEpoch:30 [004/005 (0120/0614)]\tLoss Ss: 0.004588\n","\tEpoch:30 [004/005 (0140/0614)]\tLoss Ss: 0.006048\n","\tEpoch:30 [004/005 (0160/0614)]\tLoss Ss: 0.007469\n","\tEpoch:30 [004/005 (0180/0614)]\tLoss Ss: 0.004155\n","\tEpoch:30 [004/005 (0200/0614)]\tLoss Ss: 0.005657\n","\tEpoch:30 [004/005 (0220/0614)]\tLoss Ss: 0.006052\n","\tEpoch:30 [004/005 (0240/0614)]\tLoss Ss: 0.005804\n","\tEpoch:30 [004/005 (0260/0614)]\tLoss Ss: 0.006835\n","\tEpoch:30 [004/005 (0280/0614)]\tLoss Ss: 0.003906\n","\tEpoch:30 [004/005 (0300/0614)]\tLoss Ss: 0.003867\n","\tEpoch:30 [004/005 (0320/0614)]\tLoss Ss: 0.007697\n","\tEpoch:30 [004/005 (0340/0614)]\tLoss Ss: 0.003746\n","\tEpoch:30 [004/005 (0360/0614)]\tLoss Ss: 0.006968\n","\tEpoch:30 [004/005 (0380/0614)]\tLoss Ss: 0.006195\n","\tEpoch:30 [004/005 (0400/0614)]\tLoss Ss: 0.006302\n","\tEpoch:30 [004/005 (0420/0614)]\tLoss Ss: 0.006872\n","\tEpoch:30 [004/005 (0440/0614)]\tLoss Ss: 0.006422\n","\tEpoch:30 [004/005 (0460/0614)]\tLoss Ss: 0.005267\n","\tEpoch:30 [004/005 (0480/0614)]\tLoss Ss: 0.007212\n","\tEpoch:30 [004/005 (0500/0614)]\tLoss Ss: 0.002791\n","\tEpoch:30 [004/005 (0520/0614)]\tLoss Ss: 0.004376\n","\tEpoch:30 [004/005 (0540/0614)]\tLoss Ss: 0.004483\n","\tEpoch:30 [004/005 (0560/0614)]\tLoss Ss: 0.005928\n","\tEpoch:30 [004/005 (0580/0614)]\tLoss Ss: 0.006792\n","\tEpoch:30 [004/005 (0600/0614)]\tLoss Ss: 0.007166\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:30 [005/005 (0000/0755)]\tLoss Ss: 0.013641\n","\tEpoch:30 [005/005 (0020/0755)]\tLoss Ss: 0.015752\n","\tEpoch:30 [005/005 (0040/0755)]\tLoss Ss: 0.019722\n","\tEpoch:30 [005/005 (0060/0755)]\tLoss Ss: 0.011457\n","\tEpoch:30 [005/005 (0080/0755)]\tLoss Ss: 0.014474\n","\tEpoch:30 [005/005 (0100/0755)]\tLoss Ss: 0.019270\n","\tEpoch:30 [005/005 (0120/0755)]\tLoss Ss: 0.012901\n","\tEpoch:30 [005/005 (0140/0755)]\tLoss Ss: 0.013788\n","\tEpoch:30 [005/005 (0160/0755)]\tLoss Ss: 0.017047\n","\tEpoch:30 [005/005 (0180/0755)]\tLoss Ss: 0.010944\n","\tEpoch:30 [005/005 (0200/0755)]\tLoss Ss: 0.015158\n","\tEpoch:30 [005/005 (0220/0755)]\tLoss Ss: 0.016478\n","\tEpoch:30 [005/005 (0240/0755)]\tLoss Ss: 0.015505\n","\tEpoch:30 [005/005 (0260/0755)]\tLoss Ss: 0.015806\n","\tEpoch:30 [005/005 (0280/0755)]\tLoss Ss: 0.010119\n","\tEpoch:30 [005/005 (0300/0755)]\tLoss Ss: 0.008779\n","\tEpoch:30 [005/005 (0320/0755)]\tLoss Ss: 0.010185\n","\tEpoch:30 [005/005 (0340/0755)]\tLoss Ss: 0.015499\n","\tEpoch:30 [005/005 (0360/0755)]\tLoss Ss: 0.008859\n","\tEpoch:30 [005/005 (0380/0755)]\tLoss Ss: 0.014619\n","\tEpoch:30 [005/005 (0400/0755)]\tLoss Ss: 0.010469\n","\tEpoch:30 [005/005 (0420/0755)]\tLoss Ss: 0.015850\n","\tEpoch:30 [005/005 (0440/0755)]\tLoss Ss: 0.010823\n","\tEpoch:30 [005/005 (0460/0755)]\tLoss Ss: 0.004992\n","\tEpoch:30 [005/005 (0480/0755)]\tLoss Ss: 0.012474\n","\tEpoch:30 [005/005 (0500/0755)]\tLoss Ss: 0.013957\n","\tEpoch:30 [005/005 (0520/0755)]\tLoss Ss: 0.013065\n","\tEpoch:30 [005/005 (0540/0755)]\tLoss Ss: 0.007484\n","\tEpoch:30 [005/005 (0560/0755)]\tLoss Ss: 0.013007\n","\tEpoch:30 [005/005 (0580/0755)]\tLoss Ss: 0.015282\n","\tEpoch:30 [005/005 (0600/0755)]\tLoss Ss: 0.009626\n","\tEpoch:30 [005/005 (0620/0755)]\tLoss Ss: 0.014603\n","\tEpoch:30 [005/005 (0640/0755)]\tLoss Ss: 0.012023\n","\tEpoch:30 [005/005 (0660/0755)]\tLoss Ss: 0.011404\n","\tEpoch:30 [005/005 (0680/0755)]\tLoss Ss: 0.011196\n","\tEpoch:30 [005/005 (0700/0755)]\tLoss Ss: 0.015567\n","\tEpoch:30 [005/005 (0720/0755)]\tLoss Ss: 0.013036\n","\tEpoch:30 [005/005 (0740/0755)]\tLoss Ss: 0.012914\n","Now train the rotated image\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tRotated_Epoch:30 [000/005 (0000/0693)]\tLoss Ss: 0.017068\n","\tRotated_Epoch:30 [000/005 (0020/0693)]\tLoss Ss: 0.020634\n","\tRotated_Epoch:30 [000/005 (0040/0693)]\tLoss Ss: 0.015645\n","\tRotated_Epoch:30 [000/005 (0060/0693)]\tLoss Ss: 0.018800\n","\tRotated_Epoch:30 [000/005 (0080/0693)]\tLoss Ss: 0.016338\n","\tRotated_Epoch:30 [000/005 (0100/0693)]\tLoss Ss: 0.022481\n","\tRotated_Epoch:30 [000/005 (0120/0693)]\tLoss Ss: 0.017397\n","\tRotated_Epoch:30 [000/005 (0140/0693)]\tLoss Ss: 0.011364\n","\tRotated_Epoch:30 [000/005 (0160/0693)]\tLoss Ss: 0.014718\n","\tRotated_Epoch:30 [000/005 (0180/0693)]\tLoss Ss: 0.010281\n","\tRotated_Epoch:30 [000/005 (0200/0693)]\tLoss Ss: 0.010353\n","\tRotated_Epoch:30 [000/005 (0220/0693)]\tLoss Ss: 0.015125\n","\tRotated_Epoch:30 [000/005 (0240/0693)]\tLoss Ss: 0.011113\n","\tRotated_Epoch:30 [000/005 (0260/0693)]\tLoss Ss: 0.014475\n","\tRotated_Epoch:30 [000/005 (0280/0693)]\tLoss Ss: 0.015566\n","\tRotated_Epoch:30 [000/005 (0300/0693)]\tLoss Ss: 0.013340\n","\tRotated_Epoch:30 [000/005 (0320/0693)]\tLoss Ss: 0.013390\n","\tRotated_Epoch:30 [000/005 (0340/0693)]\tLoss Ss: 0.016157\n","\tRotated_Epoch:30 [000/005 (0360/0693)]\tLoss Ss: 0.014952\n","\tRotated_Epoch:30 [000/005 (0380/0693)]\tLoss Ss: 0.009215\n","\tRotated_Epoch:30 [000/005 (0400/0693)]\tLoss Ss: 0.013590\n","\tRotated_Epoch:30 [000/005 (0420/0693)]\tLoss Ss: 0.016357\n","\tRotated_Epoch:30 [000/005 (0440/0693)]\tLoss Ss: 0.013608\n","\tRotated_Epoch:30 [000/005 (0460/0693)]\tLoss Ss: 0.010876\n","\tRotated_Epoch:30 [000/005 (0480/0693)]\tLoss Ss: 0.015925\n","\tRotated_Epoch:30 [000/005 (0500/0693)]\tLoss Ss: 0.018243\n","\tRotated_Epoch:30 [000/005 (0520/0693)]\tLoss Ss: 0.009976\n","\tRotated_Epoch:30 [000/005 (0540/0693)]\tLoss Ss: 0.014491\n","\tRotated_Epoch:30 [000/005 (0560/0693)]\tLoss Ss: 0.011542\n","\tRotated_Epoch:30 [000/005 (0580/0693)]\tLoss Ss: 0.007251\n","\tRotated_Epoch:30 [000/005 (0600/0693)]\tLoss Ss: 0.013852\n","\tRotated_Epoch:30 [000/005 (0620/0693)]\tLoss Ss: 0.011030\n","\tRotated_Epoch:30 [000/005 (0640/0693)]\tLoss Ss: 0.011843\n","\tRotated_Epoch:30 [000/005 (0660/0693)]\tLoss Ss: 0.013100\n","\tRotated_Epoch:30 [000/005 (0680/0693)]\tLoss Ss: 0.010122\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tRotated_Epoch:30 [001/005 (0000/0614)]\tLoss Ss: 0.007836\n","\tRotated_Epoch:30 [001/005 (0020/0614)]\tLoss Ss: 0.007775\n","\tRotated_Epoch:30 [001/005 (0040/0614)]\tLoss Ss: 0.006119\n","\tRotated_Epoch:30 [001/005 (0060/0614)]\tLoss Ss: 0.005182\n","\tRotated_Epoch:30 [001/005 (0080/0614)]\tLoss Ss: 0.005070\n","\tRotated_Epoch:30 [001/005 (0100/0614)]\tLoss Ss: 0.006069\n","\tRotated_Epoch:30 [001/005 (0120/0614)]\tLoss Ss: 0.006760\n","\tRotated_Epoch:30 [001/005 (0140/0614)]\tLoss Ss: 0.008542\n","\tRotated_Epoch:30 [001/005 (0160/0614)]\tLoss Ss: 0.004207\n","\tRotated_Epoch:30 [001/005 (0180/0614)]\tLoss Ss: 0.005832\n","\tRotated_Epoch:30 [001/005 (0200/0614)]\tLoss Ss: 0.003894\n","\tRotated_Epoch:30 [001/005 (0220/0614)]\tLoss Ss: 0.006477\n","\tRotated_Epoch:30 [001/005 (0240/0614)]\tLoss Ss: 0.006711\n","\tRotated_Epoch:30 [001/005 (0260/0614)]\tLoss Ss: 0.003585\n","\tRotated_Epoch:30 [001/005 (0280/0614)]\tLoss Ss: 0.006162\n","\tRotated_Epoch:30 [001/005 (0300/0614)]\tLoss Ss: 0.006454\n","\tRotated_Epoch:30 [001/005 (0320/0614)]\tLoss Ss: 0.006676\n","\tRotated_Epoch:30 [001/005 (0340/0614)]\tLoss Ss: 0.006466\n","\tRotated_Epoch:30 [001/005 (0360/0614)]\tLoss Ss: 0.007817\n","\tRotated_Epoch:30 [001/005 (0380/0614)]\tLoss Ss: 0.006929\n","\tRotated_Epoch:30 [001/005 (0400/0614)]\tLoss Ss: 0.003795\n","\tRotated_Epoch:30 [001/005 (0420/0614)]\tLoss Ss: 0.008349\n","\tRotated_Epoch:30 [001/005 (0440/0614)]\tLoss Ss: 0.005501\n","\tRotated_Epoch:30 [001/005 (0460/0614)]\tLoss Ss: 0.008351\n","\tRotated_Epoch:30 [001/005 (0480/0614)]\tLoss Ss: 0.004325\n","\tRotated_Epoch:30 [001/005 (0500/0614)]\tLoss Ss: 0.004911\n","\tRotated_Epoch:30 [001/005 (0520/0614)]\tLoss Ss: 0.006155\n","\tRotated_Epoch:30 [001/005 (0540/0614)]\tLoss Ss: 0.007544\n","\tRotated_Epoch:30 [001/005 (0560/0614)]\tLoss Ss: 0.007898\n","\tRotated_Epoch:30 [001/005 (0580/0614)]\tLoss Ss: 0.004881\n","\tRotated_Epoch:30 [001/005 (0600/0614)]\tLoss Ss: 0.006075\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tRotated_Epoch:30 [002/005 (0000/0693)]\tLoss Ss: 0.018412\n","\tRotated_Epoch:30 [002/005 (0020/0693)]\tLoss Ss: 0.019909\n","\tRotated_Epoch:30 [002/005 (0040/0693)]\tLoss Ss: 0.016933\n","\tRotated_Epoch:30 [002/005 (0060/0693)]\tLoss Ss: 0.009078\n","\tRotated_Epoch:30 [002/005 (0080/0693)]\tLoss Ss: 0.016386\n","\tRotated_Epoch:30 [002/005 (0100/0693)]\tLoss Ss: 0.018492\n","\tRotated_Epoch:30 [002/005 (0120/0693)]\tLoss Ss: 0.012896\n","\tRotated_Epoch:30 [002/005 (0140/0693)]\tLoss Ss: 0.015613\n","\tRotated_Epoch:30 [002/005 (0160/0693)]\tLoss Ss: 0.011984\n","\tRotated_Epoch:30 [002/005 (0180/0693)]\tLoss Ss: 0.017449\n","\tRotated_Epoch:30 [002/005 (0200/0693)]\tLoss Ss: 0.011448\n","\tRotated_Epoch:30 [002/005 (0220/0693)]\tLoss Ss: 0.012534\n","\tRotated_Epoch:30 [002/005 (0240/0693)]\tLoss Ss: 0.015411\n","\tRotated_Epoch:30 [002/005 (0260/0693)]\tLoss Ss: 0.015614\n","\tRotated_Epoch:30 [002/005 (0280/0693)]\tLoss Ss: 0.018371\n","\tRotated_Epoch:30 [002/005 (0300/0693)]\tLoss Ss: 0.011036\n","\tRotated_Epoch:30 [002/005 (0320/0693)]\tLoss Ss: 0.013967\n","\tRotated_Epoch:30 [002/005 (0340/0693)]\tLoss Ss: 0.017429\n","\tRotated_Epoch:30 [002/005 (0360/0693)]\tLoss Ss: 0.014027\n","\tRotated_Epoch:30 [002/005 (0380/0693)]\tLoss Ss: 0.017600\n","\tRotated_Epoch:30 [002/005 (0400/0693)]\tLoss Ss: 0.021433\n","\tRotated_Epoch:30 [002/005 (0420/0693)]\tLoss Ss: 0.016798\n","\tRotated_Epoch:30 [002/005 (0440/0693)]\tLoss Ss: 0.014072\n","\tRotated_Epoch:30 [002/005 (0460/0693)]\tLoss Ss: 0.016461\n","\tRotated_Epoch:30 [002/005 (0480/0693)]\tLoss Ss: 0.015029\n","\tRotated_Epoch:30 [002/005 (0500/0693)]\tLoss Ss: 0.015664\n","\tRotated_Epoch:30 [002/005 (0520/0693)]\tLoss Ss: 0.014834\n","\tRotated_Epoch:30 [002/005 (0540/0693)]\tLoss Ss: 0.016801\n","\tRotated_Epoch:30 [002/005 (0560/0693)]\tLoss Ss: 0.013839\n","\tRotated_Epoch:30 [002/005 (0580/0693)]\tLoss Ss: 0.010433\n","\tRotated_Epoch:30 [002/005 (0600/0693)]\tLoss Ss: 0.017402\n","\tRotated_Epoch:30 [002/005 (0620/0693)]\tLoss Ss: 0.012166\n","\tRotated_Epoch:30 [002/005 (0640/0693)]\tLoss Ss: 0.013144\n","\tRotated_Epoch:30 [002/005 (0660/0693)]\tLoss Ss: 0.016471\n","\tRotated_Epoch:30 [002/005 (0680/0693)]\tLoss Ss: 0.009993\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tRotated_Epoch:30 [003/005 (0000/0588)]\tLoss Ss: 0.146603\n","\tRotated_Epoch:30 [003/005 (0020/0588)]\tLoss Ss: 0.083637\n","\tRotated_Epoch:30 [003/005 (0040/0588)]\tLoss Ss: 0.105034\n","\tRotated_Epoch:30 [003/005 (0060/0588)]\tLoss Ss: 0.080439\n","\tRotated_Epoch:30 [003/005 (0080/0588)]\tLoss Ss: 0.053957\n","\tRotated_Epoch:30 [003/005 (0100/0588)]\tLoss Ss: 0.079862\n","\tRotated_Epoch:30 [003/005 (0120/0588)]\tLoss Ss: 0.090132\n","\tRotated_Epoch:30 [003/005 (0140/0588)]\tLoss Ss: 0.065803\n","\tRotated_Epoch:30 [003/005 (0160/0588)]\tLoss Ss: 0.056536\n","\tRotated_Epoch:30 [003/005 (0180/0588)]\tLoss Ss: 0.082038\n","\tRotated_Epoch:30 [003/005 (0200/0588)]\tLoss Ss: 0.077211\n","\tRotated_Epoch:30 [003/005 (0220/0588)]\tLoss Ss: 0.068985\n","\tRotated_Epoch:30 [003/005 (0240/0588)]\tLoss Ss: 0.060567\n","\tRotated_Epoch:30 [003/005 (0260/0588)]\tLoss Ss: 0.063048\n","\tRotated_Epoch:30 [003/005 (0280/0588)]\tLoss Ss: 0.065293\n","\tRotated_Epoch:30 [003/005 (0300/0588)]\tLoss Ss: 0.038444\n","\tRotated_Epoch:30 [003/005 (0320/0588)]\tLoss Ss: 0.083936\n","\tRotated_Epoch:30 [003/005 (0340/0588)]\tLoss Ss: 0.060908\n","\tRotated_Epoch:30 [003/005 (0360/0588)]\tLoss Ss: 0.044907\n","\tRotated_Epoch:30 [003/005 (0380/0588)]\tLoss Ss: 0.074417\n","\tRotated_Epoch:30 [003/005 (0400/0588)]\tLoss Ss: 0.067701\n","\tRotated_Epoch:30 [003/005 (0420/0588)]\tLoss Ss: 0.069049\n","\tRotated_Epoch:30 [003/005 (0440/0588)]\tLoss Ss: 0.069622\n","\tRotated_Epoch:30 [003/005 (0460/0588)]\tLoss Ss: 0.075674\n","\tRotated_Epoch:30 [003/005 (0480/0588)]\tLoss Ss: 0.050032\n","\tRotated_Epoch:30 [003/005 (0500/0588)]\tLoss Ss: 0.043580\n","\tRotated_Epoch:30 [003/005 (0520/0588)]\tLoss Ss: 0.050530\n","\tRotated_Epoch:30 [003/005 (0540/0588)]\tLoss Ss: 0.048948\n","\tRotated_Epoch:30 [003/005 (0560/0588)]\tLoss Ss: 0.061912\n","\tRotated_Epoch:30 [003/005 (0580/0588)]\tLoss Ss: 0.036810\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:30 [004/005 (0000/0755)]\tLoss Ss: 0.078469\n","\tRotated_Epoch:30 [004/005 (0020/0755)]\tLoss Ss: 0.127116\n","\tRotated_Epoch:30 [004/005 (0040/0755)]\tLoss Ss: 0.076960\n","\tRotated_Epoch:30 [004/005 (0060/0755)]\tLoss Ss: 0.058162\n","\tRotated_Epoch:30 [004/005 (0080/0755)]\tLoss Ss: 0.043553\n","\tRotated_Epoch:30 [004/005 (0100/0755)]\tLoss Ss: 0.066709\n","\tRotated_Epoch:30 [004/005 (0120/0755)]\tLoss Ss: 0.046432\n","\tRotated_Epoch:30 [004/005 (0140/0755)]\tLoss Ss: 0.050992\n","\tRotated_Epoch:30 [004/005 (0160/0755)]\tLoss Ss: 0.048913\n","\tRotated_Epoch:30 [004/005 (0180/0755)]\tLoss Ss: 0.050229\n","\tRotated_Epoch:30 [004/005 (0200/0755)]\tLoss Ss: 0.053689\n","\tRotated_Epoch:30 [004/005 (0220/0755)]\tLoss Ss: 0.052556\n","\tRotated_Epoch:30 [004/005 (0240/0755)]\tLoss Ss: 0.070016\n","\tRotated_Epoch:30 [004/005 (0260/0755)]\tLoss Ss: 0.053775\n","\tRotated_Epoch:30 [004/005 (0280/0755)]\tLoss Ss: 0.041575\n","\tRotated_Epoch:30 [004/005 (0300/0755)]\tLoss Ss: 0.037177\n","\tRotated_Epoch:30 [004/005 (0320/0755)]\tLoss Ss: 0.029335\n","\tRotated_Epoch:30 [004/005 (0340/0755)]\tLoss Ss: 0.025760\n","\tRotated_Epoch:30 [004/005 (0360/0755)]\tLoss Ss: 0.033942\n","\tRotated_Epoch:30 [004/005 (0380/0755)]\tLoss Ss: 0.031608\n","\tRotated_Epoch:30 [004/005 (0400/0755)]\tLoss Ss: 0.048623\n","\tRotated_Epoch:30 [004/005 (0420/0755)]\tLoss Ss: 0.058894\n","\tRotated_Epoch:30 [004/005 (0440/0755)]\tLoss Ss: 0.032899\n","\tRotated_Epoch:30 [004/005 (0460/0755)]\tLoss Ss: 0.029142\n","\tRotated_Epoch:30 [004/005 (0480/0755)]\tLoss Ss: 0.035144\n","\tRotated_Epoch:30 [004/005 (0500/0755)]\tLoss Ss: 0.048641\n","\tRotated_Epoch:30 [004/005 (0520/0755)]\tLoss Ss: 0.034031\n","\tRotated_Epoch:30 [004/005 (0540/0755)]\tLoss Ss: 0.045771\n","\tRotated_Epoch:30 [004/005 (0560/0755)]\tLoss Ss: 0.023348\n","\tRotated_Epoch:30 [004/005 (0580/0755)]\tLoss Ss: 0.037853\n","\tRotated_Epoch:30 [004/005 (0600/0755)]\tLoss Ss: 0.038469\n","\tRotated_Epoch:30 [004/005 (0620/0755)]\tLoss Ss: 0.043332\n","\tRotated_Epoch:30 [004/005 (0640/0755)]\tLoss Ss: 0.034413\n","\tRotated_Epoch:30 [004/005 (0660/0755)]\tLoss Ss: 0.028627\n","\tRotated_Epoch:30 [004/005 (0680/0755)]\tLoss Ss: 0.036503\n","\tRotated_Epoch:30 [004/005 (0700/0755)]\tLoss Ss: 0.046601\n","\tRotated_Epoch:30 [004/005 (0720/0755)]\tLoss Ss: 0.036491\n","\tRotated_Epoch:30 [004/005 (0740/0755)]\tLoss Ss: 0.034003\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:30 [005/005 (0000/0755)]\tLoss Ss: 0.131135\n","\tRotated_Epoch:30 [005/005 (0020/0755)]\tLoss Ss: 0.156733\n","\tRotated_Epoch:30 [005/005 (0040/0755)]\tLoss Ss: 0.090323\n","\tRotated_Epoch:30 [005/005 (0060/0755)]\tLoss Ss: 0.105010\n","\tRotated_Epoch:30 [005/005 (0080/0755)]\tLoss Ss: 0.100141\n","\tRotated_Epoch:30 [005/005 (0100/0755)]\tLoss Ss: 0.071753\n","\tRotated_Epoch:30 [005/005 (0120/0755)]\tLoss Ss: 0.044251\n","\tRotated_Epoch:30 [005/005 (0140/0755)]\tLoss Ss: 0.036960\n","\tRotated_Epoch:30 [005/005 (0160/0755)]\tLoss Ss: 0.032691\n","\tRotated_Epoch:30 [005/005 (0180/0755)]\tLoss Ss: 0.033942\n","\tRotated_Epoch:30 [005/005 (0200/0755)]\tLoss Ss: 0.034109\n","\tRotated_Epoch:30 [005/005 (0220/0755)]\tLoss Ss: 0.025382\n","\tRotated_Epoch:30 [005/005 (0240/0755)]\tLoss Ss: 0.017548\n","\tRotated_Epoch:30 [005/005 (0260/0755)]\tLoss Ss: 0.026432\n","\tRotated_Epoch:30 [005/005 (0280/0755)]\tLoss Ss: 0.024808\n","\tRotated_Epoch:30 [005/005 (0300/0755)]\tLoss Ss: 0.024131\n","\tRotated_Epoch:30 [005/005 (0320/0755)]\tLoss Ss: 0.024543\n","\tRotated_Epoch:30 [005/005 (0340/0755)]\tLoss Ss: 0.020113\n","\tRotated_Epoch:30 [005/005 (0360/0755)]\tLoss Ss: 0.023643\n","\tRotated_Epoch:30 [005/005 (0380/0755)]\tLoss Ss: 0.025925\n","\tRotated_Epoch:30 [005/005 (0400/0755)]\tLoss Ss: 0.022282\n","\tRotated_Epoch:30 [005/005 (0420/0755)]\tLoss Ss: 0.018818\n","\tRotated_Epoch:30 [005/005 (0440/0755)]\tLoss Ss: 0.026630\n","\tRotated_Epoch:30 [005/005 (0460/0755)]\tLoss Ss: 0.017194\n","\tRotated_Epoch:30 [005/005 (0480/0755)]\tLoss Ss: 0.017308\n","\tRotated_Epoch:30 [005/005 (0500/0755)]\tLoss Ss: 0.017068\n","\tRotated_Epoch:30 [005/005 (0520/0755)]\tLoss Ss: 0.019970\n","\tRotated_Epoch:30 [005/005 (0540/0755)]\tLoss Ss: 0.017102\n","\tRotated_Epoch:30 [005/005 (0560/0755)]\tLoss Ss: 0.018335\n","\tRotated_Epoch:30 [005/005 (0580/0755)]\tLoss Ss: 0.023738\n","\tRotated_Epoch:30 [005/005 (0600/0755)]\tLoss Ss: 0.018139\n","\tRotated_Epoch:30 [005/005 (0620/0755)]\tLoss Ss: 0.028267\n","\tRotated_Epoch:30 [005/005 (0640/0755)]\tLoss Ss: 0.030991\n","\tRotated_Epoch:30 [005/005 (0660/0755)]\tLoss Ss: 0.021059\n","\tRotated_Epoch:30 [005/005 (0680/0755)]\tLoss Ss: 0.020923\n","\tRotated_Epoch:30 [005/005 (0700/0755)]\tLoss Ss: 0.027845\n","\tRotated_Epoch:30 [005/005 (0720/0755)]\tLoss Ss: 0.022454\n","\tRotated_Epoch:30 [005/005 (0740/0755)]\tLoss Ss: 0.019882\n","new_1_6_d.nii\n","new_1_6_m.nii\n","new_1_7_d.nii\n","new_1_7_m.nii\n","new_2_5_d.nii\n","new_2_5_m.nii\n","new_2_6_d.nii\n","new_2_6_m.nii\n","new_2_7_d.nii\n","new_2_7_m.nii\n","new_3_d (2).nii\n","new_3_m (2).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (3).nii\n","new_3_m (3).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (7).nii\n","new_3_m (7).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (8).nii\n","new_3_m (8).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (9).nii\n","new_3_m (9).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_4_10_d_163526.nii\n","new_4_10_m_163526.nii.gz\n","new_4_11_d_084336.nii\n","new_4_11_m_084336.nii.gz\n","new_4_7_d_184422.nii\n","new_4_7_m_184422.nii.gz\n","new_4_8_d_143412.nii\n","new_4_8_m_143412.nii.gz\n","new_4_9_d_155347.nii\n","new_4_9_m_155347.nii.gz\n","\tEpoch: 30; Dice: 0.9547 +/- 0.0193; Loss: 9.0255\n","Begin Epoch 31\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tEpoch:31 [000/005 (0000/0693)]\tLoss Ss: 0.018882\n","\tEpoch:31 [000/005 (0020/0693)]\tLoss Ss: 0.013267\n","\tEpoch:31 [000/005 (0040/0693)]\tLoss Ss: 0.014344\n","\tEpoch:31 [000/005 (0060/0693)]\tLoss Ss: 0.013924\n","\tEpoch:31 [000/005 (0080/0693)]\tLoss Ss: 0.016168\n","\tEpoch:31 [000/005 (0100/0693)]\tLoss Ss: 0.013437\n","\tEpoch:31 [000/005 (0120/0693)]\tLoss Ss: 0.009806\n","\tEpoch:31 [000/005 (0140/0693)]\tLoss Ss: 0.013145\n","\tEpoch:31 [000/005 (0160/0693)]\tLoss Ss: 0.013262\n","\tEpoch:31 [000/005 (0180/0693)]\tLoss Ss: 0.015371\n","\tEpoch:31 [000/005 (0200/0693)]\tLoss Ss: 0.010602\n","\tEpoch:31 [000/005 (0220/0693)]\tLoss Ss: 0.013952\n","\tEpoch:31 [000/005 (0240/0693)]\tLoss Ss: 0.014047\n","\tEpoch:31 [000/005 (0260/0693)]\tLoss Ss: 0.013173\n","\tEpoch:31 [000/005 (0280/0693)]\tLoss Ss: 0.015946\n","\tEpoch:31 [000/005 (0300/0693)]\tLoss Ss: 0.011885\n","\tEpoch:31 [000/005 (0320/0693)]\tLoss Ss: 0.007204\n","\tEpoch:31 [000/005 (0340/0693)]\tLoss Ss: 0.010629\n","\tEpoch:31 [000/005 (0360/0693)]\tLoss Ss: 0.010177\n","\tEpoch:31 [000/005 (0380/0693)]\tLoss Ss: 0.009043\n","\tEpoch:31 [000/005 (0400/0693)]\tLoss Ss: 0.014360\n","\tEpoch:31 [000/005 (0420/0693)]\tLoss Ss: 0.011850\n","\tEpoch:31 [000/005 (0440/0693)]\tLoss Ss: 0.016697\n","\tEpoch:31 [000/005 (0460/0693)]\tLoss Ss: 0.010466\n","\tEpoch:31 [000/005 (0480/0693)]\tLoss Ss: 0.011527\n","\tEpoch:31 [000/005 (0500/0693)]\tLoss Ss: 0.011226\n","\tEpoch:31 [000/005 (0520/0693)]\tLoss Ss: 0.014800\n","\tEpoch:31 [000/005 (0540/0693)]\tLoss Ss: 0.013459\n","\tEpoch:31 [000/005 (0560/0693)]\tLoss Ss: 0.015384\n","\tEpoch:31 [000/005 (0580/0693)]\tLoss Ss: 0.012471\n","\tEpoch:31 [000/005 (0600/0693)]\tLoss Ss: 0.007037\n","\tEpoch:31 [000/005 (0620/0693)]\tLoss Ss: 0.011213\n","\tEpoch:31 [000/005 (0640/0693)]\tLoss Ss: 0.015455\n","\tEpoch:31 [000/005 (0660/0693)]\tLoss Ss: 0.017008\n","\tEpoch:31 [000/005 (0680/0693)]\tLoss Ss: 0.010891\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:31 [001/005 (0000/0755)]\tLoss Ss: 0.035320\n","\tEpoch:31 [001/005 (0020/0755)]\tLoss Ss: 0.017596\n","\tEpoch:31 [001/005 (0040/0755)]\tLoss Ss: 0.020393\n","\tEpoch:31 [001/005 (0060/0755)]\tLoss Ss: 0.023896\n","\tEpoch:31 [001/005 (0080/0755)]\tLoss Ss: 0.014946\n","\tEpoch:31 [001/005 (0100/0755)]\tLoss Ss: 0.018418\n","\tEpoch:31 [001/005 (0120/0755)]\tLoss Ss: 0.016576\n","\tEpoch:31 [001/005 (0140/0755)]\tLoss Ss: 0.024690\n","\tEpoch:31 [001/005 (0160/0755)]\tLoss Ss: 0.019883\n","\tEpoch:31 [001/005 (0180/0755)]\tLoss Ss: 0.019820\n","\tEpoch:31 [001/005 (0200/0755)]\tLoss Ss: 0.018887\n","\tEpoch:31 [001/005 (0220/0755)]\tLoss Ss: 0.022630\n","\tEpoch:31 [001/005 (0240/0755)]\tLoss Ss: 0.013637\n","\tEpoch:31 [001/005 (0260/0755)]\tLoss Ss: 0.014441\n","\tEpoch:31 [001/005 (0280/0755)]\tLoss Ss: 0.021316\n","\tEpoch:31 [001/005 (0300/0755)]\tLoss Ss: 0.017737\n","\tEpoch:31 [001/005 (0320/0755)]\tLoss Ss: 0.017497\n","\tEpoch:31 [001/005 (0340/0755)]\tLoss Ss: 0.014114\n","\tEpoch:31 [001/005 (0360/0755)]\tLoss Ss: 0.018602\n","\tEpoch:31 [001/005 (0380/0755)]\tLoss Ss: 0.020812\n","\tEpoch:31 [001/005 (0400/0755)]\tLoss Ss: 0.019131\n","\tEpoch:31 [001/005 (0420/0755)]\tLoss Ss: 0.016529\n","\tEpoch:31 [001/005 (0440/0755)]\tLoss Ss: 0.012377\n","\tEpoch:31 [001/005 (0460/0755)]\tLoss Ss: 0.009512\n","\tEpoch:31 [001/005 (0480/0755)]\tLoss Ss: 0.009824\n","\tEpoch:31 [001/005 (0500/0755)]\tLoss Ss: 0.019622\n","\tEpoch:31 [001/005 (0520/0755)]\tLoss Ss: 0.017313\n","\tEpoch:31 [001/005 (0540/0755)]\tLoss Ss: 0.015253\n","\tEpoch:31 [001/005 (0560/0755)]\tLoss Ss: 0.013869\n","\tEpoch:31 [001/005 (0580/0755)]\tLoss Ss: 0.011492\n","\tEpoch:31 [001/005 (0600/0755)]\tLoss Ss: 0.013296\n","\tEpoch:31 [001/005 (0620/0755)]\tLoss Ss: 0.015317\n","\tEpoch:31 [001/005 (0640/0755)]\tLoss Ss: 0.017035\n","\tEpoch:31 [001/005 (0660/0755)]\tLoss Ss: 0.012865\n","\tEpoch:31 [001/005 (0680/0755)]\tLoss Ss: 0.018101\n","\tEpoch:31 [001/005 (0700/0755)]\tLoss Ss: 0.013147\n","\tEpoch:31 [001/005 (0720/0755)]\tLoss Ss: 0.023120\n","\tEpoch:31 [001/005 (0740/0755)]\tLoss Ss: 0.014261\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tEpoch:31 [002/005 (0000/0693)]\tLoss Ss: 0.021819\n","\tEpoch:31 [002/005 (0020/0693)]\tLoss Ss: 0.011811\n","\tEpoch:31 [002/005 (0040/0693)]\tLoss Ss: 0.017170\n","\tEpoch:31 [002/005 (0060/0693)]\tLoss Ss: 0.015411\n","\tEpoch:31 [002/005 (0080/0693)]\tLoss Ss: 0.014833\n","\tEpoch:31 [002/005 (0100/0693)]\tLoss Ss: 0.013056\n","\tEpoch:31 [002/005 (0120/0693)]\tLoss Ss: 0.017435\n","\tEpoch:31 [002/005 (0140/0693)]\tLoss Ss: 0.012572\n","\tEpoch:31 [002/005 (0160/0693)]\tLoss Ss: 0.013001\n","\tEpoch:31 [002/005 (0180/0693)]\tLoss Ss: 0.012589\n","\tEpoch:31 [002/005 (0200/0693)]\tLoss Ss: 0.014096\n","\tEpoch:31 [002/005 (0220/0693)]\tLoss Ss: 0.011350\n","\tEpoch:31 [002/005 (0240/0693)]\tLoss Ss: 0.009865\n","\tEpoch:31 [002/005 (0260/0693)]\tLoss Ss: 0.014104\n","\tEpoch:31 [002/005 (0280/0693)]\tLoss Ss: 0.012546\n","\tEpoch:31 [002/005 (0300/0693)]\tLoss Ss: 0.014842\n","\tEpoch:31 [002/005 (0320/0693)]\tLoss Ss: 0.015624\n","\tEpoch:31 [002/005 (0340/0693)]\tLoss Ss: 0.010881\n","\tEpoch:31 [002/005 (0360/0693)]\tLoss Ss: 0.014907\n","\tEpoch:31 [002/005 (0380/0693)]\tLoss Ss: 0.014926\n","\tEpoch:31 [002/005 (0400/0693)]\tLoss Ss: 0.012808\n","\tEpoch:31 [002/005 (0420/0693)]\tLoss Ss: 0.011413\n","\tEpoch:31 [002/005 (0440/0693)]\tLoss Ss: 0.016657\n","\tEpoch:31 [002/005 (0460/0693)]\tLoss Ss: 0.012047\n","\tEpoch:31 [002/005 (0480/0693)]\tLoss Ss: 0.012354\n","\tEpoch:31 [002/005 (0500/0693)]\tLoss Ss: 0.020478\n","\tEpoch:31 [002/005 (0520/0693)]\tLoss Ss: 0.015724\n","\tEpoch:31 [002/005 (0540/0693)]\tLoss Ss: 0.015313\n","\tEpoch:31 [002/005 (0560/0693)]\tLoss Ss: 0.013013\n","\tEpoch:31 [002/005 (0580/0693)]\tLoss Ss: 0.016295\n","\tEpoch:31 [002/005 (0600/0693)]\tLoss Ss: 0.010208\n","\tEpoch:31 [002/005 (0620/0693)]\tLoss Ss: 0.010062\n","\tEpoch:31 [002/005 (0640/0693)]\tLoss Ss: 0.012129\n","\tEpoch:31 [002/005 (0660/0693)]\tLoss Ss: 0.012855\n","\tEpoch:31 [002/005 (0680/0693)]\tLoss Ss: 0.011111\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:31 [003/005 (0000/0755)]\tLoss Ss: 0.014648\n","\tEpoch:31 [003/005 (0020/0755)]\tLoss Ss: 0.017650\n","\tEpoch:31 [003/005 (0040/0755)]\tLoss Ss: 0.015345\n","\tEpoch:31 [003/005 (0060/0755)]\tLoss Ss: 0.013385\n","\tEpoch:31 [003/005 (0080/0755)]\tLoss Ss: 0.014275\n","\tEpoch:31 [003/005 (0100/0755)]\tLoss Ss: 0.014590\n","\tEpoch:31 [003/005 (0120/0755)]\tLoss Ss: 0.013379\n","\tEpoch:31 [003/005 (0140/0755)]\tLoss Ss: 0.014080\n","\tEpoch:31 [003/005 (0160/0755)]\tLoss Ss: 0.018140\n","\tEpoch:31 [003/005 (0180/0755)]\tLoss Ss: 0.011803\n","\tEpoch:31 [003/005 (0200/0755)]\tLoss Ss: 0.015551\n","\tEpoch:31 [003/005 (0220/0755)]\tLoss Ss: 0.013087\n","\tEpoch:31 [003/005 (0240/0755)]\tLoss Ss: 0.013051\n","\tEpoch:31 [003/005 (0260/0755)]\tLoss Ss: 0.014352\n","\tEpoch:31 [003/005 (0280/0755)]\tLoss Ss: 0.007082\n","\tEpoch:31 [003/005 (0300/0755)]\tLoss Ss: 0.010801\n","\tEpoch:31 [003/005 (0320/0755)]\tLoss Ss: 0.014401\n","\tEpoch:31 [003/005 (0340/0755)]\tLoss Ss: 0.008015\n","\tEpoch:31 [003/005 (0360/0755)]\tLoss Ss: 0.012328\n","\tEpoch:31 [003/005 (0380/0755)]\tLoss Ss: 0.013739\n","\tEpoch:31 [003/005 (0400/0755)]\tLoss Ss: 0.017402\n","\tEpoch:31 [003/005 (0420/0755)]\tLoss Ss: 0.011172\n","\tEpoch:31 [003/005 (0440/0755)]\tLoss Ss: 0.010017\n","\tEpoch:31 [003/005 (0460/0755)]\tLoss Ss: 0.011734\n","\tEpoch:31 [003/005 (0480/0755)]\tLoss Ss: 0.014035\n","\tEpoch:31 [003/005 (0500/0755)]\tLoss Ss: 0.012471\n","\tEpoch:31 [003/005 (0520/0755)]\tLoss Ss: 0.011458\n","\tEpoch:31 [003/005 (0540/0755)]\tLoss Ss: 0.012450\n","\tEpoch:31 [003/005 (0560/0755)]\tLoss Ss: 0.012211\n","\tEpoch:31 [003/005 (0580/0755)]\tLoss Ss: 0.013084\n","\tEpoch:31 [003/005 (0600/0755)]\tLoss Ss: 0.011889\n","\tEpoch:31 [003/005 (0620/0755)]\tLoss Ss: 0.009021\n","\tEpoch:31 [003/005 (0640/0755)]\tLoss Ss: 0.018609\n","\tEpoch:31 [003/005 (0660/0755)]\tLoss Ss: 0.011448\n","\tEpoch:31 [003/005 (0680/0755)]\tLoss Ss: 0.010672\n","\tEpoch:31 [003/005 (0700/0755)]\tLoss Ss: 0.013561\n","\tEpoch:31 [003/005 (0720/0755)]\tLoss Ss: 0.018062\n","\tEpoch:31 [003/005 (0740/0755)]\tLoss Ss: 0.010966\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tEpoch:31 [004/005 (0000/0614)]\tLoss Ss: 0.004234\n","\tEpoch:31 [004/005 (0020/0614)]\tLoss Ss: 0.006761\n","\tEpoch:31 [004/005 (0040/0614)]\tLoss Ss: 0.006659\n","\tEpoch:31 [004/005 (0060/0614)]\tLoss Ss: 0.006661\n","\tEpoch:31 [004/005 (0080/0614)]\tLoss Ss: 0.003963\n","\tEpoch:31 [004/005 (0100/0614)]\tLoss Ss: 0.006348\n","\tEpoch:31 [004/005 (0120/0614)]\tLoss Ss: 0.005364\n","\tEpoch:31 [004/005 (0140/0614)]\tLoss Ss: 0.007608\n","\tEpoch:31 [004/005 (0160/0614)]\tLoss Ss: 0.006424\n","\tEpoch:31 [004/005 (0180/0614)]\tLoss Ss: 0.006899\n","\tEpoch:31 [004/005 (0200/0614)]\tLoss Ss: 0.005416\n","\tEpoch:31 [004/005 (0220/0614)]\tLoss Ss: 0.003911\n","\tEpoch:31 [004/005 (0240/0614)]\tLoss Ss: 0.009448\n","\tEpoch:31 [004/005 (0260/0614)]\tLoss Ss: 0.005497\n","\tEpoch:31 [004/005 (0280/0614)]\tLoss Ss: 0.006201\n","\tEpoch:31 [004/005 (0300/0614)]\tLoss Ss: 0.008276\n","\tEpoch:31 [004/005 (0320/0614)]\tLoss Ss: 0.005639\n","\tEpoch:31 [004/005 (0340/0614)]\tLoss Ss: 0.005004\n","\tEpoch:31 [004/005 (0360/0614)]\tLoss Ss: 0.005623\n","\tEpoch:31 [004/005 (0380/0614)]\tLoss Ss: 0.005384\n","\tEpoch:31 [004/005 (0400/0614)]\tLoss Ss: 0.004472\n","\tEpoch:31 [004/005 (0420/0614)]\tLoss Ss: 0.002937\n","\tEpoch:31 [004/005 (0440/0614)]\tLoss Ss: 0.003961\n","\tEpoch:31 [004/005 (0460/0614)]\tLoss Ss: 0.005681\n","\tEpoch:31 [004/005 (0480/0614)]\tLoss Ss: 0.004004\n","\tEpoch:31 [004/005 (0500/0614)]\tLoss Ss: 0.007148\n","\tEpoch:31 [004/005 (0520/0614)]\tLoss Ss: 0.006515\n","\tEpoch:31 [004/005 (0540/0614)]\tLoss Ss: 0.004199\n","\tEpoch:31 [004/005 (0560/0614)]\tLoss Ss: 0.007966\n","\tEpoch:31 [004/005 (0580/0614)]\tLoss Ss: 0.004941\n","\tEpoch:31 [004/005 (0600/0614)]\tLoss Ss: 0.006600\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tEpoch:31 [005/005 (0000/0588)]\tLoss Ss: 0.008764\n","\tEpoch:31 [005/005 (0020/0588)]\tLoss Ss: 0.008821\n","\tEpoch:31 [005/005 (0040/0588)]\tLoss Ss: 0.012755\n","\tEpoch:31 [005/005 (0060/0588)]\tLoss Ss: 0.006072\n","\tEpoch:31 [005/005 (0080/0588)]\tLoss Ss: 0.008131\n","\tEpoch:31 [005/005 (0100/0588)]\tLoss Ss: 0.006075\n","\tEpoch:31 [005/005 (0120/0588)]\tLoss Ss: 0.005787\n","\tEpoch:31 [005/005 (0140/0588)]\tLoss Ss: 0.003783\n","\tEpoch:31 [005/005 (0160/0588)]\tLoss Ss: 0.005652\n","\tEpoch:31 [005/005 (0180/0588)]\tLoss Ss: 0.005941\n","\tEpoch:31 [005/005 (0200/0588)]\tLoss Ss: 0.004128\n","\tEpoch:31 [005/005 (0220/0588)]\tLoss Ss: 0.005278\n","\tEpoch:31 [005/005 (0240/0588)]\tLoss Ss: 0.005426\n","\tEpoch:31 [005/005 (0260/0588)]\tLoss Ss: 0.006593\n","\tEpoch:31 [005/005 (0280/0588)]\tLoss Ss: 0.003548\n","\tEpoch:31 [005/005 (0300/0588)]\tLoss Ss: 0.002749\n","\tEpoch:31 [005/005 (0320/0588)]\tLoss Ss: 0.003906\n","\tEpoch:31 [005/005 (0340/0588)]\tLoss Ss: 0.004957\n","\tEpoch:31 [005/005 (0360/0588)]\tLoss Ss: 0.003889\n","\tEpoch:31 [005/005 (0380/0588)]\tLoss Ss: 0.003082\n","\tEpoch:31 [005/005 (0400/0588)]\tLoss Ss: 0.005436\n","\tEpoch:31 [005/005 (0420/0588)]\tLoss Ss: 0.005799\n","\tEpoch:31 [005/005 (0440/0588)]\tLoss Ss: 0.004317\n","\tEpoch:31 [005/005 (0460/0588)]\tLoss Ss: 0.004088\n","\tEpoch:31 [005/005 (0480/0588)]\tLoss Ss: 0.004382\n","\tEpoch:31 [005/005 (0500/0588)]\tLoss Ss: 0.003783\n","\tEpoch:31 [005/005 (0520/0588)]\tLoss Ss: 0.003890\n","\tEpoch:31 [005/005 (0540/0588)]\tLoss Ss: 0.004758\n","\tEpoch:31 [005/005 (0560/0588)]\tLoss Ss: 0.004188\n","\tEpoch:31 [005/005 (0580/0588)]\tLoss Ss: 0.005319\n","Now train the rotated image\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tRotated_Epoch:31 [000/005 (0000/0614)]\tLoss Ss: 0.008268\n","\tRotated_Epoch:31 [000/005 (0020/0614)]\tLoss Ss: 0.008333\n","\tRotated_Epoch:31 [000/005 (0040/0614)]\tLoss Ss: 0.007115\n","\tRotated_Epoch:31 [000/005 (0060/0614)]\tLoss Ss: 0.007560\n","\tRotated_Epoch:31 [000/005 (0080/0614)]\tLoss Ss: 0.007005\n","\tRotated_Epoch:31 [000/005 (0100/0614)]\tLoss Ss: 0.008446\n","\tRotated_Epoch:31 [000/005 (0120/0614)]\tLoss Ss: 0.005615\n","\tRotated_Epoch:31 [000/005 (0140/0614)]\tLoss Ss: 0.005418\n","\tRotated_Epoch:31 [000/005 (0160/0614)]\tLoss Ss: 0.007607\n","\tRotated_Epoch:31 [000/005 (0180/0614)]\tLoss Ss: 0.007221\n","\tRotated_Epoch:31 [000/005 (0200/0614)]\tLoss Ss: 0.007445\n","\tRotated_Epoch:31 [000/005 (0220/0614)]\tLoss Ss: 0.006783\n","\tRotated_Epoch:31 [000/005 (0240/0614)]\tLoss Ss: 0.005868\n","\tRotated_Epoch:31 [000/005 (0260/0614)]\tLoss Ss: 0.005602\n","\tRotated_Epoch:31 [000/005 (0280/0614)]\tLoss Ss: 0.004363\n","\tRotated_Epoch:31 [000/005 (0300/0614)]\tLoss Ss: 0.006503\n","\tRotated_Epoch:31 [000/005 (0320/0614)]\tLoss Ss: 0.006381\n","\tRotated_Epoch:31 [000/005 (0340/0614)]\tLoss Ss: 0.004436\n","\tRotated_Epoch:31 [000/005 (0360/0614)]\tLoss Ss: 0.004935\n","\tRotated_Epoch:31 [000/005 (0380/0614)]\tLoss Ss: 0.004651\n","\tRotated_Epoch:31 [000/005 (0400/0614)]\tLoss Ss: 0.005046\n","\tRotated_Epoch:31 [000/005 (0420/0614)]\tLoss Ss: 0.003324\n","\tRotated_Epoch:31 [000/005 (0440/0614)]\tLoss Ss: 0.006254\n","\tRotated_Epoch:31 [000/005 (0460/0614)]\tLoss Ss: 0.006324\n","\tRotated_Epoch:31 [000/005 (0480/0614)]\tLoss Ss: 0.006153\n","\tRotated_Epoch:31 [000/005 (0500/0614)]\tLoss Ss: 0.004880\n","\tRotated_Epoch:31 [000/005 (0520/0614)]\tLoss Ss: 0.005695\n","\tRotated_Epoch:31 [000/005 (0540/0614)]\tLoss Ss: 0.005008\n","\tRotated_Epoch:31 [000/005 (0560/0614)]\tLoss Ss: 0.006317\n","\tRotated_Epoch:31 [000/005 (0580/0614)]\tLoss Ss: 0.005796\n","\tRotated_Epoch:31 [000/005 (0600/0614)]\tLoss Ss: 0.007940\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tRotated_Epoch:31 [001/005 (0000/0693)]\tLoss Ss: 0.016763\n","\tRotated_Epoch:31 [001/005 (0020/0693)]\tLoss Ss: 0.013177\n","\tRotated_Epoch:31 [001/005 (0040/0693)]\tLoss Ss: 0.015421\n","\tRotated_Epoch:31 [001/005 (0060/0693)]\tLoss Ss: 0.014001\n","\tRotated_Epoch:31 [001/005 (0080/0693)]\tLoss Ss: 0.014562\n","\tRotated_Epoch:31 [001/005 (0100/0693)]\tLoss Ss: 0.018870\n","\tRotated_Epoch:31 [001/005 (0120/0693)]\tLoss Ss: 0.021425\n","\tRotated_Epoch:31 [001/005 (0140/0693)]\tLoss Ss: 0.013410\n","\tRotated_Epoch:31 [001/005 (0160/0693)]\tLoss Ss: 0.012779\n","\tRotated_Epoch:31 [001/005 (0180/0693)]\tLoss Ss: 0.013798\n","\tRotated_Epoch:31 [001/005 (0200/0693)]\tLoss Ss: 0.011717\n","\tRotated_Epoch:31 [001/005 (0220/0693)]\tLoss Ss: 0.011475\n","\tRotated_Epoch:31 [001/005 (0240/0693)]\tLoss Ss: 0.013810\n","\tRotated_Epoch:31 [001/005 (0260/0693)]\tLoss Ss: 0.014417\n","\tRotated_Epoch:31 [001/005 (0280/0693)]\tLoss Ss: 0.009953\n","\tRotated_Epoch:31 [001/005 (0300/0693)]\tLoss Ss: 0.013461\n","\tRotated_Epoch:31 [001/005 (0320/0693)]\tLoss Ss: 0.014753\n","\tRotated_Epoch:31 [001/005 (0340/0693)]\tLoss Ss: 0.010298\n","\tRotated_Epoch:31 [001/005 (0360/0693)]\tLoss Ss: 0.017737\n","\tRotated_Epoch:31 [001/005 (0380/0693)]\tLoss Ss: 0.011211\n","\tRotated_Epoch:31 [001/005 (0400/0693)]\tLoss Ss: 0.016559\n","\tRotated_Epoch:31 [001/005 (0420/0693)]\tLoss Ss: 0.010348\n","\tRotated_Epoch:31 [001/005 (0440/0693)]\tLoss Ss: 0.012166\n","\tRotated_Epoch:31 [001/005 (0460/0693)]\tLoss Ss: 0.016360\n","\tRotated_Epoch:31 [001/005 (0480/0693)]\tLoss Ss: 0.015121\n","\tRotated_Epoch:31 [001/005 (0500/0693)]\tLoss Ss: 0.010359\n","\tRotated_Epoch:31 [001/005 (0520/0693)]\tLoss Ss: 0.012298\n","\tRotated_Epoch:31 [001/005 (0540/0693)]\tLoss Ss: 0.012128\n","\tRotated_Epoch:31 [001/005 (0560/0693)]\tLoss Ss: 0.011327\n","\tRotated_Epoch:31 [001/005 (0580/0693)]\tLoss Ss: 0.010961\n","\tRotated_Epoch:31 [001/005 (0600/0693)]\tLoss Ss: 0.011718\n","\tRotated_Epoch:31 [001/005 (0620/0693)]\tLoss Ss: 0.009677\n","\tRotated_Epoch:31 [001/005 (0640/0693)]\tLoss Ss: 0.011763\n","\tRotated_Epoch:31 [001/005 (0660/0693)]\tLoss Ss: 0.014384\n","\tRotated_Epoch:31 [001/005 (0680/0693)]\tLoss Ss: 0.014307\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tRotated_Epoch:31 [002/005 (0000/0693)]\tLoss Ss: 0.019013\n","\tRotated_Epoch:31 [002/005 (0020/0693)]\tLoss Ss: 0.012123\n","\tRotated_Epoch:31 [002/005 (0040/0693)]\tLoss Ss: 0.012583\n","\tRotated_Epoch:31 [002/005 (0060/0693)]\tLoss Ss: 0.011923\n","\tRotated_Epoch:31 [002/005 (0080/0693)]\tLoss Ss: 0.015621\n","\tRotated_Epoch:31 [002/005 (0100/0693)]\tLoss Ss: 0.019776\n","\tRotated_Epoch:31 [002/005 (0120/0693)]\tLoss Ss: 0.015566\n","\tRotated_Epoch:31 [002/005 (0140/0693)]\tLoss Ss: 0.015003\n","\tRotated_Epoch:31 [002/005 (0160/0693)]\tLoss Ss: 0.011634\n","\tRotated_Epoch:31 [002/005 (0180/0693)]\tLoss Ss: 0.022591\n","\tRotated_Epoch:31 [002/005 (0200/0693)]\tLoss Ss: 0.012624\n","\tRotated_Epoch:31 [002/005 (0220/0693)]\tLoss Ss: 0.014986\n","\tRotated_Epoch:31 [002/005 (0240/0693)]\tLoss Ss: 0.017076\n","\tRotated_Epoch:31 [002/005 (0260/0693)]\tLoss Ss: 0.012547\n","\tRotated_Epoch:31 [002/005 (0280/0693)]\tLoss Ss: 0.015813\n","\tRotated_Epoch:31 [002/005 (0300/0693)]\tLoss Ss: 0.014426\n","\tRotated_Epoch:31 [002/005 (0320/0693)]\tLoss Ss: 0.014382\n","\tRotated_Epoch:31 [002/005 (0340/0693)]\tLoss Ss: 0.010396\n","\tRotated_Epoch:31 [002/005 (0360/0693)]\tLoss Ss: 0.012661\n","\tRotated_Epoch:31 [002/005 (0380/0693)]\tLoss Ss: 0.023217\n","\tRotated_Epoch:31 [002/005 (0400/0693)]\tLoss Ss: 0.014352\n","\tRotated_Epoch:31 [002/005 (0420/0693)]\tLoss Ss: 0.017171\n","\tRotated_Epoch:31 [002/005 (0440/0693)]\tLoss Ss: 0.014313\n","\tRotated_Epoch:31 [002/005 (0460/0693)]\tLoss Ss: 0.016245\n","\tRotated_Epoch:31 [002/005 (0480/0693)]\tLoss Ss: 0.019016\n","\tRotated_Epoch:31 [002/005 (0500/0693)]\tLoss Ss: 0.014153\n","\tRotated_Epoch:31 [002/005 (0520/0693)]\tLoss Ss: 0.014732\n","\tRotated_Epoch:31 [002/005 (0540/0693)]\tLoss Ss: 0.013802\n","\tRotated_Epoch:31 [002/005 (0560/0693)]\tLoss Ss: 0.017003\n","\tRotated_Epoch:31 [002/005 (0580/0693)]\tLoss Ss: 0.015373\n","\tRotated_Epoch:31 [002/005 (0600/0693)]\tLoss Ss: 0.013984\n","\tRotated_Epoch:31 [002/005 (0620/0693)]\tLoss Ss: 0.015775\n","\tRotated_Epoch:31 [002/005 (0640/0693)]\tLoss Ss: 0.014077\n","\tRotated_Epoch:31 [002/005 (0660/0693)]\tLoss Ss: 0.010187\n","\tRotated_Epoch:31 [002/005 (0680/0693)]\tLoss Ss: 0.011543\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tRotated_Epoch:31 [003/005 (0000/0588)]\tLoss Ss: 0.080812\n","\tRotated_Epoch:31 [003/005 (0020/0588)]\tLoss Ss: 0.065299\n","\tRotated_Epoch:31 [003/005 (0040/0588)]\tLoss Ss: 0.076411\n","\tRotated_Epoch:31 [003/005 (0060/0588)]\tLoss Ss: 0.056358\n","\tRotated_Epoch:31 [003/005 (0080/0588)]\tLoss Ss: 0.082800\n","\tRotated_Epoch:31 [003/005 (0100/0588)]\tLoss Ss: 0.058822\n","\tRotated_Epoch:31 [003/005 (0120/0588)]\tLoss Ss: 0.073530\n","\tRotated_Epoch:31 [003/005 (0140/0588)]\tLoss Ss: 0.032327\n","\tRotated_Epoch:31 [003/005 (0160/0588)]\tLoss Ss: 0.056197\n","\tRotated_Epoch:31 [003/005 (0180/0588)]\tLoss Ss: 0.049024\n","\tRotated_Epoch:31 [003/005 (0200/0588)]\tLoss Ss: 0.057147\n","\tRotated_Epoch:31 [003/005 (0220/0588)]\tLoss Ss: 0.066813\n","\tRotated_Epoch:31 [003/005 (0240/0588)]\tLoss Ss: 0.057258\n","\tRotated_Epoch:31 [003/005 (0260/0588)]\tLoss Ss: 0.052207\n","\tRotated_Epoch:31 [003/005 (0280/0588)]\tLoss Ss: 0.036522\n","\tRotated_Epoch:31 [003/005 (0300/0588)]\tLoss Ss: 0.045018\n","\tRotated_Epoch:31 [003/005 (0320/0588)]\tLoss Ss: 0.059588\n","\tRotated_Epoch:31 [003/005 (0340/0588)]\tLoss Ss: 0.036860\n","\tRotated_Epoch:31 [003/005 (0360/0588)]\tLoss Ss: 0.055114\n","\tRotated_Epoch:31 [003/005 (0380/0588)]\tLoss Ss: 0.066933\n","\tRotated_Epoch:31 [003/005 (0400/0588)]\tLoss Ss: 0.068406\n","\tRotated_Epoch:31 [003/005 (0420/0588)]\tLoss Ss: 0.038419\n","\tRotated_Epoch:31 [003/005 (0440/0588)]\tLoss Ss: 0.048223\n","\tRotated_Epoch:31 [003/005 (0460/0588)]\tLoss Ss: 0.061143\n","\tRotated_Epoch:31 [003/005 (0480/0588)]\tLoss Ss: 0.061823\n","\tRotated_Epoch:31 [003/005 (0500/0588)]\tLoss Ss: 0.067366\n","\tRotated_Epoch:31 [003/005 (0520/0588)]\tLoss Ss: 0.058013\n","\tRotated_Epoch:31 [003/005 (0540/0588)]\tLoss Ss: 0.047092\n","\tRotated_Epoch:31 [003/005 (0560/0588)]\tLoss Ss: 0.068788\n","\tRotated_Epoch:31 [003/005 (0580/0588)]\tLoss Ss: 0.060013\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:31 [004/005 (0000/0755)]\tLoss Ss: 0.191205\n","\tRotated_Epoch:31 [004/005 (0020/0755)]\tLoss Ss: 0.177625\n","\tRotated_Epoch:31 [004/005 (0040/0755)]\tLoss Ss: 0.183997\n","\tRotated_Epoch:31 [004/005 (0060/0755)]\tLoss Ss: 0.099404\n","\tRotated_Epoch:31 [004/005 (0080/0755)]\tLoss Ss: 0.095791\n","\tRotated_Epoch:31 [004/005 (0100/0755)]\tLoss Ss: 0.100345\n","\tRotated_Epoch:31 [004/005 (0120/0755)]\tLoss Ss: 0.041779\n","\tRotated_Epoch:31 [004/005 (0140/0755)]\tLoss Ss: 0.066768\n","\tRotated_Epoch:31 [004/005 (0160/0755)]\tLoss Ss: 0.051428\n","\tRotated_Epoch:31 [004/005 (0180/0755)]\tLoss Ss: 0.071826\n","\tRotated_Epoch:31 [004/005 (0200/0755)]\tLoss Ss: 0.055379\n","\tRotated_Epoch:31 [004/005 (0220/0755)]\tLoss Ss: 0.042804\n","\tRotated_Epoch:31 [004/005 (0240/0755)]\tLoss Ss: 0.045779\n","\tRotated_Epoch:31 [004/005 (0260/0755)]\tLoss Ss: 0.068246\n","\tRotated_Epoch:31 [004/005 (0280/0755)]\tLoss Ss: 0.050032\n","\tRotated_Epoch:31 [004/005 (0300/0755)]\tLoss Ss: 0.045136\n","\tRotated_Epoch:31 [004/005 (0320/0755)]\tLoss Ss: 0.062162\n","\tRotated_Epoch:31 [004/005 (0340/0755)]\tLoss Ss: 0.041607\n","\tRotated_Epoch:31 [004/005 (0360/0755)]\tLoss Ss: 0.045256\n","\tRotated_Epoch:31 [004/005 (0380/0755)]\tLoss Ss: 0.045624\n","\tRotated_Epoch:31 [004/005 (0400/0755)]\tLoss Ss: 0.048629\n","\tRotated_Epoch:31 [004/005 (0420/0755)]\tLoss Ss: 0.033903\n","\tRotated_Epoch:31 [004/005 (0440/0755)]\tLoss Ss: 0.050015\n","\tRotated_Epoch:31 [004/005 (0460/0755)]\tLoss Ss: 0.047358\n","\tRotated_Epoch:31 [004/005 (0480/0755)]\tLoss Ss: 0.041219\n","\tRotated_Epoch:31 [004/005 (0500/0755)]\tLoss Ss: 0.047655\n","\tRotated_Epoch:31 [004/005 (0520/0755)]\tLoss Ss: 0.027018\n","\tRotated_Epoch:31 [004/005 (0540/0755)]\tLoss Ss: 0.059862\n","\tRotated_Epoch:31 [004/005 (0560/0755)]\tLoss Ss: 0.041691\n","\tRotated_Epoch:31 [004/005 (0580/0755)]\tLoss Ss: 0.066111\n","\tRotated_Epoch:31 [004/005 (0600/0755)]\tLoss Ss: 0.036904\n","\tRotated_Epoch:31 [004/005 (0620/0755)]\tLoss Ss: 0.031075\n","\tRotated_Epoch:31 [004/005 (0640/0755)]\tLoss Ss: 0.027945\n","\tRotated_Epoch:31 [004/005 (0660/0755)]\tLoss Ss: 0.029324\n","\tRotated_Epoch:31 [004/005 (0680/0755)]\tLoss Ss: 0.046055\n","\tRotated_Epoch:31 [004/005 (0700/0755)]\tLoss Ss: 0.043000\n","\tRotated_Epoch:31 [004/005 (0720/0755)]\tLoss Ss: 0.027286\n","\tRotated_Epoch:31 [004/005 (0740/0755)]\tLoss Ss: 0.030508\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:31 [005/005 (0000/0755)]\tLoss Ss: 0.132539\n","\tRotated_Epoch:31 [005/005 (0020/0755)]\tLoss Ss: 0.123560\n","\tRotated_Epoch:31 [005/005 (0040/0755)]\tLoss Ss: 0.194629\n","\tRotated_Epoch:31 [005/005 (0060/0755)]\tLoss Ss: 0.076746\n","\tRotated_Epoch:31 [005/005 (0080/0755)]\tLoss Ss: 0.086384\n","\tRotated_Epoch:31 [005/005 (0100/0755)]\tLoss Ss: 0.047853\n","\tRotated_Epoch:31 [005/005 (0120/0755)]\tLoss Ss: 0.046420\n","\tRotated_Epoch:31 [005/005 (0140/0755)]\tLoss Ss: 0.058957\n","\tRotated_Epoch:31 [005/005 (0160/0755)]\tLoss Ss: 0.047417\n","\tRotated_Epoch:31 [005/005 (0180/0755)]\tLoss Ss: 0.029708\n","\tRotated_Epoch:31 [005/005 (0200/0755)]\tLoss Ss: 0.031660\n","\tRotated_Epoch:31 [005/005 (0220/0755)]\tLoss Ss: 0.033359\n","\tRotated_Epoch:31 [005/005 (0240/0755)]\tLoss Ss: 0.027958\n","\tRotated_Epoch:31 [005/005 (0260/0755)]\tLoss Ss: 0.027592\n","\tRotated_Epoch:31 [005/005 (0280/0755)]\tLoss Ss: 0.030854\n","\tRotated_Epoch:31 [005/005 (0300/0755)]\tLoss Ss: 0.017859\n","\tRotated_Epoch:31 [005/005 (0320/0755)]\tLoss Ss: 0.019699\n","\tRotated_Epoch:31 [005/005 (0340/0755)]\tLoss Ss: 0.030439\n","\tRotated_Epoch:31 [005/005 (0360/0755)]\tLoss Ss: 0.026459\n","\tRotated_Epoch:31 [005/005 (0380/0755)]\tLoss Ss: 0.025603\n","\tRotated_Epoch:31 [005/005 (0400/0755)]\tLoss Ss: 0.032907\n","\tRotated_Epoch:31 [005/005 (0420/0755)]\tLoss Ss: 0.022961\n","\tRotated_Epoch:31 [005/005 (0440/0755)]\tLoss Ss: 0.019516\n","\tRotated_Epoch:31 [005/005 (0460/0755)]\tLoss Ss: 0.033001\n","\tRotated_Epoch:31 [005/005 (0480/0755)]\tLoss Ss: 0.031860\n","\tRotated_Epoch:31 [005/005 (0500/0755)]\tLoss Ss: 0.020256\n","\tRotated_Epoch:31 [005/005 (0520/0755)]\tLoss Ss: 0.030413\n","\tRotated_Epoch:31 [005/005 (0540/0755)]\tLoss Ss: 0.017700\n","\tRotated_Epoch:31 [005/005 (0560/0755)]\tLoss Ss: 0.021065\n","\tRotated_Epoch:31 [005/005 (0580/0755)]\tLoss Ss: 0.022089\n","\tRotated_Epoch:31 [005/005 (0600/0755)]\tLoss Ss: 0.021694\n","\tRotated_Epoch:31 [005/005 (0620/0755)]\tLoss Ss: 0.019377\n","\tRotated_Epoch:31 [005/005 (0640/0755)]\tLoss Ss: 0.019218\n","\tRotated_Epoch:31 [005/005 (0660/0755)]\tLoss Ss: 0.018484\n","\tRotated_Epoch:31 [005/005 (0680/0755)]\tLoss Ss: 0.016977\n","\tRotated_Epoch:31 [005/005 (0700/0755)]\tLoss Ss: 0.022250\n","\tRotated_Epoch:31 [005/005 (0720/0755)]\tLoss Ss: 0.020625\n","\tRotated_Epoch:31 [005/005 (0740/0755)]\tLoss Ss: 0.017928\n","new_1_6_d.nii\n","new_1_6_m.nii\n","new_1_7_d.nii\n","new_1_7_m.nii\n","new_2_5_d.nii\n","new_2_5_m.nii\n","new_2_6_d.nii\n","new_2_6_m.nii\n","new_2_7_d.nii\n","new_2_7_m.nii\n","new_3_d (2).nii\n","new_3_m (2).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (3).nii\n","new_3_m (3).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (7).nii\n","new_3_m (7).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (8).nii\n","new_3_m (8).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (9).nii\n","new_3_m (9).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_4_10_d_163526.nii\n","new_4_10_m_163526.nii.gz\n","new_4_11_d_084336.nii\n","new_4_11_m_084336.nii.gz\n","new_4_7_d_184422.nii\n","new_4_7_m_184422.nii.gz\n","new_4_8_d_143412.nii\n","new_4_8_m_143412.nii.gz\n","new_4_9_d_155347.nii\n","new_4_9_m_155347.nii.gz\n","\tEpoch: 31; Dice: 0.9542 +/- 0.0136; Loss: 9.2183\n","Begin Epoch 32\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tEpoch:32 [000/005 (0000/0588)]\tLoss Ss: 0.031382\n","\tEpoch:32 [000/005 (0020/0588)]\tLoss Ss: 0.042162\n","\tEpoch:32 [000/005 (0040/0588)]\tLoss Ss: 0.016176\n","\tEpoch:32 [000/005 (0060/0588)]\tLoss Ss: 0.014850\n","\tEpoch:32 [000/005 (0080/0588)]\tLoss Ss: 0.010606\n","\tEpoch:32 [000/005 (0100/0588)]\tLoss Ss: 0.007990\n","\tEpoch:32 [000/005 (0120/0588)]\tLoss Ss: 0.011516\n","\tEpoch:32 [000/005 (0140/0588)]\tLoss Ss: 0.007440\n","\tEpoch:32 [000/005 (0160/0588)]\tLoss Ss: 0.007458\n","\tEpoch:32 [000/005 (0180/0588)]\tLoss Ss: 0.005697\n","\tEpoch:32 [000/005 (0200/0588)]\tLoss Ss: 0.011283\n","\tEpoch:32 [000/005 (0220/0588)]\tLoss Ss: 0.004711\n","\tEpoch:32 [000/005 (0240/0588)]\tLoss Ss: 0.004468\n","\tEpoch:32 [000/005 (0260/0588)]\tLoss Ss: 0.009053\n","\tEpoch:32 [000/005 (0280/0588)]\tLoss Ss: 0.005445\n","\tEpoch:32 [000/005 (0300/0588)]\tLoss Ss: 0.005421\n","\tEpoch:32 [000/005 (0320/0588)]\tLoss Ss: 0.009060\n","\tEpoch:32 [000/005 (0340/0588)]\tLoss Ss: 0.006142\n","\tEpoch:32 [000/005 (0360/0588)]\tLoss Ss: 0.006951\n","\tEpoch:32 [000/005 (0380/0588)]\tLoss Ss: 0.005350\n","\tEpoch:32 [000/005 (0400/0588)]\tLoss Ss: 0.008134\n","\tEpoch:32 [000/005 (0420/0588)]\tLoss Ss: 0.007112\n","\tEpoch:32 [000/005 (0440/0588)]\tLoss Ss: 0.005815\n","\tEpoch:32 [000/005 (0460/0588)]\tLoss Ss: 0.007290\n","\tEpoch:32 [000/005 (0480/0588)]\tLoss Ss: 0.005480\n","\tEpoch:32 [000/005 (0500/0588)]\tLoss Ss: 0.006049\n","\tEpoch:32 [000/005 (0520/0588)]\tLoss Ss: 0.006020\n","\tEpoch:32 [000/005 (0540/0588)]\tLoss Ss: 0.005137\n","\tEpoch:32 [000/005 (0560/0588)]\tLoss Ss: 0.005620\n","\tEpoch:32 [000/005 (0580/0588)]\tLoss Ss: 0.004602\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:32 [001/005 (0000/0755)]\tLoss Ss: 0.025819\n","\tEpoch:32 [001/005 (0020/0755)]\tLoss Ss: 0.038918\n","\tEpoch:32 [001/005 (0040/0755)]\tLoss Ss: 0.032847\n","\tEpoch:32 [001/005 (0060/0755)]\tLoss Ss: 0.025814\n","\tEpoch:32 [001/005 (0080/0755)]\tLoss Ss: 0.021326\n","\tEpoch:32 [001/005 (0100/0755)]\tLoss Ss: 0.018719\n","\tEpoch:32 [001/005 (0120/0755)]\tLoss Ss: 0.020105\n","\tEpoch:32 [001/005 (0140/0755)]\tLoss Ss: 0.014006\n","\tEpoch:32 [001/005 (0160/0755)]\tLoss Ss: 0.020779\n","\tEpoch:32 [001/005 (0180/0755)]\tLoss Ss: 0.014706\n","\tEpoch:32 [001/005 (0200/0755)]\tLoss Ss: 0.018306\n","\tEpoch:32 [001/005 (0220/0755)]\tLoss Ss: 0.015794\n","\tEpoch:32 [001/005 (0240/0755)]\tLoss Ss: 0.015294\n","\tEpoch:32 [001/005 (0260/0755)]\tLoss Ss: 0.025082\n","\tEpoch:32 [001/005 (0280/0755)]\tLoss Ss: 0.019777\n","\tEpoch:32 [001/005 (0300/0755)]\tLoss Ss: 0.012566\n","\tEpoch:32 [001/005 (0320/0755)]\tLoss Ss: 0.016548\n","\tEpoch:32 [001/005 (0340/0755)]\tLoss Ss: 0.016773\n","\tEpoch:32 [001/005 (0360/0755)]\tLoss Ss: 0.016062\n","\tEpoch:32 [001/005 (0380/0755)]\tLoss Ss: 0.021835\n","\tEpoch:32 [001/005 (0400/0755)]\tLoss Ss: 0.020250\n","\tEpoch:32 [001/005 (0420/0755)]\tLoss Ss: 0.015321\n","\tEpoch:32 [001/005 (0440/0755)]\tLoss Ss: 0.015289\n","\tEpoch:32 [001/005 (0460/0755)]\tLoss Ss: 0.013703\n","\tEpoch:32 [001/005 (0480/0755)]\tLoss Ss: 0.014400\n","\tEpoch:32 [001/005 (0500/0755)]\tLoss Ss: 0.017527\n","\tEpoch:32 [001/005 (0520/0755)]\tLoss Ss: 0.014465\n","\tEpoch:32 [001/005 (0540/0755)]\tLoss Ss: 0.013090\n","\tEpoch:32 [001/005 (0560/0755)]\tLoss Ss: 0.013734\n","\tEpoch:32 [001/005 (0580/0755)]\tLoss Ss: 0.012824\n","\tEpoch:32 [001/005 (0600/0755)]\tLoss Ss: 0.015952\n","\tEpoch:32 [001/005 (0620/0755)]\tLoss Ss: 0.019249\n","\tEpoch:32 [001/005 (0640/0755)]\tLoss Ss: 0.013725\n","\tEpoch:32 [001/005 (0660/0755)]\tLoss Ss: 0.012811\n","\tEpoch:32 [001/005 (0680/0755)]\tLoss Ss: 0.016269\n","\tEpoch:32 [001/005 (0700/0755)]\tLoss Ss: 0.019383\n","\tEpoch:32 [001/005 (0720/0755)]\tLoss Ss: 0.017300\n","\tEpoch:32 [001/005 (0740/0755)]\tLoss Ss: 0.008649\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:32 [002/005 (0000/0755)]\tLoss Ss: 0.016334\n","\tEpoch:32 [002/005 (0020/0755)]\tLoss Ss: 0.016509\n","\tEpoch:32 [002/005 (0040/0755)]\tLoss Ss: 0.016248\n","\tEpoch:32 [002/005 (0060/0755)]\tLoss Ss: 0.013593\n","\tEpoch:32 [002/005 (0080/0755)]\tLoss Ss: 0.013842\n","\tEpoch:32 [002/005 (0100/0755)]\tLoss Ss: 0.017676\n","\tEpoch:32 [002/005 (0120/0755)]\tLoss Ss: 0.014554\n","\tEpoch:32 [002/005 (0140/0755)]\tLoss Ss: 0.021202\n","\tEpoch:32 [002/005 (0160/0755)]\tLoss Ss: 0.020454\n","\tEpoch:32 [002/005 (0180/0755)]\tLoss Ss: 0.013998\n","\tEpoch:32 [002/005 (0200/0755)]\tLoss Ss: 0.010738\n","\tEpoch:32 [002/005 (0220/0755)]\tLoss Ss: 0.009446\n","\tEpoch:32 [002/005 (0240/0755)]\tLoss Ss: 0.013796\n","\tEpoch:32 [002/005 (0260/0755)]\tLoss Ss: 0.012998\n","\tEpoch:32 [002/005 (0280/0755)]\tLoss Ss: 0.015747\n","\tEpoch:32 [002/005 (0300/0755)]\tLoss Ss: 0.014016\n","\tEpoch:32 [002/005 (0320/0755)]\tLoss Ss: 0.013845\n","\tEpoch:32 [002/005 (0340/0755)]\tLoss Ss: 0.015674\n","\tEpoch:32 [002/005 (0360/0755)]\tLoss Ss: 0.011335\n","\tEpoch:32 [002/005 (0380/0755)]\tLoss Ss: 0.012516\n","\tEpoch:32 [002/005 (0400/0755)]\tLoss Ss: 0.014522\n","\tEpoch:32 [002/005 (0420/0755)]\tLoss Ss: 0.008430\n","\tEpoch:32 [002/005 (0440/0755)]\tLoss Ss: 0.009861\n","\tEpoch:32 [002/005 (0460/0755)]\tLoss Ss: 0.018826\n","\tEpoch:32 [002/005 (0480/0755)]\tLoss Ss: 0.009033\n","\tEpoch:32 [002/005 (0500/0755)]\tLoss Ss: 0.011572\n","\tEpoch:32 [002/005 (0520/0755)]\tLoss Ss: 0.013040\n","\tEpoch:32 [002/005 (0540/0755)]\tLoss Ss: 0.013181\n","\tEpoch:32 [002/005 (0560/0755)]\tLoss Ss: 0.012541\n","\tEpoch:32 [002/005 (0580/0755)]\tLoss Ss: 0.010684\n","\tEpoch:32 [002/005 (0600/0755)]\tLoss Ss: 0.012540\n","\tEpoch:32 [002/005 (0620/0755)]\tLoss Ss: 0.008998\n","\tEpoch:32 [002/005 (0640/0755)]\tLoss Ss: 0.015318\n","\tEpoch:32 [002/005 (0660/0755)]\tLoss Ss: 0.011331\n","\tEpoch:32 [002/005 (0680/0755)]\tLoss Ss: 0.013916\n","\tEpoch:32 [002/005 (0700/0755)]\tLoss Ss: 0.019375\n","\tEpoch:32 [002/005 (0720/0755)]\tLoss Ss: 0.009415\n","\tEpoch:32 [002/005 (0740/0755)]\tLoss Ss: 0.009174\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tEpoch:32 [003/005 (0000/0614)]\tLoss Ss: 0.004532\n","\tEpoch:32 [003/005 (0020/0614)]\tLoss Ss: 0.004298\n","\tEpoch:32 [003/005 (0040/0614)]\tLoss Ss: 0.006743\n","\tEpoch:32 [003/005 (0060/0614)]\tLoss Ss: 0.004856\n","\tEpoch:32 [003/005 (0080/0614)]\tLoss Ss: 0.004049\n","\tEpoch:32 [003/005 (0100/0614)]\tLoss Ss: 0.006253\n","\tEpoch:32 [003/005 (0120/0614)]\tLoss Ss: 0.005508\n","\tEpoch:32 [003/005 (0140/0614)]\tLoss Ss: 0.006868\n","\tEpoch:32 [003/005 (0160/0614)]\tLoss Ss: 0.005603\n","\tEpoch:32 [003/005 (0180/0614)]\tLoss Ss: 0.007107\n","\tEpoch:32 [003/005 (0200/0614)]\tLoss Ss: 0.007468\n","\tEpoch:32 [003/005 (0220/0614)]\tLoss Ss: 0.006315\n","\tEpoch:32 [003/005 (0240/0614)]\tLoss Ss: 0.006968\n","\tEpoch:32 [003/005 (0260/0614)]\tLoss Ss: 0.006492\n","\tEpoch:32 [003/005 (0280/0614)]\tLoss Ss: 0.005258\n","\tEpoch:32 [003/005 (0300/0614)]\tLoss Ss: 0.005053\n","\tEpoch:32 [003/005 (0320/0614)]\tLoss Ss: 0.005521\n","\tEpoch:32 [003/005 (0340/0614)]\tLoss Ss: 0.006036\n","\tEpoch:32 [003/005 (0360/0614)]\tLoss Ss: 0.006313\n","\tEpoch:32 [003/005 (0380/0614)]\tLoss Ss: 0.007504\n","\tEpoch:32 [003/005 (0400/0614)]\tLoss Ss: 0.003732\n","\tEpoch:32 [003/005 (0420/0614)]\tLoss Ss: 0.004981\n","\tEpoch:32 [003/005 (0440/0614)]\tLoss Ss: 0.003726\n","\tEpoch:32 [003/005 (0460/0614)]\tLoss Ss: 0.004689\n","\tEpoch:32 [003/005 (0480/0614)]\tLoss Ss: 0.005720\n","\tEpoch:32 [003/005 (0500/0614)]\tLoss Ss: 0.004591\n","\tEpoch:32 [003/005 (0520/0614)]\tLoss Ss: 0.006867\n","\tEpoch:32 [003/005 (0540/0614)]\tLoss Ss: 0.006228\n","\tEpoch:32 [003/005 (0560/0614)]\tLoss Ss: 0.007363\n","\tEpoch:32 [003/005 (0580/0614)]\tLoss Ss: 0.003676\n","\tEpoch:32 [003/005 (0600/0614)]\tLoss Ss: 0.004367\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tEpoch:32 [004/005 (0000/0693)]\tLoss Ss: 0.013766\n","\tEpoch:32 [004/005 (0020/0693)]\tLoss Ss: 0.014362\n","\tEpoch:32 [004/005 (0040/0693)]\tLoss Ss: 0.013812\n","\tEpoch:32 [004/005 (0060/0693)]\tLoss Ss: 0.017510\n","\tEpoch:32 [004/005 (0080/0693)]\tLoss Ss: 0.014871\n","\tEpoch:32 [004/005 (0100/0693)]\tLoss Ss: 0.009231\n","\tEpoch:32 [004/005 (0120/0693)]\tLoss Ss: 0.011352\n","\tEpoch:32 [004/005 (0140/0693)]\tLoss Ss: 0.011119\n","\tEpoch:32 [004/005 (0160/0693)]\tLoss Ss: 0.013862\n","\tEpoch:32 [004/005 (0180/0693)]\tLoss Ss: 0.007511\n","\tEpoch:32 [004/005 (0200/0693)]\tLoss Ss: 0.013659\n","\tEpoch:32 [004/005 (0220/0693)]\tLoss Ss: 0.017973\n","\tEpoch:32 [004/005 (0240/0693)]\tLoss Ss: 0.015253\n","\tEpoch:32 [004/005 (0260/0693)]\tLoss Ss: 0.014165\n","\tEpoch:32 [004/005 (0280/0693)]\tLoss Ss: 0.013493\n","\tEpoch:32 [004/005 (0300/0693)]\tLoss Ss: 0.017483\n","\tEpoch:32 [004/005 (0320/0693)]\tLoss Ss: 0.016038\n","\tEpoch:32 [004/005 (0340/0693)]\tLoss Ss: 0.018695\n","\tEpoch:32 [004/005 (0360/0693)]\tLoss Ss: 0.013043\n","\tEpoch:32 [004/005 (0380/0693)]\tLoss Ss: 0.009988\n","\tEpoch:32 [004/005 (0400/0693)]\tLoss Ss: 0.010821\n","\tEpoch:32 [004/005 (0420/0693)]\tLoss Ss: 0.013796\n","\tEpoch:32 [004/005 (0440/0693)]\tLoss Ss: 0.017704\n","\tEpoch:32 [004/005 (0460/0693)]\tLoss Ss: 0.013778\n","\tEpoch:32 [004/005 (0480/0693)]\tLoss Ss: 0.014175\n","\tEpoch:32 [004/005 (0500/0693)]\tLoss Ss: 0.015069\n","\tEpoch:32 [004/005 (0520/0693)]\tLoss Ss: 0.015511\n","\tEpoch:32 [004/005 (0540/0693)]\tLoss Ss: 0.009299\n","\tEpoch:32 [004/005 (0560/0693)]\tLoss Ss: 0.010370\n","\tEpoch:32 [004/005 (0580/0693)]\tLoss Ss: 0.010075\n","\tEpoch:32 [004/005 (0600/0693)]\tLoss Ss: 0.015199\n","\tEpoch:32 [004/005 (0620/0693)]\tLoss Ss: 0.011704\n","\tEpoch:32 [004/005 (0640/0693)]\tLoss Ss: 0.016311\n","\tEpoch:32 [004/005 (0660/0693)]\tLoss Ss: 0.007840\n","\tEpoch:32 [004/005 (0680/0693)]\tLoss Ss: 0.012520\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tEpoch:32 [005/005 (0000/0693)]\tLoss Ss: 0.013203\n","\tEpoch:32 [005/005 (0020/0693)]\tLoss Ss: 0.013493\n","\tEpoch:32 [005/005 (0040/0693)]\tLoss Ss: 0.015969\n","\tEpoch:32 [005/005 (0060/0693)]\tLoss Ss: 0.016661\n","\tEpoch:32 [005/005 (0080/0693)]\tLoss Ss: 0.008526\n","\tEpoch:32 [005/005 (0100/0693)]\tLoss Ss: 0.015691\n","\tEpoch:32 [005/005 (0120/0693)]\tLoss Ss: 0.009781\n","\tEpoch:32 [005/005 (0140/0693)]\tLoss Ss: 0.011435\n","\tEpoch:32 [005/005 (0160/0693)]\tLoss Ss: 0.011981\n","\tEpoch:32 [005/005 (0180/0693)]\tLoss Ss: 0.012775\n","\tEpoch:32 [005/005 (0200/0693)]\tLoss Ss: 0.011799\n","\tEpoch:32 [005/005 (0220/0693)]\tLoss Ss: 0.011540\n","\tEpoch:32 [005/005 (0240/0693)]\tLoss Ss: 0.005417\n","\tEpoch:32 [005/005 (0260/0693)]\tLoss Ss: 0.008628\n","\tEpoch:32 [005/005 (0280/0693)]\tLoss Ss: 0.012831\n","\tEpoch:32 [005/005 (0300/0693)]\tLoss Ss: 0.012974\n","\tEpoch:32 [005/005 (0320/0693)]\tLoss Ss: 0.015386\n","\tEpoch:32 [005/005 (0340/0693)]\tLoss Ss: 0.009771\n","\tEpoch:32 [005/005 (0360/0693)]\tLoss Ss: 0.010743\n","\tEpoch:32 [005/005 (0380/0693)]\tLoss Ss: 0.010091\n","\tEpoch:32 [005/005 (0400/0693)]\tLoss Ss: 0.014639\n","\tEpoch:32 [005/005 (0420/0693)]\tLoss Ss: 0.010716\n","\tEpoch:32 [005/005 (0440/0693)]\tLoss Ss: 0.012891\n","\tEpoch:32 [005/005 (0460/0693)]\tLoss Ss: 0.013195\n","\tEpoch:32 [005/005 (0480/0693)]\tLoss Ss: 0.006726\n","\tEpoch:32 [005/005 (0500/0693)]\tLoss Ss: 0.014687\n","\tEpoch:32 [005/005 (0520/0693)]\tLoss Ss: 0.013283\n","\tEpoch:32 [005/005 (0540/0693)]\tLoss Ss: 0.012329\n","\tEpoch:32 [005/005 (0560/0693)]\tLoss Ss: 0.009287\n","\tEpoch:32 [005/005 (0580/0693)]\tLoss Ss: 0.011249\n","\tEpoch:32 [005/005 (0600/0693)]\tLoss Ss: 0.009492\n","\tEpoch:32 [005/005 (0620/0693)]\tLoss Ss: 0.011826\n","\tEpoch:32 [005/005 (0640/0693)]\tLoss Ss: 0.013855\n","\tEpoch:32 [005/005 (0660/0693)]\tLoss Ss: 0.011648\n","\tEpoch:32 [005/005 (0680/0693)]\tLoss Ss: 0.006922\n","Now train the rotated image\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:32 [000/005 (0000/0755)]\tLoss Ss: 0.243405\n","\tRotated_Epoch:32 [000/005 (0020/0755)]\tLoss Ss: 0.256260\n","\tRotated_Epoch:32 [000/005 (0040/0755)]\tLoss Ss: 0.149334\n","\tRotated_Epoch:32 [000/005 (0060/0755)]\tLoss Ss: 0.168778\n","\tRotated_Epoch:32 [000/005 (0080/0755)]\tLoss Ss: 0.117168\n","\tRotated_Epoch:32 [000/005 (0100/0755)]\tLoss Ss: 0.112171\n","\tRotated_Epoch:32 [000/005 (0120/0755)]\tLoss Ss: 0.132127\n","\tRotated_Epoch:32 [000/005 (0140/0755)]\tLoss Ss: 0.078024\n","\tRotated_Epoch:32 [000/005 (0160/0755)]\tLoss Ss: 0.123999\n","\tRotated_Epoch:32 [000/005 (0180/0755)]\tLoss Ss: 0.076969\n","\tRotated_Epoch:32 [000/005 (0200/0755)]\tLoss Ss: 0.107890\n","\tRotated_Epoch:32 [000/005 (0220/0755)]\tLoss Ss: 0.061909\n","\tRotated_Epoch:32 [000/005 (0240/0755)]\tLoss Ss: 0.075582\n","\tRotated_Epoch:32 [000/005 (0260/0755)]\tLoss Ss: 0.051745\n","\tRotated_Epoch:32 [000/005 (0280/0755)]\tLoss Ss: 0.074265\n","\tRotated_Epoch:32 [000/005 (0300/0755)]\tLoss Ss: 0.055468\n","\tRotated_Epoch:32 [000/005 (0320/0755)]\tLoss Ss: 0.040012\n","\tRotated_Epoch:32 [000/005 (0340/0755)]\tLoss Ss: 0.067804\n","\tRotated_Epoch:32 [000/005 (0360/0755)]\tLoss Ss: 0.060941\n","\tRotated_Epoch:32 [000/005 (0380/0755)]\tLoss Ss: 0.050241\n","\tRotated_Epoch:32 [000/005 (0400/0755)]\tLoss Ss: 0.058619\n","\tRotated_Epoch:32 [000/005 (0420/0755)]\tLoss Ss: 0.055707\n","\tRotated_Epoch:32 [000/005 (0440/0755)]\tLoss Ss: 0.054434\n","\tRotated_Epoch:32 [000/005 (0460/0755)]\tLoss Ss: 0.033423\n","\tRotated_Epoch:32 [000/005 (0480/0755)]\tLoss Ss: 0.045080\n","\tRotated_Epoch:32 [000/005 (0500/0755)]\tLoss Ss: 0.055933\n","\tRotated_Epoch:32 [000/005 (0520/0755)]\tLoss Ss: 0.046316\n","\tRotated_Epoch:32 [000/005 (0540/0755)]\tLoss Ss: 0.044472\n","\tRotated_Epoch:32 [000/005 (0560/0755)]\tLoss Ss: 0.046871\n","\tRotated_Epoch:32 [000/005 (0580/0755)]\tLoss Ss: 0.038770\n","\tRotated_Epoch:32 [000/005 (0600/0755)]\tLoss Ss: 0.052439\n","\tRotated_Epoch:32 [000/005 (0620/0755)]\tLoss Ss: 0.028384\n","\tRotated_Epoch:32 [000/005 (0640/0755)]\tLoss Ss: 0.049263\n","\tRotated_Epoch:32 [000/005 (0660/0755)]\tLoss Ss: 0.033733\n","\tRotated_Epoch:32 [000/005 (0680/0755)]\tLoss Ss: 0.037205\n","\tRotated_Epoch:32 [000/005 (0700/0755)]\tLoss Ss: 0.030793\n","\tRotated_Epoch:32 [000/005 (0720/0755)]\tLoss Ss: 0.044232\n","\tRotated_Epoch:32 [000/005 (0740/0755)]\tLoss Ss: 0.042057\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tRotated_Epoch:32 [001/005 (0000/0588)]\tLoss Ss: 0.077164\n","\tRotated_Epoch:32 [001/005 (0020/0588)]\tLoss Ss: 0.062543\n","\tRotated_Epoch:32 [001/005 (0040/0588)]\tLoss Ss: 0.064752\n","\tRotated_Epoch:32 [001/005 (0060/0588)]\tLoss Ss: 0.056355\n","\tRotated_Epoch:32 [001/005 (0080/0588)]\tLoss Ss: 0.064168\n","\tRotated_Epoch:32 [001/005 (0100/0588)]\tLoss Ss: 0.055457\n","\tRotated_Epoch:32 [001/005 (0120/0588)]\tLoss Ss: 0.069777\n","\tRotated_Epoch:32 [001/005 (0140/0588)]\tLoss Ss: 0.052398\n","\tRotated_Epoch:32 [001/005 (0160/0588)]\tLoss Ss: 0.051932\n","\tRotated_Epoch:32 [001/005 (0180/0588)]\tLoss Ss: 0.043371\n","\tRotated_Epoch:32 [001/005 (0200/0588)]\tLoss Ss: 0.044088\n","\tRotated_Epoch:32 [001/005 (0220/0588)]\tLoss Ss: 0.050482\n","\tRotated_Epoch:32 [001/005 (0240/0588)]\tLoss Ss: 0.058269\n","\tRotated_Epoch:32 [001/005 (0260/0588)]\tLoss Ss: 0.086597\n","\tRotated_Epoch:32 [001/005 (0280/0588)]\tLoss Ss: 0.052437\n","\tRotated_Epoch:32 [001/005 (0300/0588)]\tLoss Ss: 0.060914\n","\tRotated_Epoch:32 [001/005 (0320/0588)]\tLoss Ss: 0.047478\n","\tRotated_Epoch:32 [001/005 (0340/0588)]\tLoss Ss: 0.052468\n","\tRotated_Epoch:32 [001/005 (0360/0588)]\tLoss Ss: 0.037797\n","\tRotated_Epoch:32 [001/005 (0380/0588)]\tLoss Ss: 0.061081\n","\tRotated_Epoch:32 [001/005 (0400/0588)]\tLoss Ss: 0.036537\n","\tRotated_Epoch:32 [001/005 (0420/0588)]\tLoss Ss: 0.046776\n","\tRotated_Epoch:32 [001/005 (0440/0588)]\tLoss Ss: 0.046542\n","\tRotated_Epoch:32 [001/005 (0460/0588)]\tLoss Ss: 0.047777\n","\tRotated_Epoch:32 [001/005 (0480/0588)]\tLoss Ss: 0.057810\n","\tRotated_Epoch:32 [001/005 (0500/0588)]\tLoss Ss: 0.065372\n","\tRotated_Epoch:32 [001/005 (0520/0588)]\tLoss Ss: 0.045265\n","\tRotated_Epoch:32 [001/005 (0540/0588)]\tLoss Ss: 0.049191\n","\tRotated_Epoch:32 [001/005 (0560/0588)]\tLoss Ss: 0.057431\n","\tRotated_Epoch:32 [001/005 (0580/0588)]\tLoss Ss: 0.058183\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tRotated_Epoch:32 [002/005 (0000/0614)]\tLoss Ss: 0.139703\n","\tRotated_Epoch:32 [002/005 (0020/0614)]\tLoss Ss: 0.060667\n","\tRotated_Epoch:32 [002/005 (0040/0614)]\tLoss Ss: 0.059007\n","\tRotated_Epoch:32 [002/005 (0060/0614)]\tLoss Ss: 0.043799\n","\tRotated_Epoch:32 [002/005 (0080/0614)]\tLoss Ss: 0.037544\n","\tRotated_Epoch:32 [002/005 (0100/0614)]\tLoss Ss: 0.017536\n","\tRotated_Epoch:32 [002/005 (0120/0614)]\tLoss Ss: 0.016348\n","\tRotated_Epoch:32 [002/005 (0140/0614)]\tLoss Ss: 0.014985\n","\tRotated_Epoch:32 [002/005 (0160/0614)]\tLoss Ss: 0.014354\n","\tRotated_Epoch:32 [002/005 (0180/0614)]\tLoss Ss: 0.017079\n","\tRotated_Epoch:32 [002/005 (0200/0614)]\tLoss Ss: 0.016624\n","\tRotated_Epoch:32 [002/005 (0220/0614)]\tLoss Ss: 0.018420\n","\tRotated_Epoch:32 [002/005 (0240/0614)]\tLoss Ss: 0.012729\n","\tRotated_Epoch:32 [002/005 (0260/0614)]\tLoss Ss: 0.008828\n","\tRotated_Epoch:32 [002/005 (0280/0614)]\tLoss Ss: 0.011329\n","\tRotated_Epoch:32 [002/005 (0300/0614)]\tLoss Ss: 0.013624\n","\tRotated_Epoch:32 [002/005 (0320/0614)]\tLoss Ss: 0.011342\n","\tRotated_Epoch:32 [002/005 (0340/0614)]\tLoss Ss: 0.010341\n","\tRotated_Epoch:32 [002/005 (0360/0614)]\tLoss Ss: 0.008775\n","\tRotated_Epoch:32 [002/005 (0380/0614)]\tLoss Ss: 0.012498\n","\tRotated_Epoch:32 [002/005 (0400/0614)]\tLoss Ss: 0.008898\n","\tRotated_Epoch:32 [002/005 (0420/0614)]\tLoss Ss: 0.009610\n","\tRotated_Epoch:32 [002/005 (0440/0614)]\tLoss Ss: 0.010945\n","\tRotated_Epoch:32 [002/005 (0460/0614)]\tLoss Ss: 0.011314\n","\tRotated_Epoch:32 [002/005 (0480/0614)]\tLoss Ss: 0.009726\n","\tRotated_Epoch:32 [002/005 (0500/0614)]\tLoss Ss: 0.008119\n","\tRotated_Epoch:32 [002/005 (0520/0614)]\tLoss Ss: 0.007259\n","\tRotated_Epoch:32 [002/005 (0540/0614)]\tLoss Ss: 0.010103\n","\tRotated_Epoch:32 [002/005 (0560/0614)]\tLoss Ss: 0.007662\n","\tRotated_Epoch:32 [002/005 (0580/0614)]\tLoss Ss: 0.008599\n","\tRotated_Epoch:32 [002/005 (0600/0614)]\tLoss Ss: 0.006874\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tRotated_Epoch:32 [003/005 (0000/0693)]\tLoss Ss: 0.022081\n","\tRotated_Epoch:32 [003/005 (0020/0693)]\tLoss Ss: 0.023971\n","\tRotated_Epoch:32 [003/005 (0040/0693)]\tLoss Ss: 0.022150\n","\tRotated_Epoch:32 [003/005 (0060/0693)]\tLoss Ss: 0.015878\n","\tRotated_Epoch:32 [003/005 (0080/0693)]\tLoss Ss: 0.020304\n","\tRotated_Epoch:32 [003/005 (0100/0693)]\tLoss Ss: 0.012754\n","\tRotated_Epoch:32 [003/005 (0120/0693)]\tLoss Ss: 0.014331\n","\tRotated_Epoch:32 [003/005 (0140/0693)]\tLoss Ss: 0.024663\n","\tRotated_Epoch:32 [003/005 (0160/0693)]\tLoss Ss: 0.018055\n","\tRotated_Epoch:32 [003/005 (0180/0693)]\tLoss Ss: 0.014182\n","\tRotated_Epoch:32 [003/005 (0200/0693)]\tLoss Ss: 0.023612\n","\tRotated_Epoch:32 [003/005 (0220/0693)]\tLoss Ss: 0.016478\n","\tRotated_Epoch:32 [003/005 (0240/0693)]\tLoss Ss: 0.011607\n","\tRotated_Epoch:32 [003/005 (0260/0693)]\tLoss Ss: 0.011654\n","\tRotated_Epoch:32 [003/005 (0280/0693)]\tLoss Ss: 0.017568\n","\tRotated_Epoch:32 [003/005 (0300/0693)]\tLoss Ss: 0.022770\n","\tRotated_Epoch:32 [003/005 (0320/0693)]\tLoss Ss: 0.015821\n","\tRotated_Epoch:32 [003/005 (0340/0693)]\tLoss Ss: 0.018650\n","\tRotated_Epoch:32 [003/005 (0360/0693)]\tLoss Ss: 0.025764\n","\tRotated_Epoch:32 [003/005 (0380/0693)]\tLoss Ss: 0.010515\n","\tRotated_Epoch:32 [003/005 (0400/0693)]\tLoss Ss: 0.018673\n","\tRotated_Epoch:32 [003/005 (0420/0693)]\tLoss Ss: 0.014479\n","\tRotated_Epoch:32 [003/005 (0440/0693)]\tLoss Ss: 0.016445\n","\tRotated_Epoch:32 [003/005 (0460/0693)]\tLoss Ss: 0.011562\n","\tRotated_Epoch:32 [003/005 (0480/0693)]\tLoss Ss: 0.015269\n","\tRotated_Epoch:32 [003/005 (0500/0693)]\tLoss Ss: 0.012445\n","\tRotated_Epoch:32 [003/005 (0520/0693)]\tLoss Ss: 0.011422\n","\tRotated_Epoch:32 [003/005 (0540/0693)]\tLoss Ss: 0.010813\n","\tRotated_Epoch:32 [003/005 (0560/0693)]\tLoss Ss: 0.014266\n","\tRotated_Epoch:32 [003/005 (0580/0693)]\tLoss Ss: 0.011377\n","\tRotated_Epoch:32 [003/005 (0600/0693)]\tLoss Ss: 0.017271\n","\tRotated_Epoch:32 [003/005 (0620/0693)]\tLoss Ss: 0.013333\n","\tRotated_Epoch:32 [003/005 (0640/0693)]\tLoss Ss: 0.011790\n","\tRotated_Epoch:32 [003/005 (0660/0693)]\tLoss Ss: 0.014450\n","\tRotated_Epoch:32 [003/005 (0680/0693)]\tLoss Ss: 0.023045\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tRotated_Epoch:32 [004/005 (0000/0693)]\tLoss Ss: 0.014082\n","\tRotated_Epoch:32 [004/005 (0020/0693)]\tLoss Ss: 0.010896\n","\tRotated_Epoch:32 [004/005 (0040/0693)]\tLoss Ss: 0.020327\n","\tRotated_Epoch:32 [004/005 (0060/0693)]\tLoss Ss: 0.019824\n","\tRotated_Epoch:32 [004/005 (0080/0693)]\tLoss Ss: 0.009497\n","\tRotated_Epoch:32 [004/005 (0100/0693)]\tLoss Ss: 0.011985\n","\tRotated_Epoch:32 [004/005 (0120/0693)]\tLoss Ss: 0.017398\n","\tRotated_Epoch:32 [004/005 (0140/0693)]\tLoss Ss: 0.014812\n","\tRotated_Epoch:32 [004/005 (0160/0693)]\tLoss Ss: 0.015401\n","\tRotated_Epoch:32 [004/005 (0180/0693)]\tLoss Ss: 0.015414\n","\tRotated_Epoch:32 [004/005 (0200/0693)]\tLoss Ss: 0.018249\n","\tRotated_Epoch:32 [004/005 (0220/0693)]\tLoss Ss: 0.021591\n","\tRotated_Epoch:32 [004/005 (0240/0693)]\tLoss Ss: 0.013887\n","\tRotated_Epoch:32 [004/005 (0260/0693)]\tLoss Ss: 0.010323\n","\tRotated_Epoch:32 [004/005 (0280/0693)]\tLoss Ss: 0.015359\n","\tRotated_Epoch:32 [004/005 (0300/0693)]\tLoss Ss: 0.017901\n","\tRotated_Epoch:32 [004/005 (0320/0693)]\tLoss Ss: 0.016103\n","\tRotated_Epoch:32 [004/005 (0340/0693)]\tLoss Ss: 0.016099\n","\tRotated_Epoch:32 [004/005 (0360/0693)]\tLoss Ss: 0.017495\n","\tRotated_Epoch:32 [004/005 (0380/0693)]\tLoss Ss: 0.012534\n","\tRotated_Epoch:32 [004/005 (0400/0693)]\tLoss Ss: 0.013312\n","\tRotated_Epoch:32 [004/005 (0420/0693)]\tLoss Ss: 0.013413\n","\tRotated_Epoch:32 [004/005 (0440/0693)]\tLoss Ss: 0.017611\n","\tRotated_Epoch:32 [004/005 (0460/0693)]\tLoss Ss: 0.014628\n","\tRotated_Epoch:32 [004/005 (0480/0693)]\tLoss Ss: 0.012823\n","\tRotated_Epoch:32 [004/005 (0500/0693)]\tLoss Ss: 0.017862\n","\tRotated_Epoch:32 [004/005 (0520/0693)]\tLoss Ss: 0.011212\n","\tRotated_Epoch:32 [004/005 (0540/0693)]\tLoss Ss: 0.019875\n","\tRotated_Epoch:32 [004/005 (0560/0693)]\tLoss Ss: 0.013913\n","\tRotated_Epoch:32 [004/005 (0580/0693)]\tLoss Ss: 0.016639\n","\tRotated_Epoch:32 [004/005 (0600/0693)]\tLoss Ss: 0.010379\n","\tRotated_Epoch:32 [004/005 (0620/0693)]\tLoss Ss: 0.017110\n","\tRotated_Epoch:32 [004/005 (0640/0693)]\tLoss Ss: 0.014476\n","\tRotated_Epoch:32 [004/005 (0660/0693)]\tLoss Ss: 0.017733\n","\tRotated_Epoch:32 [004/005 (0680/0693)]\tLoss Ss: 0.013714\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:32 [005/005 (0000/0755)]\tLoss Ss: 0.030343\n","\tRotated_Epoch:32 [005/005 (0020/0755)]\tLoss Ss: 0.067644\n","\tRotated_Epoch:32 [005/005 (0040/0755)]\tLoss Ss: 0.050058\n","\tRotated_Epoch:32 [005/005 (0060/0755)]\tLoss Ss: 0.035008\n","\tRotated_Epoch:32 [005/005 (0080/0755)]\tLoss Ss: 0.026366\n","\tRotated_Epoch:32 [005/005 (0100/0755)]\tLoss Ss: 0.043245\n","\tRotated_Epoch:32 [005/005 (0120/0755)]\tLoss Ss: 0.029686\n","\tRotated_Epoch:32 [005/005 (0140/0755)]\tLoss Ss: 0.014805\n","\tRotated_Epoch:32 [005/005 (0160/0755)]\tLoss Ss: 0.015114\n","\tRotated_Epoch:32 [005/005 (0180/0755)]\tLoss Ss: 0.023130\n","\tRotated_Epoch:32 [005/005 (0200/0755)]\tLoss Ss: 0.017290\n","\tRotated_Epoch:32 [005/005 (0220/0755)]\tLoss Ss: 0.014323\n","\tRotated_Epoch:32 [005/005 (0240/0755)]\tLoss Ss: 0.031577\n","\tRotated_Epoch:32 [005/005 (0260/0755)]\tLoss Ss: 0.025322\n","\tRotated_Epoch:32 [005/005 (0280/0755)]\tLoss Ss: 0.023200\n","\tRotated_Epoch:32 [005/005 (0300/0755)]\tLoss Ss: 0.026430\n","\tRotated_Epoch:32 [005/005 (0320/0755)]\tLoss Ss: 0.023648\n","\tRotated_Epoch:32 [005/005 (0340/0755)]\tLoss Ss: 0.013489\n","\tRotated_Epoch:32 [005/005 (0360/0755)]\tLoss Ss: 0.015296\n","\tRotated_Epoch:32 [005/005 (0380/0755)]\tLoss Ss: 0.021108\n","\tRotated_Epoch:32 [005/005 (0400/0755)]\tLoss Ss: 0.019787\n","\tRotated_Epoch:32 [005/005 (0420/0755)]\tLoss Ss: 0.015253\n","\tRotated_Epoch:32 [005/005 (0440/0755)]\tLoss Ss: 0.018770\n","\tRotated_Epoch:32 [005/005 (0460/0755)]\tLoss Ss: 0.021442\n","\tRotated_Epoch:32 [005/005 (0480/0755)]\tLoss Ss: 0.026082\n","\tRotated_Epoch:32 [005/005 (0500/0755)]\tLoss Ss: 0.015490\n","\tRotated_Epoch:32 [005/005 (0520/0755)]\tLoss Ss: 0.019401\n","\tRotated_Epoch:32 [005/005 (0540/0755)]\tLoss Ss: 0.013576\n","\tRotated_Epoch:32 [005/005 (0560/0755)]\tLoss Ss: 0.021880\n","\tRotated_Epoch:32 [005/005 (0580/0755)]\tLoss Ss: 0.018913\n","\tRotated_Epoch:32 [005/005 (0600/0755)]\tLoss Ss: 0.018725\n","\tRotated_Epoch:32 [005/005 (0620/0755)]\tLoss Ss: 0.013786\n","\tRotated_Epoch:32 [005/005 (0640/0755)]\tLoss Ss: 0.015345\n","\tRotated_Epoch:32 [005/005 (0660/0755)]\tLoss Ss: 0.015974\n","\tRotated_Epoch:32 [005/005 (0680/0755)]\tLoss Ss: 0.012205\n","\tRotated_Epoch:32 [005/005 (0700/0755)]\tLoss Ss: 0.011630\n","\tRotated_Epoch:32 [005/005 (0720/0755)]\tLoss Ss: 0.022502\n","\tRotated_Epoch:32 [005/005 (0740/0755)]\tLoss Ss: 0.014849\n","new_1_6_d.nii\n","new_1_6_m.nii\n","new_1_7_d.nii\n","new_1_7_m.nii\n","new_2_5_d.nii\n","new_2_5_m.nii\n","new_2_6_d.nii\n","new_2_6_m.nii\n","new_2_7_d.nii\n","new_2_7_m.nii\n","new_3_d (2).nii\n","new_3_m (2).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (3).nii\n","new_3_m (3).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (7).nii\n","new_3_m (7).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (8).nii\n","new_3_m (8).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (9).nii\n","new_3_m (9).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_4_10_d_163526.nii\n","new_4_10_m_163526.nii.gz\n","new_4_11_d_084336.nii\n","new_4_11_m_084336.nii.gz\n","new_4_7_d_184422.nii\n","new_4_7_m_184422.nii.gz\n","new_4_8_d_143412.nii\n","new_4_8_m_143412.nii.gz\n","new_4_9_d_155347.nii\n","new_4_9_m_155347.nii.gz\n","\tEpoch: 32; Dice: 0.9625 +/- 0.0060; Loss: 9.7261\n","Begin Epoch 33\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tEpoch:33 [000/005 (0000/0693)]\tLoss Ss: 0.016501\n","\tEpoch:33 [000/005 (0020/0693)]\tLoss Ss: 0.010176\n","\tEpoch:33 [000/005 (0040/0693)]\tLoss Ss: 0.021470\n","\tEpoch:33 [000/005 (0060/0693)]\tLoss Ss: 0.013925\n","\tEpoch:33 [000/005 (0080/0693)]\tLoss Ss: 0.013793\n","\tEpoch:33 [000/005 (0100/0693)]\tLoss Ss: 0.020852\n","\tEpoch:33 [000/005 (0120/0693)]\tLoss Ss: 0.014985\n","\tEpoch:33 [000/005 (0140/0693)]\tLoss Ss: 0.019363\n","\tEpoch:33 [000/005 (0160/0693)]\tLoss Ss: 0.016278\n","\tEpoch:33 [000/005 (0180/0693)]\tLoss Ss: 0.019315\n","\tEpoch:33 [000/005 (0200/0693)]\tLoss Ss: 0.014793\n","\tEpoch:33 [000/005 (0220/0693)]\tLoss Ss: 0.014716\n","\tEpoch:33 [000/005 (0240/0693)]\tLoss Ss: 0.016022\n","\tEpoch:33 [000/005 (0260/0693)]\tLoss Ss: 0.013321\n","\tEpoch:33 [000/005 (0280/0693)]\tLoss Ss: 0.010572\n","\tEpoch:33 [000/005 (0300/0693)]\tLoss Ss: 0.016861\n","\tEpoch:33 [000/005 (0320/0693)]\tLoss Ss: 0.013895\n","\tEpoch:33 [000/005 (0340/0693)]\tLoss Ss: 0.011482\n","\tEpoch:33 [000/005 (0360/0693)]\tLoss Ss: 0.013392\n","\tEpoch:33 [000/005 (0380/0693)]\tLoss Ss: 0.013333\n","\tEpoch:33 [000/005 (0400/0693)]\tLoss Ss: 0.011018\n","\tEpoch:33 [000/005 (0420/0693)]\tLoss Ss: 0.009172\n","\tEpoch:33 [000/005 (0440/0693)]\tLoss Ss: 0.011436\n","\tEpoch:33 [000/005 (0460/0693)]\tLoss Ss: 0.017313\n","\tEpoch:33 [000/005 (0480/0693)]\tLoss Ss: 0.014623\n","\tEpoch:33 [000/005 (0500/0693)]\tLoss Ss: 0.017378\n","\tEpoch:33 [000/005 (0520/0693)]\tLoss Ss: 0.013118\n","\tEpoch:33 [000/005 (0540/0693)]\tLoss Ss: 0.014102\n","\tEpoch:33 [000/005 (0560/0693)]\tLoss Ss: 0.009480\n","\tEpoch:33 [000/005 (0580/0693)]\tLoss Ss: 0.011951\n","\tEpoch:33 [000/005 (0600/0693)]\tLoss Ss: 0.016422\n","\tEpoch:33 [000/005 (0620/0693)]\tLoss Ss: 0.013293\n","\tEpoch:33 [000/005 (0640/0693)]\tLoss Ss: 0.018520\n","\tEpoch:33 [000/005 (0660/0693)]\tLoss Ss: 0.011916\n","\tEpoch:33 [000/005 (0680/0693)]\tLoss Ss: 0.010280\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tEpoch:33 [001/005 (0000/0614)]\tLoss Ss: 0.004570\n","\tEpoch:33 [001/005 (0020/0614)]\tLoss Ss: 0.005531\n","\tEpoch:33 [001/005 (0040/0614)]\tLoss Ss: 0.006399\n","\tEpoch:33 [001/005 (0060/0614)]\tLoss Ss: 0.002591\n","\tEpoch:33 [001/005 (0080/0614)]\tLoss Ss: 0.007837\n","\tEpoch:33 [001/005 (0100/0614)]\tLoss Ss: 0.006139\n","\tEpoch:33 [001/005 (0120/0614)]\tLoss Ss: 0.006462\n","\tEpoch:33 [001/005 (0140/0614)]\tLoss Ss: 0.004174\n","\tEpoch:33 [001/005 (0160/0614)]\tLoss Ss: 0.005961\n","\tEpoch:33 [001/005 (0180/0614)]\tLoss Ss: 0.005866\n","\tEpoch:33 [001/005 (0200/0614)]\tLoss Ss: 0.008024\n","\tEpoch:33 [001/005 (0220/0614)]\tLoss Ss: 0.004155\n","\tEpoch:33 [001/005 (0240/0614)]\tLoss Ss: 0.007029\n","\tEpoch:33 [001/005 (0260/0614)]\tLoss Ss: 0.006293\n","\tEpoch:33 [001/005 (0280/0614)]\tLoss Ss: 0.009225\n","\tEpoch:33 [001/005 (0300/0614)]\tLoss Ss: 0.004949\n","\tEpoch:33 [001/005 (0320/0614)]\tLoss Ss: 0.006123\n","\tEpoch:33 [001/005 (0340/0614)]\tLoss Ss: 0.005084\n","\tEpoch:33 [001/005 (0360/0614)]\tLoss Ss: 0.006645\n","\tEpoch:33 [001/005 (0380/0614)]\tLoss Ss: 0.004724\n","\tEpoch:33 [001/005 (0400/0614)]\tLoss Ss: 0.006913\n","\tEpoch:33 [001/005 (0420/0614)]\tLoss Ss: 0.005812\n","\tEpoch:33 [001/005 (0440/0614)]\tLoss Ss: 0.005072\n","\tEpoch:33 [001/005 (0460/0614)]\tLoss Ss: 0.005360\n","\tEpoch:33 [001/005 (0480/0614)]\tLoss Ss: 0.004753\n","\tEpoch:33 [001/005 (0500/0614)]\tLoss Ss: 0.005438\n","\tEpoch:33 [001/005 (0520/0614)]\tLoss Ss: 0.006092\n","\tEpoch:33 [001/005 (0540/0614)]\tLoss Ss: 0.007469\n","\tEpoch:33 [001/005 (0560/0614)]\tLoss Ss: 0.006695\n","\tEpoch:33 [001/005 (0580/0614)]\tLoss Ss: 0.005373\n","\tEpoch:33 [001/005 (0600/0614)]\tLoss Ss: 0.005048\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:33 [002/005 (0000/0755)]\tLoss Ss: 0.016809\n","\tEpoch:33 [002/005 (0020/0755)]\tLoss Ss: 0.024106\n","\tEpoch:33 [002/005 (0040/0755)]\tLoss Ss: 0.018238\n","\tEpoch:33 [002/005 (0060/0755)]\tLoss Ss: 0.020227\n","\tEpoch:33 [002/005 (0080/0755)]\tLoss Ss: 0.019369\n","\tEpoch:33 [002/005 (0100/0755)]\tLoss Ss: 0.014467\n","\tEpoch:33 [002/005 (0120/0755)]\tLoss Ss: 0.020301\n","\tEpoch:33 [002/005 (0140/0755)]\tLoss Ss: 0.020293\n","\tEpoch:33 [002/005 (0160/0755)]\tLoss Ss: 0.016055\n","\tEpoch:33 [002/005 (0180/0755)]\tLoss Ss: 0.017532\n","\tEpoch:33 [002/005 (0200/0755)]\tLoss Ss: 0.016967\n","\tEpoch:33 [002/005 (0220/0755)]\tLoss Ss: 0.021992\n","\tEpoch:33 [002/005 (0240/0755)]\tLoss Ss: 0.013957\n","\tEpoch:33 [002/005 (0260/0755)]\tLoss Ss: 0.013483\n","\tEpoch:33 [002/005 (0280/0755)]\tLoss Ss: 0.016722\n","\tEpoch:33 [002/005 (0300/0755)]\tLoss Ss: 0.020013\n","\tEpoch:33 [002/005 (0320/0755)]\tLoss Ss: 0.016389\n","\tEpoch:33 [002/005 (0340/0755)]\tLoss Ss: 0.020521\n","\tEpoch:33 [002/005 (0360/0755)]\tLoss Ss: 0.016559\n","\tEpoch:33 [002/005 (0380/0755)]\tLoss Ss: 0.018822\n","\tEpoch:33 [002/005 (0400/0755)]\tLoss Ss: 0.014978\n","\tEpoch:33 [002/005 (0420/0755)]\tLoss Ss: 0.010301\n","\tEpoch:33 [002/005 (0440/0755)]\tLoss Ss: 0.014871\n","\tEpoch:33 [002/005 (0460/0755)]\tLoss Ss: 0.010058\n","\tEpoch:33 [002/005 (0480/0755)]\tLoss Ss: 0.016513\n","\tEpoch:33 [002/005 (0500/0755)]\tLoss Ss: 0.018644\n","\tEpoch:33 [002/005 (0520/0755)]\tLoss Ss: 0.015535\n","\tEpoch:33 [002/005 (0540/0755)]\tLoss Ss: 0.014116\n","\tEpoch:33 [002/005 (0560/0755)]\tLoss Ss: 0.013311\n","\tEpoch:33 [002/005 (0580/0755)]\tLoss Ss: 0.011149\n","\tEpoch:33 [002/005 (0600/0755)]\tLoss Ss: 0.017394\n","\tEpoch:33 [002/005 (0620/0755)]\tLoss Ss: 0.014020\n","\tEpoch:33 [002/005 (0640/0755)]\tLoss Ss: 0.013616\n","\tEpoch:33 [002/005 (0660/0755)]\tLoss Ss: 0.010096\n","\tEpoch:33 [002/005 (0680/0755)]\tLoss Ss: 0.010963\n","\tEpoch:33 [002/005 (0700/0755)]\tLoss Ss: 0.015377\n","\tEpoch:33 [002/005 (0720/0755)]\tLoss Ss: 0.019051\n","\tEpoch:33 [002/005 (0740/0755)]\tLoss Ss: 0.015844\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tEpoch:33 [003/005 (0000/0588)]\tLoss Ss: 0.005762\n","\tEpoch:33 [003/005 (0020/0588)]\tLoss Ss: 0.007562\n","\tEpoch:33 [003/005 (0040/0588)]\tLoss Ss: 0.006098\n","\tEpoch:33 [003/005 (0060/0588)]\tLoss Ss: 0.006738\n","\tEpoch:33 [003/005 (0080/0588)]\tLoss Ss: 0.004419\n","\tEpoch:33 [003/005 (0100/0588)]\tLoss Ss: 0.007365\n","\tEpoch:33 [003/005 (0120/0588)]\tLoss Ss: 0.006416\n","\tEpoch:33 [003/005 (0140/0588)]\tLoss Ss: 0.004436\n","\tEpoch:33 [003/005 (0160/0588)]\tLoss Ss: 0.005031\n","\tEpoch:33 [003/005 (0180/0588)]\tLoss Ss: 0.004022\n","\tEpoch:33 [003/005 (0200/0588)]\tLoss Ss: 0.004434\n","\tEpoch:33 [003/005 (0220/0588)]\tLoss Ss: 0.004410\n","\tEpoch:33 [003/005 (0240/0588)]\tLoss Ss: 0.005412\n","\tEpoch:33 [003/005 (0260/0588)]\tLoss Ss: 0.002676\n","\tEpoch:33 [003/005 (0280/0588)]\tLoss Ss: 0.004955\n","\tEpoch:33 [003/005 (0300/0588)]\tLoss Ss: 0.006224\n","\tEpoch:33 [003/005 (0320/0588)]\tLoss Ss: 0.003085\n","\tEpoch:33 [003/005 (0340/0588)]\tLoss Ss: 0.003881\n","\tEpoch:33 [003/005 (0360/0588)]\tLoss Ss: 0.002694\n","\tEpoch:33 [003/005 (0380/0588)]\tLoss Ss: 0.004701\n","\tEpoch:33 [003/005 (0400/0588)]\tLoss Ss: 0.004583\n","\tEpoch:33 [003/005 (0420/0588)]\tLoss Ss: 0.004311\n","\tEpoch:33 [003/005 (0440/0588)]\tLoss Ss: 0.005406\n","\tEpoch:33 [003/005 (0460/0588)]\tLoss Ss: 0.004554\n","\tEpoch:33 [003/005 (0480/0588)]\tLoss Ss: 0.003483\n","\tEpoch:33 [003/005 (0500/0588)]\tLoss Ss: 0.002721\n","\tEpoch:33 [003/005 (0520/0588)]\tLoss Ss: 0.004961\n","\tEpoch:33 [003/005 (0540/0588)]\tLoss Ss: 0.003266\n","\tEpoch:33 [003/005 (0560/0588)]\tLoss Ss: 0.005345\n","\tEpoch:33 [003/005 (0580/0588)]\tLoss Ss: 0.004078\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tEpoch:33 [004/005 (0000/0693)]\tLoss Ss: 0.023966\n","\tEpoch:33 [004/005 (0020/0693)]\tLoss Ss: 0.018868\n","\tEpoch:33 [004/005 (0040/0693)]\tLoss Ss: 0.014733\n","\tEpoch:33 [004/005 (0060/0693)]\tLoss Ss: 0.016460\n","\tEpoch:33 [004/005 (0080/0693)]\tLoss Ss: 0.014074\n","\tEpoch:33 [004/005 (0100/0693)]\tLoss Ss: 0.013248\n","\tEpoch:33 [004/005 (0120/0693)]\tLoss Ss: 0.010820\n","\tEpoch:33 [004/005 (0140/0693)]\tLoss Ss: 0.008456\n","\tEpoch:33 [004/005 (0160/0693)]\tLoss Ss: 0.014050\n","\tEpoch:33 [004/005 (0180/0693)]\tLoss Ss: 0.009394\n","\tEpoch:33 [004/005 (0200/0693)]\tLoss Ss: 0.013577\n","\tEpoch:33 [004/005 (0220/0693)]\tLoss Ss: 0.013608\n","\tEpoch:33 [004/005 (0240/0693)]\tLoss Ss: 0.011662\n","\tEpoch:33 [004/005 (0260/0693)]\tLoss Ss: 0.011563\n","\tEpoch:33 [004/005 (0280/0693)]\tLoss Ss: 0.010139\n","\tEpoch:33 [004/005 (0300/0693)]\tLoss Ss: 0.012870\n","\tEpoch:33 [004/005 (0320/0693)]\tLoss Ss: 0.011965\n","\tEpoch:33 [004/005 (0340/0693)]\tLoss Ss: 0.016332\n","\tEpoch:33 [004/005 (0360/0693)]\tLoss Ss: 0.012434\n","\tEpoch:33 [004/005 (0380/0693)]\tLoss Ss: 0.007928\n","\tEpoch:33 [004/005 (0400/0693)]\tLoss Ss: 0.011898\n","\tEpoch:33 [004/005 (0420/0693)]\tLoss Ss: 0.005273\n","\tEpoch:33 [004/005 (0440/0693)]\tLoss Ss: 0.009725\n","\tEpoch:33 [004/005 (0460/0693)]\tLoss Ss: 0.010115\n","\tEpoch:33 [004/005 (0480/0693)]\tLoss Ss: 0.010917\n","\tEpoch:33 [004/005 (0500/0693)]\tLoss Ss: 0.013449\n","\tEpoch:33 [004/005 (0520/0693)]\tLoss Ss: 0.009328\n","\tEpoch:33 [004/005 (0540/0693)]\tLoss Ss: 0.016311\n","\tEpoch:33 [004/005 (0560/0693)]\tLoss Ss: 0.009102\n","\tEpoch:33 [004/005 (0580/0693)]\tLoss Ss: 0.015267\n","\tEpoch:33 [004/005 (0600/0693)]\tLoss Ss: 0.008238\n","\tEpoch:33 [004/005 (0620/0693)]\tLoss Ss: 0.008556\n","\tEpoch:33 [004/005 (0640/0693)]\tLoss Ss: 0.009832\n","\tEpoch:33 [004/005 (0660/0693)]\tLoss Ss: 0.007489\n","\tEpoch:33 [004/005 (0680/0693)]\tLoss Ss: 0.009963\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:33 [005/005 (0000/0755)]\tLoss Ss: 0.015154\n","\tEpoch:33 [005/005 (0020/0755)]\tLoss Ss: 0.014903\n","\tEpoch:33 [005/005 (0040/0755)]\tLoss Ss: 0.008593\n","\tEpoch:33 [005/005 (0060/0755)]\tLoss Ss: 0.015050\n","\tEpoch:33 [005/005 (0080/0755)]\tLoss Ss: 0.009295\n","\tEpoch:33 [005/005 (0100/0755)]\tLoss Ss: 0.011716\n","\tEpoch:33 [005/005 (0120/0755)]\tLoss Ss: 0.013159\n","\tEpoch:33 [005/005 (0140/0755)]\tLoss Ss: 0.010514\n","\tEpoch:33 [005/005 (0160/0755)]\tLoss Ss: 0.011197\n","\tEpoch:33 [005/005 (0180/0755)]\tLoss Ss: 0.010805\n","\tEpoch:33 [005/005 (0200/0755)]\tLoss Ss: 0.009852\n","\tEpoch:33 [005/005 (0220/0755)]\tLoss Ss: 0.012987\n","\tEpoch:33 [005/005 (0240/0755)]\tLoss Ss: 0.010670\n","\tEpoch:33 [005/005 (0260/0755)]\tLoss Ss: 0.010349\n","\tEpoch:33 [005/005 (0280/0755)]\tLoss Ss: 0.009826\n","\tEpoch:33 [005/005 (0300/0755)]\tLoss Ss: 0.015348\n","\tEpoch:33 [005/005 (0320/0755)]\tLoss Ss: 0.013130\n","\tEpoch:33 [005/005 (0340/0755)]\tLoss Ss: 0.011795\n","\tEpoch:33 [005/005 (0360/0755)]\tLoss Ss: 0.012756\n","\tEpoch:33 [005/005 (0380/0755)]\tLoss Ss: 0.014514\n","\tEpoch:33 [005/005 (0400/0755)]\tLoss Ss: 0.012659\n","\tEpoch:33 [005/005 (0420/0755)]\tLoss Ss: 0.013125\n","\tEpoch:33 [005/005 (0440/0755)]\tLoss Ss: 0.012581\n","\tEpoch:33 [005/005 (0460/0755)]\tLoss Ss: 0.013215\n","\tEpoch:33 [005/005 (0480/0755)]\tLoss Ss: 0.011353\n","\tEpoch:33 [005/005 (0500/0755)]\tLoss Ss: 0.013561\n","\tEpoch:33 [005/005 (0520/0755)]\tLoss Ss: 0.010997\n","\tEpoch:33 [005/005 (0540/0755)]\tLoss Ss: 0.010263\n","\tEpoch:33 [005/005 (0560/0755)]\tLoss Ss: 0.008962\n","\tEpoch:33 [005/005 (0580/0755)]\tLoss Ss: 0.013925\n","\tEpoch:33 [005/005 (0600/0755)]\tLoss Ss: 0.014838\n","\tEpoch:33 [005/005 (0620/0755)]\tLoss Ss: 0.010308\n","\tEpoch:33 [005/005 (0640/0755)]\tLoss Ss: 0.011583\n","\tEpoch:33 [005/005 (0660/0755)]\tLoss Ss: 0.011727\n","\tEpoch:33 [005/005 (0680/0755)]\tLoss Ss: 0.010792\n","\tEpoch:33 [005/005 (0700/0755)]\tLoss Ss: 0.012434\n","\tEpoch:33 [005/005 (0720/0755)]\tLoss Ss: 0.011787\n","\tEpoch:33 [005/005 (0740/0755)]\tLoss Ss: 0.016178\n","Now train the rotated image\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tRotated_Epoch:33 [000/005 (0000/0693)]\tLoss Ss: 0.015982\n","\tRotated_Epoch:33 [000/005 (0020/0693)]\tLoss Ss: 0.016807\n","\tRotated_Epoch:33 [000/005 (0040/0693)]\tLoss Ss: 0.015080\n","\tRotated_Epoch:33 [000/005 (0060/0693)]\tLoss Ss: 0.018236\n","\tRotated_Epoch:33 [000/005 (0080/0693)]\tLoss Ss: 0.017511\n","\tRotated_Epoch:33 [000/005 (0100/0693)]\tLoss Ss: 0.014602\n","\tRotated_Epoch:33 [000/005 (0120/0693)]\tLoss Ss: 0.026957\n","\tRotated_Epoch:33 [000/005 (0140/0693)]\tLoss Ss: 0.014338\n","\tRotated_Epoch:33 [000/005 (0160/0693)]\tLoss Ss: 0.011578\n","\tRotated_Epoch:33 [000/005 (0180/0693)]\tLoss Ss: 0.012652\n","\tRotated_Epoch:33 [000/005 (0200/0693)]\tLoss Ss: 0.014636\n","\tRotated_Epoch:33 [000/005 (0220/0693)]\tLoss Ss: 0.013607\n","\tRotated_Epoch:33 [000/005 (0240/0693)]\tLoss Ss: 0.016107\n","\tRotated_Epoch:33 [000/005 (0260/0693)]\tLoss Ss: 0.011980\n","\tRotated_Epoch:33 [000/005 (0280/0693)]\tLoss Ss: 0.014871\n","\tRotated_Epoch:33 [000/005 (0300/0693)]\tLoss Ss: 0.014271\n","\tRotated_Epoch:33 [000/005 (0320/0693)]\tLoss Ss: 0.016870\n","\tRotated_Epoch:33 [000/005 (0340/0693)]\tLoss Ss: 0.014714\n","\tRotated_Epoch:33 [000/005 (0360/0693)]\tLoss Ss: 0.017447\n","\tRotated_Epoch:33 [000/005 (0380/0693)]\tLoss Ss: 0.015484\n","\tRotated_Epoch:33 [000/005 (0400/0693)]\tLoss Ss: 0.012176\n","\tRotated_Epoch:33 [000/005 (0420/0693)]\tLoss Ss: 0.014344\n","\tRotated_Epoch:33 [000/005 (0440/0693)]\tLoss Ss: 0.015595\n","\tRotated_Epoch:33 [000/005 (0460/0693)]\tLoss Ss: 0.015596\n","\tRotated_Epoch:33 [000/005 (0480/0693)]\tLoss Ss: 0.015435\n","\tRotated_Epoch:33 [000/005 (0500/0693)]\tLoss Ss: 0.017755\n","\tRotated_Epoch:33 [000/005 (0520/0693)]\tLoss Ss: 0.015699\n","\tRotated_Epoch:33 [000/005 (0540/0693)]\tLoss Ss: 0.011575\n","\tRotated_Epoch:33 [000/005 (0560/0693)]\tLoss Ss: 0.015813\n","\tRotated_Epoch:33 [000/005 (0580/0693)]\tLoss Ss: 0.013429\n","\tRotated_Epoch:33 [000/005 (0600/0693)]\tLoss Ss: 0.013462\n","\tRotated_Epoch:33 [000/005 (0620/0693)]\tLoss Ss: 0.013095\n","\tRotated_Epoch:33 [000/005 (0640/0693)]\tLoss Ss: 0.011303\n","\tRotated_Epoch:33 [000/005 (0660/0693)]\tLoss Ss: 0.008742\n","\tRotated_Epoch:33 [000/005 (0680/0693)]\tLoss Ss: 0.010666\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:33 [001/005 (0000/0755)]\tLoss Ss: 0.010843\n","\tRotated_Epoch:33 [001/005 (0020/0755)]\tLoss Ss: 0.013797\n","\tRotated_Epoch:33 [001/005 (0040/0755)]\tLoss Ss: 0.014223\n","\tRotated_Epoch:33 [001/005 (0060/0755)]\tLoss Ss: 0.028071\n","\tRotated_Epoch:33 [001/005 (0080/0755)]\tLoss Ss: 0.017426\n","\tRotated_Epoch:33 [001/005 (0100/0755)]\tLoss Ss: 0.015089\n","\tRotated_Epoch:33 [001/005 (0120/0755)]\tLoss Ss: 0.016724\n","\tRotated_Epoch:33 [001/005 (0140/0755)]\tLoss Ss: 0.015050\n","\tRotated_Epoch:33 [001/005 (0160/0755)]\tLoss Ss: 0.026082\n","\tRotated_Epoch:33 [001/005 (0180/0755)]\tLoss Ss: 0.017727\n","\tRotated_Epoch:33 [001/005 (0200/0755)]\tLoss Ss: 0.013179\n","\tRotated_Epoch:33 [001/005 (0220/0755)]\tLoss Ss: 0.014448\n","\tRotated_Epoch:33 [001/005 (0240/0755)]\tLoss Ss: 0.013260\n","\tRotated_Epoch:33 [001/005 (0260/0755)]\tLoss Ss: 0.011400\n","\tRotated_Epoch:33 [001/005 (0280/0755)]\tLoss Ss: 0.012443\n","\tRotated_Epoch:33 [001/005 (0300/0755)]\tLoss Ss: 0.009454\n","\tRotated_Epoch:33 [001/005 (0320/0755)]\tLoss Ss: 0.015180\n","\tRotated_Epoch:33 [001/005 (0340/0755)]\tLoss Ss: 0.018689\n","\tRotated_Epoch:33 [001/005 (0360/0755)]\tLoss Ss: 0.024698\n","\tRotated_Epoch:33 [001/005 (0380/0755)]\tLoss Ss: 0.014471\n","\tRotated_Epoch:33 [001/005 (0400/0755)]\tLoss Ss: 0.012759\n","\tRotated_Epoch:33 [001/005 (0420/0755)]\tLoss Ss: 0.018009\n","\tRotated_Epoch:33 [001/005 (0440/0755)]\tLoss Ss: 0.007130\n","\tRotated_Epoch:33 [001/005 (0460/0755)]\tLoss Ss: 0.011007\n","\tRotated_Epoch:33 [001/005 (0480/0755)]\tLoss Ss: 0.012887\n","\tRotated_Epoch:33 [001/005 (0500/0755)]\tLoss Ss: 0.012495\n","\tRotated_Epoch:33 [001/005 (0520/0755)]\tLoss Ss: 0.017012\n","\tRotated_Epoch:33 [001/005 (0540/0755)]\tLoss Ss: 0.013252\n","\tRotated_Epoch:33 [001/005 (0560/0755)]\tLoss Ss: 0.011044\n","\tRotated_Epoch:33 [001/005 (0580/0755)]\tLoss Ss: 0.019740\n","\tRotated_Epoch:33 [001/005 (0600/0755)]\tLoss Ss: 0.013001\n","\tRotated_Epoch:33 [001/005 (0620/0755)]\tLoss Ss: 0.010873\n","\tRotated_Epoch:33 [001/005 (0640/0755)]\tLoss Ss: 0.016247\n","\tRotated_Epoch:33 [001/005 (0660/0755)]\tLoss Ss: 0.017315\n","\tRotated_Epoch:33 [001/005 (0680/0755)]\tLoss Ss: 0.012980\n","\tRotated_Epoch:33 [001/005 (0700/0755)]\tLoss Ss: 0.018160\n","\tRotated_Epoch:33 [001/005 (0720/0755)]\tLoss Ss: 0.014730\n","\tRotated_Epoch:33 [001/005 (0740/0755)]\tLoss Ss: 0.016247\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tRotated_Epoch:33 [002/005 (0000/0693)]\tLoss Ss: 0.013129\n","\tRotated_Epoch:33 [002/005 (0020/0693)]\tLoss Ss: 0.016128\n","\tRotated_Epoch:33 [002/005 (0040/0693)]\tLoss Ss: 0.016151\n","\tRotated_Epoch:33 [002/005 (0060/0693)]\tLoss Ss: 0.012340\n","\tRotated_Epoch:33 [002/005 (0080/0693)]\tLoss Ss: 0.011744\n","\tRotated_Epoch:33 [002/005 (0100/0693)]\tLoss Ss: 0.011161\n","\tRotated_Epoch:33 [002/005 (0120/0693)]\tLoss Ss: 0.013431\n","\tRotated_Epoch:33 [002/005 (0140/0693)]\tLoss Ss: 0.012874\n","\tRotated_Epoch:33 [002/005 (0160/0693)]\tLoss Ss: 0.013774\n","\tRotated_Epoch:33 [002/005 (0180/0693)]\tLoss Ss: 0.013619\n","\tRotated_Epoch:33 [002/005 (0200/0693)]\tLoss Ss: 0.015355\n","\tRotated_Epoch:33 [002/005 (0220/0693)]\tLoss Ss: 0.014046\n","\tRotated_Epoch:33 [002/005 (0240/0693)]\tLoss Ss: 0.010200\n","\tRotated_Epoch:33 [002/005 (0260/0693)]\tLoss Ss: 0.011536\n","\tRotated_Epoch:33 [002/005 (0280/0693)]\tLoss Ss: 0.009146\n","\tRotated_Epoch:33 [002/005 (0300/0693)]\tLoss Ss: 0.011941\n","\tRotated_Epoch:33 [002/005 (0320/0693)]\tLoss Ss: 0.017266\n","\tRotated_Epoch:33 [002/005 (0340/0693)]\tLoss Ss: 0.017294\n","\tRotated_Epoch:33 [002/005 (0360/0693)]\tLoss Ss: 0.012201\n","\tRotated_Epoch:33 [002/005 (0380/0693)]\tLoss Ss: 0.015071\n","\tRotated_Epoch:33 [002/005 (0400/0693)]\tLoss Ss: 0.009858\n","\tRotated_Epoch:33 [002/005 (0420/0693)]\tLoss Ss: 0.009234\n","\tRotated_Epoch:33 [002/005 (0440/0693)]\tLoss Ss: 0.015223\n","\tRotated_Epoch:33 [002/005 (0460/0693)]\tLoss Ss: 0.012721\n","\tRotated_Epoch:33 [002/005 (0480/0693)]\tLoss Ss: 0.010306\n","\tRotated_Epoch:33 [002/005 (0500/0693)]\tLoss Ss: 0.016687\n","\tRotated_Epoch:33 [002/005 (0520/0693)]\tLoss Ss: 0.012272\n","\tRotated_Epoch:33 [002/005 (0540/0693)]\tLoss Ss: 0.015513\n","\tRotated_Epoch:33 [002/005 (0560/0693)]\tLoss Ss: 0.013134\n","\tRotated_Epoch:33 [002/005 (0580/0693)]\tLoss Ss: 0.011232\n","\tRotated_Epoch:33 [002/005 (0600/0693)]\tLoss Ss: 0.015599\n","\tRotated_Epoch:33 [002/005 (0620/0693)]\tLoss Ss: 0.018202\n","\tRotated_Epoch:33 [002/005 (0640/0693)]\tLoss Ss: 0.012336\n","\tRotated_Epoch:33 [002/005 (0660/0693)]\tLoss Ss: 0.008628\n","\tRotated_Epoch:33 [002/005 (0680/0693)]\tLoss Ss: 0.012949\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tRotated_Epoch:33 [003/005 (0000/0614)]\tLoss Ss: 0.009660\n","\tRotated_Epoch:33 [003/005 (0020/0614)]\tLoss Ss: 0.006053\n","\tRotated_Epoch:33 [003/005 (0040/0614)]\tLoss Ss: 0.008618\n","\tRotated_Epoch:33 [003/005 (0060/0614)]\tLoss Ss: 0.006161\n","\tRotated_Epoch:33 [003/005 (0080/0614)]\tLoss Ss: 0.007317\n","\tRotated_Epoch:33 [003/005 (0100/0614)]\tLoss Ss: 0.004100\n","\tRotated_Epoch:33 [003/005 (0120/0614)]\tLoss Ss: 0.005339\n","\tRotated_Epoch:33 [003/005 (0140/0614)]\tLoss Ss: 0.005159\n","\tRotated_Epoch:33 [003/005 (0160/0614)]\tLoss Ss: 0.008448\n","\tRotated_Epoch:33 [003/005 (0180/0614)]\tLoss Ss: 0.005936\n","\tRotated_Epoch:33 [003/005 (0200/0614)]\tLoss Ss: 0.006237\n","\tRotated_Epoch:33 [003/005 (0220/0614)]\tLoss Ss: 0.005498\n","\tRotated_Epoch:33 [003/005 (0240/0614)]\tLoss Ss: 0.007714\n","\tRotated_Epoch:33 [003/005 (0260/0614)]\tLoss Ss: 0.007525\n","\tRotated_Epoch:33 [003/005 (0280/0614)]\tLoss Ss: 0.005900\n","\tRotated_Epoch:33 [003/005 (0300/0614)]\tLoss Ss: 0.006440\n","\tRotated_Epoch:33 [003/005 (0320/0614)]\tLoss Ss: 0.006188\n","\tRotated_Epoch:33 [003/005 (0340/0614)]\tLoss Ss: 0.004593\n","\tRotated_Epoch:33 [003/005 (0360/0614)]\tLoss Ss: 0.005217\n","\tRotated_Epoch:33 [003/005 (0380/0614)]\tLoss Ss: 0.005016\n","\tRotated_Epoch:33 [003/005 (0400/0614)]\tLoss Ss: 0.004381\n","\tRotated_Epoch:33 [003/005 (0420/0614)]\tLoss Ss: 0.007364\n","\tRotated_Epoch:33 [003/005 (0440/0614)]\tLoss Ss: 0.005513\n","\tRotated_Epoch:33 [003/005 (0460/0614)]\tLoss Ss: 0.008046\n","\tRotated_Epoch:33 [003/005 (0480/0614)]\tLoss Ss: 0.005982\n","\tRotated_Epoch:33 [003/005 (0500/0614)]\tLoss Ss: 0.005323\n","\tRotated_Epoch:33 [003/005 (0520/0614)]\tLoss Ss: 0.004285\n","\tRotated_Epoch:33 [003/005 (0540/0614)]\tLoss Ss: 0.008045\n","\tRotated_Epoch:33 [003/005 (0560/0614)]\tLoss Ss: 0.004519\n","\tRotated_Epoch:33 [003/005 (0580/0614)]\tLoss Ss: 0.005678\n","\tRotated_Epoch:33 [003/005 (0600/0614)]\tLoss Ss: 0.005655\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:33 [004/005 (0000/0755)]\tLoss Ss: 0.300353\n","\tRotated_Epoch:33 [004/005 (0020/0755)]\tLoss Ss: 0.134627\n","\tRotated_Epoch:33 [004/005 (0040/0755)]\tLoss Ss: 0.126993\n","\tRotated_Epoch:33 [004/005 (0060/0755)]\tLoss Ss: 0.077599\n","\tRotated_Epoch:33 [004/005 (0080/0755)]\tLoss Ss: 0.170868\n","\tRotated_Epoch:33 [004/005 (0100/0755)]\tLoss Ss: 0.101545\n","\tRotated_Epoch:33 [004/005 (0120/0755)]\tLoss Ss: 0.081364\n","\tRotated_Epoch:33 [004/005 (0140/0755)]\tLoss Ss: 0.071159\n","\tRotated_Epoch:33 [004/005 (0160/0755)]\tLoss Ss: 0.093555\n","\tRotated_Epoch:33 [004/005 (0180/0755)]\tLoss Ss: 0.025245\n","\tRotated_Epoch:33 [004/005 (0200/0755)]\tLoss Ss: 0.054784\n","\tRotated_Epoch:33 [004/005 (0220/0755)]\tLoss Ss: 0.037634\n","\tRotated_Epoch:33 [004/005 (0240/0755)]\tLoss Ss: 0.041136\n","\tRotated_Epoch:33 [004/005 (0260/0755)]\tLoss Ss: 0.059238\n","\tRotated_Epoch:33 [004/005 (0280/0755)]\tLoss Ss: 0.051005\n","\tRotated_Epoch:33 [004/005 (0300/0755)]\tLoss Ss: 0.038683\n","\tRotated_Epoch:33 [004/005 (0320/0755)]\tLoss Ss: 0.034300\n","\tRotated_Epoch:33 [004/005 (0340/0755)]\tLoss Ss: 0.049663\n","\tRotated_Epoch:33 [004/005 (0360/0755)]\tLoss Ss: 0.061108\n","\tRotated_Epoch:33 [004/005 (0380/0755)]\tLoss Ss: 0.053810\n","\tRotated_Epoch:33 [004/005 (0400/0755)]\tLoss Ss: 0.043656\n","\tRotated_Epoch:33 [004/005 (0420/0755)]\tLoss Ss: 0.049218\n","\tRotated_Epoch:33 [004/005 (0440/0755)]\tLoss Ss: 0.033354\n","\tRotated_Epoch:33 [004/005 (0460/0755)]\tLoss Ss: 0.031515\n","\tRotated_Epoch:33 [004/005 (0480/0755)]\tLoss Ss: 0.053834\n","\tRotated_Epoch:33 [004/005 (0500/0755)]\tLoss Ss: 0.048611\n","\tRotated_Epoch:33 [004/005 (0520/0755)]\tLoss Ss: 0.037995\n","\tRotated_Epoch:33 [004/005 (0540/0755)]\tLoss Ss: 0.039120\n","\tRotated_Epoch:33 [004/005 (0560/0755)]\tLoss Ss: 0.029021\n","\tRotated_Epoch:33 [004/005 (0580/0755)]\tLoss Ss: 0.030474\n","\tRotated_Epoch:33 [004/005 (0600/0755)]\tLoss Ss: 0.027231\n","\tRotated_Epoch:33 [004/005 (0620/0755)]\tLoss Ss: 0.037663\n","\tRotated_Epoch:33 [004/005 (0640/0755)]\tLoss Ss: 0.046677\n","\tRotated_Epoch:33 [004/005 (0660/0755)]\tLoss Ss: 0.029825\n","\tRotated_Epoch:33 [004/005 (0680/0755)]\tLoss Ss: 0.033758\n","\tRotated_Epoch:33 [004/005 (0700/0755)]\tLoss Ss: 0.035097\n","\tRotated_Epoch:33 [004/005 (0720/0755)]\tLoss Ss: 0.026325\n","\tRotated_Epoch:33 [004/005 (0740/0755)]\tLoss Ss: 0.040164\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tRotated_Epoch:33 [005/005 (0000/0588)]\tLoss Ss: 0.096683\n","\tRotated_Epoch:33 [005/005 (0020/0588)]\tLoss Ss: 0.073176\n","\tRotated_Epoch:33 [005/005 (0040/0588)]\tLoss Ss: 0.041766\n","\tRotated_Epoch:33 [005/005 (0060/0588)]\tLoss Ss: 0.042733\n","\tRotated_Epoch:33 [005/005 (0080/0588)]\tLoss Ss: 0.051925\n","\tRotated_Epoch:33 [005/005 (0100/0588)]\tLoss Ss: 0.062528\n","\tRotated_Epoch:33 [005/005 (0120/0588)]\tLoss Ss: 0.091024\n","\tRotated_Epoch:33 [005/005 (0140/0588)]\tLoss Ss: 0.071894\n","\tRotated_Epoch:33 [005/005 (0160/0588)]\tLoss Ss: 0.058896\n","\tRotated_Epoch:33 [005/005 (0180/0588)]\tLoss Ss: 0.058912\n","\tRotated_Epoch:33 [005/005 (0200/0588)]\tLoss Ss: 0.080415\n","\tRotated_Epoch:33 [005/005 (0220/0588)]\tLoss Ss: 0.049536\n","\tRotated_Epoch:33 [005/005 (0240/0588)]\tLoss Ss: 0.041208\n","\tRotated_Epoch:33 [005/005 (0260/0588)]\tLoss Ss: 0.049522\n","\tRotated_Epoch:33 [005/005 (0280/0588)]\tLoss Ss: 0.061107\n","\tRotated_Epoch:33 [005/005 (0300/0588)]\tLoss Ss: 0.059242\n","\tRotated_Epoch:33 [005/005 (0320/0588)]\tLoss Ss: 0.043633\n","\tRotated_Epoch:33 [005/005 (0340/0588)]\tLoss Ss: 0.069091\n","\tRotated_Epoch:33 [005/005 (0360/0588)]\tLoss Ss: 0.051827\n","\tRotated_Epoch:33 [005/005 (0380/0588)]\tLoss Ss: 0.038680\n","\tRotated_Epoch:33 [005/005 (0400/0588)]\tLoss Ss: 0.042533\n","\tRotated_Epoch:33 [005/005 (0420/0588)]\tLoss Ss: 0.071296\n","\tRotated_Epoch:33 [005/005 (0440/0588)]\tLoss Ss: 0.066705\n","\tRotated_Epoch:33 [005/005 (0460/0588)]\tLoss Ss: 0.062750\n","\tRotated_Epoch:33 [005/005 (0480/0588)]\tLoss Ss: 0.036860\n","\tRotated_Epoch:33 [005/005 (0500/0588)]\tLoss Ss: 0.056882\n","\tRotated_Epoch:33 [005/005 (0520/0588)]\tLoss Ss: 0.049455\n","\tRotated_Epoch:33 [005/005 (0540/0588)]\tLoss Ss: 0.051425\n","\tRotated_Epoch:33 [005/005 (0560/0588)]\tLoss Ss: 0.036086\n","\tRotated_Epoch:33 [005/005 (0580/0588)]\tLoss Ss: 0.064000\n","new_1_6_d.nii\n","new_1_6_m.nii\n","new_1_7_d.nii\n","new_1_7_m.nii\n","new_2_5_d.nii\n","new_2_5_m.nii\n","new_2_6_d.nii\n","new_2_6_m.nii\n","new_2_7_d.nii\n","new_2_7_m.nii\n","new_3_d (2).nii\n","new_3_m (2).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (3).nii\n","new_3_m (3).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (7).nii\n","new_3_m (7).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (8).nii\n","new_3_m (8).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (9).nii\n","new_3_m (9).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_4_10_d_163526.nii\n","new_4_10_m_163526.nii.gz\n","new_4_11_d_084336.nii\n","new_4_11_m_084336.nii.gz\n","new_4_7_d_184422.nii\n","new_4_7_m_184422.nii.gz\n","new_4_8_d_143412.nii\n","new_4_8_m_143412.nii.gz\n","new_4_9_d_155347.nii\n","new_4_9_m_155347.nii.gz\n","\tEpoch: 33; Dice: 0.8838 +/- 0.0165; Loss: 8.1518\n","Begin Epoch 34\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:34 [000/005 (0000/0755)]\tLoss Ss: 0.055090\n","\tEpoch:34 [000/005 (0020/0755)]\tLoss Ss: 0.088684\n","\tEpoch:34 [000/005 (0040/0755)]\tLoss Ss: 0.017106\n","\tEpoch:34 [000/005 (0060/0755)]\tLoss Ss: 0.032994\n","\tEpoch:34 [000/005 (0080/0755)]\tLoss Ss: 0.029242\n","\tEpoch:34 [000/005 (0100/0755)]\tLoss Ss: 0.031045\n","\tEpoch:34 [000/005 (0120/0755)]\tLoss Ss: 0.018611\n","\tEpoch:34 [000/005 (0140/0755)]\tLoss Ss: 0.026606\n","\tEpoch:34 [000/005 (0160/0755)]\tLoss Ss: 0.026664\n","\tEpoch:34 [000/005 (0180/0755)]\tLoss Ss: 0.018642\n","\tEpoch:34 [000/005 (0200/0755)]\tLoss Ss: 0.023630\n","\tEpoch:34 [000/005 (0220/0755)]\tLoss Ss: 0.014889\n","\tEpoch:34 [000/005 (0240/0755)]\tLoss Ss: 0.022836\n","\tEpoch:34 [000/005 (0260/0755)]\tLoss Ss: 0.019282\n","\tEpoch:34 [000/005 (0280/0755)]\tLoss Ss: 0.018597\n","\tEpoch:34 [000/005 (0300/0755)]\tLoss Ss: 0.021481\n","\tEpoch:34 [000/005 (0320/0755)]\tLoss Ss: 0.017393\n","\tEpoch:34 [000/005 (0340/0755)]\tLoss Ss: 0.015713\n","\tEpoch:34 [000/005 (0360/0755)]\tLoss Ss: 0.013764\n","\tEpoch:34 [000/005 (0380/0755)]\tLoss Ss: 0.022401\n","\tEpoch:34 [000/005 (0400/0755)]\tLoss Ss: 0.015092\n","\tEpoch:34 [000/005 (0420/0755)]\tLoss Ss: 0.018241\n","\tEpoch:34 [000/005 (0440/0755)]\tLoss Ss: 0.017063\n","\tEpoch:34 [000/005 (0460/0755)]\tLoss Ss: 0.017102\n","\tEpoch:34 [000/005 (0480/0755)]\tLoss Ss: 0.015797\n","\tEpoch:34 [000/005 (0500/0755)]\tLoss Ss: 0.022525\n","\tEpoch:34 [000/005 (0520/0755)]\tLoss Ss: 0.012735\n","\tEpoch:34 [000/005 (0540/0755)]\tLoss Ss: 0.018246\n","\tEpoch:34 [000/005 (0560/0755)]\tLoss Ss: 0.014126\n","\tEpoch:34 [000/005 (0580/0755)]\tLoss Ss: 0.020119\n","\tEpoch:34 [000/005 (0600/0755)]\tLoss Ss: 0.013104\n","\tEpoch:34 [000/005 (0620/0755)]\tLoss Ss: 0.009983\n","\tEpoch:34 [000/005 (0640/0755)]\tLoss Ss: 0.013940\n","\tEpoch:34 [000/005 (0660/0755)]\tLoss Ss: 0.014201\n","\tEpoch:34 [000/005 (0680/0755)]\tLoss Ss: 0.015744\n","\tEpoch:34 [000/005 (0700/0755)]\tLoss Ss: 0.011726\n","\tEpoch:34 [000/005 (0720/0755)]\tLoss Ss: 0.015330\n","\tEpoch:34 [000/005 (0740/0755)]\tLoss Ss: 0.026641\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tEpoch:34 [001/005 (0000/0588)]\tLoss Ss: 0.004590\n","\tEpoch:34 [001/005 (0020/0588)]\tLoss Ss: 0.009301\n","\tEpoch:34 [001/005 (0040/0588)]\tLoss Ss: 0.008169\n","\tEpoch:34 [001/005 (0060/0588)]\tLoss Ss: 0.007937\n","\tEpoch:34 [001/005 (0080/0588)]\tLoss Ss: 0.009802\n","\tEpoch:34 [001/005 (0100/0588)]\tLoss Ss: 0.005702\n","\tEpoch:34 [001/005 (0120/0588)]\tLoss Ss: 0.005120\n","\tEpoch:34 [001/005 (0140/0588)]\tLoss Ss: 0.006377\n","\tEpoch:34 [001/005 (0160/0588)]\tLoss Ss: 0.003610\n","\tEpoch:34 [001/005 (0180/0588)]\tLoss Ss: 0.005520\n","\tEpoch:34 [001/005 (0200/0588)]\tLoss Ss: 0.007092\n","\tEpoch:34 [001/005 (0220/0588)]\tLoss Ss: 0.004819\n","\tEpoch:34 [001/005 (0240/0588)]\tLoss Ss: 0.004797\n","\tEpoch:34 [001/005 (0260/0588)]\tLoss Ss: 0.004355\n","\tEpoch:34 [001/005 (0280/0588)]\tLoss Ss: 0.005396\n","\tEpoch:34 [001/005 (0300/0588)]\tLoss Ss: 0.003974\n","\tEpoch:34 [001/005 (0320/0588)]\tLoss Ss: 0.007088\n","\tEpoch:34 [001/005 (0340/0588)]\tLoss Ss: 0.006191\n","\tEpoch:34 [001/005 (0360/0588)]\tLoss Ss: 0.004195\n","\tEpoch:34 [001/005 (0380/0588)]\tLoss Ss: 0.004268\n","\tEpoch:34 [001/005 (0400/0588)]\tLoss Ss: 0.003813\n","\tEpoch:34 [001/005 (0420/0588)]\tLoss Ss: 0.005298\n","\tEpoch:34 [001/005 (0440/0588)]\tLoss Ss: 0.004814\n","\tEpoch:34 [001/005 (0460/0588)]\tLoss Ss: 0.003759\n","\tEpoch:34 [001/005 (0480/0588)]\tLoss Ss: 0.004048\n","\tEpoch:34 [001/005 (0500/0588)]\tLoss Ss: 0.005440\n","\tEpoch:34 [001/005 (0520/0588)]\tLoss Ss: 0.005124\n","\tEpoch:34 [001/005 (0540/0588)]\tLoss Ss: 0.004108\n","\tEpoch:34 [001/005 (0560/0588)]\tLoss Ss: 0.003616\n","\tEpoch:34 [001/005 (0580/0588)]\tLoss Ss: 0.003011\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tEpoch:34 [002/005 (0000/0693)]\tLoss Ss: 0.016307\n","\tEpoch:34 [002/005 (0020/0693)]\tLoss Ss: 0.019906\n","\tEpoch:34 [002/005 (0040/0693)]\tLoss Ss: 0.014669\n","\tEpoch:34 [002/005 (0060/0693)]\tLoss Ss: 0.018762\n","\tEpoch:34 [002/005 (0080/0693)]\tLoss Ss: 0.014765\n","\tEpoch:34 [002/005 (0100/0693)]\tLoss Ss: 0.015756\n","\tEpoch:34 [002/005 (0120/0693)]\tLoss Ss: 0.016710\n","\tEpoch:34 [002/005 (0140/0693)]\tLoss Ss: 0.014006\n","\tEpoch:34 [002/005 (0160/0693)]\tLoss Ss: 0.018219\n","\tEpoch:34 [002/005 (0180/0693)]\tLoss Ss: 0.016853\n","\tEpoch:34 [002/005 (0200/0693)]\tLoss Ss: 0.016664\n","\tEpoch:34 [002/005 (0220/0693)]\tLoss Ss: 0.012887\n","\tEpoch:34 [002/005 (0240/0693)]\tLoss Ss: 0.013966\n","\tEpoch:34 [002/005 (0260/0693)]\tLoss Ss: 0.009206\n","\tEpoch:34 [002/005 (0280/0693)]\tLoss Ss: 0.014740\n","\tEpoch:34 [002/005 (0300/0693)]\tLoss Ss: 0.009876\n","\tEpoch:34 [002/005 (0320/0693)]\tLoss Ss: 0.016525\n","\tEpoch:34 [002/005 (0340/0693)]\tLoss Ss: 0.016583\n","\tEpoch:34 [002/005 (0360/0693)]\tLoss Ss: 0.018397\n","\tEpoch:34 [002/005 (0380/0693)]\tLoss Ss: 0.009710\n","\tEpoch:34 [002/005 (0400/0693)]\tLoss Ss: 0.010754\n","\tEpoch:34 [002/005 (0420/0693)]\tLoss Ss: 0.014017\n","\tEpoch:34 [002/005 (0440/0693)]\tLoss Ss: 0.011034\n","\tEpoch:34 [002/005 (0460/0693)]\tLoss Ss: 0.010788\n","\tEpoch:34 [002/005 (0480/0693)]\tLoss Ss: 0.014567\n","\tEpoch:34 [002/005 (0500/0693)]\tLoss Ss: 0.010668\n","\tEpoch:34 [002/005 (0520/0693)]\tLoss Ss: 0.009870\n","\tEpoch:34 [002/005 (0540/0693)]\tLoss Ss: 0.014159\n","\tEpoch:34 [002/005 (0560/0693)]\tLoss Ss: 0.012356\n","\tEpoch:34 [002/005 (0580/0693)]\tLoss Ss: 0.021503\n","\tEpoch:34 [002/005 (0600/0693)]\tLoss Ss: 0.016011\n","\tEpoch:34 [002/005 (0620/0693)]\tLoss Ss: 0.008516\n","\tEpoch:34 [002/005 (0640/0693)]\tLoss Ss: 0.011545\n","\tEpoch:34 [002/005 (0660/0693)]\tLoss Ss: 0.018264\n","\tEpoch:34 [002/005 (0680/0693)]\tLoss Ss: 0.012093\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tEpoch:34 [003/005 (0000/0614)]\tLoss Ss: 0.008790\n","\tEpoch:34 [003/005 (0020/0614)]\tLoss Ss: 0.004265\n","\tEpoch:34 [003/005 (0040/0614)]\tLoss Ss: 0.005810\n","\tEpoch:34 [003/005 (0060/0614)]\tLoss Ss: 0.007933\n","\tEpoch:34 [003/005 (0080/0614)]\tLoss Ss: 0.005789\n","\tEpoch:34 [003/005 (0100/0614)]\tLoss Ss: 0.005162\n","\tEpoch:34 [003/005 (0120/0614)]\tLoss Ss: 0.004216\n","\tEpoch:34 [003/005 (0140/0614)]\tLoss Ss: 0.004049\n","\tEpoch:34 [003/005 (0160/0614)]\tLoss Ss: 0.007054\n","\tEpoch:34 [003/005 (0180/0614)]\tLoss Ss: 0.007272\n","\tEpoch:34 [003/005 (0200/0614)]\tLoss Ss: 0.005112\n","\tEpoch:34 [003/005 (0220/0614)]\tLoss Ss: 0.005809\n","\tEpoch:34 [003/005 (0240/0614)]\tLoss Ss: 0.006340\n","\tEpoch:34 [003/005 (0260/0614)]\tLoss Ss: 0.006153\n","\tEpoch:34 [003/005 (0280/0614)]\tLoss Ss: 0.004830\n","\tEpoch:34 [003/005 (0300/0614)]\tLoss Ss: 0.004037\n","\tEpoch:34 [003/005 (0320/0614)]\tLoss Ss: 0.005589\n","\tEpoch:34 [003/005 (0340/0614)]\tLoss Ss: 0.004804\n","\tEpoch:34 [003/005 (0360/0614)]\tLoss Ss: 0.006301\n","\tEpoch:34 [003/005 (0380/0614)]\tLoss Ss: 0.002810\n","\tEpoch:34 [003/005 (0400/0614)]\tLoss Ss: 0.004309\n","\tEpoch:34 [003/005 (0420/0614)]\tLoss Ss: 0.004507\n","\tEpoch:34 [003/005 (0440/0614)]\tLoss Ss: 0.004612\n","\tEpoch:34 [003/005 (0460/0614)]\tLoss Ss: 0.003301\n","\tEpoch:34 [003/005 (0480/0614)]\tLoss Ss: 0.003872\n","\tEpoch:34 [003/005 (0500/0614)]\tLoss Ss: 0.006249\n","\tEpoch:34 [003/005 (0520/0614)]\tLoss Ss: 0.005215\n","\tEpoch:34 [003/005 (0540/0614)]\tLoss Ss: 0.005892\n","\tEpoch:34 [003/005 (0560/0614)]\tLoss Ss: 0.005213\n","\tEpoch:34 [003/005 (0580/0614)]\tLoss Ss: 0.004663\n","\tEpoch:34 [003/005 (0600/0614)]\tLoss Ss: 0.006731\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tEpoch:34 [004/005 (0000/0693)]\tLoss Ss: 0.012072\n","\tEpoch:34 [004/005 (0020/0693)]\tLoss Ss: 0.014566\n","\tEpoch:34 [004/005 (0040/0693)]\tLoss Ss: 0.013708\n","\tEpoch:34 [004/005 (0060/0693)]\tLoss Ss: 0.009801\n","\tEpoch:34 [004/005 (0080/0693)]\tLoss Ss: 0.011386\n","\tEpoch:34 [004/005 (0100/0693)]\tLoss Ss: 0.014488\n","\tEpoch:34 [004/005 (0120/0693)]\tLoss Ss: 0.008668\n","\tEpoch:34 [004/005 (0140/0693)]\tLoss Ss: 0.013105\n","\tEpoch:34 [004/005 (0160/0693)]\tLoss Ss: 0.012352\n","\tEpoch:34 [004/005 (0180/0693)]\tLoss Ss: 0.014421\n","\tEpoch:34 [004/005 (0200/0693)]\tLoss Ss: 0.006809\n","\tEpoch:34 [004/005 (0220/0693)]\tLoss Ss: 0.012799\n","\tEpoch:34 [004/005 (0240/0693)]\tLoss Ss: 0.011717\n","\tEpoch:34 [004/005 (0260/0693)]\tLoss Ss: 0.007357\n","\tEpoch:34 [004/005 (0280/0693)]\tLoss Ss: 0.013672\n","\tEpoch:34 [004/005 (0300/0693)]\tLoss Ss: 0.009627\n","\tEpoch:34 [004/005 (0320/0693)]\tLoss Ss: 0.012702\n","\tEpoch:34 [004/005 (0340/0693)]\tLoss Ss: 0.018060\n","\tEpoch:34 [004/005 (0360/0693)]\tLoss Ss: 0.010100\n","\tEpoch:34 [004/005 (0380/0693)]\tLoss Ss: 0.011058\n","\tEpoch:34 [004/005 (0400/0693)]\tLoss Ss: 0.011224\n","\tEpoch:34 [004/005 (0420/0693)]\tLoss Ss: 0.012309\n","\tEpoch:34 [004/005 (0440/0693)]\tLoss Ss: 0.012378\n","\tEpoch:34 [004/005 (0460/0693)]\tLoss Ss: 0.011882\n","\tEpoch:34 [004/005 (0480/0693)]\tLoss Ss: 0.007400\n","\tEpoch:34 [004/005 (0500/0693)]\tLoss Ss: 0.009006\n","\tEpoch:34 [004/005 (0520/0693)]\tLoss Ss: 0.010684\n","\tEpoch:34 [004/005 (0540/0693)]\tLoss Ss: 0.012627\n","\tEpoch:34 [004/005 (0560/0693)]\tLoss Ss: 0.010956\n","\tEpoch:34 [004/005 (0580/0693)]\tLoss Ss: 0.007566\n","\tEpoch:34 [004/005 (0600/0693)]\tLoss Ss: 0.015275\n","\tEpoch:34 [004/005 (0620/0693)]\tLoss Ss: 0.011987\n","\tEpoch:34 [004/005 (0640/0693)]\tLoss Ss: 0.013780\n","\tEpoch:34 [004/005 (0660/0693)]\tLoss Ss: 0.018016\n","\tEpoch:34 [004/005 (0680/0693)]\tLoss Ss: 0.012410\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:34 [005/005 (0000/0755)]\tLoss Ss: 0.023724\n","\tEpoch:34 [005/005 (0020/0755)]\tLoss Ss: 0.033252\n","\tEpoch:34 [005/005 (0040/0755)]\tLoss Ss: 0.018866\n","\tEpoch:34 [005/005 (0060/0755)]\tLoss Ss: 0.022984\n","\tEpoch:34 [005/005 (0080/0755)]\tLoss Ss: 0.023074\n","\tEpoch:34 [005/005 (0100/0755)]\tLoss Ss: 0.021120\n","\tEpoch:34 [005/005 (0120/0755)]\tLoss Ss: 0.025424\n","\tEpoch:34 [005/005 (0140/0755)]\tLoss Ss: 0.016753\n","\tEpoch:34 [005/005 (0160/0755)]\tLoss Ss: 0.014906\n","\tEpoch:34 [005/005 (0180/0755)]\tLoss Ss: 0.014067\n","\tEpoch:34 [005/005 (0200/0755)]\tLoss Ss: 0.010405\n","\tEpoch:34 [005/005 (0220/0755)]\tLoss Ss: 0.018270\n","\tEpoch:34 [005/005 (0240/0755)]\tLoss Ss: 0.014412\n","\tEpoch:34 [005/005 (0260/0755)]\tLoss Ss: 0.015143\n","\tEpoch:34 [005/005 (0280/0755)]\tLoss Ss: 0.016923\n","\tEpoch:34 [005/005 (0300/0755)]\tLoss Ss: 0.024720\n","\tEpoch:34 [005/005 (0320/0755)]\tLoss Ss: 0.017400\n","\tEpoch:34 [005/005 (0340/0755)]\tLoss Ss: 0.013451\n","\tEpoch:34 [005/005 (0360/0755)]\tLoss Ss: 0.014306\n","\tEpoch:34 [005/005 (0380/0755)]\tLoss Ss: 0.014752\n","\tEpoch:34 [005/005 (0400/0755)]\tLoss Ss: 0.011555\n","\tEpoch:34 [005/005 (0420/0755)]\tLoss Ss: 0.019113\n","\tEpoch:34 [005/005 (0440/0755)]\tLoss Ss: 0.013237\n","\tEpoch:34 [005/005 (0460/0755)]\tLoss Ss: 0.009542\n","\tEpoch:34 [005/005 (0480/0755)]\tLoss Ss: 0.021354\n","\tEpoch:34 [005/005 (0500/0755)]\tLoss Ss: 0.012737\n","\tEpoch:34 [005/005 (0520/0755)]\tLoss Ss: 0.015630\n","\tEpoch:34 [005/005 (0540/0755)]\tLoss Ss: 0.018548\n","\tEpoch:34 [005/005 (0560/0755)]\tLoss Ss: 0.011176\n","\tEpoch:34 [005/005 (0580/0755)]\tLoss Ss: 0.010883\n","\tEpoch:34 [005/005 (0600/0755)]\tLoss Ss: 0.020433\n","\tEpoch:34 [005/005 (0620/0755)]\tLoss Ss: 0.016037\n","\tEpoch:34 [005/005 (0640/0755)]\tLoss Ss: 0.015214\n","\tEpoch:34 [005/005 (0660/0755)]\tLoss Ss: 0.012788\n","\tEpoch:34 [005/005 (0680/0755)]\tLoss Ss: 0.017073\n","\tEpoch:34 [005/005 (0700/0755)]\tLoss Ss: 0.012497\n","\tEpoch:34 [005/005 (0720/0755)]\tLoss Ss: 0.013936\n","\tEpoch:34 [005/005 (0740/0755)]\tLoss Ss: 0.009635\n","Now train the rotated image\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tRotated_Epoch:34 [000/005 (0000/0588)]\tLoss Ss: 0.086004\n","\tRotated_Epoch:34 [000/005 (0020/0588)]\tLoss Ss: 0.101709\n","\tRotated_Epoch:34 [000/005 (0040/0588)]\tLoss Ss: 0.071877\n","\tRotated_Epoch:34 [000/005 (0060/0588)]\tLoss Ss: 0.083176\n","\tRotated_Epoch:34 [000/005 (0080/0588)]\tLoss Ss: 0.063063\n","\tRotated_Epoch:34 [000/005 (0100/0588)]\tLoss Ss: 0.065925\n","\tRotated_Epoch:34 [000/005 (0120/0588)]\tLoss Ss: 0.052388\n","\tRotated_Epoch:34 [000/005 (0140/0588)]\tLoss Ss: 0.068376\n","\tRotated_Epoch:34 [000/005 (0160/0588)]\tLoss Ss: 0.099278\n","\tRotated_Epoch:34 [000/005 (0180/0588)]\tLoss Ss: 0.047305\n","\tRotated_Epoch:34 [000/005 (0200/0588)]\tLoss Ss: 0.062341\n","\tRotated_Epoch:34 [000/005 (0220/0588)]\tLoss Ss: 0.065961\n","\tRotated_Epoch:34 [000/005 (0240/0588)]\tLoss Ss: 0.079313\n","\tRotated_Epoch:34 [000/005 (0260/0588)]\tLoss Ss: 0.047766\n","\tRotated_Epoch:34 [000/005 (0280/0588)]\tLoss Ss: 0.075679\n","\tRotated_Epoch:34 [000/005 (0300/0588)]\tLoss Ss: 0.036001\n","\tRotated_Epoch:34 [000/005 (0320/0588)]\tLoss Ss: 0.074982\n","\tRotated_Epoch:34 [000/005 (0340/0588)]\tLoss Ss: 0.056773\n","\tRotated_Epoch:34 [000/005 (0360/0588)]\tLoss Ss: 0.064468\n","\tRotated_Epoch:34 [000/005 (0380/0588)]\tLoss Ss: 0.039336\n","\tRotated_Epoch:34 [000/005 (0400/0588)]\tLoss Ss: 0.061122\n","\tRotated_Epoch:34 [000/005 (0420/0588)]\tLoss Ss: 0.048540\n","\tRotated_Epoch:34 [000/005 (0440/0588)]\tLoss Ss: 0.054335\n","\tRotated_Epoch:34 [000/005 (0460/0588)]\tLoss Ss: 0.058973\n","\tRotated_Epoch:34 [000/005 (0480/0588)]\tLoss Ss: 0.079658\n","\tRotated_Epoch:34 [000/005 (0500/0588)]\tLoss Ss: 0.061427\n","\tRotated_Epoch:34 [000/005 (0520/0588)]\tLoss Ss: 0.050955\n","\tRotated_Epoch:34 [000/005 (0540/0588)]\tLoss Ss: 0.052482\n","\tRotated_Epoch:34 [000/005 (0560/0588)]\tLoss Ss: 0.067500\n","\tRotated_Epoch:34 [000/005 (0580/0588)]\tLoss Ss: 0.033743\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:34 [001/005 (0000/0755)]\tLoss Ss: 0.091263\n","\tRotated_Epoch:34 [001/005 (0020/0755)]\tLoss Ss: 0.052729\n","\tRotated_Epoch:34 [001/005 (0040/0755)]\tLoss Ss: 0.073730\n","\tRotated_Epoch:34 [001/005 (0060/0755)]\tLoss Ss: 0.057491\n","\tRotated_Epoch:34 [001/005 (0080/0755)]\tLoss Ss: 0.047829\n","\tRotated_Epoch:34 [001/005 (0100/0755)]\tLoss Ss: 0.050407\n","\tRotated_Epoch:34 [001/005 (0120/0755)]\tLoss Ss: 0.054210\n","\tRotated_Epoch:34 [001/005 (0140/0755)]\tLoss Ss: 0.062825\n","\tRotated_Epoch:34 [001/005 (0160/0755)]\tLoss Ss: 0.043128\n","\tRotated_Epoch:34 [001/005 (0180/0755)]\tLoss Ss: 0.028578\n","\tRotated_Epoch:34 [001/005 (0200/0755)]\tLoss Ss: 0.037584\n","\tRotated_Epoch:34 [001/005 (0220/0755)]\tLoss Ss: 0.037950\n","\tRotated_Epoch:34 [001/005 (0240/0755)]\tLoss Ss: 0.029335\n","\tRotated_Epoch:34 [001/005 (0260/0755)]\tLoss Ss: 0.025226\n","\tRotated_Epoch:34 [001/005 (0280/0755)]\tLoss Ss: 0.033981\n","\tRotated_Epoch:34 [001/005 (0300/0755)]\tLoss Ss: 0.038846\n","\tRotated_Epoch:34 [001/005 (0320/0755)]\tLoss Ss: 0.042932\n","\tRotated_Epoch:34 [001/005 (0340/0755)]\tLoss Ss: 0.024042\n","\tRotated_Epoch:34 [001/005 (0360/0755)]\tLoss Ss: 0.042461\n","\tRotated_Epoch:34 [001/005 (0380/0755)]\tLoss Ss: 0.037528\n","\tRotated_Epoch:34 [001/005 (0400/0755)]\tLoss Ss: 0.031282\n","\tRotated_Epoch:34 [001/005 (0420/0755)]\tLoss Ss: 0.047282\n","\tRotated_Epoch:34 [001/005 (0440/0755)]\tLoss Ss: 0.032523\n","\tRotated_Epoch:34 [001/005 (0460/0755)]\tLoss Ss: 0.029933\n","\tRotated_Epoch:34 [001/005 (0480/0755)]\tLoss Ss: 0.027022\n","\tRotated_Epoch:34 [001/005 (0500/0755)]\tLoss Ss: 0.030204\n","\tRotated_Epoch:34 [001/005 (0520/0755)]\tLoss Ss: 0.024710\n","\tRotated_Epoch:34 [001/005 (0540/0755)]\tLoss Ss: 0.030769\n","\tRotated_Epoch:34 [001/005 (0560/0755)]\tLoss Ss: 0.027448\n","\tRotated_Epoch:34 [001/005 (0580/0755)]\tLoss Ss: 0.037481\n","\tRotated_Epoch:34 [001/005 (0600/0755)]\tLoss Ss: 0.035536\n","\tRotated_Epoch:34 [001/005 (0620/0755)]\tLoss Ss: 0.037032\n","\tRotated_Epoch:34 [001/005 (0640/0755)]\tLoss Ss: 0.032070\n","\tRotated_Epoch:34 [001/005 (0660/0755)]\tLoss Ss: 0.038474\n","\tRotated_Epoch:34 [001/005 (0680/0755)]\tLoss Ss: 0.027324\n","\tRotated_Epoch:34 [001/005 (0700/0755)]\tLoss Ss: 0.017665\n","\tRotated_Epoch:34 [001/005 (0720/0755)]\tLoss Ss: 0.036193\n","\tRotated_Epoch:34 [001/005 (0740/0755)]\tLoss Ss: 0.022370\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tRotated_Epoch:34 [002/005 (0000/0693)]\tLoss Ss: 0.024378\n","\tRotated_Epoch:34 [002/005 (0020/0693)]\tLoss Ss: 0.026006\n","\tRotated_Epoch:34 [002/005 (0040/0693)]\tLoss Ss: 0.021626\n","\tRotated_Epoch:34 [002/005 (0060/0693)]\tLoss Ss: 0.020162\n","\tRotated_Epoch:34 [002/005 (0080/0693)]\tLoss Ss: 0.025003\n","\tRotated_Epoch:34 [002/005 (0100/0693)]\tLoss Ss: 0.017630\n","\tRotated_Epoch:34 [002/005 (0120/0693)]\tLoss Ss: 0.017778\n","\tRotated_Epoch:34 [002/005 (0140/0693)]\tLoss Ss: 0.017250\n","\tRotated_Epoch:34 [002/005 (0160/0693)]\tLoss Ss: 0.022748\n","\tRotated_Epoch:34 [002/005 (0180/0693)]\tLoss Ss: 0.012646\n","\tRotated_Epoch:34 [002/005 (0200/0693)]\tLoss Ss: 0.017318\n","\tRotated_Epoch:34 [002/005 (0220/0693)]\tLoss Ss: 0.015181\n","\tRotated_Epoch:34 [002/005 (0240/0693)]\tLoss Ss: 0.011334\n","\tRotated_Epoch:34 [002/005 (0260/0693)]\tLoss Ss: 0.015543\n","\tRotated_Epoch:34 [002/005 (0280/0693)]\tLoss Ss: 0.013837\n","\tRotated_Epoch:34 [002/005 (0300/0693)]\tLoss Ss: 0.014522\n","\tRotated_Epoch:34 [002/005 (0320/0693)]\tLoss Ss: 0.012969\n","\tRotated_Epoch:34 [002/005 (0340/0693)]\tLoss Ss: 0.018122\n","\tRotated_Epoch:34 [002/005 (0360/0693)]\tLoss Ss: 0.013071\n","\tRotated_Epoch:34 [002/005 (0380/0693)]\tLoss Ss: 0.017252\n","\tRotated_Epoch:34 [002/005 (0400/0693)]\tLoss Ss: 0.015857\n","\tRotated_Epoch:34 [002/005 (0420/0693)]\tLoss Ss: 0.017971\n","\tRotated_Epoch:34 [002/005 (0440/0693)]\tLoss Ss: 0.013399\n","\tRotated_Epoch:34 [002/005 (0460/0693)]\tLoss Ss: 0.013750\n","\tRotated_Epoch:34 [002/005 (0480/0693)]\tLoss Ss: 0.017923\n","\tRotated_Epoch:34 [002/005 (0500/0693)]\tLoss Ss: 0.019176\n","\tRotated_Epoch:34 [002/005 (0520/0693)]\tLoss Ss: 0.012121\n","\tRotated_Epoch:34 [002/005 (0540/0693)]\tLoss Ss: 0.012762\n","\tRotated_Epoch:34 [002/005 (0560/0693)]\tLoss Ss: 0.012883\n","\tRotated_Epoch:34 [002/005 (0580/0693)]\tLoss Ss: 0.017055\n","\tRotated_Epoch:34 [002/005 (0600/0693)]\tLoss Ss: 0.013426\n","\tRotated_Epoch:34 [002/005 (0620/0693)]\tLoss Ss: 0.014479\n","\tRotated_Epoch:34 [002/005 (0640/0693)]\tLoss Ss: 0.019499\n","\tRotated_Epoch:34 [002/005 (0660/0693)]\tLoss Ss: 0.013984\n","\tRotated_Epoch:34 [002/005 (0680/0693)]\tLoss Ss: 0.009637\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tRotated_Epoch:34 [003/005 (0000/0693)]\tLoss Ss: 0.016926\n","\tRotated_Epoch:34 [003/005 (0020/0693)]\tLoss Ss: 0.009949\n","\tRotated_Epoch:34 [003/005 (0040/0693)]\tLoss Ss: 0.014274\n","\tRotated_Epoch:34 [003/005 (0060/0693)]\tLoss Ss: 0.015384\n","\tRotated_Epoch:34 [003/005 (0080/0693)]\tLoss Ss: 0.013767\n","\tRotated_Epoch:34 [003/005 (0100/0693)]\tLoss Ss: 0.016806\n","\tRotated_Epoch:34 [003/005 (0120/0693)]\tLoss Ss: 0.010838\n","\tRotated_Epoch:34 [003/005 (0140/0693)]\tLoss Ss: 0.021670\n","\tRotated_Epoch:34 [003/005 (0160/0693)]\tLoss Ss: 0.010010\n","\tRotated_Epoch:34 [003/005 (0180/0693)]\tLoss Ss: 0.015724\n","\tRotated_Epoch:34 [003/005 (0200/0693)]\tLoss Ss: 0.014688\n","\tRotated_Epoch:34 [003/005 (0220/0693)]\tLoss Ss: 0.016468\n","\tRotated_Epoch:34 [003/005 (0240/0693)]\tLoss Ss: 0.015001\n","\tRotated_Epoch:34 [003/005 (0260/0693)]\tLoss Ss: 0.016030\n","\tRotated_Epoch:34 [003/005 (0280/0693)]\tLoss Ss: 0.012459\n","\tRotated_Epoch:34 [003/005 (0300/0693)]\tLoss Ss: 0.016127\n","\tRotated_Epoch:34 [003/005 (0320/0693)]\tLoss Ss: 0.011559\n","\tRotated_Epoch:34 [003/005 (0340/0693)]\tLoss Ss: 0.019907\n","\tRotated_Epoch:34 [003/005 (0360/0693)]\tLoss Ss: 0.012840\n","\tRotated_Epoch:34 [003/005 (0380/0693)]\tLoss Ss: 0.013063\n","\tRotated_Epoch:34 [003/005 (0400/0693)]\tLoss Ss: 0.010568\n","\tRotated_Epoch:34 [003/005 (0420/0693)]\tLoss Ss: 0.008660\n","\tRotated_Epoch:34 [003/005 (0440/0693)]\tLoss Ss: 0.010039\n","\tRotated_Epoch:34 [003/005 (0460/0693)]\tLoss Ss: 0.013294\n","\tRotated_Epoch:34 [003/005 (0480/0693)]\tLoss Ss: 0.011776\n","\tRotated_Epoch:34 [003/005 (0500/0693)]\tLoss Ss: 0.011378\n","\tRotated_Epoch:34 [003/005 (0520/0693)]\tLoss Ss: 0.011253\n","\tRotated_Epoch:34 [003/005 (0540/0693)]\tLoss Ss: 0.009765\n","\tRotated_Epoch:34 [003/005 (0560/0693)]\tLoss Ss: 0.011471\n","\tRotated_Epoch:34 [003/005 (0580/0693)]\tLoss Ss: 0.012696\n","\tRotated_Epoch:34 [003/005 (0600/0693)]\tLoss Ss: 0.015819\n","\tRotated_Epoch:34 [003/005 (0620/0693)]\tLoss Ss: 0.011692\n","\tRotated_Epoch:34 [003/005 (0640/0693)]\tLoss Ss: 0.013336\n","\tRotated_Epoch:34 [003/005 (0660/0693)]\tLoss Ss: 0.010480\n","\tRotated_Epoch:34 [003/005 (0680/0693)]\tLoss Ss: 0.010492\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:34 [004/005 (0000/0755)]\tLoss Ss: 0.067170\n","\tRotated_Epoch:34 [004/005 (0020/0755)]\tLoss Ss: 0.028793\n","\tRotated_Epoch:34 [004/005 (0040/0755)]\tLoss Ss: 0.039947\n","\tRotated_Epoch:34 [004/005 (0060/0755)]\tLoss Ss: 0.047887\n","\tRotated_Epoch:34 [004/005 (0080/0755)]\tLoss Ss: 0.029131\n","\tRotated_Epoch:34 [004/005 (0100/0755)]\tLoss Ss: 0.022642\n","\tRotated_Epoch:34 [004/005 (0120/0755)]\tLoss Ss: 0.017167\n","\tRotated_Epoch:34 [004/005 (0140/0755)]\tLoss Ss: 0.045220\n","\tRotated_Epoch:34 [004/005 (0160/0755)]\tLoss Ss: 0.033321\n","\tRotated_Epoch:34 [004/005 (0180/0755)]\tLoss Ss: 0.025043\n","\tRotated_Epoch:34 [004/005 (0200/0755)]\tLoss Ss: 0.037830\n","\tRotated_Epoch:34 [004/005 (0220/0755)]\tLoss Ss: 0.020338\n","\tRotated_Epoch:34 [004/005 (0240/0755)]\tLoss Ss: 0.018052\n","\tRotated_Epoch:34 [004/005 (0260/0755)]\tLoss Ss: 0.016202\n","\tRotated_Epoch:34 [004/005 (0280/0755)]\tLoss Ss: 0.017307\n","\tRotated_Epoch:34 [004/005 (0300/0755)]\tLoss Ss: 0.021812\n","\tRotated_Epoch:34 [004/005 (0320/0755)]\tLoss Ss: 0.021367\n","\tRotated_Epoch:34 [004/005 (0340/0755)]\tLoss Ss: 0.017797\n","\tRotated_Epoch:34 [004/005 (0360/0755)]\tLoss Ss: 0.021129\n","\tRotated_Epoch:34 [004/005 (0380/0755)]\tLoss Ss: 0.019842\n","\tRotated_Epoch:34 [004/005 (0400/0755)]\tLoss Ss: 0.012647\n","\tRotated_Epoch:34 [004/005 (0420/0755)]\tLoss Ss: 0.014923\n","\tRotated_Epoch:34 [004/005 (0440/0755)]\tLoss Ss: 0.022787\n","\tRotated_Epoch:34 [004/005 (0460/0755)]\tLoss Ss: 0.013209\n","\tRotated_Epoch:34 [004/005 (0480/0755)]\tLoss Ss: 0.012134\n","\tRotated_Epoch:34 [004/005 (0500/0755)]\tLoss Ss: 0.010543\n","\tRotated_Epoch:34 [004/005 (0520/0755)]\tLoss Ss: 0.022187\n","\tRotated_Epoch:34 [004/005 (0540/0755)]\tLoss Ss: 0.010522\n","\tRotated_Epoch:34 [004/005 (0560/0755)]\tLoss Ss: 0.016553\n","\tRotated_Epoch:34 [004/005 (0580/0755)]\tLoss Ss: 0.009433\n","\tRotated_Epoch:34 [004/005 (0600/0755)]\tLoss Ss: 0.014679\n","\tRotated_Epoch:34 [004/005 (0620/0755)]\tLoss Ss: 0.016806\n","\tRotated_Epoch:34 [004/005 (0640/0755)]\tLoss Ss: 0.011634\n","\tRotated_Epoch:34 [004/005 (0660/0755)]\tLoss Ss: 0.017640\n","\tRotated_Epoch:34 [004/005 (0680/0755)]\tLoss Ss: 0.012280\n","\tRotated_Epoch:34 [004/005 (0700/0755)]\tLoss Ss: 0.015014\n","\tRotated_Epoch:34 [004/005 (0720/0755)]\tLoss Ss: 0.019209\n","\tRotated_Epoch:34 [004/005 (0740/0755)]\tLoss Ss: 0.012822\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tRotated_Epoch:34 [005/005 (0000/0614)]\tLoss Ss: 0.019639\n","\tRotated_Epoch:34 [005/005 (0020/0614)]\tLoss Ss: 0.017640\n","\tRotated_Epoch:34 [005/005 (0040/0614)]\tLoss Ss: 0.011910\n","\tRotated_Epoch:34 [005/005 (0060/0614)]\tLoss Ss: 0.005261\n","\tRotated_Epoch:34 [005/005 (0080/0614)]\tLoss Ss: 0.007637\n","\tRotated_Epoch:34 [005/005 (0100/0614)]\tLoss Ss: 0.004566\n","\tRotated_Epoch:34 [005/005 (0120/0614)]\tLoss Ss: 0.009028\n","\tRotated_Epoch:34 [005/005 (0140/0614)]\tLoss Ss: 0.006397\n","\tRotated_Epoch:34 [005/005 (0160/0614)]\tLoss Ss: 0.010372\n","\tRotated_Epoch:34 [005/005 (0180/0614)]\tLoss Ss: 0.004384\n","\tRotated_Epoch:34 [005/005 (0200/0614)]\tLoss Ss: 0.010008\n","\tRotated_Epoch:34 [005/005 (0220/0614)]\tLoss Ss: 0.007384\n","\tRotated_Epoch:34 [005/005 (0240/0614)]\tLoss Ss: 0.004477\n","\tRotated_Epoch:34 [005/005 (0260/0614)]\tLoss Ss: 0.005178\n","\tRotated_Epoch:34 [005/005 (0280/0614)]\tLoss Ss: 0.010268\n","\tRotated_Epoch:34 [005/005 (0300/0614)]\tLoss Ss: 0.007169\n","\tRotated_Epoch:34 [005/005 (0320/0614)]\tLoss Ss: 0.006150\n","\tRotated_Epoch:34 [005/005 (0340/0614)]\tLoss Ss: 0.006841\n","\tRotated_Epoch:34 [005/005 (0360/0614)]\tLoss Ss: 0.006737\n","\tRotated_Epoch:34 [005/005 (0380/0614)]\tLoss Ss: 0.007509\n","\tRotated_Epoch:34 [005/005 (0400/0614)]\tLoss Ss: 0.005437\n","\tRotated_Epoch:34 [005/005 (0420/0614)]\tLoss Ss: 0.007583\n","\tRotated_Epoch:34 [005/005 (0440/0614)]\tLoss Ss: 0.003418\n","\tRotated_Epoch:34 [005/005 (0460/0614)]\tLoss Ss: 0.006805\n","\tRotated_Epoch:34 [005/005 (0480/0614)]\tLoss Ss: 0.005838\n","\tRotated_Epoch:34 [005/005 (0500/0614)]\tLoss Ss: 0.005106\n","\tRotated_Epoch:34 [005/005 (0520/0614)]\tLoss Ss: 0.006671\n","\tRotated_Epoch:34 [005/005 (0540/0614)]\tLoss Ss: 0.007998\n","\tRotated_Epoch:34 [005/005 (0560/0614)]\tLoss Ss: 0.006371\n","\tRotated_Epoch:34 [005/005 (0580/0614)]\tLoss Ss: 0.006392\n","\tRotated_Epoch:34 [005/005 (0600/0614)]\tLoss Ss: 0.005882\n","new_1_6_d.nii\n","new_1_6_m.nii\n","new_1_7_d.nii\n","new_1_7_m.nii\n","new_2_5_d.nii\n","new_2_5_m.nii\n","new_2_6_d.nii\n","new_2_6_m.nii\n","new_2_7_d.nii\n","new_2_7_m.nii\n","new_3_d (2).nii\n","new_3_m (2).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (3).nii\n","new_3_m (3).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (7).nii\n","new_3_m (7).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (8).nii\n","new_3_m (8).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (9).nii\n","new_3_m (9).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_4_10_d_163526.nii\n","new_4_10_m_163526.nii.gz\n","new_4_11_d_084336.nii\n","new_4_11_m_084336.nii.gz\n","new_4_7_d_184422.nii\n","new_4_7_m_184422.nii.gz\n","new_4_8_d_143412.nii\n","new_4_8_m_143412.nii.gz\n","new_4_9_d_155347.nii\n","new_4_9_m_155347.nii.gz\n","\tEpoch: 34; Dice: 0.9672 +/- 0.0043; Loss: 8.2078\n","Begin Epoch 35\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tEpoch:35 [000/005 (0000/0693)]\tLoss Ss: 0.017925\n","\tEpoch:35 [000/005 (0020/0693)]\tLoss Ss: 0.017206\n","\tEpoch:35 [000/005 (0040/0693)]\tLoss Ss: 0.021614\n","\tEpoch:35 [000/005 (0060/0693)]\tLoss Ss: 0.016437\n","\tEpoch:35 [000/005 (0080/0693)]\tLoss Ss: 0.017996\n","\tEpoch:35 [000/005 (0100/0693)]\tLoss Ss: 0.015696\n","\tEpoch:35 [000/005 (0120/0693)]\tLoss Ss: 0.011479\n","\tEpoch:35 [000/005 (0140/0693)]\tLoss Ss: 0.015966\n","\tEpoch:35 [000/005 (0160/0693)]\tLoss Ss: 0.017736\n","\tEpoch:35 [000/005 (0180/0693)]\tLoss Ss: 0.012533\n","\tEpoch:35 [000/005 (0200/0693)]\tLoss Ss: 0.013233\n","\tEpoch:35 [000/005 (0220/0693)]\tLoss Ss: 0.015109\n","\tEpoch:35 [000/005 (0240/0693)]\tLoss Ss: 0.009220\n","\tEpoch:35 [000/005 (0260/0693)]\tLoss Ss: 0.013849\n","\tEpoch:35 [000/005 (0280/0693)]\tLoss Ss: 0.019371\n","\tEpoch:35 [000/005 (0300/0693)]\tLoss Ss: 0.011436\n","\tEpoch:35 [000/005 (0320/0693)]\tLoss Ss: 0.016371\n","\tEpoch:35 [000/005 (0340/0693)]\tLoss Ss: 0.013520\n","\tEpoch:35 [000/005 (0360/0693)]\tLoss Ss: 0.012977\n","\tEpoch:35 [000/005 (0380/0693)]\tLoss Ss: 0.014460\n","\tEpoch:35 [000/005 (0400/0693)]\tLoss Ss: 0.013087\n","\tEpoch:35 [000/005 (0420/0693)]\tLoss Ss: 0.014938\n","\tEpoch:35 [000/005 (0440/0693)]\tLoss Ss: 0.010320\n","\tEpoch:35 [000/005 (0460/0693)]\tLoss Ss: 0.010906\n","\tEpoch:35 [000/005 (0480/0693)]\tLoss Ss: 0.018351\n","\tEpoch:35 [000/005 (0500/0693)]\tLoss Ss: 0.013446\n","\tEpoch:35 [000/005 (0520/0693)]\tLoss Ss: 0.010163\n","\tEpoch:35 [000/005 (0540/0693)]\tLoss Ss: 0.009741\n","\tEpoch:35 [000/005 (0560/0693)]\tLoss Ss: 0.011145\n","\tEpoch:35 [000/005 (0580/0693)]\tLoss Ss: 0.012637\n","\tEpoch:35 [000/005 (0600/0693)]\tLoss Ss: 0.018182\n","\tEpoch:35 [000/005 (0620/0693)]\tLoss Ss: 0.010599\n","\tEpoch:35 [000/005 (0640/0693)]\tLoss Ss: 0.021819\n","\tEpoch:35 [000/005 (0660/0693)]\tLoss Ss: 0.010566\n","\tEpoch:35 [000/005 (0680/0693)]\tLoss Ss: 0.012364\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:35 [001/005 (0000/0755)]\tLoss Ss: 0.012505\n","\tEpoch:35 [001/005 (0020/0755)]\tLoss Ss: 0.017514\n","\tEpoch:35 [001/005 (0040/0755)]\tLoss Ss: 0.022630\n","\tEpoch:35 [001/005 (0060/0755)]\tLoss Ss: 0.017530\n","\tEpoch:35 [001/005 (0080/0755)]\tLoss Ss: 0.012221\n","\tEpoch:35 [001/005 (0100/0755)]\tLoss Ss: 0.009712\n","\tEpoch:35 [001/005 (0120/0755)]\tLoss Ss: 0.012565\n","\tEpoch:35 [001/005 (0140/0755)]\tLoss Ss: 0.013926\n","\tEpoch:35 [001/005 (0160/0755)]\tLoss Ss: 0.014004\n","\tEpoch:35 [001/005 (0180/0755)]\tLoss Ss: 0.012579\n","\tEpoch:35 [001/005 (0200/0755)]\tLoss Ss: 0.008010\n","\tEpoch:35 [001/005 (0220/0755)]\tLoss Ss: 0.016545\n","\tEpoch:35 [001/005 (0240/0755)]\tLoss Ss: 0.015642\n","\tEpoch:35 [001/005 (0260/0755)]\tLoss Ss: 0.011659\n","\tEpoch:35 [001/005 (0280/0755)]\tLoss Ss: 0.015281\n","\tEpoch:35 [001/005 (0300/0755)]\tLoss Ss: 0.012374\n","\tEpoch:35 [001/005 (0320/0755)]\tLoss Ss: 0.016258\n","\tEpoch:35 [001/005 (0340/0755)]\tLoss Ss: 0.013417\n","\tEpoch:35 [001/005 (0360/0755)]\tLoss Ss: 0.009484\n","\tEpoch:35 [001/005 (0380/0755)]\tLoss Ss: 0.009844\n","\tEpoch:35 [001/005 (0400/0755)]\tLoss Ss: 0.015145\n","\tEpoch:35 [001/005 (0420/0755)]\tLoss Ss: 0.012729\n","\tEpoch:35 [001/005 (0440/0755)]\tLoss Ss: 0.011318\n","\tEpoch:35 [001/005 (0460/0755)]\tLoss Ss: 0.009801\n","\tEpoch:35 [001/005 (0480/0755)]\tLoss Ss: 0.010858\n","\tEpoch:35 [001/005 (0500/0755)]\tLoss Ss: 0.013114\n","\tEpoch:35 [001/005 (0520/0755)]\tLoss Ss: 0.007103\n","\tEpoch:35 [001/005 (0540/0755)]\tLoss Ss: 0.010648\n","\tEpoch:35 [001/005 (0560/0755)]\tLoss Ss: 0.011920\n","\tEpoch:35 [001/005 (0580/0755)]\tLoss Ss: 0.007245\n","\tEpoch:35 [001/005 (0600/0755)]\tLoss Ss: 0.011695\n","\tEpoch:35 [001/005 (0620/0755)]\tLoss Ss: 0.008356\n","\tEpoch:35 [001/005 (0640/0755)]\tLoss Ss: 0.016388\n","\tEpoch:35 [001/005 (0660/0755)]\tLoss Ss: 0.012408\n","\tEpoch:35 [001/005 (0680/0755)]\tLoss Ss: 0.008989\n","\tEpoch:35 [001/005 (0700/0755)]\tLoss Ss: 0.009462\n","\tEpoch:35 [001/005 (0720/0755)]\tLoss Ss: 0.012604\n","\tEpoch:35 [001/005 (0740/0755)]\tLoss Ss: 0.008700\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tEpoch:35 [002/005 (0000/0614)]\tLoss Ss: 0.008719\n","\tEpoch:35 [002/005 (0020/0614)]\tLoss Ss: 0.005436\n","\tEpoch:35 [002/005 (0040/0614)]\tLoss Ss: 0.004582\n","\tEpoch:35 [002/005 (0060/0614)]\tLoss Ss: 0.006636\n","\tEpoch:35 [002/005 (0080/0614)]\tLoss Ss: 0.007031\n","\tEpoch:35 [002/005 (0100/0614)]\tLoss Ss: 0.003548\n","\tEpoch:35 [002/005 (0120/0614)]\tLoss Ss: 0.006947\n","\tEpoch:35 [002/005 (0140/0614)]\tLoss Ss: 0.005602\n","\tEpoch:35 [002/005 (0160/0614)]\tLoss Ss: 0.005479\n","\tEpoch:35 [002/005 (0180/0614)]\tLoss Ss: 0.005834\n","\tEpoch:35 [002/005 (0200/0614)]\tLoss Ss: 0.004451\n","\tEpoch:35 [002/005 (0220/0614)]\tLoss Ss: 0.003192\n","\tEpoch:35 [002/005 (0240/0614)]\tLoss Ss: 0.005691\n","\tEpoch:35 [002/005 (0260/0614)]\tLoss Ss: 0.003575\n","\tEpoch:35 [002/005 (0280/0614)]\tLoss Ss: 0.003443\n","\tEpoch:35 [002/005 (0300/0614)]\tLoss Ss: 0.002963\n","\tEpoch:35 [002/005 (0320/0614)]\tLoss Ss: 0.004426\n","\tEpoch:35 [002/005 (0340/0614)]\tLoss Ss: 0.003563\n","\tEpoch:35 [002/005 (0360/0614)]\tLoss Ss: 0.004490\n","\tEpoch:35 [002/005 (0380/0614)]\tLoss Ss: 0.005460\n","\tEpoch:35 [002/005 (0400/0614)]\tLoss Ss: 0.004071\n","\tEpoch:35 [002/005 (0420/0614)]\tLoss Ss: 0.005326\n","\tEpoch:35 [002/005 (0440/0614)]\tLoss Ss: 0.006290\n","\tEpoch:35 [002/005 (0460/0614)]\tLoss Ss: 0.005433\n","\tEpoch:35 [002/005 (0480/0614)]\tLoss Ss: 0.008339\n","\tEpoch:35 [002/005 (0500/0614)]\tLoss Ss: 0.005569\n","\tEpoch:35 [002/005 (0520/0614)]\tLoss Ss: 0.004583\n","\tEpoch:35 [002/005 (0540/0614)]\tLoss Ss: 0.005090\n","\tEpoch:35 [002/005 (0560/0614)]\tLoss Ss: 0.004565\n","\tEpoch:35 [002/005 (0580/0614)]\tLoss Ss: 0.007329\n","\tEpoch:35 [002/005 (0600/0614)]\tLoss Ss: 0.003962\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tEpoch:35 [003/005 (0000/0588)]\tLoss Ss: 0.004194\n","\tEpoch:35 [003/005 (0020/0588)]\tLoss Ss: 0.002729\n","\tEpoch:35 [003/005 (0040/0588)]\tLoss Ss: 0.005984\n","\tEpoch:35 [003/005 (0060/0588)]\tLoss Ss: 0.002635\n","\tEpoch:35 [003/005 (0080/0588)]\tLoss Ss: 0.004041\n","\tEpoch:35 [003/005 (0100/0588)]\tLoss Ss: 0.002347\n","\tEpoch:35 [003/005 (0120/0588)]\tLoss Ss: 0.004806\n","\tEpoch:35 [003/005 (0140/0588)]\tLoss Ss: 0.003250\n","\tEpoch:35 [003/005 (0160/0588)]\tLoss Ss: 0.005695\n","\tEpoch:35 [003/005 (0180/0588)]\tLoss Ss: 0.005673\n","\tEpoch:35 [003/005 (0200/0588)]\tLoss Ss: 0.005438\n","\tEpoch:35 [003/005 (0220/0588)]\tLoss Ss: 0.004336\n","\tEpoch:35 [003/005 (0240/0588)]\tLoss Ss: 0.004022\n","\tEpoch:35 [003/005 (0260/0588)]\tLoss Ss: 0.003943\n","\tEpoch:35 [003/005 (0280/0588)]\tLoss Ss: 0.004208\n","\tEpoch:35 [003/005 (0300/0588)]\tLoss Ss: 0.003381\n","\tEpoch:35 [003/005 (0320/0588)]\tLoss Ss: 0.004534\n","\tEpoch:35 [003/005 (0340/0588)]\tLoss Ss: 0.005582\n","\tEpoch:35 [003/005 (0360/0588)]\tLoss Ss: 0.005154\n","\tEpoch:35 [003/005 (0380/0588)]\tLoss Ss: 0.005296\n","\tEpoch:35 [003/005 (0400/0588)]\tLoss Ss: 0.002223\n","\tEpoch:35 [003/005 (0420/0588)]\tLoss Ss: 0.004225\n","\tEpoch:35 [003/005 (0440/0588)]\tLoss Ss: 0.003353\n","\tEpoch:35 [003/005 (0460/0588)]\tLoss Ss: 0.003531\n","\tEpoch:35 [003/005 (0480/0588)]\tLoss Ss: 0.004221\n","\tEpoch:35 [003/005 (0500/0588)]\tLoss Ss: 0.003107\n","\tEpoch:35 [003/005 (0520/0588)]\tLoss Ss: 0.003465\n","\tEpoch:35 [003/005 (0540/0588)]\tLoss Ss: 0.002961\n","\tEpoch:35 [003/005 (0560/0588)]\tLoss Ss: 0.004421\n","\tEpoch:35 [003/005 (0580/0588)]\tLoss Ss: 0.004489\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tEpoch:35 [004/005 (0000/0693)]\tLoss Ss: 0.014014\n","\tEpoch:35 [004/005 (0020/0693)]\tLoss Ss: 0.011062\n","\tEpoch:35 [004/005 (0040/0693)]\tLoss Ss: 0.013779\n","\tEpoch:35 [004/005 (0060/0693)]\tLoss Ss: 0.008507\n","\tEpoch:35 [004/005 (0080/0693)]\tLoss Ss: 0.009628\n","\tEpoch:35 [004/005 (0100/0693)]\tLoss Ss: 0.015770\n","\tEpoch:35 [004/005 (0120/0693)]\tLoss Ss: 0.014530\n","\tEpoch:35 [004/005 (0140/0693)]\tLoss Ss: 0.009465\n","\tEpoch:35 [004/005 (0160/0693)]\tLoss Ss: 0.014617\n","\tEpoch:35 [004/005 (0180/0693)]\tLoss Ss: 0.011528\n","\tEpoch:35 [004/005 (0200/0693)]\tLoss Ss: 0.012156\n","\tEpoch:35 [004/005 (0220/0693)]\tLoss Ss: 0.015810\n","\tEpoch:35 [004/005 (0240/0693)]\tLoss Ss: 0.013206\n","\tEpoch:35 [004/005 (0260/0693)]\tLoss Ss: 0.008759\n","\tEpoch:35 [004/005 (0280/0693)]\tLoss Ss: 0.007811\n","\tEpoch:35 [004/005 (0300/0693)]\tLoss Ss: 0.012417\n","\tEpoch:35 [004/005 (0320/0693)]\tLoss Ss: 0.007670\n","\tEpoch:35 [004/005 (0340/0693)]\tLoss Ss: 0.012011\n","\tEpoch:35 [004/005 (0360/0693)]\tLoss Ss: 0.011360\n","\tEpoch:35 [004/005 (0380/0693)]\tLoss Ss: 0.015735\n","\tEpoch:35 [004/005 (0400/0693)]\tLoss Ss: 0.009969\n","\tEpoch:35 [004/005 (0420/0693)]\tLoss Ss: 0.012879\n","\tEpoch:35 [004/005 (0440/0693)]\tLoss Ss: 0.010457\n","\tEpoch:35 [004/005 (0460/0693)]\tLoss Ss: 0.007737\n","\tEpoch:35 [004/005 (0480/0693)]\tLoss Ss: 0.013306\n","\tEpoch:35 [004/005 (0500/0693)]\tLoss Ss: 0.008883\n","\tEpoch:35 [004/005 (0520/0693)]\tLoss Ss: 0.010472\n","\tEpoch:35 [004/005 (0540/0693)]\tLoss Ss: 0.010724\n","\tEpoch:35 [004/005 (0560/0693)]\tLoss Ss: 0.009537\n","\tEpoch:35 [004/005 (0580/0693)]\tLoss Ss: 0.009476\n","\tEpoch:35 [004/005 (0600/0693)]\tLoss Ss: 0.009788\n","\tEpoch:35 [004/005 (0620/0693)]\tLoss Ss: 0.009577\n","\tEpoch:35 [004/005 (0640/0693)]\tLoss Ss: 0.015739\n","\tEpoch:35 [004/005 (0660/0693)]\tLoss Ss: 0.008379\n","\tEpoch:35 [004/005 (0680/0693)]\tLoss Ss: 0.009327\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:35 [005/005 (0000/0755)]\tLoss Ss: 0.027508\n","\tEpoch:35 [005/005 (0020/0755)]\tLoss Ss: 0.020191\n","\tEpoch:35 [005/005 (0040/0755)]\tLoss Ss: 0.022963\n","\tEpoch:35 [005/005 (0060/0755)]\tLoss Ss: 0.015100\n","\tEpoch:35 [005/005 (0080/0755)]\tLoss Ss: 0.017033\n","\tEpoch:35 [005/005 (0100/0755)]\tLoss Ss: 0.012262\n","\tEpoch:35 [005/005 (0120/0755)]\tLoss Ss: 0.014931\n","\tEpoch:35 [005/005 (0140/0755)]\tLoss Ss: 0.016408\n","\tEpoch:35 [005/005 (0160/0755)]\tLoss Ss: 0.011425\n","\tEpoch:35 [005/005 (0180/0755)]\tLoss Ss: 0.019240\n","\tEpoch:35 [005/005 (0200/0755)]\tLoss Ss: 0.012805\n","\tEpoch:35 [005/005 (0220/0755)]\tLoss Ss: 0.014589\n","\tEpoch:35 [005/005 (0240/0755)]\tLoss Ss: 0.017715\n","\tEpoch:35 [005/005 (0260/0755)]\tLoss Ss: 0.011192\n","\tEpoch:35 [005/005 (0280/0755)]\tLoss Ss: 0.018450\n","\tEpoch:35 [005/005 (0300/0755)]\tLoss Ss: 0.012974\n","\tEpoch:35 [005/005 (0320/0755)]\tLoss Ss: 0.013386\n","\tEpoch:35 [005/005 (0340/0755)]\tLoss Ss: 0.013913\n","\tEpoch:35 [005/005 (0360/0755)]\tLoss Ss: 0.013850\n","\tEpoch:35 [005/005 (0380/0755)]\tLoss Ss: 0.015024\n","\tEpoch:35 [005/005 (0400/0755)]\tLoss Ss: 0.019470\n","\tEpoch:35 [005/005 (0420/0755)]\tLoss Ss: 0.013606\n","\tEpoch:35 [005/005 (0440/0755)]\tLoss Ss: 0.012903\n","\tEpoch:35 [005/005 (0460/0755)]\tLoss Ss: 0.013117\n","\tEpoch:35 [005/005 (0480/0755)]\tLoss Ss: 0.012454\n","\tEpoch:35 [005/005 (0500/0755)]\tLoss Ss: 0.014355\n","\tEpoch:35 [005/005 (0520/0755)]\tLoss Ss: 0.015673\n","\tEpoch:35 [005/005 (0540/0755)]\tLoss Ss: 0.015037\n","\tEpoch:35 [005/005 (0560/0755)]\tLoss Ss: 0.017231\n","\tEpoch:35 [005/005 (0580/0755)]\tLoss Ss: 0.013214\n","\tEpoch:35 [005/005 (0600/0755)]\tLoss Ss: 0.011358\n","\tEpoch:35 [005/005 (0620/0755)]\tLoss Ss: 0.015975\n","\tEpoch:35 [005/005 (0640/0755)]\tLoss Ss: 0.014143\n","\tEpoch:35 [005/005 (0660/0755)]\tLoss Ss: 0.013419\n","\tEpoch:35 [005/005 (0680/0755)]\tLoss Ss: 0.013859\n","\tEpoch:35 [005/005 (0700/0755)]\tLoss Ss: 0.010502\n","\tEpoch:35 [005/005 (0720/0755)]\tLoss Ss: 0.013688\n","\tEpoch:35 [005/005 (0740/0755)]\tLoss Ss: 0.009754\n","Now train the rotated image\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tRotated_Epoch:35 [000/005 (0000/0614)]\tLoss Ss: 0.006890\n","\tRotated_Epoch:35 [000/005 (0020/0614)]\tLoss Ss: 0.016263\n","\tRotated_Epoch:35 [000/005 (0040/0614)]\tLoss Ss: 0.007288\n","\tRotated_Epoch:35 [000/005 (0060/0614)]\tLoss Ss: 0.009301\n","\tRotated_Epoch:35 [000/005 (0080/0614)]\tLoss Ss: 0.011867\n","\tRotated_Epoch:35 [000/005 (0100/0614)]\tLoss Ss: 0.007563\n","\tRotated_Epoch:35 [000/005 (0120/0614)]\tLoss Ss: 0.004919\n","\tRotated_Epoch:35 [000/005 (0140/0614)]\tLoss Ss: 0.006446\n","\tRotated_Epoch:35 [000/005 (0160/0614)]\tLoss Ss: 0.005702\n","\tRotated_Epoch:35 [000/005 (0180/0614)]\tLoss Ss: 0.003260\n","\tRotated_Epoch:35 [000/005 (0200/0614)]\tLoss Ss: 0.006174\n","\tRotated_Epoch:35 [000/005 (0220/0614)]\tLoss Ss: 0.008779\n","\tRotated_Epoch:35 [000/005 (0240/0614)]\tLoss Ss: 0.004972\n","\tRotated_Epoch:35 [000/005 (0260/0614)]\tLoss Ss: 0.006548\n","\tRotated_Epoch:35 [000/005 (0280/0614)]\tLoss Ss: 0.006961\n","\tRotated_Epoch:35 [000/005 (0300/0614)]\tLoss Ss: 0.004727\n","\tRotated_Epoch:35 [000/005 (0320/0614)]\tLoss Ss: 0.004692\n","\tRotated_Epoch:35 [000/005 (0340/0614)]\tLoss Ss: 0.005077\n","\tRotated_Epoch:35 [000/005 (0360/0614)]\tLoss Ss: 0.003893\n","\tRotated_Epoch:35 [000/005 (0380/0614)]\tLoss Ss: 0.006966\n","\tRotated_Epoch:35 [000/005 (0400/0614)]\tLoss Ss: 0.007384\n","\tRotated_Epoch:35 [000/005 (0420/0614)]\tLoss Ss: 0.003875\n","\tRotated_Epoch:35 [000/005 (0440/0614)]\tLoss Ss: 0.004870\n","\tRotated_Epoch:35 [000/005 (0460/0614)]\tLoss Ss: 0.006445\n","\tRotated_Epoch:35 [000/005 (0480/0614)]\tLoss Ss: 0.006635\n","\tRotated_Epoch:35 [000/005 (0500/0614)]\tLoss Ss: 0.003923\n","\tRotated_Epoch:35 [000/005 (0520/0614)]\tLoss Ss: 0.005675\n","\tRotated_Epoch:35 [000/005 (0540/0614)]\tLoss Ss: 0.004592\n","\tRotated_Epoch:35 [000/005 (0560/0614)]\tLoss Ss: 0.003552\n","\tRotated_Epoch:35 [000/005 (0580/0614)]\tLoss Ss: 0.004613\n","\tRotated_Epoch:35 [000/005 (0600/0614)]\tLoss Ss: 0.005334\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tRotated_Epoch:35 [001/005 (0000/0693)]\tLoss Ss: 0.023217\n","\tRotated_Epoch:35 [001/005 (0020/0693)]\tLoss Ss: 0.017623\n","\tRotated_Epoch:35 [001/005 (0040/0693)]\tLoss Ss: 0.019092\n","\tRotated_Epoch:35 [001/005 (0060/0693)]\tLoss Ss: 0.028790\n","\tRotated_Epoch:35 [001/005 (0080/0693)]\tLoss Ss: 0.016923\n","\tRotated_Epoch:35 [001/005 (0100/0693)]\tLoss Ss: 0.013479\n","\tRotated_Epoch:35 [001/005 (0120/0693)]\tLoss Ss: 0.018844\n","\tRotated_Epoch:35 [001/005 (0140/0693)]\tLoss Ss: 0.013492\n","\tRotated_Epoch:35 [001/005 (0160/0693)]\tLoss Ss: 0.013484\n","\tRotated_Epoch:35 [001/005 (0180/0693)]\tLoss Ss: 0.017211\n","\tRotated_Epoch:35 [001/005 (0200/0693)]\tLoss Ss: 0.011347\n","\tRotated_Epoch:35 [001/005 (0220/0693)]\tLoss Ss: 0.009124\n","\tRotated_Epoch:35 [001/005 (0240/0693)]\tLoss Ss: 0.014942\n","\tRotated_Epoch:35 [001/005 (0260/0693)]\tLoss Ss: 0.013412\n","\tRotated_Epoch:35 [001/005 (0280/0693)]\tLoss Ss: 0.009001\n","\tRotated_Epoch:35 [001/005 (0300/0693)]\tLoss Ss: 0.010813\n","\tRotated_Epoch:35 [001/005 (0320/0693)]\tLoss Ss: 0.012751\n","\tRotated_Epoch:35 [001/005 (0340/0693)]\tLoss Ss: 0.015348\n","\tRotated_Epoch:35 [001/005 (0360/0693)]\tLoss Ss: 0.011088\n","\tRotated_Epoch:35 [001/005 (0380/0693)]\tLoss Ss: 0.011277\n","\tRotated_Epoch:35 [001/005 (0400/0693)]\tLoss Ss: 0.007298\n","\tRotated_Epoch:35 [001/005 (0420/0693)]\tLoss Ss: 0.013206\n","\tRotated_Epoch:35 [001/005 (0440/0693)]\tLoss Ss: 0.016214\n","\tRotated_Epoch:35 [001/005 (0460/0693)]\tLoss Ss: 0.008880\n","\tRotated_Epoch:35 [001/005 (0480/0693)]\tLoss Ss: 0.009802\n","\tRotated_Epoch:35 [001/005 (0500/0693)]\tLoss Ss: 0.010384\n","\tRotated_Epoch:35 [001/005 (0520/0693)]\tLoss Ss: 0.012338\n","\tRotated_Epoch:35 [001/005 (0540/0693)]\tLoss Ss: 0.011443\n","\tRotated_Epoch:35 [001/005 (0560/0693)]\tLoss Ss: 0.012364\n","\tRotated_Epoch:35 [001/005 (0580/0693)]\tLoss Ss: 0.010208\n","\tRotated_Epoch:35 [001/005 (0600/0693)]\tLoss Ss: 0.008472\n","\tRotated_Epoch:35 [001/005 (0620/0693)]\tLoss Ss: 0.008177\n","\tRotated_Epoch:35 [001/005 (0640/0693)]\tLoss Ss: 0.010699\n","\tRotated_Epoch:35 [001/005 (0660/0693)]\tLoss Ss: 0.008464\n","\tRotated_Epoch:35 [001/005 (0680/0693)]\tLoss Ss: 0.016562\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tRotated_Epoch:35 [002/005 (0000/0588)]\tLoss Ss: 0.156369\n","\tRotated_Epoch:35 [002/005 (0020/0588)]\tLoss Ss: 0.118219\n","\tRotated_Epoch:35 [002/005 (0040/0588)]\tLoss Ss: 0.067957\n","\tRotated_Epoch:35 [002/005 (0060/0588)]\tLoss Ss: 0.061996\n","\tRotated_Epoch:35 [002/005 (0080/0588)]\tLoss Ss: 0.046498\n","\tRotated_Epoch:35 [002/005 (0100/0588)]\tLoss Ss: 0.044884\n","\tRotated_Epoch:35 [002/005 (0120/0588)]\tLoss Ss: 0.068031\n","\tRotated_Epoch:35 [002/005 (0140/0588)]\tLoss Ss: 0.055863\n","\tRotated_Epoch:35 [002/005 (0160/0588)]\tLoss Ss: 0.060202\n","\tRotated_Epoch:35 [002/005 (0180/0588)]\tLoss Ss: 0.056442\n","\tRotated_Epoch:35 [002/005 (0200/0588)]\tLoss Ss: 0.056160\n","\tRotated_Epoch:35 [002/005 (0220/0588)]\tLoss Ss: 0.061627\n","\tRotated_Epoch:35 [002/005 (0240/0588)]\tLoss Ss: 0.048209\n","\tRotated_Epoch:35 [002/005 (0260/0588)]\tLoss Ss: 0.033502\n","\tRotated_Epoch:35 [002/005 (0280/0588)]\tLoss Ss: 0.049876\n","\tRotated_Epoch:35 [002/005 (0300/0588)]\tLoss Ss: 0.054654\n","\tRotated_Epoch:35 [002/005 (0320/0588)]\tLoss Ss: 0.078232\n","\tRotated_Epoch:35 [002/005 (0340/0588)]\tLoss Ss: 0.048265\n","\tRotated_Epoch:35 [002/005 (0360/0588)]\tLoss Ss: 0.066388\n","\tRotated_Epoch:35 [002/005 (0380/0588)]\tLoss Ss: 0.053384\n","\tRotated_Epoch:35 [002/005 (0400/0588)]\tLoss Ss: 0.053718\n","\tRotated_Epoch:35 [002/005 (0420/0588)]\tLoss Ss: 0.046004\n","\tRotated_Epoch:35 [002/005 (0440/0588)]\tLoss Ss: 0.062299\n","\tRotated_Epoch:35 [002/005 (0460/0588)]\tLoss Ss: 0.035351\n","\tRotated_Epoch:35 [002/005 (0480/0588)]\tLoss Ss: 0.056063\n","\tRotated_Epoch:35 [002/005 (0500/0588)]\tLoss Ss: 0.035855\n","\tRotated_Epoch:35 [002/005 (0520/0588)]\tLoss Ss: 0.041993\n","\tRotated_Epoch:35 [002/005 (0540/0588)]\tLoss Ss: 0.092235\n","\tRotated_Epoch:35 [002/005 (0560/0588)]\tLoss Ss: 0.054261\n","\tRotated_Epoch:35 [002/005 (0580/0588)]\tLoss Ss: 0.023157\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:35 [003/005 (0000/0755)]\tLoss Ss: 0.017124\n","\tRotated_Epoch:35 [003/005 (0020/0755)]\tLoss Ss: 0.025782\n","\tRotated_Epoch:35 [003/005 (0040/0755)]\tLoss Ss: 0.026319\n","\tRotated_Epoch:35 [003/005 (0060/0755)]\tLoss Ss: 0.024688\n","\tRotated_Epoch:35 [003/005 (0080/0755)]\tLoss Ss: 0.022512\n","\tRotated_Epoch:35 [003/005 (0100/0755)]\tLoss Ss: 0.020817\n","\tRotated_Epoch:35 [003/005 (0120/0755)]\tLoss Ss: 0.019501\n","\tRotated_Epoch:35 [003/005 (0140/0755)]\tLoss Ss: 0.015359\n","\tRotated_Epoch:35 [003/005 (0160/0755)]\tLoss Ss: 0.014400\n","\tRotated_Epoch:35 [003/005 (0180/0755)]\tLoss Ss: 0.019284\n","\tRotated_Epoch:35 [003/005 (0200/0755)]\tLoss Ss: 0.012886\n","\tRotated_Epoch:35 [003/005 (0220/0755)]\tLoss Ss: 0.015307\n","\tRotated_Epoch:35 [003/005 (0240/0755)]\tLoss Ss: 0.022123\n","\tRotated_Epoch:35 [003/005 (0260/0755)]\tLoss Ss: 0.012851\n","\tRotated_Epoch:35 [003/005 (0280/0755)]\tLoss Ss: 0.010974\n","\tRotated_Epoch:35 [003/005 (0300/0755)]\tLoss Ss: 0.011780\n","\tRotated_Epoch:35 [003/005 (0320/0755)]\tLoss Ss: 0.019622\n","\tRotated_Epoch:35 [003/005 (0340/0755)]\tLoss Ss: 0.014832\n","\tRotated_Epoch:35 [003/005 (0360/0755)]\tLoss Ss: 0.025329\n","\tRotated_Epoch:35 [003/005 (0380/0755)]\tLoss Ss: 0.018198\n","\tRotated_Epoch:35 [003/005 (0400/0755)]\tLoss Ss: 0.015710\n","\tRotated_Epoch:35 [003/005 (0420/0755)]\tLoss Ss: 0.014363\n","\tRotated_Epoch:35 [003/005 (0440/0755)]\tLoss Ss: 0.018107\n","\tRotated_Epoch:35 [003/005 (0460/0755)]\tLoss Ss: 0.012417\n","\tRotated_Epoch:35 [003/005 (0480/0755)]\tLoss Ss: 0.013437\n","\tRotated_Epoch:35 [003/005 (0500/0755)]\tLoss Ss: 0.012191\n","\tRotated_Epoch:35 [003/005 (0520/0755)]\tLoss Ss: 0.010519\n","\tRotated_Epoch:35 [003/005 (0540/0755)]\tLoss Ss: 0.011212\n","\tRotated_Epoch:35 [003/005 (0560/0755)]\tLoss Ss: 0.013318\n","\tRotated_Epoch:35 [003/005 (0580/0755)]\tLoss Ss: 0.010537\n","\tRotated_Epoch:35 [003/005 (0600/0755)]\tLoss Ss: 0.013134\n","\tRotated_Epoch:35 [003/005 (0620/0755)]\tLoss Ss: 0.014994\n","\tRotated_Epoch:35 [003/005 (0640/0755)]\tLoss Ss: 0.022874\n","\tRotated_Epoch:35 [003/005 (0660/0755)]\tLoss Ss: 0.011097\n","\tRotated_Epoch:35 [003/005 (0680/0755)]\tLoss Ss: 0.010288\n","\tRotated_Epoch:35 [003/005 (0700/0755)]\tLoss Ss: 0.020842\n","\tRotated_Epoch:35 [003/005 (0720/0755)]\tLoss Ss: 0.014056\n","\tRotated_Epoch:35 [003/005 (0740/0755)]\tLoss Ss: 0.013112\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:35 [004/005 (0000/0755)]\tLoss Ss: 0.290365\n","\tRotated_Epoch:35 [004/005 (0020/0755)]\tLoss Ss: 0.189061\n","\tRotated_Epoch:35 [004/005 (0040/0755)]\tLoss Ss: 0.245195\n","\tRotated_Epoch:35 [004/005 (0060/0755)]\tLoss Ss: 0.172854\n","\tRotated_Epoch:35 [004/005 (0080/0755)]\tLoss Ss: 0.194000\n","\tRotated_Epoch:35 [004/005 (0100/0755)]\tLoss Ss: 0.071465\n","\tRotated_Epoch:35 [004/005 (0120/0755)]\tLoss Ss: 0.110062\n","\tRotated_Epoch:35 [004/005 (0140/0755)]\tLoss Ss: 0.080925\n","\tRotated_Epoch:35 [004/005 (0160/0755)]\tLoss Ss: 0.068169\n","\tRotated_Epoch:35 [004/005 (0180/0755)]\tLoss Ss: 0.078726\n","\tRotated_Epoch:35 [004/005 (0200/0755)]\tLoss Ss: 0.042850\n","\tRotated_Epoch:35 [004/005 (0220/0755)]\tLoss Ss: 0.047380\n","\tRotated_Epoch:35 [004/005 (0240/0755)]\tLoss Ss: 0.048584\n","\tRotated_Epoch:35 [004/005 (0260/0755)]\tLoss Ss: 0.046094\n","\tRotated_Epoch:35 [004/005 (0280/0755)]\tLoss Ss: 0.057257\n","\tRotated_Epoch:35 [004/005 (0300/0755)]\tLoss Ss: 0.084618\n","\tRotated_Epoch:35 [004/005 (0320/0755)]\tLoss Ss: 0.061297\n","\tRotated_Epoch:35 [004/005 (0340/0755)]\tLoss Ss: 0.067104\n","\tRotated_Epoch:35 [004/005 (0360/0755)]\tLoss Ss: 0.052494\n","\tRotated_Epoch:35 [004/005 (0380/0755)]\tLoss Ss: 0.047383\n","\tRotated_Epoch:35 [004/005 (0400/0755)]\tLoss Ss: 0.048178\n","\tRotated_Epoch:35 [004/005 (0420/0755)]\tLoss Ss: 0.046907\n","\tRotated_Epoch:35 [004/005 (0440/0755)]\tLoss Ss: 0.069339\n","\tRotated_Epoch:35 [004/005 (0460/0755)]\tLoss Ss: 0.046169\n","\tRotated_Epoch:35 [004/005 (0480/0755)]\tLoss Ss: 0.034111\n","\tRotated_Epoch:35 [004/005 (0500/0755)]\tLoss Ss: 0.045419\n","\tRotated_Epoch:35 [004/005 (0520/0755)]\tLoss Ss: 0.042973\n","\tRotated_Epoch:35 [004/005 (0540/0755)]\tLoss Ss: 0.054958\n","\tRotated_Epoch:35 [004/005 (0560/0755)]\tLoss Ss: 0.051535\n","\tRotated_Epoch:35 [004/005 (0580/0755)]\tLoss Ss: 0.047326\n","\tRotated_Epoch:35 [004/005 (0600/0755)]\tLoss Ss: 0.043489\n","\tRotated_Epoch:35 [004/005 (0620/0755)]\tLoss Ss: 0.051121\n","\tRotated_Epoch:35 [004/005 (0640/0755)]\tLoss Ss: 0.049401\n","\tRotated_Epoch:35 [004/005 (0660/0755)]\tLoss Ss: 0.065829\n","\tRotated_Epoch:35 [004/005 (0680/0755)]\tLoss Ss: 0.032983\n","\tRotated_Epoch:35 [004/005 (0700/0755)]\tLoss Ss: 0.026539\n","\tRotated_Epoch:35 [004/005 (0720/0755)]\tLoss Ss: 0.032639\n","\tRotated_Epoch:35 [004/005 (0740/0755)]\tLoss Ss: 0.036500\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tRotated_Epoch:35 [005/005 (0000/0693)]\tLoss Ss: 0.084333\n","\tRotated_Epoch:35 [005/005 (0020/0693)]\tLoss Ss: 0.047893\n","\tRotated_Epoch:35 [005/005 (0040/0693)]\tLoss Ss: 0.051333\n","\tRotated_Epoch:35 [005/005 (0060/0693)]\tLoss Ss: 0.032698\n","\tRotated_Epoch:35 [005/005 (0080/0693)]\tLoss Ss: 0.026877\n","\tRotated_Epoch:35 [005/005 (0100/0693)]\tLoss Ss: 0.036966\n","\tRotated_Epoch:35 [005/005 (0120/0693)]\tLoss Ss: 0.022384\n","\tRotated_Epoch:35 [005/005 (0140/0693)]\tLoss Ss: 0.018709\n","\tRotated_Epoch:35 [005/005 (0160/0693)]\tLoss Ss: 0.023278\n","\tRotated_Epoch:35 [005/005 (0180/0693)]\tLoss Ss: 0.018533\n","\tRotated_Epoch:35 [005/005 (0200/0693)]\tLoss Ss: 0.022141\n","\tRotated_Epoch:35 [005/005 (0220/0693)]\tLoss Ss: 0.021386\n","\tRotated_Epoch:35 [005/005 (0240/0693)]\tLoss Ss: 0.011586\n","\tRotated_Epoch:35 [005/005 (0260/0693)]\tLoss Ss: 0.016418\n","\tRotated_Epoch:35 [005/005 (0280/0693)]\tLoss Ss: 0.022108\n","\tRotated_Epoch:35 [005/005 (0300/0693)]\tLoss Ss: 0.017931\n","\tRotated_Epoch:35 [005/005 (0320/0693)]\tLoss Ss: 0.023858\n","\tRotated_Epoch:35 [005/005 (0340/0693)]\tLoss Ss: 0.016874\n","\tRotated_Epoch:35 [005/005 (0360/0693)]\tLoss Ss: 0.013779\n","\tRotated_Epoch:35 [005/005 (0380/0693)]\tLoss Ss: 0.018662\n","\tRotated_Epoch:35 [005/005 (0400/0693)]\tLoss Ss: 0.014924\n","\tRotated_Epoch:35 [005/005 (0420/0693)]\tLoss Ss: 0.012405\n","\tRotated_Epoch:35 [005/005 (0440/0693)]\tLoss Ss: 0.016916\n","\tRotated_Epoch:35 [005/005 (0460/0693)]\tLoss Ss: 0.010846\n","\tRotated_Epoch:35 [005/005 (0480/0693)]\tLoss Ss: 0.018774\n","\tRotated_Epoch:35 [005/005 (0500/0693)]\tLoss Ss: 0.015374\n","\tRotated_Epoch:35 [005/005 (0520/0693)]\tLoss Ss: 0.010119\n","\tRotated_Epoch:35 [005/005 (0540/0693)]\tLoss Ss: 0.016565\n","\tRotated_Epoch:35 [005/005 (0560/0693)]\tLoss Ss: 0.013545\n","\tRotated_Epoch:35 [005/005 (0580/0693)]\tLoss Ss: 0.019715\n","\tRotated_Epoch:35 [005/005 (0600/0693)]\tLoss Ss: 0.015503\n","\tRotated_Epoch:35 [005/005 (0620/0693)]\tLoss Ss: 0.019802\n","\tRotated_Epoch:35 [005/005 (0640/0693)]\tLoss Ss: 0.019009\n","\tRotated_Epoch:35 [005/005 (0660/0693)]\tLoss Ss: 0.014625\n","\tRotated_Epoch:35 [005/005 (0680/0693)]\tLoss Ss: 0.015432\n","new_1_6_d.nii\n","new_1_6_m.nii\n","new_1_7_d.nii\n","new_1_7_m.nii\n","new_2_5_d.nii\n","new_2_5_m.nii\n","new_2_6_d.nii\n","new_2_6_m.nii\n","new_2_7_d.nii\n","new_2_7_m.nii\n","new_3_d (2).nii\n","new_3_m (2).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (3).nii\n","new_3_m (3).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (7).nii\n","new_3_m (7).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (8).nii\n","new_3_m (8).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (9).nii\n","new_3_m (9).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_4_10_d_163526.nii\n","new_4_10_m_163526.nii.gz\n","new_4_11_d_084336.nii\n","new_4_11_m_084336.nii.gz\n","new_4_7_d_184422.nii\n","new_4_7_m_184422.nii.gz\n","new_4_8_d_143412.nii\n","new_4_8_m_143412.nii.gz\n","new_4_9_d_155347.nii\n","new_4_9_m_155347.nii.gz\n","\tEpoch: 35; Dice: 0.9578 +/- 0.0095; Loss: 8.9594\n","Begin Epoch 36\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tEpoch:36 [000/005 (0000/0693)]\tLoss Ss: 0.014050\n","\tEpoch:36 [000/005 (0020/0693)]\tLoss Ss: 0.013424\n","\tEpoch:36 [000/005 (0040/0693)]\tLoss Ss: 0.012766\n","\tEpoch:36 [000/005 (0060/0693)]\tLoss Ss: 0.018255\n","\tEpoch:36 [000/005 (0080/0693)]\tLoss Ss: 0.012371\n","\tEpoch:36 [000/005 (0100/0693)]\tLoss Ss: 0.012227\n","\tEpoch:36 [000/005 (0120/0693)]\tLoss Ss: 0.015784\n","\tEpoch:36 [000/005 (0140/0693)]\tLoss Ss: 0.015035\n","\tEpoch:36 [000/005 (0160/0693)]\tLoss Ss: 0.010522\n","\tEpoch:36 [000/005 (0180/0693)]\tLoss Ss: 0.012159\n","\tEpoch:36 [000/005 (0200/0693)]\tLoss Ss: 0.011611\n","\tEpoch:36 [000/005 (0220/0693)]\tLoss Ss: 0.012701\n","\tEpoch:36 [000/005 (0240/0693)]\tLoss Ss: 0.016237\n","\tEpoch:36 [000/005 (0260/0693)]\tLoss Ss: 0.015903\n","\tEpoch:36 [000/005 (0280/0693)]\tLoss Ss: 0.013998\n","\tEpoch:36 [000/005 (0300/0693)]\tLoss Ss: 0.015509\n","\tEpoch:36 [000/005 (0320/0693)]\tLoss Ss: 0.016236\n","\tEpoch:36 [000/005 (0340/0693)]\tLoss Ss: 0.015887\n","\tEpoch:36 [000/005 (0360/0693)]\tLoss Ss: 0.023821\n","\tEpoch:36 [000/005 (0380/0693)]\tLoss Ss: 0.008674\n","\tEpoch:36 [000/005 (0400/0693)]\tLoss Ss: 0.013062\n","\tEpoch:36 [000/005 (0420/0693)]\tLoss Ss: 0.011254\n","\tEpoch:36 [000/005 (0440/0693)]\tLoss Ss: 0.011444\n","\tEpoch:36 [000/005 (0460/0693)]\tLoss Ss: 0.013976\n","\tEpoch:36 [000/005 (0480/0693)]\tLoss Ss: 0.011200\n","\tEpoch:36 [000/005 (0500/0693)]\tLoss Ss: 0.011145\n","\tEpoch:36 [000/005 (0520/0693)]\tLoss Ss: 0.014646\n","\tEpoch:36 [000/005 (0540/0693)]\tLoss Ss: 0.014000\n","\tEpoch:36 [000/005 (0560/0693)]\tLoss Ss: 0.014119\n","\tEpoch:36 [000/005 (0580/0693)]\tLoss Ss: 0.009006\n","\tEpoch:36 [000/005 (0600/0693)]\tLoss Ss: 0.012536\n","\tEpoch:36 [000/005 (0620/0693)]\tLoss Ss: 0.014610\n","\tEpoch:36 [000/005 (0640/0693)]\tLoss Ss: 0.016917\n","\tEpoch:36 [000/005 (0660/0693)]\tLoss Ss: 0.015344\n","\tEpoch:36 [000/005 (0680/0693)]\tLoss Ss: 0.015485\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tEpoch:36 [001/005 (0000/0614)]\tLoss Ss: 0.007090\n","\tEpoch:36 [001/005 (0020/0614)]\tLoss Ss: 0.008121\n","\tEpoch:36 [001/005 (0040/0614)]\tLoss Ss: 0.004323\n","\tEpoch:36 [001/005 (0060/0614)]\tLoss Ss: 0.009733\n","\tEpoch:36 [001/005 (0080/0614)]\tLoss Ss: 0.008554\n","\tEpoch:36 [001/005 (0100/0614)]\tLoss Ss: 0.009839\n","\tEpoch:36 [001/005 (0120/0614)]\tLoss Ss: 0.005153\n","\tEpoch:36 [001/005 (0140/0614)]\tLoss Ss: 0.004341\n","\tEpoch:36 [001/005 (0160/0614)]\tLoss Ss: 0.005556\n","\tEpoch:36 [001/005 (0180/0614)]\tLoss Ss: 0.004178\n","\tEpoch:36 [001/005 (0200/0614)]\tLoss Ss: 0.006408\n","\tEpoch:36 [001/005 (0220/0614)]\tLoss Ss: 0.007385\n","\tEpoch:36 [001/005 (0240/0614)]\tLoss Ss: 0.007160\n","\tEpoch:36 [001/005 (0260/0614)]\tLoss Ss: 0.004571\n","\tEpoch:36 [001/005 (0280/0614)]\tLoss Ss: 0.006860\n","\tEpoch:36 [001/005 (0300/0614)]\tLoss Ss: 0.006934\n","\tEpoch:36 [001/005 (0320/0614)]\tLoss Ss: 0.005170\n","\tEpoch:36 [001/005 (0340/0614)]\tLoss Ss: 0.004370\n","\tEpoch:36 [001/005 (0360/0614)]\tLoss Ss: 0.006133\n","\tEpoch:36 [001/005 (0380/0614)]\tLoss Ss: 0.008829\n","\tEpoch:36 [001/005 (0400/0614)]\tLoss Ss: 0.007900\n","\tEpoch:36 [001/005 (0420/0614)]\tLoss Ss: 0.005239\n","\tEpoch:36 [001/005 (0440/0614)]\tLoss Ss: 0.002476\n","\tEpoch:36 [001/005 (0460/0614)]\tLoss Ss: 0.006838\n","\tEpoch:36 [001/005 (0480/0614)]\tLoss Ss: 0.005758\n","\tEpoch:36 [001/005 (0500/0614)]\tLoss Ss: 0.006004\n","\tEpoch:36 [001/005 (0520/0614)]\tLoss Ss: 0.005006\n","\tEpoch:36 [001/005 (0540/0614)]\tLoss Ss: 0.004796\n","\tEpoch:36 [001/005 (0560/0614)]\tLoss Ss: 0.006592\n","\tEpoch:36 [001/005 (0580/0614)]\tLoss Ss: 0.005520\n","\tEpoch:36 [001/005 (0600/0614)]\tLoss Ss: 0.005003\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tEpoch:36 [002/005 (0000/0588)]\tLoss Ss: 0.029356\n","\tEpoch:36 [002/005 (0020/0588)]\tLoss Ss: 0.019892\n","\tEpoch:36 [002/005 (0040/0588)]\tLoss Ss: 0.009398\n","\tEpoch:36 [002/005 (0060/0588)]\tLoss Ss: 0.005077\n","\tEpoch:36 [002/005 (0080/0588)]\tLoss Ss: 0.004218\n","\tEpoch:36 [002/005 (0100/0588)]\tLoss Ss: 0.004726\n","\tEpoch:36 [002/005 (0120/0588)]\tLoss Ss: 0.004400\n","\tEpoch:36 [002/005 (0140/0588)]\tLoss Ss: 0.004701\n","\tEpoch:36 [002/005 (0160/0588)]\tLoss Ss: 0.006850\n","\tEpoch:36 [002/005 (0180/0588)]\tLoss Ss: 0.005231\n","\tEpoch:36 [002/005 (0200/0588)]\tLoss Ss: 0.004396\n","\tEpoch:36 [002/005 (0220/0588)]\tLoss Ss: 0.006303\n","\tEpoch:36 [002/005 (0240/0588)]\tLoss Ss: 0.008270\n","\tEpoch:36 [002/005 (0260/0588)]\tLoss Ss: 0.004213\n","\tEpoch:36 [002/005 (0280/0588)]\tLoss Ss: 0.007314\n","\tEpoch:36 [002/005 (0300/0588)]\tLoss Ss: 0.007586\n","\tEpoch:36 [002/005 (0320/0588)]\tLoss Ss: 0.003075\n","\tEpoch:36 [002/005 (0340/0588)]\tLoss Ss: 0.004860\n","\tEpoch:36 [002/005 (0360/0588)]\tLoss Ss: 0.004314\n","\tEpoch:36 [002/005 (0380/0588)]\tLoss Ss: 0.006124\n","\tEpoch:36 [002/005 (0400/0588)]\tLoss Ss: 0.006353\n","\tEpoch:36 [002/005 (0420/0588)]\tLoss Ss: 0.004315\n","\tEpoch:36 [002/005 (0440/0588)]\tLoss Ss: 0.003894\n","\tEpoch:36 [002/005 (0460/0588)]\tLoss Ss: 0.004607\n","\tEpoch:36 [002/005 (0480/0588)]\tLoss Ss: 0.002893\n","\tEpoch:36 [002/005 (0500/0588)]\tLoss Ss: 0.003248\n","\tEpoch:36 [002/005 (0520/0588)]\tLoss Ss: 0.004987\n","\tEpoch:36 [002/005 (0540/0588)]\tLoss Ss: 0.003833\n","\tEpoch:36 [002/005 (0560/0588)]\tLoss Ss: 0.004602\n","\tEpoch:36 [002/005 (0580/0588)]\tLoss Ss: 0.005713\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:36 [003/005 (0000/0755)]\tLoss Ss: 0.015351\n","\tEpoch:36 [003/005 (0020/0755)]\tLoss Ss: 0.016181\n","\tEpoch:36 [003/005 (0040/0755)]\tLoss Ss: 0.022562\n","\tEpoch:36 [003/005 (0060/0755)]\tLoss Ss: 0.021383\n","\tEpoch:36 [003/005 (0080/0755)]\tLoss Ss: 0.014591\n","\tEpoch:36 [003/005 (0100/0755)]\tLoss Ss: 0.015845\n","\tEpoch:36 [003/005 (0120/0755)]\tLoss Ss: 0.017586\n","\tEpoch:36 [003/005 (0140/0755)]\tLoss Ss: 0.018236\n","\tEpoch:36 [003/005 (0160/0755)]\tLoss Ss: 0.013265\n","\tEpoch:36 [003/005 (0180/0755)]\tLoss Ss: 0.008900\n","\tEpoch:36 [003/005 (0200/0755)]\tLoss Ss: 0.019914\n","\tEpoch:36 [003/005 (0220/0755)]\tLoss Ss: 0.017322\n","\tEpoch:36 [003/005 (0240/0755)]\tLoss Ss: 0.015465\n","\tEpoch:36 [003/005 (0260/0755)]\tLoss Ss: 0.015135\n","\tEpoch:36 [003/005 (0280/0755)]\tLoss Ss: 0.014121\n","\tEpoch:36 [003/005 (0300/0755)]\tLoss Ss: 0.015397\n","\tEpoch:36 [003/005 (0320/0755)]\tLoss Ss: 0.015046\n","\tEpoch:36 [003/005 (0340/0755)]\tLoss Ss: 0.012796\n","\tEpoch:36 [003/005 (0360/0755)]\tLoss Ss: 0.009294\n","\tEpoch:36 [003/005 (0380/0755)]\tLoss Ss: 0.020038\n","\tEpoch:36 [003/005 (0400/0755)]\tLoss Ss: 0.013179\n","\tEpoch:36 [003/005 (0420/0755)]\tLoss Ss: 0.008969\n","\tEpoch:36 [003/005 (0440/0755)]\tLoss Ss: 0.007830\n","\tEpoch:36 [003/005 (0460/0755)]\tLoss Ss: 0.014746\n","\tEpoch:36 [003/005 (0480/0755)]\tLoss Ss: 0.017278\n","\tEpoch:36 [003/005 (0500/0755)]\tLoss Ss: 0.007851\n","\tEpoch:36 [003/005 (0520/0755)]\tLoss Ss: 0.014666\n","\tEpoch:36 [003/005 (0540/0755)]\tLoss Ss: 0.010764\n","\tEpoch:36 [003/005 (0560/0755)]\tLoss Ss: 0.015367\n","\tEpoch:36 [003/005 (0580/0755)]\tLoss Ss: 0.013162\n","\tEpoch:36 [003/005 (0600/0755)]\tLoss Ss: 0.014753\n","\tEpoch:36 [003/005 (0620/0755)]\tLoss Ss: 0.010706\n","\tEpoch:36 [003/005 (0640/0755)]\tLoss Ss: 0.011218\n","\tEpoch:36 [003/005 (0660/0755)]\tLoss Ss: 0.009826\n","\tEpoch:36 [003/005 (0680/0755)]\tLoss Ss: 0.016778\n","\tEpoch:36 [003/005 (0700/0755)]\tLoss Ss: 0.011129\n","\tEpoch:36 [003/005 (0720/0755)]\tLoss Ss: 0.012376\n","\tEpoch:36 [003/005 (0740/0755)]\tLoss Ss: 0.013324\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:36 [004/005 (0000/0755)]\tLoss Ss: 0.024449\n","\tEpoch:36 [004/005 (0020/0755)]\tLoss Ss: 0.023022\n","\tEpoch:36 [004/005 (0040/0755)]\tLoss Ss: 0.018796\n","\tEpoch:36 [004/005 (0060/0755)]\tLoss Ss: 0.015394\n","\tEpoch:36 [004/005 (0080/0755)]\tLoss Ss: 0.021671\n","\tEpoch:36 [004/005 (0100/0755)]\tLoss Ss: 0.011180\n","\tEpoch:36 [004/005 (0120/0755)]\tLoss Ss: 0.016320\n","\tEpoch:36 [004/005 (0140/0755)]\tLoss Ss: 0.013756\n","\tEpoch:36 [004/005 (0160/0755)]\tLoss Ss: 0.012982\n","\tEpoch:36 [004/005 (0180/0755)]\tLoss Ss: 0.011434\n","\tEpoch:36 [004/005 (0200/0755)]\tLoss Ss: 0.018315\n","\tEpoch:36 [004/005 (0220/0755)]\tLoss Ss: 0.011411\n","\tEpoch:36 [004/005 (0240/0755)]\tLoss Ss: 0.014139\n","\tEpoch:36 [004/005 (0260/0755)]\tLoss Ss: 0.015902\n","\tEpoch:36 [004/005 (0280/0755)]\tLoss Ss: 0.013422\n","\tEpoch:36 [004/005 (0300/0755)]\tLoss Ss: 0.015817\n","\tEpoch:36 [004/005 (0320/0755)]\tLoss Ss: 0.015037\n","\tEpoch:36 [004/005 (0340/0755)]\tLoss Ss: 0.018621\n","\tEpoch:36 [004/005 (0360/0755)]\tLoss Ss: 0.016987\n","\tEpoch:36 [004/005 (0380/0755)]\tLoss Ss: 0.013470\n","\tEpoch:36 [004/005 (0400/0755)]\tLoss Ss: 0.010893\n","\tEpoch:36 [004/005 (0420/0755)]\tLoss Ss: 0.013027\n","\tEpoch:36 [004/005 (0440/0755)]\tLoss Ss: 0.021859\n","\tEpoch:36 [004/005 (0460/0755)]\tLoss Ss: 0.013051\n","\tEpoch:36 [004/005 (0480/0755)]\tLoss Ss: 0.019999\n","\tEpoch:36 [004/005 (0500/0755)]\tLoss Ss: 0.016628\n","\tEpoch:36 [004/005 (0520/0755)]\tLoss Ss: 0.011559\n","\tEpoch:36 [004/005 (0540/0755)]\tLoss Ss: 0.012437\n","\tEpoch:36 [004/005 (0560/0755)]\tLoss Ss: 0.012288\n","\tEpoch:36 [004/005 (0580/0755)]\tLoss Ss: 0.013120\n","\tEpoch:36 [004/005 (0600/0755)]\tLoss Ss: 0.008949\n","\tEpoch:36 [004/005 (0620/0755)]\tLoss Ss: 0.012764\n","\tEpoch:36 [004/005 (0640/0755)]\tLoss Ss: 0.012251\n","\tEpoch:36 [004/005 (0660/0755)]\tLoss Ss: 0.012009\n","\tEpoch:36 [004/005 (0680/0755)]\tLoss Ss: 0.015326\n","\tEpoch:36 [004/005 (0700/0755)]\tLoss Ss: 0.008807\n","\tEpoch:36 [004/005 (0720/0755)]\tLoss Ss: 0.011298\n","\tEpoch:36 [004/005 (0740/0755)]\tLoss Ss: 0.018573\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tEpoch:36 [005/005 (0000/0693)]\tLoss Ss: 0.018918\n","\tEpoch:36 [005/005 (0020/0693)]\tLoss Ss: 0.023827\n","\tEpoch:36 [005/005 (0040/0693)]\tLoss Ss: 0.014574\n","\tEpoch:36 [005/005 (0060/0693)]\tLoss Ss: 0.020331\n","\tEpoch:36 [005/005 (0080/0693)]\tLoss Ss: 0.014757\n","\tEpoch:36 [005/005 (0100/0693)]\tLoss Ss: 0.012294\n","\tEpoch:36 [005/005 (0120/0693)]\tLoss Ss: 0.011596\n","\tEpoch:36 [005/005 (0140/0693)]\tLoss Ss: 0.012577\n","\tEpoch:36 [005/005 (0160/0693)]\tLoss Ss: 0.016045\n","\tEpoch:36 [005/005 (0180/0693)]\tLoss Ss: 0.008783\n","\tEpoch:36 [005/005 (0200/0693)]\tLoss Ss: 0.013362\n","\tEpoch:36 [005/005 (0220/0693)]\tLoss Ss: 0.012090\n","\tEpoch:36 [005/005 (0240/0693)]\tLoss Ss: 0.010452\n","\tEpoch:36 [005/005 (0260/0693)]\tLoss Ss: 0.014011\n","\tEpoch:36 [005/005 (0280/0693)]\tLoss Ss: 0.012282\n","\tEpoch:36 [005/005 (0300/0693)]\tLoss Ss: 0.011956\n","\tEpoch:36 [005/005 (0320/0693)]\tLoss Ss: 0.013272\n","\tEpoch:36 [005/005 (0340/0693)]\tLoss Ss: 0.011411\n","\tEpoch:36 [005/005 (0360/0693)]\tLoss Ss: 0.012986\n","\tEpoch:36 [005/005 (0380/0693)]\tLoss Ss: 0.011627\n","\tEpoch:36 [005/005 (0400/0693)]\tLoss Ss: 0.012633\n","\tEpoch:36 [005/005 (0420/0693)]\tLoss Ss: 0.013149\n","\tEpoch:36 [005/005 (0440/0693)]\tLoss Ss: 0.010724\n","\tEpoch:36 [005/005 (0460/0693)]\tLoss Ss: 0.008922\n","\tEpoch:36 [005/005 (0480/0693)]\tLoss Ss: 0.007340\n","\tEpoch:36 [005/005 (0500/0693)]\tLoss Ss: 0.010657\n","\tEpoch:36 [005/005 (0520/0693)]\tLoss Ss: 0.011345\n","\tEpoch:36 [005/005 (0540/0693)]\tLoss Ss: 0.011677\n","\tEpoch:36 [005/005 (0560/0693)]\tLoss Ss: 0.013531\n","\tEpoch:36 [005/005 (0580/0693)]\tLoss Ss: 0.010142\n","\tEpoch:36 [005/005 (0600/0693)]\tLoss Ss: 0.013723\n","\tEpoch:36 [005/005 (0620/0693)]\tLoss Ss: 0.008450\n","\tEpoch:36 [005/005 (0640/0693)]\tLoss Ss: 0.014336\n","\tEpoch:36 [005/005 (0660/0693)]\tLoss Ss: 0.009837\n","\tEpoch:36 [005/005 (0680/0693)]\tLoss Ss: 0.009236\n","Now train the rotated image\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tRotated_Epoch:36 [000/005 (0000/0693)]\tLoss Ss: 0.010783\n","\tRotated_Epoch:36 [000/005 (0020/0693)]\tLoss Ss: 0.016945\n","\tRotated_Epoch:36 [000/005 (0040/0693)]\tLoss Ss: 0.016714\n","\tRotated_Epoch:36 [000/005 (0060/0693)]\tLoss Ss: 0.013914\n","\tRotated_Epoch:36 [000/005 (0080/0693)]\tLoss Ss: 0.018724\n","\tRotated_Epoch:36 [000/005 (0100/0693)]\tLoss Ss: 0.022275\n","\tRotated_Epoch:36 [000/005 (0120/0693)]\tLoss Ss: 0.018186\n","\tRotated_Epoch:36 [000/005 (0140/0693)]\tLoss Ss: 0.013852\n","\tRotated_Epoch:36 [000/005 (0160/0693)]\tLoss Ss: 0.011005\n","\tRotated_Epoch:36 [000/005 (0180/0693)]\tLoss Ss: 0.014024\n","\tRotated_Epoch:36 [000/005 (0200/0693)]\tLoss Ss: 0.014282\n","\tRotated_Epoch:36 [000/005 (0220/0693)]\tLoss Ss: 0.018163\n","\tRotated_Epoch:36 [000/005 (0240/0693)]\tLoss Ss: 0.014010\n","\tRotated_Epoch:36 [000/005 (0260/0693)]\tLoss Ss: 0.016778\n","\tRotated_Epoch:36 [000/005 (0280/0693)]\tLoss Ss: 0.018117\n","\tRotated_Epoch:36 [000/005 (0300/0693)]\tLoss Ss: 0.013589\n","\tRotated_Epoch:36 [000/005 (0320/0693)]\tLoss Ss: 0.008111\n","\tRotated_Epoch:36 [000/005 (0340/0693)]\tLoss Ss: 0.016138\n","\tRotated_Epoch:36 [000/005 (0360/0693)]\tLoss Ss: 0.015077\n","\tRotated_Epoch:36 [000/005 (0380/0693)]\tLoss Ss: 0.014340\n","\tRotated_Epoch:36 [000/005 (0400/0693)]\tLoss Ss: 0.013945\n","\tRotated_Epoch:36 [000/005 (0420/0693)]\tLoss Ss: 0.012746\n","\tRotated_Epoch:36 [000/005 (0440/0693)]\tLoss Ss: 0.014744\n","\tRotated_Epoch:36 [000/005 (0460/0693)]\tLoss Ss: 0.014047\n","\tRotated_Epoch:36 [000/005 (0480/0693)]\tLoss Ss: 0.010999\n","\tRotated_Epoch:36 [000/005 (0500/0693)]\tLoss Ss: 0.011818\n","\tRotated_Epoch:36 [000/005 (0520/0693)]\tLoss Ss: 0.013060\n","\tRotated_Epoch:36 [000/005 (0540/0693)]\tLoss Ss: 0.014447\n","\tRotated_Epoch:36 [000/005 (0560/0693)]\tLoss Ss: 0.022188\n","\tRotated_Epoch:36 [000/005 (0580/0693)]\tLoss Ss: 0.013749\n","\tRotated_Epoch:36 [000/005 (0600/0693)]\tLoss Ss: 0.011255\n","\tRotated_Epoch:36 [000/005 (0620/0693)]\tLoss Ss: 0.015927\n","\tRotated_Epoch:36 [000/005 (0640/0693)]\tLoss Ss: 0.010386\n","\tRotated_Epoch:36 [000/005 (0660/0693)]\tLoss Ss: 0.012389\n","\tRotated_Epoch:36 [000/005 (0680/0693)]\tLoss Ss: 0.020026\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:36 [001/005 (0000/0755)]\tLoss Ss: 0.023546\n","\tRotated_Epoch:36 [001/005 (0020/0755)]\tLoss Ss: 0.046133\n","\tRotated_Epoch:36 [001/005 (0040/0755)]\tLoss Ss: 0.038490\n","\tRotated_Epoch:36 [001/005 (0060/0755)]\tLoss Ss: 0.021263\n","\tRotated_Epoch:36 [001/005 (0080/0755)]\tLoss Ss: 0.034100\n","\tRotated_Epoch:36 [001/005 (0100/0755)]\tLoss Ss: 0.023104\n","\tRotated_Epoch:36 [001/005 (0120/0755)]\tLoss Ss: 0.017124\n","\tRotated_Epoch:36 [001/005 (0140/0755)]\tLoss Ss: 0.019111\n","\tRotated_Epoch:36 [001/005 (0160/0755)]\tLoss Ss: 0.035854\n","\tRotated_Epoch:36 [001/005 (0180/0755)]\tLoss Ss: 0.022951\n","\tRotated_Epoch:36 [001/005 (0200/0755)]\tLoss Ss: 0.022141\n","\tRotated_Epoch:36 [001/005 (0220/0755)]\tLoss Ss: 0.015362\n","\tRotated_Epoch:36 [001/005 (0240/0755)]\tLoss Ss: 0.013486\n","\tRotated_Epoch:36 [001/005 (0260/0755)]\tLoss Ss: 0.016481\n","\tRotated_Epoch:36 [001/005 (0280/0755)]\tLoss Ss: 0.014298\n","\tRotated_Epoch:36 [001/005 (0300/0755)]\tLoss Ss: 0.020319\n","\tRotated_Epoch:36 [001/005 (0320/0755)]\tLoss Ss: 0.018250\n","\tRotated_Epoch:36 [001/005 (0340/0755)]\tLoss Ss: 0.020137\n","\tRotated_Epoch:36 [001/005 (0360/0755)]\tLoss Ss: 0.019427\n","\tRotated_Epoch:36 [001/005 (0380/0755)]\tLoss Ss: 0.013428\n","\tRotated_Epoch:36 [001/005 (0400/0755)]\tLoss Ss: 0.016827\n","\tRotated_Epoch:36 [001/005 (0420/0755)]\tLoss Ss: 0.011780\n","\tRotated_Epoch:36 [001/005 (0440/0755)]\tLoss Ss: 0.009430\n","\tRotated_Epoch:36 [001/005 (0460/0755)]\tLoss Ss: 0.019138\n","\tRotated_Epoch:36 [001/005 (0480/0755)]\tLoss Ss: 0.013518\n","\tRotated_Epoch:36 [001/005 (0500/0755)]\tLoss Ss: 0.015839\n","\tRotated_Epoch:36 [001/005 (0520/0755)]\tLoss Ss: 0.016746\n","\tRotated_Epoch:36 [001/005 (0540/0755)]\tLoss Ss: 0.011920\n","\tRotated_Epoch:36 [001/005 (0560/0755)]\tLoss Ss: 0.014236\n","\tRotated_Epoch:36 [001/005 (0580/0755)]\tLoss Ss: 0.012594\n","\tRotated_Epoch:36 [001/005 (0600/0755)]\tLoss Ss: 0.007837\n","\tRotated_Epoch:36 [001/005 (0620/0755)]\tLoss Ss: 0.018011\n","\tRotated_Epoch:36 [001/005 (0640/0755)]\tLoss Ss: 0.011622\n","\tRotated_Epoch:36 [001/005 (0660/0755)]\tLoss Ss: 0.015741\n","\tRotated_Epoch:36 [001/005 (0680/0755)]\tLoss Ss: 0.013443\n","\tRotated_Epoch:36 [001/005 (0700/0755)]\tLoss Ss: 0.016188\n","\tRotated_Epoch:36 [001/005 (0720/0755)]\tLoss Ss: 0.012860\n","\tRotated_Epoch:36 [001/005 (0740/0755)]\tLoss Ss: 0.014190\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:36 [002/005 (0000/0755)]\tLoss Ss: 0.353477\n","\tRotated_Epoch:36 [002/005 (0020/0755)]\tLoss Ss: 0.281212\n","\tRotated_Epoch:36 [002/005 (0040/0755)]\tLoss Ss: 0.250853\n","\tRotated_Epoch:36 [002/005 (0060/0755)]\tLoss Ss: 0.144106\n","\tRotated_Epoch:36 [002/005 (0080/0755)]\tLoss Ss: 0.083089\n","\tRotated_Epoch:36 [002/005 (0100/0755)]\tLoss Ss: 0.081776\n","\tRotated_Epoch:36 [002/005 (0120/0755)]\tLoss Ss: 0.098283\n","\tRotated_Epoch:36 [002/005 (0140/0755)]\tLoss Ss: 0.091561\n","\tRotated_Epoch:36 [002/005 (0160/0755)]\tLoss Ss: 0.079646\n","\tRotated_Epoch:36 [002/005 (0180/0755)]\tLoss Ss: 0.060728\n","\tRotated_Epoch:36 [002/005 (0200/0755)]\tLoss Ss: 0.050578\n","\tRotated_Epoch:36 [002/005 (0220/0755)]\tLoss Ss: 0.042174\n","\tRotated_Epoch:36 [002/005 (0240/0755)]\tLoss Ss: 0.059175\n","\tRotated_Epoch:36 [002/005 (0260/0755)]\tLoss Ss: 0.046040\n","\tRotated_Epoch:36 [002/005 (0280/0755)]\tLoss Ss: 0.046025\n","\tRotated_Epoch:36 [002/005 (0300/0755)]\tLoss Ss: 0.050919\n","\tRotated_Epoch:36 [002/005 (0320/0755)]\tLoss Ss: 0.033780\n","\tRotated_Epoch:36 [002/005 (0340/0755)]\tLoss Ss: 0.049335\n","\tRotated_Epoch:36 [002/005 (0360/0755)]\tLoss Ss: 0.042573\n","\tRotated_Epoch:36 [002/005 (0380/0755)]\tLoss Ss: 0.043517\n","\tRotated_Epoch:36 [002/005 (0400/0755)]\tLoss Ss: 0.043736\n","\tRotated_Epoch:36 [002/005 (0420/0755)]\tLoss Ss: 0.039578\n","\tRotated_Epoch:36 [002/005 (0440/0755)]\tLoss Ss: 0.045505\n","\tRotated_Epoch:36 [002/005 (0460/0755)]\tLoss Ss: 0.040070\n","\tRotated_Epoch:36 [002/005 (0480/0755)]\tLoss Ss: 0.038286\n","\tRotated_Epoch:36 [002/005 (0500/0755)]\tLoss Ss: 0.041187\n","\tRotated_Epoch:36 [002/005 (0520/0755)]\tLoss Ss: 0.037835\n","\tRotated_Epoch:36 [002/005 (0540/0755)]\tLoss Ss: 0.027247\n","\tRotated_Epoch:36 [002/005 (0560/0755)]\tLoss Ss: 0.044647\n","\tRotated_Epoch:36 [002/005 (0580/0755)]\tLoss Ss: 0.032639\n","\tRotated_Epoch:36 [002/005 (0600/0755)]\tLoss Ss: 0.031398\n","\tRotated_Epoch:36 [002/005 (0620/0755)]\tLoss Ss: 0.037940\n","\tRotated_Epoch:36 [002/005 (0640/0755)]\tLoss Ss: 0.031629\n","\tRotated_Epoch:36 [002/005 (0660/0755)]\tLoss Ss: 0.029071\n","\tRotated_Epoch:36 [002/005 (0680/0755)]\tLoss Ss: 0.027493\n","\tRotated_Epoch:36 [002/005 (0700/0755)]\tLoss Ss: 0.034775\n","\tRotated_Epoch:36 [002/005 (0720/0755)]\tLoss Ss: 0.033461\n","\tRotated_Epoch:36 [002/005 (0740/0755)]\tLoss Ss: 0.038570\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tRotated_Epoch:36 [003/005 (0000/0614)]\tLoss Ss: 0.140861\n","\tRotated_Epoch:36 [003/005 (0020/0614)]\tLoss Ss: 0.057758\n","\tRotated_Epoch:36 [003/005 (0040/0614)]\tLoss Ss: 0.063178\n","\tRotated_Epoch:36 [003/005 (0060/0614)]\tLoss Ss: 0.028870\n","\tRotated_Epoch:36 [003/005 (0080/0614)]\tLoss Ss: 0.022147\n","\tRotated_Epoch:36 [003/005 (0100/0614)]\tLoss Ss: 0.014675\n","\tRotated_Epoch:36 [003/005 (0120/0614)]\tLoss Ss: 0.013304\n","\tRotated_Epoch:36 [003/005 (0140/0614)]\tLoss Ss: 0.015110\n","\tRotated_Epoch:36 [003/005 (0160/0614)]\tLoss Ss: 0.014684\n","\tRotated_Epoch:36 [003/005 (0180/0614)]\tLoss Ss: 0.021728\n","\tRotated_Epoch:36 [003/005 (0200/0614)]\tLoss Ss: 0.011858\n","\tRotated_Epoch:36 [003/005 (0220/0614)]\tLoss Ss: 0.015153\n","\tRotated_Epoch:36 [003/005 (0240/0614)]\tLoss Ss: 0.016933\n","\tRotated_Epoch:36 [003/005 (0260/0614)]\tLoss Ss: 0.012460\n","\tRotated_Epoch:36 [003/005 (0280/0614)]\tLoss Ss: 0.015255\n","\tRotated_Epoch:36 [003/005 (0300/0614)]\tLoss Ss: 0.022594\n","\tRotated_Epoch:36 [003/005 (0320/0614)]\tLoss Ss: 0.009226\n","\tRotated_Epoch:36 [003/005 (0340/0614)]\tLoss Ss: 0.013566\n","\tRotated_Epoch:36 [003/005 (0360/0614)]\tLoss Ss: 0.026185\n","\tRotated_Epoch:36 [003/005 (0380/0614)]\tLoss Ss: 0.009212\n","\tRotated_Epoch:36 [003/005 (0400/0614)]\tLoss Ss: 0.010558\n","\tRotated_Epoch:36 [003/005 (0420/0614)]\tLoss Ss: 0.013440\n","\tRotated_Epoch:36 [003/005 (0440/0614)]\tLoss Ss: 0.007308\n","\tRotated_Epoch:36 [003/005 (0460/0614)]\tLoss Ss: 0.011785\n","\tRotated_Epoch:36 [003/005 (0480/0614)]\tLoss Ss: 0.011916\n","\tRotated_Epoch:36 [003/005 (0500/0614)]\tLoss Ss: 0.007345\n","\tRotated_Epoch:36 [003/005 (0520/0614)]\tLoss Ss: 0.008556\n","\tRotated_Epoch:36 [003/005 (0540/0614)]\tLoss Ss: 0.012540\n","\tRotated_Epoch:36 [003/005 (0560/0614)]\tLoss Ss: 0.011592\n","\tRotated_Epoch:36 [003/005 (0580/0614)]\tLoss Ss: 0.006894\n","\tRotated_Epoch:36 [003/005 (0600/0614)]\tLoss Ss: 0.011467\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tRotated_Epoch:36 [004/005 (0000/0693)]\tLoss Ss: 0.030857\n","\tRotated_Epoch:36 [004/005 (0020/0693)]\tLoss Ss: 0.034615\n","\tRotated_Epoch:36 [004/005 (0040/0693)]\tLoss Ss: 0.045723\n","\tRotated_Epoch:36 [004/005 (0060/0693)]\tLoss Ss: 0.015232\n","\tRotated_Epoch:36 [004/005 (0080/0693)]\tLoss Ss: 0.025923\n","\tRotated_Epoch:36 [004/005 (0100/0693)]\tLoss Ss: 0.019337\n","\tRotated_Epoch:36 [004/005 (0120/0693)]\tLoss Ss: 0.015960\n","\tRotated_Epoch:36 [004/005 (0140/0693)]\tLoss Ss: 0.028959\n","\tRotated_Epoch:36 [004/005 (0160/0693)]\tLoss Ss: 0.010517\n","\tRotated_Epoch:36 [004/005 (0180/0693)]\tLoss Ss: 0.015875\n","\tRotated_Epoch:36 [004/005 (0200/0693)]\tLoss Ss: 0.013163\n","\tRotated_Epoch:36 [004/005 (0220/0693)]\tLoss Ss: 0.010062\n","\tRotated_Epoch:36 [004/005 (0240/0693)]\tLoss Ss: 0.019319\n","\tRotated_Epoch:36 [004/005 (0260/0693)]\tLoss Ss: 0.018042\n","\tRotated_Epoch:36 [004/005 (0280/0693)]\tLoss Ss: 0.010459\n","\tRotated_Epoch:36 [004/005 (0300/0693)]\tLoss Ss: 0.021276\n","\tRotated_Epoch:36 [004/005 (0320/0693)]\tLoss Ss: 0.014585\n","\tRotated_Epoch:36 [004/005 (0340/0693)]\tLoss Ss: 0.015156\n","\tRotated_Epoch:36 [004/005 (0360/0693)]\tLoss Ss: 0.012567\n","\tRotated_Epoch:36 [004/005 (0380/0693)]\tLoss Ss: 0.017708\n","\tRotated_Epoch:36 [004/005 (0400/0693)]\tLoss Ss: 0.013879\n","\tRotated_Epoch:36 [004/005 (0420/0693)]\tLoss Ss: 0.016794\n","\tRotated_Epoch:36 [004/005 (0440/0693)]\tLoss Ss: 0.015975\n","\tRotated_Epoch:36 [004/005 (0460/0693)]\tLoss Ss: 0.014262\n","\tRotated_Epoch:36 [004/005 (0480/0693)]\tLoss Ss: 0.013949\n","\tRotated_Epoch:36 [004/005 (0500/0693)]\tLoss Ss: 0.014071\n","\tRotated_Epoch:36 [004/005 (0520/0693)]\tLoss Ss: 0.014354\n","\tRotated_Epoch:36 [004/005 (0540/0693)]\tLoss Ss: 0.011596\n","\tRotated_Epoch:36 [004/005 (0560/0693)]\tLoss Ss: 0.013330\n","\tRotated_Epoch:36 [004/005 (0580/0693)]\tLoss Ss: 0.013408\n","\tRotated_Epoch:36 [004/005 (0600/0693)]\tLoss Ss: 0.012428\n","\tRotated_Epoch:36 [004/005 (0620/0693)]\tLoss Ss: 0.016583\n","\tRotated_Epoch:36 [004/005 (0640/0693)]\tLoss Ss: 0.012721\n","\tRotated_Epoch:36 [004/005 (0660/0693)]\tLoss Ss: 0.011021\n","\tRotated_Epoch:36 [004/005 (0680/0693)]\tLoss Ss: 0.018890\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tRotated_Epoch:36 [005/005 (0000/0588)]\tLoss Ss: 0.103706\n","\tRotated_Epoch:36 [005/005 (0020/0588)]\tLoss Ss: 0.134169\n","\tRotated_Epoch:36 [005/005 (0040/0588)]\tLoss Ss: 0.060260\n","\tRotated_Epoch:36 [005/005 (0060/0588)]\tLoss Ss: 0.098084\n","\tRotated_Epoch:36 [005/005 (0080/0588)]\tLoss Ss: 0.056901\n","\tRotated_Epoch:36 [005/005 (0100/0588)]\tLoss Ss: 0.082639\n","\tRotated_Epoch:36 [005/005 (0120/0588)]\tLoss Ss: 0.067399\n","\tRotated_Epoch:36 [005/005 (0140/0588)]\tLoss Ss: 0.055908\n","\tRotated_Epoch:36 [005/005 (0160/0588)]\tLoss Ss: 0.078250\n","\tRotated_Epoch:36 [005/005 (0180/0588)]\tLoss Ss: 0.068519\n","\tRotated_Epoch:36 [005/005 (0200/0588)]\tLoss Ss: 0.081002\n","\tRotated_Epoch:36 [005/005 (0220/0588)]\tLoss Ss: 0.066983\n","\tRotated_Epoch:36 [005/005 (0240/0588)]\tLoss Ss: 0.061538\n","\tRotated_Epoch:36 [005/005 (0260/0588)]\tLoss Ss: 0.050382\n","\tRotated_Epoch:36 [005/005 (0280/0588)]\tLoss Ss: 0.079670\n","\tRotated_Epoch:36 [005/005 (0300/0588)]\tLoss Ss: 0.053474\n","\tRotated_Epoch:36 [005/005 (0320/0588)]\tLoss Ss: 0.063996\n","\tRotated_Epoch:36 [005/005 (0340/0588)]\tLoss Ss: 0.042462\n","\tRotated_Epoch:36 [005/005 (0360/0588)]\tLoss Ss: 0.054067\n","\tRotated_Epoch:36 [005/005 (0380/0588)]\tLoss Ss: 0.049066\n","\tRotated_Epoch:36 [005/005 (0400/0588)]\tLoss Ss: 0.059314\n","\tRotated_Epoch:36 [005/005 (0420/0588)]\tLoss Ss: 0.058349\n","\tRotated_Epoch:36 [005/005 (0440/0588)]\tLoss Ss: 0.042491\n","\tRotated_Epoch:36 [005/005 (0460/0588)]\tLoss Ss: 0.054397\n","\tRotated_Epoch:36 [005/005 (0480/0588)]\tLoss Ss: 0.040425\n","\tRotated_Epoch:36 [005/005 (0500/0588)]\tLoss Ss: 0.034155\n","\tRotated_Epoch:36 [005/005 (0520/0588)]\tLoss Ss: 0.028098\n","\tRotated_Epoch:36 [005/005 (0540/0588)]\tLoss Ss: 0.045628\n","\tRotated_Epoch:36 [005/005 (0560/0588)]\tLoss Ss: 0.067060\n","\tRotated_Epoch:36 [005/005 (0580/0588)]\tLoss Ss: 0.070296\n","new_1_6_d.nii\n","new_1_6_m.nii\n","new_1_7_d.nii\n","new_1_7_m.nii\n","new_2_5_d.nii\n","new_2_5_m.nii\n","new_2_6_d.nii\n","new_2_6_m.nii\n","new_2_7_d.nii\n","new_2_7_m.nii\n","new_3_d (2).nii\n","new_3_m (2).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (3).nii\n","new_3_m (3).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (7).nii\n","new_3_m (7).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (8).nii\n","new_3_m (8).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (9).nii\n","new_3_m (9).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_4_10_d_163526.nii\n","new_4_10_m_163526.nii.gz\n","new_4_11_d_084336.nii\n","new_4_11_m_084336.nii.gz\n","new_4_7_d_184422.nii\n","new_4_7_m_184422.nii.gz\n","new_4_8_d_143412.nii\n","new_4_8_m_143412.nii.gz\n","new_4_9_d_155347.nii\n","new_4_9_m_155347.nii.gz\n","\tEpoch: 36; Dice: 0.8497 +/- 0.0663; Loss: 9.4677\n","Begin Epoch 37\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tEpoch:37 [000/005 (0000/0588)]\tLoss Ss: 0.065253\n","\tEpoch:37 [000/005 (0020/0588)]\tLoss Ss: 0.060839\n","\tEpoch:37 [000/005 (0040/0588)]\tLoss Ss: 0.022804\n","\tEpoch:37 [000/005 (0060/0588)]\tLoss Ss: 0.019228\n","\tEpoch:37 [000/005 (0080/0588)]\tLoss Ss: 0.014069\n","\tEpoch:37 [000/005 (0100/0588)]\tLoss Ss: 0.020166\n","\tEpoch:37 [000/005 (0120/0588)]\tLoss Ss: 0.010749\n","\tEpoch:37 [000/005 (0140/0588)]\tLoss Ss: 0.013486\n","\tEpoch:37 [000/005 (0160/0588)]\tLoss Ss: 0.013330\n","\tEpoch:37 [000/005 (0180/0588)]\tLoss Ss: 0.017187\n","\tEpoch:37 [000/005 (0200/0588)]\tLoss Ss: 0.014701\n","\tEpoch:37 [000/005 (0220/0588)]\tLoss Ss: 0.011816\n","\tEpoch:37 [000/005 (0240/0588)]\tLoss Ss: 0.011885\n","\tEpoch:37 [000/005 (0260/0588)]\tLoss Ss: 0.007494\n","\tEpoch:37 [000/005 (0280/0588)]\tLoss Ss: 0.008365\n","\tEpoch:37 [000/005 (0300/0588)]\tLoss Ss: 0.008675\n","\tEpoch:37 [000/005 (0320/0588)]\tLoss Ss: 0.009373\n","\tEpoch:37 [000/005 (0340/0588)]\tLoss Ss: 0.007764\n","\tEpoch:37 [000/005 (0360/0588)]\tLoss Ss: 0.010674\n","\tEpoch:37 [000/005 (0380/0588)]\tLoss Ss: 0.008604\n","\tEpoch:37 [000/005 (0400/0588)]\tLoss Ss: 0.006892\n","\tEpoch:37 [000/005 (0420/0588)]\tLoss Ss: 0.004351\n","\tEpoch:37 [000/005 (0440/0588)]\tLoss Ss: 0.006400\n","\tEpoch:37 [000/005 (0460/0588)]\tLoss Ss: 0.005487\n","\tEpoch:37 [000/005 (0480/0588)]\tLoss Ss: 0.005265\n","\tEpoch:37 [000/005 (0500/0588)]\tLoss Ss: 0.006087\n","\tEpoch:37 [000/005 (0520/0588)]\tLoss Ss: 0.004434\n","\tEpoch:37 [000/005 (0540/0588)]\tLoss Ss: 0.006579\n","\tEpoch:37 [000/005 (0560/0588)]\tLoss Ss: 0.005155\n","\tEpoch:37 [000/005 (0580/0588)]\tLoss Ss: 0.005896\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:37 [001/005 (0000/0755)]\tLoss Ss: 0.028705\n","\tEpoch:37 [001/005 (0020/0755)]\tLoss Ss: 0.018545\n","\tEpoch:37 [001/005 (0040/0755)]\tLoss Ss: 0.020383\n","\tEpoch:37 [001/005 (0060/0755)]\tLoss Ss: 0.016598\n","\tEpoch:37 [001/005 (0080/0755)]\tLoss Ss: 0.020637\n","\tEpoch:37 [001/005 (0100/0755)]\tLoss Ss: 0.024186\n","\tEpoch:37 [001/005 (0120/0755)]\tLoss Ss: 0.022976\n","\tEpoch:37 [001/005 (0140/0755)]\tLoss Ss: 0.014547\n","\tEpoch:37 [001/005 (0160/0755)]\tLoss Ss: 0.012718\n","\tEpoch:37 [001/005 (0180/0755)]\tLoss Ss: 0.014268\n","\tEpoch:37 [001/005 (0200/0755)]\tLoss Ss: 0.014805\n","\tEpoch:37 [001/005 (0220/0755)]\tLoss Ss: 0.011541\n","\tEpoch:37 [001/005 (0240/0755)]\tLoss Ss: 0.010805\n","\tEpoch:37 [001/005 (0260/0755)]\tLoss Ss: 0.009980\n","\tEpoch:37 [001/005 (0280/0755)]\tLoss Ss: 0.009985\n","\tEpoch:37 [001/005 (0300/0755)]\tLoss Ss: 0.018982\n","\tEpoch:37 [001/005 (0320/0755)]\tLoss Ss: 0.022923\n","\tEpoch:37 [001/005 (0340/0755)]\tLoss Ss: 0.010343\n","\tEpoch:37 [001/005 (0360/0755)]\tLoss Ss: 0.011892\n","\tEpoch:37 [001/005 (0380/0755)]\tLoss Ss: 0.019222\n","\tEpoch:37 [001/005 (0400/0755)]\tLoss Ss: 0.015208\n","\tEpoch:37 [001/005 (0420/0755)]\tLoss Ss: 0.013690\n","\tEpoch:37 [001/005 (0440/0755)]\tLoss Ss: 0.017542\n","\tEpoch:37 [001/005 (0460/0755)]\tLoss Ss: 0.014196\n","\tEpoch:37 [001/005 (0480/0755)]\tLoss Ss: 0.012059\n","\tEpoch:37 [001/005 (0500/0755)]\tLoss Ss: 0.013223\n","\tEpoch:37 [001/005 (0520/0755)]\tLoss Ss: 0.008817\n","\tEpoch:37 [001/005 (0540/0755)]\tLoss Ss: 0.014044\n","\tEpoch:37 [001/005 (0560/0755)]\tLoss Ss: 0.015835\n","\tEpoch:37 [001/005 (0580/0755)]\tLoss Ss: 0.012530\n","\tEpoch:37 [001/005 (0600/0755)]\tLoss Ss: 0.010919\n","\tEpoch:37 [001/005 (0620/0755)]\tLoss Ss: 0.006393\n","\tEpoch:37 [001/005 (0640/0755)]\tLoss Ss: 0.010562\n","\tEpoch:37 [001/005 (0660/0755)]\tLoss Ss: 0.015287\n","\tEpoch:37 [001/005 (0680/0755)]\tLoss Ss: 0.009921\n","\tEpoch:37 [001/005 (0700/0755)]\tLoss Ss: 0.009817\n","\tEpoch:37 [001/005 (0720/0755)]\tLoss Ss: 0.008129\n","\tEpoch:37 [001/005 (0740/0755)]\tLoss Ss: 0.015610\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tEpoch:37 [002/005 (0000/0614)]\tLoss Ss: 0.007189\n","\tEpoch:37 [002/005 (0020/0614)]\tLoss Ss: 0.005500\n","\tEpoch:37 [002/005 (0040/0614)]\tLoss Ss: 0.006295\n","\tEpoch:37 [002/005 (0060/0614)]\tLoss Ss: 0.006404\n","\tEpoch:37 [002/005 (0080/0614)]\tLoss Ss: 0.006217\n","\tEpoch:37 [002/005 (0100/0614)]\tLoss Ss: 0.004151\n","\tEpoch:37 [002/005 (0120/0614)]\tLoss Ss: 0.008652\n","\tEpoch:37 [002/005 (0140/0614)]\tLoss Ss: 0.005055\n","\tEpoch:37 [002/005 (0160/0614)]\tLoss Ss: 0.004594\n","\tEpoch:37 [002/005 (0180/0614)]\tLoss Ss: 0.003511\n","\tEpoch:37 [002/005 (0200/0614)]\tLoss Ss: 0.006807\n","\tEpoch:37 [002/005 (0220/0614)]\tLoss Ss: 0.007114\n","\tEpoch:37 [002/005 (0240/0614)]\tLoss Ss: 0.006274\n","\tEpoch:37 [002/005 (0260/0614)]\tLoss Ss: 0.005743\n","\tEpoch:37 [002/005 (0280/0614)]\tLoss Ss: 0.008506\n","\tEpoch:37 [002/005 (0300/0614)]\tLoss Ss: 0.004252\n","\tEpoch:37 [002/005 (0320/0614)]\tLoss Ss: 0.006385\n","\tEpoch:37 [002/005 (0340/0614)]\tLoss Ss: 0.006806\n","\tEpoch:37 [002/005 (0360/0614)]\tLoss Ss: 0.003811\n","\tEpoch:37 [002/005 (0380/0614)]\tLoss Ss: 0.005998\n","\tEpoch:37 [002/005 (0400/0614)]\tLoss Ss: 0.004655\n","\tEpoch:37 [002/005 (0420/0614)]\tLoss Ss: 0.005609\n","\tEpoch:37 [002/005 (0440/0614)]\tLoss Ss: 0.004437\n","\tEpoch:37 [002/005 (0460/0614)]\tLoss Ss: 0.004960\n","\tEpoch:37 [002/005 (0480/0614)]\tLoss Ss: 0.006612\n","\tEpoch:37 [002/005 (0500/0614)]\tLoss Ss: 0.005620\n","\tEpoch:37 [002/005 (0520/0614)]\tLoss Ss: 0.003269\n","\tEpoch:37 [002/005 (0540/0614)]\tLoss Ss: 0.006656\n","\tEpoch:37 [002/005 (0560/0614)]\tLoss Ss: 0.003611\n","\tEpoch:37 [002/005 (0580/0614)]\tLoss Ss: 0.007301\n","\tEpoch:37 [002/005 (0600/0614)]\tLoss Ss: 0.005692\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tEpoch:37 [003/005 (0000/0693)]\tLoss Ss: 0.013388\n","\tEpoch:37 [003/005 (0020/0693)]\tLoss Ss: 0.015183\n","\tEpoch:37 [003/005 (0040/0693)]\tLoss Ss: 0.010643\n","\tEpoch:37 [003/005 (0060/0693)]\tLoss Ss: 0.009269\n","\tEpoch:37 [003/005 (0080/0693)]\tLoss Ss: 0.012319\n","\tEpoch:37 [003/005 (0100/0693)]\tLoss Ss: 0.019730\n","\tEpoch:37 [003/005 (0120/0693)]\tLoss Ss: 0.012895\n","\tEpoch:37 [003/005 (0140/0693)]\tLoss Ss: 0.024805\n","\tEpoch:37 [003/005 (0160/0693)]\tLoss Ss: 0.011821\n","\tEpoch:37 [003/005 (0180/0693)]\tLoss Ss: 0.014240\n","\tEpoch:37 [003/005 (0200/0693)]\tLoss Ss: 0.007005\n","\tEpoch:37 [003/005 (0220/0693)]\tLoss Ss: 0.011570\n","\tEpoch:37 [003/005 (0240/0693)]\tLoss Ss: 0.013499\n","\tEpoch:37 [003/005 (0260/0693)]\tLoss Ss: 0.010316\n","\tEpoch:37 [003/005 (0280/0693)]\tLoss Ss: 0.012448\n","\tEpoch:37 [003/005 (0300/0693)]\tLoss Ss: 0.016165\n","\tEpoch:37 [003/005 (0320/0693)]\tLoss Ss: 0.016333\n","\tEpoch:37 [003/005 (0340/0693)]\tLoss Ss: 0.018736\n","\tEpoch:37 [003/005 (0360/0693)]\tLoss Ss: 0.017741\n","\tEpoch:37 [003/005 (0380/0693)]\tLoss Ss: 0.012403\n","\tEpoch:37 [003/005 (0400/0693)]\tLoss Ss: 0.012542\n","\tEpoch:37 [003/005 (0420/0693)]\tLoss Ss: 0.010396\n","\tEpoch:37 [003/005 (0440/0693)]\tLoss Ss: 0.020729\n","\tEpoch:37 [003/005 (0460/0693)]\tLoss Ss: 0.013941\n","\tEpoch:37 [003/005 (0480/0693)]\tLoss Ss: 0.008249\n","\tEpoch:37 [003/005 (0500/0693)]\tLoss Ss: 0.012958\n","\tEpoch:37 [003/005 (0520/0693)]\tLoss Ss: 0.013245\n","\tEpoch:37 [003/005 (0540/0693)]\tLoss Ss: 0.012953\n","\tEpoch:37 [003/005 (0560/0693)]\tLoss Ss: 0.013164\n","\tEpoch:37 [003/005 (0580/0693)]\tLoss Ss: 0.009659\n","\tEpoch:37 [003/005 (0600/0693)]\tLoss Ss: 0.013345\n","\tEpoch:37 [003/005 (0620/0693)]\tLoss Ss: 0.012460\n","\tEpoch:37 [003/005 (0640/0693)]\tLoss Ss: 0.014284\n","\tEpoch:37 [003/005 (0660/0693)]\tLoss Ss: 0.008987\n","\tEpoch:37 [003/005 (0680/0693)]\tLoss Ss: 0.010609\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tEpoch:37 [004/005 (0000/0693)]\tLoss Ss: 0.009839\n","\tEpoch:37 [004/005 (0020/0693)]\tLoss Ss: 0.010247\n","\tEpoch:37 [004/005 (0040/0693)]\tLoss Ss: 0.014321\n","\tEpoch:37 [004/005 (0060/0693)]\tLoss Ss: 0.014185\n","\tEpoch:37 [004/005 (0080/0693)]\tLoss Ss: 0.013869\n","\tEpoch:37 [004/005 (0100/0693)]\tLoss Ss: 0.011140\n","\tEpoch:37 [004/005 (0120/0693)]\tLoss Ss: 0.013786\n","\tEpoch:37 [004/005 (0140/0693)]\tLoss Ss: 0.008409\n","\tEpoch:37 [004/005 (0160/0693)]\tLoss Ss: 0.011532\n","\tEpoch:37 [004/005 (0180/0693)]\tLoss Ss: 0.009047\n","\tEpoch:37 [004/005 (0200/0693)]\tLoss Ss: 0.010165\n","\tEpoch:37 [004/005 (0220/0693)]\tLoss Ss: 0.010323\n","\tEpoch:37 [004/005 (0240/0693)]\tLoss Ss: 0.012991\n","\tEpoch:37 [004/005 (0260/0693)]\tLoss Ss: 0.012597\n","\tEpoch:37 [004/005 (0280/0693)]\tLoss Ss: 0.009487\n","\tEpoch:37 [004/005 (0300/0693)]\tLoss Ss: 0.015573\n","\tEpoch:37 [004/005 (0320/0693)]\tLoss Ss: 0.011787\n","\tEpoch:37 [004/005 (0340/0693)]\tLoss Ss: 0.012490\n","\tEpoch:37 [004/005 (0360/0693)]\tLoss Ss: 0.014456\n","\tEpoch:37 [004/005 (0380/0693)]\tLoss Ss: 0.014659\n","\tEpoch:37 [004/005 (0400/0693)]\tLoss Ss: 0.007543\n","\tEpoch:37 [004/005 (0420/0693)]\tLoss Ss: 0.008720\n","\tEpoch:37 [004/005 (0440/0693)]\tLoss Ss: 0.011036\n","\tEpoch:37 [004/005 (0460/0693)]\tLoss Ss: 0.010819\n","\tEpoch:37 [004/005 (0480/0693)]\tLoss Ss: 0.011212\n","\tEpoch:37 [004/005 (0500/0693)]\tLoss Ss: 0.014659\n","\tEpoch:37 [004/005 (0520/0693)]\tLoss Ss: 0.009842\n","\tEpoch:37 [004/005 (0540/0693)]\tLoss Ss: 0.012750\n","\tEpoch:37 [004/005 (0560/0693)]\tLoss Ss: 0.016331\n","\tEpoch:37 [004/005 (0580/0693)]\tLoss Ss: 0.009933\n","\tEpoch:37 [004/005 (0600/0693)]\tLoss Ss: 0.010283\n","\tEpoch:37 [004/005 (0620/0693)]\tLoss Ss: 0.009595\n","\tEpoch:37 [004/005 (0640/0693)]\tLoss Ss: 0.007589\n","\tEpoch:37 [004/005 (0660/0693)]\tLoss Ss: 0.011111\n","\tEpoch:37 [004/005 (0680/0693)]\tLoss Ss: 0.013069\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:37 [005/005 (0000/0755)]\tLoss Ss: 0.015716\n","\tEpoch:37 [005/005 (0020/0755)]\tLoss Ss: 0.013335\n","\tEpoch:37 [005/005 (0040/0755)]\tLoss Ss: 0.021818\n","\tEpoch:37 [005/005 (0060/0755)]\tLoss Ss: 0.010987\n","\tEpoch:37 [005/005 (0080/0755)]\tLoss Ss: 0.017795\n","\tEpoch:37 [005/005 (0100/0755)]\tLoss Ss: 0.024330\n","\tEpoch:37 [005/005 (0120/0755)]\tLoss Ss: 0.017724\n","\tEpoch:37 [005/005 (0140/0755)]\tLoss Ss: 0.013507\n","\tEpoch:37 [005/005 (0160/0755)]\tLoss Ss: 0.012515\n","\tEpoch:37 [005/005 (0180/0755)]\tLoss Ss: 0.013448\n","\tEpoch:37 [005/005 (0200/0755)]\tLoss Ss: 0.015434\n","\tEpoch:37 [005/005 (0220/0755)]\tLoss Ss: 0.025675\n","\tEpoch:37 [005/005 (0240/0755)]\tLoss Ss: 0.012871\n","\tEpoch:37 [005/005 (0260/0755)]\tLoss Ss: 0.016852\n","\tEpoch:37 [005/005 (0280/0755)]\tLoss Ss: 0.012006\n","\tEpoch:37 [005/005 (0300/0755)]\tLoss Ss: 0.011377\n","\tEpoch:37 [005/005 (0320/0755)]\tLoss Ss: 0.015216\n","\tEpoch:37 [005/005 (0340/0755)]\tLoss Ss: 0.013171\n","\tEpoch:37 [005/005 (0360/0755)]\tLoss Ss: 0.012317\n","\tEpoch:37 [005/005 (0380/0755)]\tLoss Ss: 0.012885\n","\tEpoch:37 [005/005 (0400/0755)]\tLoss Ss: 0.016733\n","\tEpoch:37 [005/005 (0420/0755)]\tLoss Ss: 0.011779\n","\tEpoch:37 [005/005 (0440/0755)]\tLoss Ss: 0.011340\n","\tEpoch:37 [005/005 (0460/0755)]\tLoss Ss: 0.015085\n","\tEpoch:37 [005/005 (0480/0755)]\tLoss Ss: 0.012066\n","\tEpoch:37 [005/005 (0500/0755)]\tLoss Ss: 0.015603\n","\tEpoch:37 [005/005 (0520/0755)]\tLoss Ss: 0.015809\n","\tEpoch:37 [005/005 (0540/0755)]\tLoss Ss: 0.016827\n","\tEpoch:37 [005/005 (0560/0755)]\tLoss Ss: 0.011136\n","\tEpoch:37 [005/005 (0580/0755)]\tLoss Ss: 0.022459\n","\tEpoch:37 [005/005 (0600/0755)]\tLoss Ss: 0.012936\n","\tEpoch:37 [005/005 (0620/0755)]\tLoss Ss: 0.015219\n","\tEpoch:37 [005/005 (0640/0755)]\tLoss Ss: 0.010165\n","\tEpoch:37 [005/005 (0660/0755)]\tLoss Ss: 0.010388\n","\tEpoch:37 [005/005 (0680/0755)]\tLoss Ss: 0.013253\n","\tEpoch:37 [005/005 (0700/0755)]\tLoss Ss: 0.014826\n","\tEpoch:37 [005/005 (0720/0755)]\tLoss Ss: 0.013527\n","\tEpoch:37 [005/005 (0740/0755)]\tLoss Ss: 0.018840\n","Now train the rotated image\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tRotated_Epoch:37 [000/005 (0000/0693)]\tLoss Ss: 0.018147\n","\tRotated_Epoch:37 [000/005 (0020/0693)]\tLoss Ss: 0.015599\n","\tRotated_Epoch:37 [000/005 (0040/0693)]\tLoss Ss: 0.012464\n","\tRotated_Epoch:37 [000/005 (0060/0693)]\tLoss Ss: 0.020700\n","\tRotated_Epoch:37 [000/005 (0080/0693)]\tLoss Ss: 0.019126\n","\tRotated_Epoch:37 [000/005 (0100/0693)]\tLoss Ss: 0.020963\n","\tRotated_Epoch:37 [000/005 (0120/0693)]\tLoss Ss: 0.016412\n","\tRotated_Epoch:37 [000/005 (0140/0693)]\tLoss Ss: 0.014114\n","\tRotated_Epoch:37 [000/005 (0160/0693)]\tLoss Ss: 0.011499\n","\tRotated_Epoch:37 [000/005 (0180/0693)]\tLoss Ss: 0.011982\n","\tRotated_Epoch:37 [000/005 (0200/0693)]\tLoss Ss: 0.011792\n","\tRotated_Epoch:37 [000/005 (0220/0693)]\tLoss Ss: 0.010483\n","\tRotated_Epoch:37 [000/005 (0240/0693)]\tLoss Ss: 0.013871\n","\tRotated_Epoch:37 [000/005 (0260/0693)]\tLoss Ss: 0.014239\n","\tRotated_Epoch:37 [000/005 (0280/0693)]\tLoss Ss: 0.013820\n","\tRotated_Epoch:37 [000/005 (0300/0693)]\tLoss Ss: 0.012254\n","\tRotated_Epoch:37 [000/005 (0320/0693)]\tLoss Ss: 0.013318\n","\tRotated_Epoch:37 [000/005 (0340/0693)]\tLoss Ss: 0.011320\n","\tRotated_Epoch:37 [000/005 (0360/0693)]\tLoss Ss: 0.008890\n","\tRotated_Epoch:37 [000/005 (0380/0693)]\tLoss Ss: 0.010836\n","\tRotated_Epoch:37 [000/005 (0400/0693)]\tLoss Ss: 0.009502\n","\tRotated_Epoch:37 [000/005 (0420/0693)]\tLoss Ss: 0.010208\n","\tRotated_Epoch:37 [000/005 (0440/0693)]\tLoss Ss: 0.016785\n","\tRotated_Epoch:37 [000/005 (0460/0693)]\tLoss Ss: 0.014460\n","\tRotated_Epoch:37 [000/005 (0480/0693)]\tLoss Ss: 0.013865\n","\tRotated_Epoch:37 [000/005 (0500/0693)]\tLoss Ss: 0.015829\n","\tRotated_Epoch:37 [000/005 (0520/0693)]\tLoss Ss: 0.012631\n","\tRotated_Epoch:37 [000/005 (0540/0693)]\tLoss Ss: 0.011497\n","\tRotated_Epoch:37 [000/005 (0560/0693)]\tLoss Ss: 0.009975\n","\tRotated_Epoch:37 [000/005 (0580/0693)]\tLoss Ss: 0.013184\n","\tRotated_Epoch:37 [000/005 (0600/0693)]\tLoss Ss: 0.011914\n","\tRotated_Epoch:37 [000/005 (0620/0693)]\tLoss Ss: 0.017764\n","\tRotated_Epoch:37 [000/005 (0640/0693)]\tLoss Ss: 0.012657\n","\tRotated_Epoch:37 [000/005 (0660/0693)]\tLoss Ss: 0.015680\n","\tRotated_Epoch:37 [000/005 (0680/0693)]\tLoss Ss: 0.017930\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:37 [001/005 (0000/0755)]\tLoss Ss: 0.025047\n","\tRotated_Epoch:37 [001/005 (0020/0755)]\tLoss Ss: 0.028427\n","\tRotated_Epoch:37 [001/005 (0040/0755)]\tLoss Ss: 0.026923\n","\tRotated_Epoch:37 [001/005 (0060/0755)]\tLoss Ss: 0.026915\n","\tRotated_Epoch:37 [001/005 (0080/0755)]\tLoss Ss: 0.033013\n","\tRotated_Epoch:37 [001/005 (0100/0755)]\tLoss Ss: 0.023416\n","\tRotated_Epoch:37 [001/005 (0120/0755)]\tLoss Ss: 0.013379\n","\tRotated_Epoch:37 [001/005 (0140/0755)]\tLoss Ss: 0.013959\n","\tRotated_Epoch:37 [001/005 (0160/0755)]\tLoss Ss: 0.016133\n","\tRotated_Epoch:37 [001/005 (0180/0755)]\tLoss Ss: 0.019441\n","\tRotated_Epoch:37 [001/005 (0200/0755)]\tLoss Ss: 0.018615\n","\tRotated_Epoch:37 [001/005 (0220/0755)]\tLoss Ss: 0.015496\n","\tRotated_Epoch:37 [001/005 (0240/0755)]\tLoss Ss: 0.012259\n","\tRotated_Epoch:37 [001/005 (0260/0755)]\tLoss Ss: 0.021307\n","\tRotated_Epoch:37 [001/005 (0280/0755)]\tLoss Ss: 0.020342\n","\tRotated_Epoch:37 [001/005 (0300/0755)]\tLoss Ss: 0.017532\n","\tRotated_Epoch:37 [001/005 (0320/0755)]\tLoss Ss: 0.019381\n","\tRotated_Epoch:37 [001/005 (0340/0755)]\tLoss Ss: 0.009763\n","\tRotated_Epoch:37 [001/005 (0360/0755)]\tLoss Ss: 0.015865\n","\tRotated_Epoch:37 [001/005 (0380/0755)]\tLoss Ss: 0.024174\n","\tRotated_Epoch:37 [001/005 (0400/0755)]\tLoss Ss: 0.015139\n","\tRotated_Epoch:37 [001/005 (0420/0755)]\tLoss Ss: 0.015189\n","\tRotated_Epoch:37 [001/005 (0440/0755)]\tLoss Ss: 0.012716\n","\tRotated_Epoch:37 [001/005 (0460/0755)]\tLoss Ss: 0.011174\n","\tRotated_Epoch:37 [001/005 (0480/0755)]\tLoss Ss: 0.014410\n","\tRotated_Epoch:37 [001/005 (0500/0755)]\tLoss Ss: 0.017213\n","\tRotated_Epoch:37 [001/005 (0520/0755)]\tLoss Ss: 0.013705\n","\tRotated_Epoch:37 [001/005 (0540/0755)]\tLoss Ss: 0.018499\n","\tRotated_Epoch:37 [001/005 (0560/0755)]\tLoss Ss: 0.015065\n","\tRotated_Epoch:37 [001/005 (0580/0755)]\tLoss Ss: 0.013786\n","\tRotated_Epoch:37 [001/005 (0600/0755)]\tLoss Ss: 0.009860\n","\tRotated_Epoch:37 [001/005 (0620/0755)]\tLoss Ss: 0.013222\n","\tRotated_Epoch:37 [001/005 (0640/0755)]\tLoss Ss: 0.011979\n","\tRotated_Epoch:37 [001/005 (0660/0755)]\tLoss Ss: 0.017792\n","\tRotated_Epoch:37 [001/005 (0680/0755)]\tLoss Ss: 0.016019\n","\tRotated_Epoch:37 [001/005 (0700/0755)]\tLoss Ss: 0.009124\n","\tRotated_Epoch:37 [001/005 (0720/0755)]\tLoss Ss: 0.013633\n","\tRotated_Epoch:37 [001/005 (0740/0755)]\tLoss Ss: 0.012045\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tRotated_Epoch:37 [002/005 (0000/0614)]\tLoss Ss: 0.020556\n","\tRotated_Epoch:37 [002/005 (0020/0614)]\tLoss Ss: 0.008420\n","\tRotated_Epoch:37 [002/005 (0040/0614)]\tLoss Ss: 0.011763\n","\tRotated_Epoch:37 [002/005 (0060/0614)]\tLoss Ss: 0.006092\n","\tRotated_Epoch:37 [002/005 (0080/0614)]\tLoss Ss: 0.007382\n","\tRotated_Epoch:37 [002/005 (0100/0614)]\tLoss Ss: 0.008035\n","\tRotated_Epoch:37 [002/005 (0120/0614)]\tLoss Ss: 0.004637\n","\tRotated_Epoch:37 [002/005 (0140/0614)]\tLoss Ss: 0.005169\n","\tRotated_Epoch:37 [002/005 (0160/0614)]\tLoss Ss: 0.007268\n","\tRotated_Epoch:37 [002/005 (0180/0614)]\tLoss Ss: 0.003942\n","\tRotated_Epoch:37 [002/005 (0200/0614)]\tLoss Ss: 0.007000\n","\tRotated_Epoch:37 [002/005 (0220/0614)]\tLoss Ss: 0.005154\n","\tRotated_Epoch:37 [002/005 (0240/0614)]\tLoss Ss: 0.005374\n","\tRotated_Epoch:37 [002/005 (0260/0614)]\tLoss Ss: 0.007398\n","\tRotated_Epoch:37 [002/005 (0280/0614)]\tLoss Ss: 0.007279\n","\tRotated_Epoch:37 [002/005 (0300/0614)]\tLoss Ss: 0.007381\n","\tRotated_Epoch:37 [002/005 (0320/0614)]\tLoss Ss: 0.007059\n","\tRotated_Epoch:37 [002/005 (0340/0614)]\tLoss Ss: 0.004573\n","\tRotated_Epoch:37 [002/005 (0360/0614)]\tLoss Ss: 0.005823\n","\tRotated_Epoch:37 [002/005 (0380/0614)]\tLoss Ss: 0.003708\n","\tRotated_Epoch:37 [002/005 (0400/0614)]\tLoss Ss: 0.005023\n","\tRotated_Epoch:37 [002/005 (0420/0614)]\tLoss Ss: 0.006811\n","\tRotated_Epoch:37 [002/005 (0440/0614)]\tLoss Ss: 0.005326\n","\tRotated_Epoch:37 [002/005 (0460/0614)]\tLoss Ss: 0.006974\n","\tRotated_Epoch:37 [002/005 (0480/0614)]\tLoss Ss: 0.005618\n","\tRotated_Epoch:37 [002/005 (0500/0614)]\tLoss Ss: 0.007265\n","\tRotated_Epoch:37 [002/005 (0520/0614)]\tLoss Ss: 0.007511\n","\tRotated_Epoch:37 [002/005 (0540/0614)]\tLoss Ss: 0.007531\n","\tRotated_Epoch:37 [002/005 (0560/0614)]\tLoss Ss: 0.005316\n","\tRotated_Epoch:37 [002/005 (0580/0614)]\tLoss Ss: 0.007666\n","\tRotated_Epoch:37 [002/005 (0600/0614)]\tLoss Ss: 0.007034\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:37 [003/005 (0000/0755)]\tLoss Ss: 0.160510\n","\tRotated_Epoch:37 [003/005 (0020/0755)]\tLoss Ss: 0.199183\n","\tRotated_Epoch:37 [003/005 (0040/0755)]\tLoss Ss: 0.162559\n","\tRotated_Epoch:37 [003/005 (0060/0755)]\tLoss Ss: 0.082019\n","\tRotated_Epoch:37 [003/005 (0080/0755)]\tLoss Ss: 0.083911\n","\tRotated_Epoch:37 [003/005 (0100/0755)]\tLoss Ss: 0.097470\n","\tRotated_Epoch:37 [003/005 (0120/0755)]\tLoss Ss: 0.057554\n","\tRotated_Epoch:37 [003/005 (0140/0755)]\tLoss Ss: 0.073028\n","\tRotated_Epoch:37 [003/005 (0160/0755)]\tLoss Ss: 0.051615\n","\tRotated_Epoch:37 [003/005 (0180/0755)]\tLoss Ss: 0.038360\n","\tRotated_Epoch:37 [003/005 (0200/0755)]\tLoss Ss: 0.040058\n","\tRotated_Epoch:37 [003/005 (0220/0755)]\tLoss Ss: 0.047391\n","\tRotated_Epoch:37 [003/005 (0240/0755)]\tLoss Ss: 0.046941\n","\tRotated_Epoch:37 [003/005 (0260/0755)]\tLoss Ss: 0.043612\n","\tRotated_Epoch:37 [003/005 (0280/0755)]\tLoss Ss: 0.037662\n","\tRotated_Epoch:37 [003/005 (0300/0755)]\tLoss Ss: 0.046096\n","\tRotated_Epoch:37 [003/005 (0320/0755)]\tLoss Ss: 0.045266\n","\tRotated_Epoch:37 [003/005 (0340/0755)]\tLoss Ss: 0.039906\n","\tRotated_Epoch:37 [003/005 (0360/0755)]\tLoss Ss: 0.051006\n","\tRotated_Epoch:37 [003/005 (0380/0755)]\tLoss Ss: 0.041728\n","\tRotated_Epoch:37 [003/005 (0400/0755)]\tLoss Ss: 0.031984\n","\tRotated_Epoch:37 [003/005 (0420/0755)]\tLoss Ss: 0.044295\n","\tRotated_Epoch:37 [003/005 (0440/0755)]\tLoss Ss: 0.043120\n","\tRotated_Epoch:37 [003/005 (0460/0755)]\tLoss Ss: 0.042177\n","\tRotated_Epoch:37 [003/005 (0480/0755)]\tLoss Ss: 0.036450\n","\tRotated_Epoch:37 [003/005 (0500/0755)]\tLoss Ss: 0.035343\n","\tRotated_Epoch:37 [003/005 (0520/0755)]\tLoss Ss: 0.057486\n","\tRotated_Epoch:37 [003/005 (0540/0755)]\tLoss Ss: 0.035104\n","\tRotated_Epoch:37 [003/005 (0560/0755)]\tLoss Ss: 0.017035\n","\tRotated_Epoch:37 [003/005 (0580/0755)]\tLoss Ss: 0.046685\n","\tRotated_Epoch:37 [003/005 (0600/0755)]\tLoss Ss: 0.045435\n","\tRotated_Epoch:37 [003/005 (0620/0755)]\tLoss Ss: 0.037416\n","\tRotated_Epoch:37 [003/005 (0640/0755)]\tLoss Ss: 0.035670\n","\tRotated_Epoch:37 [003/005 (0660/0755)]\tLoss Ss: 0.039264\n","\tRotated_Epoch:37 [003/005 (0680/0755)]\tLoss Ss: 0.040124\n","\tRotated_Epoch:37 [003/005 (0700/0755)]\tLoss Ss: 0.033933\n","\tRotated_Epoch:37 [003/005 (0720/0755)]\tLoss Ss: 0.024116\n","\tRotated_Epoch:37 [003/005 (0740/0755)]\tLoss Ss: 0.026050\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tRotated_Epoch:37 [004/005 (0000/0588)]\tLoss Ss: 0.075786\n","\tRotated_Epoch:37 [004/005 (0020/0588)]\tLoss Ss: 0.067747\n","\tRotated_Epoch:37 [004/005 (0040/0588)]\tLoss Ss: 0.069256\n","\tRotated_Epoch:37 [004/005 (0060/0588)]\tLoss Ss: 0.034062\n","\tRotated_Epoch:37 [004/005 (0080/0588)]\tLoss Ss: 0.066613\n","\tRotated_Epoch:37 [004/005 (0100/0588)]\tLoss Ss: 0.061543\n","\tRotated_Epoch:37 [004/005 (0120/0588)]\tLoss Ss: 0.035589\n","\tRotated_Epoch:37 [004/005 (0140/0588)]\tLoss Ss: 0.043588\n","\tRotated_Epoch:37 [004/005 (0160/0588)]\tLoss Ss: 0.087545\n","\tRotated_Epoch:37 [004/005 (0180/0588)]\tLoss Ss: 0.057460\n","\tRotated_Epoch:37 [004/005 (0200/0588)]\tLoss Ss: 0.045435\n","\tRotated_Epoch:37 [004/005 (0220/0588)]\tLoss Ss: 0.061061\n","\tRotated_Epoch:37 [004/005 (0240/0588)]\tLoss Ss: 0.061796\n","\tRotated_Epoch:37 [004/005 (0260/0588)]\tLoss Ss: 0.056020\n","\tRotated_Epoch:37 [004/005 (0280/0588)]\tLoss Ss: 0.061625\n","\tRotated_Epoch:37 [004/005 (0300/0588)]\tLoss Ss: 0.046757\n","\tRotated_Epoch:37 [004/005 (0320/0588)]\tLoss Ss: 0.048013\n","\tRotated_Epoch:37 [004/005 (0340/0588)]\tLoss Ss: 0.045837\n","\tRotated_Epoch:37 [004/005 (0360/0588)]\tLoss Ss: 0.060943\n","\tRotated_Epoch:37 [004/005 (0380/0588)]\tLoss Ss: 0.055289\n","\tRotated_Epoch:37 [004/005 (0400/0588)]\tLoss Ss: 0.059800\n","\tRotated_Epoch:37 [004/005 (0420/0588)]\tLoss Ss: 0.036018\n","\tRotated_Epoch:37 [004/005 (0440/0588)]\tLoss Ss: 0.044102\n","\tRotated_Epoch:37 [004/005 (0460/0588)]\tLoss Ss: 0.051419\n","\tRotated_Epoch:37 [004/005 (0480/0588)]\tLoss Ss: 0.046123\n","\tRotated_Epoch:37 [004/005 (0500/0588)]\tLoss Ss: 0.046524\n","\tRotated_Epoch:37 [004/005 (0520/0588)]\tLoss Ss: 0.044199\n","\tRotated_Epoch:37 [004/005 (0540/0588)]\tLoss Ss: 0.067752\n","\tRotated_Epoch:37 [004/005 (0560/0588)]\tLoss Ss: 0.038086\n","\tRotated_Epoch:37 [004/005 (0580/0588)]\tLoss Ss: 0.071843\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tRotated_Epoch:37 [005/005 (0000/0693)]\tLoss Ss: 0.025121\n","\tRotated_Epoch:37 [005/005 (0020/0693)]\tLoss Ss: 0.025613\n","\tRotated_Epoch:37 [005/005 (0040/0693)]\tLoss Ss: 0.023307\n","\tRotated_Epoch:37 [005/005 (0060/0693)]\tLoss Ss: 0.018695\n","\tRotated_Epoch:37 [005/005 (0080/0693)]\tLoss Ss: 0.021990\n","\tRotated_Epoch:37 [005/005 (0100/0693)]\tLoss Ss: 0.024351\n","\tRotated_Epoch:37 [005/005 (0120/0693)]\tLoss Ss: 0.028115\n","\tRotated_Epoch:37 [005/005 (0140/0693)]\tLoss Ss: 0.014907\n","\tRotated_Epoch:37 [005/005 (0160/0693)]\tLoss Ss: 0.024441\n","\tRotated_Epoch:37 [005/005 (0180/0693)]\tLoss Ss: 0.018865\n","\tRotated_Epoch:37 [005/005 (0200/0693)]\tLoss Ss: 0.022335\n","\tRotated_Epoch:37 [005/005 (0220/0693)]\tLoss Ss: 0.016262\n","\tRotated_Epoch:37 [005/005 (0240/0693)]\tLoss Ss: 0.017330\n","\tRotated_Epoch:37 [005/005 (0260/0693)]\tLoss Ss: 0.019452\n","\tRotated_Epoch:37 [005/005 (0280/0693)]\tLoss Ss: 0.017194\n","\tRotated_Epoch:37 [005/005 (0300/0693)]\tLoss Ss: 0.014031\n","\tRotated_Epoch:37 [005/005 (0320/0693)]\tLoss Ss: 0.014155\n","\tRotated_Epoch:37 [005/005 (0340/0693)]\tLoss Ss: 0.011150\n","\tRotated_Epoch:37 [005/005 (0360/0693)]\tLoss Ss: 0.012639\n","\tRotated_Epoch:37 [005/005 (0380/0693)]\tLoss Ss: 0.010851\n","\tRotated_Epoch:37 [005/005 (0400/0693)]\tLoss Ss: 0.014716\n","\tRotated_Epoch:37 [005/005 (0420/0693)]\tLoss Ss: 0.012574\n","\tRotated_Epoch:37 [005/005 (0440/0693)]\tLoss Ss: 0.015433\n","\tRotated_Epoch:37 [005/005 (0460/0693)]\tLoss Ss: 0.015944\n","\tRotated_Epoch:37 [005/005 (0480/0693)]\tLoss Ss: 0.014836\n","\tRotated_Epoch:37 [005/005 (0500/0693)]\tLoss Ss: 0.019704\n","\tRotated_Epoch:37 [005/005 (0520/0693)]\tLoss Ss: 0.013494\n","\tRotated_Epoch:37 [005/005 (0540/0693)]\tLoss Ss: 0.013388\n","\tRotated_Epoch:37 [005/005 (0560/0693)]\tLoss Ss: 0.017517\n","\tRotated_Epoch:37 [005/005 (0580/0693)]\tLoss Ss: 0.013383\n","\tRotated_Epoch:37 [005/005 (0600/0693)]\tLoss Ss: 0.012477\n","\tRotated_Epoch:37 [005/005 (0620/0693)]\tLoss Ss: 0.012200\n","\tRotated_Epoch:37 [005/005 (0640/0693)]\tLoss Ss: 0.012328\n","\tRotated_Epoch:37 [005/005 (0660/0693)]\tLoss Ss: 0.011988\n","\tRotated_Epoch:37 [005/005 (0680/0693)]\tLoss Ss: 0.012445\n","new_1_6_d.nii\n","new_1_6_m.nii\n","new_1_7_d.nii\n","new_1_7_m.nii\n","new_2_5_d.nii\n","new_2_5_m.nii\n","new_2_6_d.nii\n","new_2_6_m.nii\n","new_2_7_d.nii\n","new_2_7_m.nii\n","new_3_d (2).nii\n","new_3_m (2).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (3).nii\n","new_3_m (3).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (7).nii\n","new_3_m (7).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (8).nii\n","new_3_m (8).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (9).nii\n","new_3_m (9).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_4_10_d_163526.nii\n","new_4_10_m_163526.nii.gz\n","new_4_11_d_084336.nii\n","new_4_11_m_084336.nii.gz\n","new_4_7_d_184422.nii\n","new_4_7_m_184422.nii.gz\n","new_4_8_d_143412.nii\n","new_4_8_m_143412.nii.gz\n","new_4_9_d_155347.nii\n","new_4_9_m_155347.nii.gz\n","\tEpoch: 37; Dice: 0.9516 +/- 0.0099; Loss: 8.3013\n","Begin Epoch 38\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tEpoch:38 [000/005 (0000/0693)]\tLoss Ss: 0.005949\n","\tEpoch:38 [000/005 (0020/0693)]\tLoss Ss: 0.015188\n","\tEpoch:38 [000/005 (0040/0693)]\tLoss Ss: 0.015537\n","\tEpoch:38 [000/005 (0060/0693)]\tLoss Ss: 0.012026\n","\tEpoch:38 [000/005 (0080/0693)]\tLoss Ss: 0.013961\n","\tEpoch:38 [000/005 (0100/0693)]\tLoss Ss: 0.009609\n","\tEpoch:38 [000/005 (0120/0693)]\tLoss Ss: 0.009505\n","\tEpoch:38 [000/005 (0140/0693)]\tLoss Ss: 0.011428\n","\tEpoch:38 [000/005 (0160/0693)]\tLoss Ss: 0.010469\n","\tEpoch:38 [000/005 (0180/0693)]\tLoss Ss: 0.012693\n","\tEpoch:38 [000/005 (0200/0693)]\tLoss Ss: 0.013994\n","\tEpoch:38 [000/005 (0220/0693)]\tLoss Ss: 0.008846\n","\tEpoch:38 [000/005 (0240/0693)]\tLoss Ss: 0.012540\n","\tEpoch:38 [000/005 (0260/0693)]\tLoss Ss: 0.012546\n","\tEpoch:38 [000/005 (0280/0693)]\tLoss Ss: 0.011646\n","\tEpoch:38 [000/005 (0300/0693)]\tLoss Ss: 0.013378\n","\tEpoch:38 [000/005 (0320/0693)]\tLoss Ss: 0.008602\n","\tEpoch:38 [000/005 (0340/0693)]\tLoss Ss: 0.011560\n","\tEpoch:38 [000/005 (0360/0693)]\tLoss Ss: 0.011449\n","\tEpoch:38 [000/005 (0380/0693)]\tLoss Ss: 0.010968\n","\tEpoch:38 [000/005 (0400/0693)]\tLoss Ss: 0.012968\n","\tEpoch:38 [000/005 (0420/0693)]\tLoss Ss: 0.011623\n","\tEpoch:38 [000/005 (0440/0693)]\tLoss Ss: 0.011520\n","\tEpoch:38 [000/005 (0460/0693)]\tLoss Ss: 0.010621\n","\tEpoch:38 [000/005 (0480/0693)]\tLoss Ss: 0.010586\n","\tEpoch:38 [000/005 (0500/0693)]\tLoss Ss: 0.013378\n","\tEpoch:38 [000/005 (0520/0693)]\tLoss Ss: 0.009133\n","\tEpoch:38 [000/005 (0540/0693)]\tLoss Ss: 0.006906\n","\tEpoch:38 [000/005 (0560/0693)]\tLoss Ss: 0.012129\n","\tEpoch:38 [000/005 (0580/0693)]\tLoss Ss: 0.009473\n","\tEpoch:38 [000/005 (0600/0693)]\tLoss Ss: 0.011256\n","\tEpoch:38 [000/005 (0620/0693)]\tLoss Ss: 0.012684\n","\tEpoch:38 [000/005 (0640/0693)]\tLoss Ss: 0.012544\n","\tEpoch:38 [000/005 (0660/0693)]\tLoss Ss: 0.011450\n","\tEpoch:38 [000/005 (0680/0693)]\tLoss Ss: 0.009076\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tEpoch:38 [001/005 (0000/0588)]\tLoss Ss: 0.007182\n","\tEpoch:38 [001/005 (0020/0588)]\tLoss Ss: 0.006258\n","\tEpoch:38 [001/005 (0040/0588)]\tLoss Ss: 0.010409\n","\tEpoch:38 [001/005 (0060/0588)]\tLoss Ss: 0.007331\n","\tEpoch:38 [001/005 (0080/0588)]\tLoss Ss: 0.004174\n","\tEpoch:38 [001/005 (0100/0588)]\tLoss Ss: 0.005898\n","\tEpoch:38 [001/005 (0120/0588)]\tLoss Ss: 0.005544\n","\tEpoch:38 [001/005 (0140/0588)]\tLoss Ss: 0.007386\n","\tEpoch:38 [001/005 (0160/0588)]\tLoss Ss: 0.007137\n","\tEpoch:38 [001/005 (0180/0588)]\tLoss Ss: 0.006969\n","\tEpoch:38 [001/005 (0200/0588)]\tLoss Ss: 0.006006\n","\tEpoch:38 [001/005 (0220/0588)]\tLoss Ss: 0.005422\n","\tEpoch:38 [001/005 (0240/0588)]\tLoss Ss: 0.004090\n","\tEpoch:38 [001/005 (0260/0588)]\tLoss Ss: 0.007104\n","\tEpoch:38 [001/005 (0280/0588)]\tLoss Ss: 0.003354\n","\tEpoch:38 [001/005 (0300/0588)]\tLoss Ss: 0.005688\n","\tEpoch:38 [001/005 (0320/0588)]\tLoss Ss: 0.003646\n","\tEpoch:38 [001/005 (0340/0588)]\tLoss Ss: 0.005053\n","\tEpoch:38 [001/005 (0360/0588)]\tLoss Ss: 0.004620\n","\tEpoch:38 [001/005 (0380/0588)]\tLoss Ss: 0.004294\n","\tEpoch:38 [001/005 (0400/0588)]\tLoss Ss: 0.005788\n","\tEpoch:38 [001/005 (0420/0588)]\tLoss Ss: 0.005198\n","\tEpoch:38 [001/005 (0440/0588)]\tLoss Ss: 0.003550\n","\tEpoch:38 [001/005 (0460/0588)]\tLoss Ss: 0.003323\n","\tEpoch:38 [001/005 (0480/0588)]\tLoss Ss: 0.004702\n","\tEpoch:38 [001/005 (0500/0588)]\tLoss Ss: 0.004219\n","\tEpoch:38 [001/005 (0520/0588)]\tLoss Ss: 0.002424\n","\tEpoch:38 [001/005 (0540/0588)]\tLoss Ss: 0.002850\n","\tEpoch:38 [001/005 (0560/0588)]\tLoss Ss: 0.004862\n","\tEpoch:38 [001/005 (0580/0588)]\tLoss Ss: 0.004549\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:38 [002/005 (0000/0755)]\tLoss Ss: 0.016669\n","\tEpoch:38 [002/005 (0020/0755)]\tLoss Ss: 0.014000\n","\tEpoch:38 [002/005 (0040/0755)]\tLoss Ss: 0.016715\n","\tEpoch:38 [002/005 (0060/0755)]\tLoss Ss: 0.013395\n","\tEpoch:38 [002/005 (0080/0755)]\tLoss Ss: 0.013326\n","\tEpoch:38 [002/005 (0100/0755)]\tLoss Ss: 0.013752\n","\tEpoch:38 [002/005 (0120/0755)]\tLoss Ss: 0.022544\n","\tEpoch:38 [002/005 (0140/0755)]\tLoss Ss: 0.014957\n","\tEpoch:38 [002/005 (0160/0755)]\tLoss Ss: 0.015634\n","\tEpoch:38 [002/005 (0180/0755)]\tLoss Ss: 0.012244\n","\tEpoch:38 [002/005 (0200/0755)]\tLoss Ss: 0.017907\n","\tEpoch:38 [002/005 (0220/0755)]\tLoss Ss: 0.013273\n","\tEpoch:38 [002/005 (0240/0755)]\tLoss Ss: 0.009764\n","\tEpoch:38 [002/005 (0260/0755)]\tLoss Ss: 0.013177\n","\tEpoch:38 [002/005 (0280/0755)]\tLoss Ss: 0.015612\n","\tEpoch:38 [002/005 (0300/0755)]\tLoss Ss: 0.007398\n","\tEpoch:38 [002/005 (0320/0755)]\tLoss Ss: 0.016734\n","\tEpoch:38 [002/005 (0340/0755)]\tLoss Ss: 0.012661\n","\tEpoch:38 [002/005 (0360/0755)]\tLoss Ss: 0.013462\n","\tEpoch:38 [002/005 (0380/0755)]\tLoss Ss: 0.010502\n","\tEpoch:38 [002/005 (0400/0755)]\tLoss Ss: 0.014838\n","\tEpoch:38 [002/005 (0420/0755)]\tLoss Ss: 0.005742\n","\tEpoch:38 [002/005 (0440/0755)]\tLoss Ss: 0.006613\n","\tEpoch:38 [002/005 (0460/0755)]\tLoss Ss: 0.010583\n","\tEpoch:38 [002/005 (0480/0755)]\tLoss Ss: 0.011086\n","\tEpoch:38 [002/005 (0500/0755)]\tLoss Ss: 0.009613\n","\tEpoch:38 [002/005 (0520/0755)]\tLoss Ss: 0.010645\n","\tEpoch:38 [002/005 (0540/0755)]\tLoss Ss: 0.017083\n","\tEpoch:38 [002/005 (0560/0755)]\tLoss Ss: 0.011951\n","\tEpoch:38 [002/005 (0580/0755)]\tLoss Ss: 0.017409\n","\tEpoch:38 [002/005 (0600/0755)]\tLoss Ss: 0.009193\n","\tEpoch:38 [002/005 (0620/0755)]\tLoss Ss: 0.012419\n","\tEpoch:38 [002/005 (0640/0755)]\tLoss Ss: 0.013635\n","\tEpoch:38 [002/005 (0660/0755)]\tLoss Ss: 0.009972\n","\tEpoch:38 [002/005 (0680/0755)]\tLoss Ss: 0.010698\n","\tEpoch:38 [002/005 (0700/0755)]\tLoss Ss: 0.013528\n","\tEpoch:38 [002/005 (0720/0755)]\tLoss Ss: 0.013597\n","\tEpoch:38 [002/005 (0740/0755)]\tLoss Ss: 0.010377\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tEpoch:38 [003/005 (0000/0614)]\tLoss Ss: 0.005622\n","\tEpoch:38 [003/005 (0020/0614)]\tLoss Ss: 0.007039\n","\tEpoch:38 [003/005 (0040/0614)]\tLoss Ss: 0.005276\n","\tEpoch:38 [003/005 (0060/0614)]\tLoss Ss: 0.006679\n","\tEpoch:38 [003/005 (0080/0614)]\tLoss Ss: 0.007006\n","\tEpoch:38 [003/005 (0100/0614)]\tLoss Ss: 0.006185\n","\tEpoch:38 [003/005 (0120/0614)]\tLoss Ss: 0.005684\n","\tEpoch:38 [003/005 (0140/0614)]\tLoss Ss: 0.007811\n","\tEpoch:38 [003/005 (0160/0614)]\tLoss Ss: 0.004157\n","\tEpoch:38 [003/005 (0180/0614)]\tLoss Ss: 0.004102\n","\tEpoch:38 [003/005 (0200/0614)]\tLoss Ss: 0.005825\n","\tEpoch:38 [003/005 (0220/0614)]\tLoss Ss: 0.004012\n","\tEpoch:38 [003/005 (0240/0614)]\tLoss Ss: 0.003711\n","\tEpoch:38 [003/005 (0260/0614)]\tLoss Ss: 0.005846\n","\tEpoch:38 [003/005 (0280/0614)]\tLoss Ss: 0.005532\n","\tEpoch:38 [003/005 (0300/0614)]\tLoss Ss: 0.006244\n","\tEpoch:38 [003/005 (0320/0614)]\tLoss Ss: 0.002319\n","\tEpoch:38 [003/005 (0340/0614)]\tLoss Ss: 0.005354\n","\tEpoch:38 [003/005 (0360/0614)]\tLoss Ss: 0.003446\n","\tEpoch:38 [003/005 (0380/0614)]\tLoss Ss: 0.004577\n","\tEpoch:38 [003/005 (0400/0614)]\tLoss Ss: 0.006252\n","\tEpoch:38 [003/005 (0420/0614)]\tLoss Ss: 0.007803\n","\tEpoch:38 [003/005 (0440/0614)]\tLoss Ss: 0.004881\n","\tEpoch:38 [003/005 (0460/0614)]\tLoss Ss: 0.006164\n","\tEpoch:38 [003/005 (0480/0614)]\tLoss Ss: 0.003069\n","\tEpoch:38 [003/005 (0500/0614)]\tLoss Ss: 0.006584\n","\tEpoch:38 [003/005 (0520/0614)]\tLoss Ss: 0.004811\n","\tEpoch:38 [003/005 (0540/0614)]\tLoss Ss: 0.004794\n","\tEpoch:38 [003/005 (0560/0614)]\tLoss Ss: 0.005740\n","\tEpoch:38 [003/005 (0580/0614)]\tLoss Ss: 0.004452\n","\tEpoch:38 [003/005 (0600/0614)]\tLoss Ss: 0.002460\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tEpoch:38 [004/005 (0000/0693)]\tLoss Ss: 0.011391\n","\tEpoch:38 [004/005 (0020/0693)]\tLoss Ss: 0.015988\n","\tEpoch:38 [004/005 (0040/0693)]\tLoss Ss: 0.007025\n","\tEpoch:38 [004/005 (0060/0693)]\tLoss Ss: 0.011013\n","\tEpoch:38 [004/005 (0080/0693)]\tLoss Ss: 0.014278\n","\tEpoch:38 [004/005 (0100/0693)]\tLoss Ss: 0.012047\n","\tEpoch:38 [004/005 (0120/0693)]\tLoss Ss: 0.015870\n","\tEpoch:38 [004/005 (0140/0693)]\tLoss Ss: 0.014362\n","\tEpoch:38 [004/005 (0160/0693)]\tLoss Ss: 0.010447\n","\tEpoch:38 [004/005 (0180/0693)]\tLoss Ss: 0.022463\n","\tEpoch:38 [004/005 (0200/0693)]\tLoss Ss: 0.012016\n","\tEpoch:38 [004/005 (0220/0693)]\tLoss Ss: 0.022166\n","\tEpoch:38 [004/005 (0240/0693)]\tLoss Ss: 0.015005\n","\tEpoch:38 [004/005 (0260/0693)]\tLoss Ss: 0.007578\n","\tEpoch:38 [004/005 (0280/0693)]\tLoss Ss: 0.015558\n","\tEpoch:38 [004/005 (0300/0693)]\tLoss Ss: 0.014853\n","\tEpoch:38 [004/005 (0320/0693)]\tLoss Ss: 0.008623\n","\tEpoch:38 [004/005 (0340/0693)]\tLoss Ss: 0.013550\n","\tEpoch:38 [004/005 (0360/0693)]\tLoss Ss: 0.011485\n","\tEpoch:38 [004/005 (0380/0693)]\tLoss Ss: 0.012099\n","\tEpoch:38 [004/005 (0400/0693)]\tLoss Ss: 0.012777\n","\tEpoch:38 [004/005 (0420/0693)]\tLoss Ss: 0.016805\n","\tEpoch:38 [004/005 (0440/0693)]\tLoss Ss: 0.012196\n","\tEpoch:38 [004/005 (0460/0693)]\tLoss Ss: 0.012390\n","\tEpoch:38 [004/005 (0480/0693)]\tLoss Ss: 0.009702\n","\tEpoch:38 [004/005 (0500/0693)]\tLoss Ss: 0.011484\n","\tEpoch:38 [004/005 (0520/0693)]\tLoss Ss: 0.009794\n","\tEpoch:38 [004/005 (0540/0693)]\tLoss Ss: 0.014948\n","\tEpoch:38 [004/005 (0560/0693)]\tLoss Ss: 0.011445\n","\tEpoch:38 [004/005 (0580/0693)]\tLoss Ss: 0.014012\n","\tEpoch:38 [004/005 (0600/0693)]\tLoss Ss: 0.011828\n","\tEpoch:38 [004/005 (0620/0693)]\tLoss Ss: 0.013466\n","\tEpoch:38 [004/005 (0640/0693)]\tLoss Ss: 0.011927\n","\tEpoch:38 [004/005 (0660/0693)]\tLoss Ss: 0.011706\n","\tEpoch:38 [004/005 (0680/0693)]\tLoss Ss: 0.011691\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:38 [005/005 (0000/0755)]\tLoss Ss: 0.016043\n","\tEpoch:38 [005/005 (0020/0755)]\tLoss Ss: 0.015390\n","\tEpoch:38 [005/005 (0040/0755)]\tLoss Ss: 0.011539\n","\tEpoch:38 [005/005 (0060/0755)]\tLoss Ss: 0.016479\n","\tEpoch:38 [005/005 (0080/0755)]\tLoss Ss: 0.019136\n","\tEpoch:38 [005/005 (0100/0755)]\tLoss Ss: 0.019609\n","\tEpoch:38 [005/005 (0120/0755)]\tLoss Ss: 0.016755\n","\tEpoch:38 [005/005 (0140/0755)]\tLoss Ss: 0.010622\n","\tEpoch:38 [005/005 (0160/0755)]\tLoss Ss: 0.016884\n","\tEpoch:38 [005/005 (0180/0755)]\tLoss Ss: 0.016249\n","\tEpoch:38 [005/005 (0200/0755)]\tLoss Ss: 0.015651\n","\tEpoch:38 [005/005 (0220/0755)]\tLoss Ss: 0.017702\n","\tEpoch:38 [005/005 (0240/0755)]\tLoss Ss: 0.016937\n","\tEpoch:38 [005/005 (0260/0755)]\tLoss Ss: 0.012110\n","\tEpoch:38 [005/005 (0280/0755)]\tLoss Ss: 0.012609\n","\tEpoch:38 [005/005 (0300/0755)]\tLoss Ss: 0.017197\n","\tEpoch:38 [005/005 (0320/0755)]\tLoss Ss: 0.016520\n","\tEpoch:38 [005/005 (0340/0755)]\tLoss Ss: 0.008420\n","\tEpoch:38 [005/005 (0360/0755)]\tLoss Ss: 0.015220\n","\tEpoch:38 [005/005 (0380/0755)]\tLoss Ss: 0.017414\n","\tEpoch:38 [005/005 (0400/0755)]\tLoss Ss: 0.011999\n","\tEpoch:38 [005/005 (0420/0755)]\tLoss Ss: 0.013191\n","\tEpoch:38 [005/005 (0440/0755)]\tLoss Ss: 0.016690\n","\tEpoch:38 [005/005 (0460/0755)]\tLoss Ss: 0.018507\n","\tEpoch:38 [005/005 (0480/0755)]\tLoss Ss: 0.014878\n","\tEpoch:38 [005/005 (0500/0755)]\tLoss Ss: 0.010018\n","\tEpoch:38 [005/005 (0520/0755)]\tLoss Ss: 0.008458\n","\tEpoch:38 [005/005 (0540/0755)]\tLoss Ss: 0.015985\n","\tEpoch:38 [005/005 (0560/0755)]\tLoss Ss: 0.017169\n","\tEpoch:38 [005/005 (0580/0755)]\tLoss Ss: 0.010303\n","\tEpoch:38 [005/005 (0600/0755)]\tLoss Ss: 0.013787\n","\tEpoch:38 [005/005 (0620/0755)]\tLoss Ss: 0.014185\n","\tEpoch:38 [005/005 (0640/0755)]\tLoss Ss: 0.007597\n","\tEpoch:38 [005/005 (0660/0755)]\tLoss Ss: 0.008742\n","\tEpoch:38 [005/005 (0680/0755)]\tLoss Ss: 0.010073\n","\tEpoch:38 [005/005 (0700/0755)]\tLoss Ss: 0.015572\n","\tEpoch:38 [005/005 (0720/0755)]\tLoss Ss: 0.007354\n","\tEpoch:38 [005/005 (0740/0755)]\tLoss Ss: 0.010782\n","Now train the rotated image\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tRotated_Epoch:38 [000/005 (0000/0693)]\tLoss Ss: 0.019370\n","\tRotated_Epoch:38 [000/005 (0020/0693)]\tLoss Ss: 0.016784\n","\tRotated_Epoch:38 [000/005 (0040/0693)]\tLoss Ss: 0.020624\n","\tRotated_Epoch:38 [000/005 (0060/0693)]\tLoss Ss: 0.026222\n","\tRotated_Epoch:38 [000/005 (0080/0693)]\tLoss Ss: 0.012422\n","\tRotated_Epoch:38 [000/005 (0100/0693)]\tLoss Ss: 0.020439\n","\tRotated_Epoch:38 [000/005 (0120/0693)]\tLoss Ss: 0.017878\n","\tRotated_Epoch:38 [000/005 (0140/0693)]\tLoss Ss: 0.014780\n","\tRotated_Epoch:38 [000/005 (0160/0693)]\tLoss Ss: 0.016362\n","\tRotated_Epoch:38 [000/005 (0180/0693)]\tLoss Ss: 0.015965\n","\tRotated_Epoch:38 [000/005 (0200/0693)]\tLoss Ss: 0.018127\n","\tRotated_Epoch:38 [000/005 (0220/0693)]\tLoss Ss: 0.015368\n","\tRotated_Epoch:38 [000/005 (0240/0693)]\tLoss Ss: 0.014979\n","\tRotated_Epoch:38 [000/005 (0260/0693)]\tLoss Ss: 0.014291\n","\tRotated_Epoch:38 [000/005 (0280/0693)]\tLoss Ss: 0.014972\n","\tRotated_Epoch:38 [000/005 (0300/0693)]\tLoss Ss: 0.018020\n","\tRotated_Epoch:38 [000/005 (0320/0693)]\tLoss Ss: 0.022471\n","\tRotated_Epoch:38 [000/005 (0340/0693)]\tLoss Ss: 0.010633\n","\tRotated_Epoch:38 [000/005 (0360/0693)]\tLoss Ss: 0.013853\n","\tRotated_Epoch:38 [000/005 (0380/0693)]\tLoss Ss: 0.009758\n","\tRotated_Epoch:38 [000/005 (0400/0693)]\tLoss Ss: 0.015025\n","\tRotated_Epoch:38 [000/005 (0420/0693)]\tLoss Ss: 0.011426\n","\tRotated_Epoch:38 [000/005 (0440/0693)]\tLoss Ss: 0.009641\n","\tRotated_Epoch:38 [000/005 (0460/0693)]\tLoss Ss: 0.010018\n","\tRotated_Epoch:38 [000/005 (0480/0693)]\tLoss Ss: 0.013737\n","\tRotated_Epoch:38 [000/005 (0500/0693)]\tLoss Ss: 0.015263\n","\tRotated_Epoch:38 [000/005 (0520/0693)]\tLoss Ss: 0.009732\n","\tRotated_Epoch:38 [000/005 (0540/0693)]\tLoss Ss: 0.017318\n","\tRotated_Epoch:38 [000/005 (0560/0693)]\tLoss Ss: 0.010940\n","\tRotated_Epoch:38 [000/005 (0580/0693)]\tLoss Ss: 0.010264\n","\tRotated_Epoch:38 [000/005 (0600/0693)]\tLoss Ss: 0.013013\n","\tRotated_Epoch:38 [000/005 (0620/0693)]\tLoss Ss: 0.015981\n","\tRotated_Epoch:38 [000/005 (0640/0693)]\tLoss Ss: 0.017365\n","\tRotated_Epoch:38 [000/005 (0660/0693)]\tLoss Ss: 0.012876\n","\tRotated_Epoch:38 [000/005 (0680/0693)]\tLoss Ss: 0.008538\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tRotated_Epoch:38 [001/005 (0000/0614)]\tLoss Ss: 0.005288\n","\tRotated_Epoch:38 [001/005 (0020/0614)]\tLoss Ss: 0.008109\n","\tRotated_Epoch:38 [001/005 (0040/0614)]\tLoss Ss: 0.008745\n","\tRotated_Epoch:38 [001/005 (0060/0614)]\tLoss Ss: 0.004144\n","\tRotated_Epoch:38 [001/005 (0080/0614)]\tLoss Ss: 0.008210\n","\tRotated_Epoch:38 [001/005 (0100/0614)]\tLoss Ss: 0.007932\n","\tRotated_Epoch:38 [001/005 (0120/0614)]\tLoss Ss: 0.007505\n","\tRotated_Epoch:38 [001/005 (0140/0614)]\tLoss Ss: 0.004600\n","\tRotated_Epoch:38 [001/005 (0160/0614)]\tLoss Ss: 0.005272\n","\tRotated_Epoch:38 [001/005 (0180/0614)]\tLoss Ss: 0.006864\n","\tRotated_Epoch:38 [001/005 (0200/0614)]\tLoss Ss: 0.006547\n","\tRotated_Epoch:38 [001/005 (0220/0614)]\tLoss Ss: 0.004913\n","\tRotated_Epoch:38 [001/005 (0240/0614)]\tLoss Ss: 0.006849\n","\tRotated_Epoch:38 [001/005 (0260/0614)]\tLoss Ss: 0.007306\n","\tRotated_Epoch:38 [001/005 (0280/0614)]\tLoss Ss: 0.006746\n","\tRotated_Epoch:38 [001/005 (0300/0614)]\tLoss Ss: 0.005641\n","\tRotated_Epoch:38 [001/005 (0320/0614)]\tLoss Ss: 0.004086\n","\tRotated_Epoch:38 [001/005 (0340/0614)]\tLoss Ss: 0.006030\n","\tRotated_Epoch:38 [001/005 (0360/0614)]\tLoss Ss: 0.004521\n","\tRotated_Epoch:38 [001/005 (0380/0614)]\tLoss Ss: 0.005858\n","\tRotated_Epoch:38 [001/005 (0400/0614)]\tLoss Ss: 0.003995\n","\tRotated_Epoch:38 [001/005 (0420/0614)]\tLoss Ss: 0.006217\n","\tRotated_Epoch:38 [001/005 (0440/0614)]\tLoss Ss: 0.004467\n","\tRotated_Epoch:38 [001/005 (0460/0614)]\tLoss Ss: 0.003367\n","\tRotated_Epoch:38 [001/005 (0480/0614)]\tLoss Ss: 0.006158\n","\tRotated_Epoch:38 [001/005 (0500/0614)]\tLoss Ss: 0.005511\n","\tRotated_Epoch:38 [001/005 (0520/0614)]\tLoss Ss: 0.007038\n","\tRotated_Epoch:38 [001/005 (0540/0614)]\tLoss Ss: 0.005805\n","\tRotated_Epoch:38 [001/005 (0560/0614)]\tLoss Ss: 0.003603\n","\tRotated_Epoch:38 [001/005 (0580/0614)]\tLoss Ss: 0.005442\n","\tRotated_Epoch:38 [001/005 (0600/0614)]\tLoss Ss: 0.007897\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tRotated_Epoch:38 [002/005 (0000/0693)]\tLoss Ss: 0.013559\n","\tRotated_Epoch:38 [002/005 (0020/0693)]\tLoss Ss: 0.015110\n","\tRotated_Epoch:38 [002/005 (0040/0693)]\tLoss Ss: 0.013006\n","\tRotated_Epoch:38 [002/005 (0060/0693)]\tLoss Ss: 0.015086\n","\tRotated_Epoch:38 [002/005 (0080/0693)]\tLoss Ss: 0.012923\n","\tRotated_Epoch:38 [002/005 (0100/0693)]\tLoss Ss: 0.010452\n","\tRotated_Epoch:38 [002/005 (0120/0693)]\tLoss Ss: 0.010798\n","\tRotated_Epoch:38 [002/005 (0140/0693)]\tLoss Ss: 0.012070\n","\tRotated_Epoch:38 [002/005 (0160/0693)]\tLoss Ss: 0.012207\n","\tRotated_Epoch:38 [002/005 (0180/0693)]\tLoss Ss: 0.017964\n","\tRotated_Epoch:38 [002/005 (0200/0693)]\tLoss Ss: 0.011240\n","\tRotated_Epoch:38 [002/005 (0220/0693)]\tLoss Ss: 0.011240\n","\tRotated_Epoch:38 [002/005 (0240/0693)]\tLoss Ss: 0.010319\n","\tRotated_Epoch:38 [002/005 (0260/0693)]\tLoss Ss: 0.013455\n","\tRotated_Epoch:38 [002/005 (0280/0693)]\tLoss Ss: 0.013188\n","\tRotated_Epoch:38 [002/005 (0300/0693)]\tLoss Ss: 0.014324\n","\tRotated_Epoch:38 [002/005 (0320/0693)]\tLoss Ss: 0.013840\n","\tRotated_Epoch:38 [002/005 (0340/0693)]\tLoss Ss: 0.011001\n","\tRotated_Epoch:38 [002/005 (0360/0693)]\tLoss Ss: 0.011989\n","\tRotated_Epoch:38 [002/005 (0380/0693)]\tLoss Ss: 0.012512\n","\tRotated_Epoch:38 [002/005 (0400/0693)]\tLoss Ss: 0.016033\n","\tRotated_Epoch:38 [002/005 (0420/0693)]\tLoss Ss: 0.008221\n","\tRotated_Epoch:38 [002/005 (0440/0693)]\tLoss Ss: 0.010064\n","\tRotated_Epoch:38 [002/005 (0460/0693)]\tLoss Ss: 0.011124\n","\tRotated_Epoch:38 [002/005 (0480/0693)]\tLoss Ss: 0.009765\n","\tRotated_Epoch:38 [002/005 (0500/0693)]\tLoss Ss: 0.012764\n","\tRotated_Epoch:38 [002/005 (0520/0693)]\tLoss Ss: 0.016498\n","\tRotated_Epoch:38 [002/005 (0540/0693)]\tLoss Ss: 0.008831\n","\tRotated_Epoch:38 [002/005 (0560/0693)]\tLoss Ss: 0.009992\n","\tRotated_Epoch:38 [002/005 (0580/0693)]\tLoss Ss: 0.010758\n","\tRotated_Epoch:38 [002/005 (0600/0693)]\tLoss Ss: 0.009097\n","\tRotated_Epoch:38 [002/005 (0620/0693)]\tLoss Ss: 0.011410\n","\tRotated_Epoch:38 [002/005 (0640/0693)]\tLoss Ss: 0.011118\n","\tRotated_Epoch:38 [002/005 (0660/0693)]\tLoss Ss: 0.011217\n","\tRotated_Epoch:38 [002/005 (0680/0693)]\tLoss Ss: 0.011168\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tRotated_Epoch:38 [003/005 (0000/0588)]\tLoss Ss: 0.077002\n","\tRotated_Epoch:38 [003/005 (0020/0588)]\tLoss Ss: 0.047368\n","\tRotated_Epoch:38 [003/005 (0040/0588)]\tLoss Ss: 0.058669\n","\tRotated_Epoch:38 [003/005 (0060/0588)]\tLoss Ss: 0.062093\n","\tRotated_Epoch:38 [003/005 (0080/0588)]\tLoss Ss: 0.064980\n","\tRotated_Epoch:38 [003/005 (0100/0588)]\tLoss Ss: 0.048827\n","\tRotated_Epoch:38 [003/005 (0120/0588)]\tLoss Ss: 0.059552\n","\tRotated_Epoch:38 [003/005 (0140/0588)]\tLoss Ss: 0.065555\n","\tRotated_Epoch:38 [003/005 (0160/0588)]\tLoss Ss: 0.075520\n","\tRotated_Epoch:38 [003/005 (0180/0588)]\tLoss Ss: 0.054887\n","\tRotated_Epoch:38 [003/005 (0200/0588)]\tLoss Ss: 0.047421\n","\tRotated_Epoch:38 [003/005 (0220/0588)]\tLoss Ss: 0.050112\n","\tRotated_Epoch:38 [003/005 (0240/0588)]\tLoss Ss: 0.064008\n","\tRotated_Epoch:38 [003/005 (0260/0588)]\tLoss Ss: 0.035947\n","\tRotated_Epoch:38 [003/005 (0280/0588)]\tLoss Ss: 0.057848\n","\tRotated_Epoch:38 [003/005 (0300/0588)]\tLoss Ss: 0.055020\n","\tRotated_Epoch:38 [003/005 (0320/0588)]\tLoss Ss: 0.052404\n","\tRotated_Epoch:38 [003/005 (0340/0588)]\tLoss Ss: 0.061053\n","\tRotated_Epoch:38 [003/005 (0360/0588)]\tLoss Ss: 0.045675\n","\tRotated_Epoch:38 [003/005 (0380/0588)]\tLoss Ss: 0.044615\n","\tRotated_Epoch:38 [003/005 (0400/0588)]\tLoss Ss: 0.045099\n","\tRotated_Epoch:38 [003/005 (0420/0588)]\tLoss Ss: 0.065437\n","\tRotated_Epoch:38 [003/005 (0440/0588)]\tLoss Ss: 0.055450\n","\tRotated_Epoch:38 [003/005 (0460/0588)]\tLoss Ss: 0.056002\n","\tRotated_Epoch:38 [003/005 (0480/0588)]\tLoss Ss: 0.051260\n","\tRotated_Epoch:38 [003/005 (0500/0588)]\tLoss Ss: 0.037151\n","\tRotated_Epoch:38 [003/005 (0520/0588)]\tLoss Ss: 0.045888\n","\tRotated_Epoch:38 [003/005 (0540/0588)]\tLoss Ss: 0.048982\n","\tRotated_Epoch:38 [003/005 (0560/0588)]\tLoss Ss: 0.038091\n","\tRotated_Epoch:38 [003/005 (0580/0588)]\tLoss Ss: 0.059998\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:38 [004/005 (0000/0755)]\tLoss Ss: 0.062978\n","\tRotated_Epoch:38 [004/005 (0020/0755)]\tLoss Ss: 0.038716\n","\tRotated_Epoch:38 [004/005 (0040/0755)]\tLoss Ss: 0.047895\n","\tRotated_Epoch:38 [004/005 (0060/0755)]\tLoss Ss: 0.031505\n","\tRotated_Epoch:38 [004/005 (0080/0755)]\tLoss Ss: 0.033952\n","\tRotated_Epoch:38 [004/005 (0100/0755)]\tLoss Ss: 0.021113\n","\tRotated_Epoch:38 [004/005 (0120/0755)]\tLoss Ss: 0.018184\n","\tRotated_Epoch:38 [004/005 (0140/0755)]\tLoss Ss: 0.030053\n","\tRotated_Epoch:38 [004/005 (0160/0755)]\tLoss Ss: 0.025674\n","\tRotated_Epoch:38 [004/005 (0180/0755)]\tLoss Ss: 0.025593\n","\tRotated_Epoch:38 [004/005 (0200/0755)]\tLoss Ss: 0.019651\n","\tRotated_Epoch:38 [004/005 (0220/0755)]\tLoss Ss: 0.011710\n","\tRotated_Epoch:38 [004/005 (0240/0755)]\tLoss Ss: 0.016223\n","\tRotated_Epoch:38 [004/005 (0260/0755)]\tLoss Ss: 0.020025\n","\tRotated_Epoch:38 [004/005 (0280/0755)]\tLoss Ss: 0.013044\n","\tRotated_Epoch:38 [004/005 (0300/0755)]\tLoss Ss: 0.015058\n","\tRotated_Epoch:38 [004/005 (0320/0755)]\tLoss Ss: 0.020372\n","\tRotated_Epoch:38 [004/005 (0340/0755)]\tLoss Ss: 0.016244\n","\tRotated_Epoch:38 [004/005 (0360/0755)]\tLoss Ss: 0.016205\n","\tRotated_Epoch:38 [004/005 (0380/0755)]\tLoss Ss: 0.016368\n","\tRotated_Epoch:38 [004/005 (0400/0755)]\tLoss Ss: 0.010610\n","\tRotated_Epoch:38 [004/005 (0420/0755)]\tLoss Ss: 0.014261\n","\tRotated_Epoch:38 [004/005 (0440/0755)]\tLoss Ss: 0.017735\n","\tRotated_Epoch:38 [004/005 (0460/0755)]\tLoss Ss: 0.011162\n","\tRotated_Epoch:38 [004/005 (0480/0755)]\tLoss Ss: 0.018394\n","\tRotated_Epoch:38 [004/005 (0500/0755)]\tLoss Ss: 0.016188\n","\tRotated_Epoch:38 [004/005 (0520/0755)]\tLoss Ss: 0.011917\n","\tRotated_Epoch:38 [004/005 (0540/0755)]\tLoss Ss: 0.018735\n","\tRotated_Epoch:38 [004/005 (0560/0755)]\tLoss Ss: 0.011076\n","\tRotated_Epoch:38 [004/005 (0580/0755)]\tLoss Ss: 0.014312\n","\tRotated_Epoch:38 [004/005 (0600/0755)]\tLoss Ss: 0.017763\n","\tRotated_Epoch:38 [004/005 (0620/0755)]\tLoss Ss: 0.007008\n","\tRotated_Epoch:38 [004/005 (0640/0755)]\tLoss Ss: 0.019945\n","\tRotated_Epoch:38 [004/005 (0660/0755)]\tLoss Ss: 0.008464\n","\tRotated_Epoch:38 [004/005 (0680/0755)]\tLoss Ss: 0.014195\n","\tRotated_Epoch:38 [004/005 (0700/0755)]\tLoss Ss: 0.016552\n","\tRotated_Epoch:38 [004/005 (0720/0755)]\tLoss Ss: 0.014791\n","\tRotated_Epoch:38 [004/005 (0740/0755)]\tLoss Ss: 0.013064\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:38 [005/005 (0000/0755)]\tLoss Ss: 0.224820\n","\tRotated_Epoch:38 [005/005 (0020/0755)]\tLoss Ss: 0.142035\n","\tRotated_Epoch:38 [005/005 (0040/0755)]\tLoss Ss: 0.076370\n","\tRotated_Epoch:38 [005/005 (0060/0755)]\tLoss Ss: 0.082119\n","\tRotated_Epoch:38 [005/005 (0080/0755)]\tLoss Ss: 0.048208\n","\tRotated_Epoch:38 [005/005 (0100/0755)]\tLoss Ss: 0.037792\n","\tRotated_Epoch:38 [005/005 (0120/0755)]\tLoss Ss: 0.038480\n","\tRotated_Epoch:38 [005/005 (0140/0755)]\tLoss Ss: 0.063816\n","\tRotated_Epoch:38 [005/005 (0160/0755)]\tLoss Ss: 0.048013\n","\tRotated_Epoch:38 [005/005 (0180/0755)]\tLoss Ss: 0.057461\n","\tRotated_Epoch:38 [005/005 (0200/0755)]\tLoss Ss: 0.040755\n","\tRotated_Epoch:38 [005/005 (0220/0755)]\tLoss Ss: 0.047993\n","\tRotated_Epoch:38 [005/005 (0240/0755)]\tLoss Ss: 0.049584\n","\tRotated_Epoch:38 [005/005 (0260/0755)]\tLoss Ss: 0.027610\n","\tRotated_Epoch:38 [005/005 (0280/0755)]\tLoss Ss: 0.032977\n","\tRotated_Epoch:38 [005/005 (0300/0755)]\tLoss Ss: 0.040322\n","\tRotated_Epoch:38 [005/005 (0320/0755)]\tLoss Ss: 0.032830\n","\tRotated_Epoch:38 [005/005 (0340/0755)]\tLoss Ss: 0.034845\n","\tRotated_Epoch:38 [005/005 (0360/0755)]\tLoss Ss: 0.035537\n","\tRotated_Epoch:38 [005/005 (0380/0755)]\tLoss Ss: 0.018030\n","\tRotated_Epoch:38 [005/005 (0400/0755)]\tLoss Ss: 0.021850\n","\tRotated_Epoch:38 [005/005 (0420/0755)]\tLoss Ss: 0.039118\n","\tRotated_Epoch:38 [005/005 (0440/0755)]\tLoss Ss: 0.033648\n","\tRotated_Epoch:38 [005/005 (0460/0755)]\tLoss Ss: 0.041423\n","\tRotated_Epoch:38 [005/005 (0480/0755)]\tLoss Ss: 0.028673\n","\tRotated_Epoch:38 [005/005 (0500/0755)]\tLoss Ss: 0.031435\n","\tRotated_Epoch:38 [005/005 (0520/0755)]\tLoss Ss: 0.030958\n","\tRotated_Epoch:38 [005/005 (0540/0755)]\tLoss Ss: 0.038695\n","\tRotated_Epoch:38 [005/005 (0560/0755)]\tLoss Ss: 0.031766\n","\tRotated_Epoch:38 [005/005 (0580/0755)]\tLoss Ss: 0.031894\n","\tRotated_Epoch:38 [005/005 (0600/0755)]\tLoss Ss: 0.029662\n","\tRotated_Epoch:38 [005/005 (0620/0755)]\tLoss Ss: 0.037982\n","\tRotated_Epoch:38 [005/005 (0640/0755)]\tLoss Ss: 0.038433\n","\tRotated_Epoch:38 [005/005 (0660/0755)]\tLoss Ss: 0.041589\n","\tRotated_Epoch:38 [005/005 (0680/0755)]\tLoss Ss: 0.027962\n","\tRotated_Epoch:38 [005/005 (0700/0755)]\tLoss Ss: 0.030547\n","\tRotated_Epoch:38 [005/005 (0720/0755)]\tLoss Ss: 0.024658\n","\tRotated_Epoch:38 [005/005 (0740/0755)]\tLoss Ss: 0.022756\n","new_1_6_d.nii\n","new_1_6_m.nii\n","new_1_7_d.nii\n","new_1_7_m.nii\n","new_2_5_d.nii\n","new_2_5_m.nii\n","new_2_6_d.nii\n","new_2_6_m.nii\n","new_2_7_d.nii\n","new_2_7_m.nii\n","new_3_d (2).nii\n","new_3_m (2).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (3).nii\n","new_3_m (3).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (7).nii\n","new_3_m (7).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (8).nii\n","new_3_m (8).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (9).nii\n","new_3_m (9).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_4_10_d_163526.nii\n","new_4_10_m_163526.nii.gz\n","new_4_11_d_084336.nii\n","new_4_11_m_084336.nii.gz\n","new_4_7_d_184422.nii\n","new_4_7_m_184422.nii.gz\n","new_4_8_d_143412.nii\n","new_4_8_m_143412.nii.gz\n","new_4_9_d_155347.nii\n","new_4_9_m_155347.nii.gz\n","\tEpoch: 38; Dice: 0.9293 +/- 0.0127; Loss: 7.4849\n","Begin Epoch 39\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tEpoch:39 [000/005 (0000/0693)]\tLoss Ss: 0.015870\n","\tEpoch:39 [000/005 (0020/0693)]\tLoss Ss: 0.017101\n","\tEpoch:39 [000/005 (0040/0693)]\tLoss Ss: 0.022218\n","\tEpoch:39 [000/005 (0060/0693)]\tLoss Ss: 0.013770\n","\tEpoch:39 [000/005 (0080/0693)]\tLoss Ss: 0.011400\n","\tEpoch:39 [000/005 (0100/0693)]\tLoss Ss: 0.011265\n","\tEpoch:39 [000/005 (0120/0693)]\tLoss Ss: 0.011866\n","\tEpoch:39 [000/005 (0140/0693)]\tLoss Ss: 0.013203\n","\tEpoch:39 [000/005 (0160/0693)]\tLoss Ss: 0.011146\n","\tEpoch:39 [000/005 (0180/0693)]\tLoss Ss: 0.014731\n","\tEpoch:39 [000/005 (0200/0693)]\tLoss Ss: 0.013633\n","\tEpoch:39 [000/005 (0220/0693)]\tLoss Ss: 0.007015\n","\tEpoch:39 [000/005 (0240/0693)]\tLoss Ss: 0.010486\n","\tEpoch:39 [000/005 (0260/0693)]\tLoss Ss: 0.010460\n","\tEpoch:39 [000/005 (0280/0693)]\tLoss Ss: 0.013152\n","\tEpoch:39 [000/005 (0300/0693)]\tLoss Ss: 0.016454\n","\tEpoch:39 [000/005 (0320/0693)]\tLoss Ss: 0.008542\n","\tEpoch:39 [000/005 (0340/0693)]\tLoss Ss: 0.010526\n","\tEpoch:39 [000/005 (0360/0693)]\tLoss Ss: 0.016976\n","\tEpoch:39 [000/005 (0380/0693)]\tLoss Ss: 0.011422\n","\tEpoch:39 [000/005 (0400/0693)]\tLoss Ss: 0.011934\n","\tEpoch:39 [000/005 (0420/0693)]\tLoss Ss: 0.012428\n","\tEpoch:39 [000/005 (0440/0693)]\tLoss Ss: 0.007418\n","\tEpoch:39 [000/005 (0460/0693)]\tLoss Ss: 0.011701\n","\tEpoch:39 [000/005 (0480/0693)]\tLoss Ss: 0.010457\n","\tEpoch:39 [000/005 (0500/0693)]\tLoss Ss: 0.007525\n","\tEpoch:39 [000/005 (0520/0693)]\tLoss Ss: 0.010083\n","\tEpoch:39 [000/005 (0540/0693)]\tLoss Ss: 0.012554\n","\tEpoch:39 [000/005 (0560/0693)]\tLoss Ss: 0.012547\n","\tEpoch:39 [000/005 (0580/0693)]\tLoss Ss: 0.010328\n","\tEpoch:39 [000/005 (0600/0693)]\tLoss Ss: 0.014271\n","\tEpoch:39 [000/005 (0620/0693)]\tLoss Ss: 0.010096\n","\tEpoch:39 [000/005 (0640/0693)]\tLoss Ss: 0.013109\n","\tEpoch:39 [000/005 (0660/0693)]\tLoss Ss: 0.007251\n","\tEpoch:39 [000/005 (0680/0693)]\tLoss Ss: 0.008793\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tEpoch:39 [001/005 (0000/0614)]\tLoss Ss: 0.004338\n","\tEpoch:39 [001/005 (0020/0614)]\tLoss Ss: 0.004614\n","\tEpoch:39 [001/005 (0040/0614)]\tLoss Ss: 0.006924\n","\tEpoch:39 [001/005 (0060/0614)]\tLoss Ss: 0.006579\n","\tEpoch:39 [001/005 (0080/0614)]\tLoss Ss: 0.005475\n","\tEpoch:39 [001/005 (0100/0614)]\tLoss Ss: 0.006692\n","\tEpoch:39 [001/005 (0120/0614)]\tLoss Ss: 0.005478\n","\tEpoch:39 [001/005 (0140/0614)]\tLoss Ss: 0.008531\n","\tEpoch:39 [001/005 (0160/0614)]\tLoss Ss: 0.006761\n","\tEpoch:39 [001/005 (0180/0614)]\tLoss Ss: 0.005854\n","\tEpoch:39 [001/005 (0200/0614)]\tLoss Ss: 0.005158\n","\tEpoch:39 [001/005 (0220/0614)]\tLoss Ss: 0.005014\n","\tEpoch:39 [001/005 (0240/0614)]\tLoss Ss: 0.005345\n","\tEpoch:39 [001/005 (0260/0614)]\tLoss Ss: 0.004107\n","\tEpoch:39 [001/005 (0280/0614)]\tLoss Ss: 0.006576\n","\tEpoch:39 [001/005 (0300/0614)]\tLoss Ss: 0.007161\n","\tEpoch:39 [001/005 (0320/0614)]\tLoss Ss: 0.005952\n","\tEpoch:39 [001/005 (0340/0614)]\tLoss Ss: 0.004798\n","\tEpoch:39 [001/005 (0360/0614)]\tLoss Ss: 0.005852\n","\tEpoch:39 [001/005 (0380/0614)]\tLoss Ss: 0.005807\n","\tEpoch:39 [001/005 (0400/0614)]\tLoss Ss: 0.007133\n","\tEpoch:39 [001/005 (0420/0614)]\tLoss Ss: 0.004705\n","\tEpoch:39 [001/005 (0440/0614)]\tLoss Ss: 0.010352\n","\tEpoch:39 [001/005 (0460/0614)]\tLoss Ss: 0.003783\n","\tEpoch:39 [001/005 (0480/0614)]\tLoss Ss: 0.006705\n","\tEpoch:39 [001/005 (0500/0614)]\tLoss Ss: 0.005299\n","\tEpoch:39 [001/005 (0520/0614)]\tLoss Ss: 0.005082\n","\tEpoch:39 [001/005 (0540/0614)]\tLoss Ss: 0.003609\n","\tEpoch:39 [001/005 (0560/0614)]\tLoss Ss: 0.004847\n","\tEpoch:39 [001/005 (0580/0614)]\tLoss Ss: 0.004318\n","\tEpoch:39 [001/005 (0600/0614)]\tLoss Ss: 0.003178\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tEpoch:39 [002/005 (0000/0588)]\tLoss Ss: 0.003504\n","\tEpoch:39 [002/005 (0020/0588)]\tLoss Ss: 0.003853\n","\tEpoch:39 [002/005 (0040/0588)]\tLoss Ss: 0.005355\n","\tEpoch:39 [002/005 (0060/0588)]\tLoss Ss: 0.006277\n","\tEpoch:39 [002/005 (0080/0588)]\tLoss Ss: 0.005400\n","\tEpoch:39 [002/005 (0100/0588)]\tLoss Ss: 0.005794\n","\tEpoch:39 [002/005 (0120/0588)]\tLoss Ss: 0.005037\n","\tEpoch:39 [002/005 (0140/0588)]\tLoss Ss: 0.004713\n","\tEpoch:39 [002/005 (0160/0588)]\tLoss Ss: 0.003048\n","\tEpoch:39 [002/005 (0180/0588)]\tLoss Ss: 0.005838\n","\tEpoch:39 [002/005 (0200/0588)]\tLoss Ss: 0.005505\n","\tEpoch:39 [002/005 (0220/0588)]\tLoss Ss: 0.005503\n","\tEpoch:39 [002/005 (0240/0588)]\tLoss Ss: 0.003375\n","\tEpoch:39 [002/005 (0260/0588)]\tLoss Ss: 0.005201\n","\tEpoch:39 [002/005 (0280/0588)]\tLoss Ss: 0.003785\n","\tEpoch:39 [002/005 (0300/0588)]\tLoss Ss: 0.003920\n","\tEpoch:39 [002/005 (0320/0588)]\tLoss Ss: 0.004578\n","\tEpoch:39 [002/005 (0340/0588)]\tLoss Ss: 0.003192\n","\tEpoch:39 [002/005 (0360/0588)]\tLoss Ss: 0.002236\n","\tEpoch:39 [002/005 (0380/0588)]\tLoss Ss: 0.005867\n","\tEpoch:39 [002/005 (0400/0588)]\tLoss Ss: 0.002740\n","\tEpoch:39 [002/005 (0420/0588)]\tLoss Ss: 0.003507\n","\tEpoch:39 [002/005 (0440/0588)]\tLoss Ss: 0.005852\n","\tEpoch:39 [002/005 (0460/0588)]\tLoss Ss: 0.005651\n","\tEpoch:39 [002/005 (0480/0588)]\tLoss Ss: 0.002755\n","\tEpoch:39 [002/005 (0500/0588)]\tLoss Ss: 0.003420\n","\tEpoch:39 [002/005 (0520/0588)]\tLoss Ss: 0.004324\n","\tEpoch:39 [002/005 (0540/0588)]\tLoss Ss: 0.004051\n","\tEpoch:39 [002/005 (0560/0588)]\tLoss Ss: 0.003938\n","\tEpoch:39 [002/005 (0580/0588)]\tLoss Ss: 0.243709\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tEpoch:39 [003/005 (0000/0693)]\tLoss Ss: 0.015730\n","\tEpoch:39 [003/005 (0020/0693)]\tLoss Ss: 0.012468\n","\tEpoch:39 [003/005 (0040/0693)]\tLoss Ss: 0.019553\n","\tEpoch:39 [003/005 (0060/0693)]\tLoss Ss: 0.010234\n","\tEpoch:39 [003/005 (0080/0693)]\tLoss Ss: 0.015657\n","\tEpoch:39 [003/005 (0100/0693)]\tLoss Ss: 0.009964\n","\tEpoch:39 [003/005 (0120/0693)]\tLoss Ss: 0.031576\n","\tEpoch:39 [003/005 (0140/0693)]\tLoss Ss: 0.015216\n","\tEpoch:39 [003/005 (0160/0693)]\tLoss Ss: 0.015595\n","\tEpoch:39 [003/005 (0180/0693)]\tLoss Ss: 0.012152\n","\tEpoch:39 [003/005 (0200/0693)]\tLoss Ss: 0.013261\n","\tEpoch:39 [003/005 (0220/0693)]\tLoss Ss: 0.008034\n","\tEpoch:39 [003/005 (0240/0693)]\tLoss Ss: 0.008156\n","\tEpoch:39 [003/005 (0260/0693)]\tLoss Ss: 0.011837\n","\tEpoch:39 [003/005 (0280/0693)]\tLoss Ss: 0.018924\n","\tEpoch:39 [003/005 (0300/0693)]\tLoss Ss: 0.013215\n","\tEpoch:39 [003/005 (0320/0693)]\tLoss Ss: 0.010638\n","\tEpoch:39 [003/005 (0340/0693)]\tLoss Ss: 0.017420\n","\tEpoch:39 [003/005 (0360/0693)]\tLoss Ss: 0.006319\n","\tEpoch:39 [003/005 (0380/0693)]\tLoss Ss: 0.018806\n","\tEpoch:39 [003/005 (0400/0693)]\tLoss Ss: 0.015685\n","\tEpoch:39 [003/005 (0420/0693)]\tLoss Ss: 0.027212\n","\tEpoch:39 [003/005 (0440/0693)]\tLoss Ss: 0.019488\n","\tEpoch:39 [003/005 (0460/0693)]\tLoss Ss: 0.015711\n","\tEpoch:39 [003/005 (0480/0693)]\tLoss Ss: 0.014202\n","\tEpoch:39 [003/005 (0500/0693)]\tLoss Ss: 0.016399\n","\tEpoch:39 [003/005 (0520/0693)]\tLoss Ss: 0.008951\n","\tEpoch:39 [003/005 (0540/0693)]\tLoss Ss: 0.015728\n","\tEpoch:39 [003/005 (0560/0693)]\tLoss Ss: 0.010861\n","\tEpoch:39 [003/005 (0580/0693)]\tLoss Ss: 0.009340\n","\tEpoch:39 [003/005 (0600/0693)]\tLoss Ss: 0.012710\n","\tEpoch:39 [003/005 (0620/0693)]\tLoss Ss: 0.014192\n","\tEpoch:39 [003/005 (0640/0693)]\tLoss Ss: 0.010149\n","\tEpoch:39 [003/005 (0660/0693)]\tLoss Ss: 0.017365\n","\tEpoch:39 [003/005 (0680/0693)]\tLoss Ss: 0.011678\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:39 [004/005 (0000/0755)]\tLoss Ss: 0.086470\n","\tEpoch:39 [004/005 (0020/0755)]\tLoss Ss: 0.060271\n","\tEpoch:39 [004/005 (0040/0755)]\tLoss Ss: 0.020289\n","\tEpoch:39 [004/005 (0060/0755)]\tLoss Ss: 0.030816\n","\tEpoch:39 [004/005 (0080/0755)]\tLoss Ss: 0.025193\n","\tEpoch:39 [004/005 (0100/0755)]\tLoss Ss: 0.034360\n","\tEpoch:39 [004/005 (0120/0755)]\tLoss Ss: 0.027506\n","\tEpoch:39 [004/005 (0140/0755)]\tLoss Ss: 0.025802\n","\tEpoch:39 [004/005 (0160/0755)]\tLoss Ss: 0.019201\n","\tEpoch:39 [004/005 (0180/0755)]\tLoss Ss: 0.027656\n","\tEpoch:39 [004/005 (0200/0755)]\tLoss Ss: 0.022757\n","\tEpoch:39 [004/005 (0220/0755)]\tLoss Ss: 0.015585\n","\tEpoch:39 [004/005 (0240/0755)]\tLoss Ss: 0.020120\n","\tEpoch:39 [004/005 (0260/0755)]\tLoss Ss: 0.025986\n","\tEpoch:39 [004/005 (0280/0755)]\tLoss Ss: 0.022034\n","\tEpoch:39 [004/005 (0300/0755)]\tLoss Ss: 0.017600\n","\tEpoch:39 [004/005 (0320/0755)]\tLoss Ss: 0.017361\n","\tEpoch:39 [004/005 (0340/0755)]\tLoss Ss: 0.012371\n","\tEpoch:39 [004/005 (0360/0755)]\tLoss Ss: 0.014525\n","\tEpoch:39 [004/005 (0380/0755)]\tLoss Ss: 0.017578\n","\tEpoch:39 [004/005 (0400/0755)]\tLoss Ss: 0.015472\n","\tEpoch:39 [004/005 (0420/0755)]\tLoss Ss: 0.013567\n","\tEpoch:39 [004/005 (0440/0755)]\tLoss Ss: 0.017199\n","\tEpoch:39 [004/005 (0460/0755)]\tLoss Ss: 0.014519\n","\tEpoch:39 [004/005 (0480/0755)]\tLoss Ss: 0.016606\n","\tEpoch:39 [004/005 (0500/0755)]\tLoss Ss: 0.021022\n","\tEpoch:39 [004/005 (0520/0755)]\tLoss Ss: 0.016235\n","\tEpoch:39 [004/005 (0540/0755)]\tLoss Ss: 0.017717\n","\tEpoch:39 [004/005 (0560/0755)]\tLoss Ss: 0.011898\n","\tEpoch:39 [004/005 (0580/0755)]\tLoss Ss: 0.010557\n","\tEpoch:39 [004/005 (0600/0755)]\tLoss Ss: 0.012285\n","\tEpoch:39 [004/005 (0620/0755)]\tLoss Ss: 0.010017\n","\tEpoch:39 [004/005 (0640/0755)]\tLoss Ss: 0.017620\n","\tEpoch:39 [004/005 (0660/0755)]\tLoss Ss: 0.019309\n","\tEpoch:39 [004/005 (0680/0755)]\tLoss Ss: 0.011945\n","\tEpoch:39 [004/005 (0700/0755)]\tLoss Ss: 0.018968\n","\tEpoch:39 [004/005 (0720/0755)]\tLoss Ss: 0.016452\n","\tEpoch:39 [004/005 (0740/0755)]\tLoss Ss: 0.014674\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:39 [005/005 (0000/0755)]\tLoss Ss: 0.016826\n","\tEpoch:39 [005/005 (0020/0755)]\tLoss Ss: 0.016410\n","\tEpoch:39 [005/005 (0040/0755)]\tLoss Ss: 0.011775\n","\tEpoch:39 [005/005 (0060/0755)]\tLoss Ss: 0.015249\n","\tEpoch:39 [005/005 (0080/0755)]\tLoss Ss: 0.017506\n","\tEpoch:39 [005/005 (0100/0755)]\tLoss Ss: 0.011403\n","\tEpoch:39 [005/005 (0120/0755)]\tLoss Ss: 0.012994\n","\tEpoch:39 [005/005 (0140/0755)]\tLoss Ss: 0.011325\n","\tEpoch:39 [005/005 (0160/0755)]\tLoss Ss: 0.009099\n","\tEpoch:39 [005/005 (0180/0755)]\tLoss Ss: 0.007161\n","\tEpoch:39 [005/005 (0200/0755)]\tLoss Ss: 0.010665\n","\tEpoch:39 [005/005 (0220/0755)]\tLoss Ss: 0.015174\n","\tEpoch:39 [005/005 (0240/0755)]\tLoss Ss: 0.012497\n","\tEpoch:39 [005/005 (0260/0755)]\tLoss Ss: 0.013261\n","\tEpoch:39 [005/005 (0280/0755)]\tLoss Ss: 0.008757\n","\tEpoch:39 [005/005 (0300/0755)]\tLoss Ss: 0.013494\n","\tEpoch:39 [005/005 (0320/0755)]\tLoss Ss: 0.014985\n","\tEpoch:39 [005/005 (0340/0755)]\tLoss Ss: 0.014092\n","\tEpoch:39 [005/005 (0360/0755)]\tLoss Ss: 0.018943\n","\tEpoch:39 [005/005 (0380/0755)]\tLoss Ss: 0.015567\n","\tEpoch:39 [005/005 (0400/0755)]\tLoss Ss: 0.014829\n","\tEpoch:39 [005/005 (0420/0755)]\tLoss Ss: 0.014648\n","\tEpoch:39 [005/005 (0440/0755)]\tLoss Ss: 0.010219\n","\tEpoch:39 [005/005 (0460/0755)]\tLoss Ss: 0.009137\n","\tEpoch:39 [005/005 (0480/0755)]\tLoss Ss: 0.011713\n","\tEpoch:39 [005/005 (0500/0755)]\tLoss Ss: 0.015489\n","\tEpoch:39 [005/005 (0520/0755)]\tLoss Ss: 0.010697\n","\tEpoch:39 [005/005 (0540/0755)]\tLoss Ss: 0.013332\n","\tEpoch:39 [005/005 (0560/0755)]\tLoss Ss: 0.009717\n","\tEpoch:39 [005/005 (0580/0755)]\tLoss Ss: 0.010624\n","\tEpoch:39 [005/005 (0600/0755)]\tLoss Ss: 0.014331\n","\tEpoch:39 [005/005 (0620/0755)]\tLoss Ss: 0.011446\n","\tEpoch:39 [005/005 (0640/0755)]\tLoss Ss: 0.012981\n","\tEpoch:39 [005/005 (0660/0755)]\tLoss Ss: 0.010641\n","\tEpoch:39 [005/005 (0680/0755)]\tLoss Ss: 0.013138\n","\tEpoch:39 [005/005 (0700/0755)]\tLoss Ss: 0.014520\n","\tEpoch:39 [005/005 (0720/0755)]\tLoss Ss: 0.009693\n","\tEpoch:39 [005/005 (0740/0755)]\tLoss Ss: 0.008057\n","Now train the rotated image\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tRotated_Epoch:39 [000/005 (0000/0693)]\tLoss Ss: 0.015444\n","\tRotated_Epoch:39 [000/005 (0020/0693)]\tLoss Ss: 0.019686\n","\tRotated_Epoch:39 [000/005 (0040/0693)]\tLoss Ss: 0.016444\n","\tRotated_Epoch:39 [000/005 (0060/0693)]\tLoss Ss: 0.018437\n","\tRotated_Epoch:39 [000/005 (0080/0693)]\tLoss Ss: 0.008859\n","\tRotated_Epoch:39 [000/005 (0100/0693)]\tLoss Ss: 0.012730\n","\tRotated_Epoch:39 [000/005 (0120/0693)]\tLoss Ss: 0.009547\n","\tRotated_Epoch:39 [000/005 (0140/0693)]\tLoss Ss: 0.012660\n","\tRotated_Epoch:39 [000/005 (0160/0693)]\tLoss Ss: 0.010876\n","\tRotated_Epoch:39 [000/005 (0180/0693)]\tLoss Ss: 0.016675\n","\tRotated_Epoch:39 [000/005 (0200/0693)]\tLoss Ss: 0.013992\n","\tRotated_Epoch:39 [000/005 (0220/0693)]\tLoss Ss: 0.016411\n","\tRotated_Epoch:39 [000/005 (0240/0693)]\tLoss Ss: 0.012375\n","\tRotated_Epoch:39 [000/005 (0260/0693)]\tLoss Ss: 0.011999\n","\tRotated_Epoch:39 [000/005 (0280/0693)]\tLoss Ss: 0.018534\n","\tRotated_Epoch:39 [000/005 (0300/0693)]\tLoss Ss: 0.011332\n","\tRotated_Epoch:39 [000/005 (0320/0693)]\tLoss Ss: 0.011843\n","\tRotated_Epoch:39 [000/005 (0340/0693)]\tLoss Ss: 0.010582\n","\tRotated_Epoch:39 [000/005 (0360/0693)]\tLoss Ss: 0.012509\n","\tRotated_Epoch:39 [000/005 (0380/0693)]\tLoss Ss: 0.009747\n","\tRotated_Epoch:39 [000/005 (0400/0693)]\tLoss Ss: 0.013288\n","\tRotated_Epoch:39 [000/005 (0420/0693)]\tLoss Ss: 0.013583\n","\tRotated_Epoch:39 [000/005 (0440/0693)]\tLoss Ss: 0.008837\n","\tRotated_Epoch:39 [000/005 (0460/0693)]\tLoss Ss: 0.011141\n","\tRotated_Epoch:39 [000/005 (0480/0693)]\tLoss Ss: 0.011502\n","\tRotated_Epoch:39 [000/005 (0500/0693)]\tLoss Ss: 0.013177\n","\tRotated_Epoch:39 [000/005 (0520/0693)]\tLoss Ss: 0.018099\n","\tRotated_Epoch:39 [000/005 (0540/0693)]\tLoss Ss: 0.009082\n","\tRotated_Epoch:39 [000/005 (0560/0693)]\tLoss Ss: 0.014638\n","\tRotated_Epoch:39 [000/005 (0580/0693)]\tLoss Ss: 0.015099\n","\tRotated_Epoch:39 [000/005 (0600/0693)]\tLoss Ss: 0.009877\n","\tRotated_Epoch:39 [000/005 (0620/0693)]\tLoss Ss: 0.012080\n","\tRotated_Epoch:39 [000/005 (0640/0693)]\tLoss Ss: 0.012198\n","\tRotated_Epoch:39 [000/005 (0660/0693)]\tLoss Ss: 0.014752\n","\tRotated_Epoch:39 [000/005 (0680/0693)]\tLoss Ss: 0.012241\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:39 [001/005 (0000/0755)]\tLoss Ss: 0.075186\n","\tRotated_Epoch:39 [001/005 (0020/0755)]\tLoss Ss: 0.069197\n","\tRotated_Epoch:39 [001/005 (0040/0755)]\tLoss Ss: 0.044336\n","\tRotated_Epoch:39 [001/005 (0060/0755)]\tLoss Ss: 0.045536\n","\tRotated_Epoch:39 [001/005 (0080/0755)]\tLoss Ss: 0.068945\n","\tRotated_Epoch:39 [001/005 (0100/0755)]\tLoss Ss: 0.045125\n","\tRotated_Epoch:39 [001/005 (0120/0755)]\tLoss Ss: 0.049682\n","\tRotated_Epoch:39 [001/005 (0140/0755)]\tLoss Ss: 0.024756\n","\tRotated_Epoch:39 [001/005 (0160/0755)]\tLoss Ss: 0.038390\n","\tRotated_Epoch:39 [001/005 (0180/0755)]\tLoss Ss: 0.035108\n","\tRotated_Epoch:39 [001/005 (0200/0755)]\tLoss Ss: 0.027467\n","\tRotated_Epoch:39 [001/005 (0220/0755)]\tLoss Ss: 0.033147\n","\tRotated_Epoch:39 [001/005 (0240/0755)]\tLoss Ss: 0.032400\n","\tRotated_Epoch:39 [001/005 (0260/0755)]\tLoss Ss: 0.033350\n","\tRotated_Epoch:39 [001/005 (0280/0755)]\tLoss Ss: 0.031695\n","\tRotated_Epoch:39 [001/005 (0300/0755)]\tLoss Ss: 0.036974\n","\tRotated_Epoch:39 [001/005 (0320/0755)]\tLoss Ss: 0.028697\n","\tRotated_Epoch:39 [001/005 (0340/0755)]\tLoss Ss: 0.019392\n","\tRotated_Epoch:39 [001/005 (0360/0755)]\tLoss Ss: 0.022632\n","\tRotated_Epoch:39 [001/005 (0380/0755)]\tLoss Ss: 0.034479\n","\tRotated_Epoch:39 [001/005 (0400/0755)]\tLoss Ss: 0.032775\n","\tRotated_Epoch:39 [001/005 (0420/0755)]\tLoss Ss: 0.025587\n","\tRotated_Epoch:39 [001/005 (0440/0755)]\tLoss Ss: 0.026397\n","\tRotated_Epoch:39 [001/005 (0460/0755)]\tLoss Ss: 0.021415\n","\tRotated_Epoch:39 [001/005 (0480/0755)]\tLoss Ss: 0.027398\n","\tRotated_Epoch:39 [001/005 (0500/0755)]\tLoss Ss: 0.028190\n","\tRotated_Epoch:39 [001/005 (0520/0755)]\tLoss Ss: 0.019932\n","\tRotated_Epoch:39 [001/005 (0540/0755)]\tLoss Ss: 0.025125\n","\tRotated_Epoch:39 [001/005 (0560/0755)]\tLoss Ss: 0.044393\n","\tRotated_Epoch:39 [001/005 (0580/0755)]\tLoss Ss: 0.029012\n","\tRotated_Epoch:39 [001/005 (0600/0755)]\tLoss Ss: 0.024292\n","\tRotated_Epoch:39 [001/005 (0620/0755)]\tLoss Ss: 0.030383\n","\tRotated_Epoch:39 [001/005 (0640/0755)]\tLoss Ss: 0.033764\n","\tRotated_Epoch:39 [001/005 (0660/0755)]\tLoss Ss: 0.026708\n","\tRotated_Epoch:39 [001/005 (0680/0755)]\tLoss Ss: 0.027444\n","\tRotated_Epoch:39 [001/005 (0700/0755)]\tLoss Ss: 0.018501\n","\tRotated_Epoch:39 [001/005 (0720/0755)]\tLoss Ss: 0.030853\n","\tRotated_Epoch:39 [001/005 (0740/0755)]\tLoss Ss: 0.031205\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:39 [002/005 (0000/0755)]\tLoss Ss: 0.144227\n","\tRotated_Epoch:39 [002/005 (0020/0755)]\tLoss Ss: 0.082987\n","\tRotated_Epoch:39 [002/005 (0040/0755)]\tLoss Ss: 0.081695\n","\tRotated_Epoch:39 [002/005 (0060/0755)]\tLoss Ss: 0.063461\n","\tRotated_Epoch:39 [002/005 (0080/0755)]\tLoss Ss: 0.029983\n","\tRotated_Epoch:39 [002/005 (0100/0755)]\tLoss Ss: 0.031877\n","\tRotated_Epoch:39 [002/005 (0120/0755)]\tLoss Ss: 0.031570\n","\tRotated_Epoch:39 [002/005 (0140/0755)]\tLoss Ss: 0.040928\n","\tRotated_Epoch:39 [002/005 (0160/0755)]\tLoss Ss: 0.044102\n","\tRotated_Epoch:39 [002/005 (0180/0755)]\tLoss Ss: 0.031343\n","\tRotated_Epoch:39 [002/005 (0200/0755)]\tLoss Ss: 0.045520\n","\tRotated_Epoch:39 [002/005 (0220/0755)]\tLoss Ss: 0.027655\n","\tRotated_Epoch:39 [002/005 (0240/0755)]\tLoss Ss: 0.023137\n","\tRotated_Epoch:39 [002/005 (0260/0755)]\tLoss Ss: 0.025134\n","\tRotated_Epoch:39 [002/005 (0280/0755)]\tLoss Ss: 0.024305\n","\tRotated_Epoch:39 [002/005 (0300/0755)]\tLoss Ss: 0.018499\n","\tRotated_Epoch:39 [002/005 (0320/0755)]\tLoss Ss: 0.013450\n","\tRotated_Epoch:39 [002/005 (0340/0755)]\tLoss Ss: 0.020629\n","\tRotated_Epoch:39 [002/005 (0360/0755)]\tLoss Ss: 0.014437\n","\tRotated_Epoch:39 [002/005 (0380/0755)]\tLoss Ss: 0.021330\n","\tRotated_Epoch:39 [002/005 (0400/0755)]\tLoss Ss: 0.016241\n","\tRotated_Epoch:39 [002/005 (0420/0755)]\tLoss Ss: 0.014018\n","\tRotated_Epoch:39 [002/005 (0440/0755)]\tLoss Ss: 0.019803\n","\tRotated_Epoch:39 [002/005 (0460/0755)]\tLoss Ss: 0.019602\n","\tRotated_Epoch:39 [002/005 (0480/0755)]\tLoss Ss: 0.017780\n","\tRotated_Epoch:39 [002/005 (0500/0755)]\tLoss Ss: 0.015805\n","\tRotated_Epoch:39 [002/005 (0520/0755)]\tLoss Ss: 0.024352\n","\tRotated_Epoch:39 [002/005 (0540/0755)]\tLoss Ss: 0.018176\n","\tRotated_Epoch:39 [002/005 (0560/0755)]\tLoss Ss: 0.010313\n","\tRotated_Epoch:39 [002/005 (0580/0755)]\tLoss Ss: 0.015481\n","\tRotated_Epoch:39 [002/005 (0600/0755)]\tLoss Ss: 0.016929\n","\tRotated_Epoch:39 [002/005 (0620/0755)]\tLoss Ss: 0.013258\n","\tRotated_Epoch:39 [002/005 (0640/0755)]\tLoss Ss: 0.016859\n","\tRotated_Epoch:39 [002/005 (0660/0755)]\tLoss Ss: 0.016137\n","\tRotated_Epoch:39 [002/005 (0680/0755)]\tLoss Ss: 0.014321\n","\tRotated_Epoch:39 [002/005 (0700/0755)]\tLoss Ss: 0.021264\n","\tRotated_Epoch:39 [002/005 (0720/0755)]\tLoss Ss: 0.017287\n","\tRotated_Epoch:39 [002/005 (0740/0755)]\tLoss Ss: 0.018102\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tRotated_Epoch:39 [003/005 (0000/0588)]\tLoss Ss: 0.079236\n","\tRotated_Epoch:39 [003/005 (0020/0588)]\tLoss Ss: 0.079606\n","\tRotated_Epoch:39 [003/005 (0040/0588)]\tLoss Ss: 0.053137\n","\tRotated_Epoch:39 [003/005 (0060/0588)]\tLoss Ss: 0.089383\n","\tRotated_Epoch:39 [003/005 (0080/0588)]\tLoss Ss: 0.054192\n","\tRotated_Epoch:39 [003/005 (0100/0588)]\tLoss Ss: 0.073487\n","\tRotated_Epoch:39 [003/005 (0120/0588)]\tLoss Ss: 0.053465\n","\tRotated_Epoch:39 [003/005 (0140/0588)]\tLoss Ss: 0.047909\n","\tRotated_Epoch:39 [003/005 (0160/0588)]\tLoss Ss: 0.047745\n","\tRotated_Epoch:39 [003/005 (0180/0588)]\tLoss Ss: 0.027902\n","\tRotated_Epoch:39 [003/005 (0200/0588)]\tLoss Ss: 0.097803\n","\tRotated_Epoch:39 [003/005 (0220/0588)]\tLoss Ss: 0.036761\n","\tRotated_Epoch:39 [003/005 (0240/0588)]\tLoss Ss: 0.074671\n","\tRotated_Epoch:39 [003/005 (0260/0588)]\tLoss Ss: 0.068310\n","\tRotated_Epoch:39 [003/005 (0280/0588)]\tLoss Ss: 0.056785\n","\tRotated_Epoch:39 [003/005 (0300/0588)]\tLoss Ss: 0.055985\n","\tRotated_Epoch:39 [003/005 (0320/0588)]\tLoss Ss: 0.056306\n","\tRotated_Epoch:39 [003/005 (0340/0588)]\tLoss Ss: 0.068172\n","\tRotated_Epoch:39 [003/005 (0360/0588)]\tLoss Ss: 0.050104\n","\tRotated_Epoch:39 [003/005 (0380/0588)]\tLoss Ss: 0.067496\n","\tRotated_Epoch:39 [003/005 (0400/0588)]\tLoss Ss: 0.049471\n","\tRotated_Epoch:39 [003/005 (0420/0588)]\tLoss Ss: 0.060000\n","\tRotated_Epoch:39 [003/005 (0440/0588)]\tLoss Ss: 0.056788\n","\tRotated_Epoch:39 [003/005 (0460/0588)]\tLoss Ss: 0.045715\n","\tRotated_Epoch:39 [003/005 (0480/0588)]\tLoss Ss: 0.061950\n","\tRotated_Epoch:39 [003/005 (0500/0588)]\tLoss Ss: 0.064046\n","\tRotated_Epoch:39 [003/005 (0520/0588)]\tLoss Ss: 0.060816\n","\tRotated_Epoch:39 [003/005 (0540/0588)]\tLoss Ss: 0.044925\n","\tRotated_Epoch:39 [003/005 (0560/0588)]\tLoss Ss: 0.040026\n","\tRotated_Epoch:39 [003/005 (0580/0588)]\tLoss Ss: 0.045367\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tRotated_Epoch:39 [004/005 (0000/0693)]\tLoss Ss: 0.014922\n","\tRotated_Epoch:39 [004/005 (0020/0693)]\tLoss Ss: 0.017400\n","\tRotated_Epoch:39 [004/005 (0040/0693)]\tLoss Ss: 0.018375\n","\tRotated_Epoch:39 [004/005 (0060/0693)]\tLoss Ss: 0.015955\n","\tRotated_Epoch:39 [004/005 (0080/0693)]\tLoss Ss: 0.016423\n","\tRotated_Epoch:39 [004/005 (0100/0693)]\tLoss Ss: 0.012065\n","\tRotated_Epoch:39 [004/005 (0120/0693)]\tLoss Ss: 0.017491\n","\tRotated_Epoch:39 [004/005 (0140/0693)]\tLoss Ss: 0.014310\n","\tRotated_Epoch:39 [004/005 (0160/0693)]\tLoss Ss: 0.019839\n","\tRotated_Epoch:39 [004/005 (0180/0693)]\tLoss Ss: 0.013999\n","\tRotated_Epoch:39 [004/005 (0200/0693)]\tLoss Ss: 0.012532\n","\tRotated_Epoch:39 [004/005 (0220/0693)]\tLoss Ss: 0.010547\n","\tRotated_Epoch:39 [004/005 (0240/0693)]\tLoss Ss: 0.010911\n","\tRotated_Epoch:39 [004/005 (0260/0693)]\tLoss Ss: 0.023056\n","\tRotated_Epoch:39 [004/005 (0280/0693)]\tLoss Ss: 0.013257\n","\tRotated_Epoch:39 [004/005 (0300/0693)]\tLoss Ss: 0.013943\n","\tRotated_Epoch:39 [004/005 (0320/0693)]\tLoss Ss: 0.011442\n","\tRotated_Epoch:39 [004/005 (0340/0693)]\tLoss Ss: 0.015885\n","\tRotated_Epoch:39 [004/005 (0360/0693)]\tLoss Ss: 0.019851\n","\tRotated_Epoch:39 [004/005 (0380/0693)]\tLoss Ss: 0.010206\n","\tRotated_Epoch:39 [004/005 (0400/0693)]\tLoss Ss: 0.017712\n","\tRotated_Epoch:39 [004/005 (0420/0693)]\tLoss Ss: 0.012963\n","\tRotated_Epoch:39 [004/005 (0440/0693)]\tLoss Ss: 0.019216\n","\tRotated_Epoch:39 [004/005 (0460/0693)]\tLoss Ss: 0.019243\n","\tRotated_Epoch:39 [004/005 (0480/0693)]\tLoss Ss: 0.018031\n","\tRotated_Epoch:39 [004/005 (0500/0693)]\tLoss Ss: 0.013150\n","\tRotated_Epoch:39 [004/005 (0520/0693)]\tLoss Ss: 0.016123\n","\tRotated_Epoch:39 [004/005 (0540/0693)]\tLoss Ss: 0.012620\n","\tRotated_Epoch:39 [004/005 (0560/0693)]\tLoss Ss: 0.015083\n","\tRotated_Epoch:39 [004/005 (0580/0693)]\tLoss Ss: 0.021123\n","\tRotated_Epoch:39 [004/005 (0600/0693)]\tLoss Ss: 0.015019\n","\tRotated_Epoch:39 [004/005 (0620/0693)]\tLoss Ss: 0.011645\n","\tRotated_Epoch:39 [004/005 (0640/0693)]\tLoss Ss: 0.011464\n","\tRotated_Epoch:39 [004/005 (0660/0693)]\tLoss Ss: 0.014411\n","\tRotated_Epoch:39 [004/005 (0680/0693)]\tLoss Ss: 0.017739\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tRotated_Epoch:39 [005/005 (0000/0614)]\tLoss Ss: 0.015876\n","\tRotated_Epoch:39 [005/005 (0020/0614)]\tLoss Ss: 0.038196\n","\tRotated_Epoch:39 [005/005 (0040/0614)]\tLoss Ss: 0.025922\n","\tRotated_Epoch:39 [005/005 (0060/0614)]\tLoss Ss: 0.009108\n","\tRotated_Epoch:39 [005/005 (0080/0614)]\tLoss Ss: 0.009496\n","\tRotated_Epoch:39 [005/005 (0100/0614)]\tLoss Ss: 0.012046\n","\tRotated_Epoch:39 [005/005 (0120/0614)]\tLoss Ss: 0.007400\n","\tRotated_Epoch:39 [005/005 (0140/0614)]\tLoss Ss: 0.010474\n","\tRotated_Epoch:39 [005/005 (0160/0614)]\tLoss Ss: 0.005416\n","\tRotated_Epoch:39 [005/005 (0180/0614)]\tLoss Ss: 0.008334\n","\tRotated_Epoch:39 [005/005 (0200/0614)]\tLoss Ss: 0.007094\n","\tRotated_Epoch:39 [005/005 (0220/0614)]\tLoss Ss: 0.008474\n","\tRotated_Epoch:39 [005/005 (0240/0614)]\tLoss Ss: 0.007000\n","\tRotated_Epoch:39 [005/005 (0260/0614)]\tLoss Ss: 0.005210\n","\tRotated_Epoch:39 [005/005 (0280/0614)]\tLoss Ss: 0.007060\n","\tRotated_Epoch:39 [005/005 (0300/0614)]\tLoss Ss: 0.006728\n","\tRotated_Epoch:39 [005/005 (0320/0614)]\tLoss Ss: 0.008245\n","\tRotated_Epoch:39 [005/005 (0340/0614)]\tLoss Ss: 0.004263\n","\tRotated_Epoch:39 [005/005 (0360/0614)]\tLoss Ss: 0.008245\n","\tRotated_Epoch:39 [005/005 (0380/0614)]\tLoss Ss: 0.004230\n","\tRotated_Epoch:39 [005/005 (0400/0614)]\tLoss Ss: 0.007198\n","\tRotated_Epoch:39 [005/005 (0420/0614)]\tLoss Ss: 0.008342\n","\tRotated_Epoch:39 [005/005 (0440/0614)]\tLoss Ss: 0.006775\n","\tRotated_Epoch:39 [005/005 (0460/0614)]\tLoss Ss: 0.005327\n","\tRotated_Epoch:39 [005/005 (0480/0614)]\tLoss Ss: 0.005891\n","\tRotated_Epoch:39 [005/005 (0500/0614)]\tLoss Ss: 0.007773\n","\tRotated_Epoch:39 [005/005 (0520/0614)]\tLoss Ss: 0.005470\n","\tRotated_Epoch:39 [005/005 (0540/0614)]\tLoss Ss: 0.007806\n","\tRotated_Epoch:39 [005/005 (0560/0614)]\tLoss Ss: 0.006040\n","\tRotated_Epoch:39 [005/005 (0580/0614)]\tLoss Ss: 0.008243\n","\tRotated_Epoch:39 [005/005 (0600/0614)]\tLoss Ss: 0.007066\n","new_1_6_d.nii\n","new_1_6_m.nii\n","new_1_7_d.nii\n","new_1_7_m.nii\n","new_2_5_d.nii\n","new_2_5_m.nii\n","new_2_6_d.nii\n","new_2_6_m.nii\n","new_2_7_d.nii\n","new_2_7_m.nii\n","new_3_d (2).nii\n","new_3_m (2).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (3).nii\n","new_3_m (3).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (7).nii\n","new_3_m (7).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (8).nii\n","new_3_m (8).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (9).nii\n","new_3_m (9).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_4_10_d_163526.nii\n","new_4_10_m_163526.nii.gz\n","new_4_11_d_084336.nii\n","new_4_11_m_084336.nii.gz\n","new_4_7_d_184422.nii\n","new_4_7_m_184422.nii.gz\n","new_4_8_d_143412.nii\n","new_4_8_m_143412.nii.gz\n","new_4_9_d_155347.nii\n","new_4_9_m_155347.nii.gz\n","\tEpoch: 39; Dice: 0.9674 +/- 0.0032; Loss: 8.2484\n","Begin Epoch 40\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tEpoch:40 [000/005 (0000/0693)]\tLoss Ss: 0.012473\n","\tEpoch:40 [000/005 (0020/0693)]\tLoss Ss: 0.013978\n","\tEpoch:40 [000/005 (0040/0693)]\tLoss Ss: 0.015639\n","\tEpoch:40 [000/005 (0060/0693)]\tLoss Ss: 0.013605\n","\tEpoch:40 [000/005 (0080/0693)]\tLoss Ss: 0.012869\n","\tEpoch:40 [000/005 (0100/0693)]\tLoss Ss: 0.012894\n","\tEpoch:40 [000/005 (0120/0693)]\tLoss Ss: 0.006498\n","\tEpoch:40 [000/005 (0140/0693)]\tLoss Ss: 0.008487\n","\tEpoch:40 [000/005 (0160/0693)]\tLoss Ss: 0.009761\n","\tEpoch:40 [000/005 (0180/0693)]\tLoss Ss: 0.010096\n","\tEpoch:40 [000/005 (0200/0693)]\tLoss Ss: 0.009492\n","\tEpoch:40 [000/005 (0220/0693)]\tLoss Ss: 0.016462\n","\tEpoch:40 [000/005 (0240/0693)]\tLoss Ss: 0.016987\n","\tEpoch:40 [000/005 (0260/0693)]\tLoss Ss: 0.010447\n","\tEpoch:40 [000/005 (0280/0693)]\tLoss Ss: 0.012234\n","\tEpoch:40 [000/005 (0300/0693)]\tLoss Ss: 0.016952\n","\tEpoch:40 [000/005 (0320/0693)]\tLoss Ss: 0.013254\n","\tEpoch:40 [000/005 (0340/0693)]\tLoss Ss: 0.013494\n","\tEpoch:40 [000/005 (0360/0693)]\tLoss Ss: 0.008591\n","\tEpoch:40 [000/005 (0380/0693)]\tLoss Ss: 0.012202\n","\tEpoch:40 [000/005 (0400/0693)]\tLoss Ss: 0.013753\n","\tEpoch:40 [000/005 (0420/0693)]\tLoss Ss: 0.011409\n","\tEpoch:40 [000/005 (0440/0693)]\tLoss Ss: 0.012485\n","\tEpoch:40 [000/005 (0460/0693)]\tLoss Ss: 0.007831\n","\tEpoch:40 [000/005 (0480/0693)]\tLoss Ss: 0.009933\n","\tEpoch:40 [000/005 (0500/0693)]\tLoss Ss: 0.013216\n","\tEpoch:40 [000/005 (0520/0693)]\tLoss Ss: 0.012364\n","\tEpoch:40 [000/005 (0540/0693)]\tLoss Ss: 0.010292\n","\tEpoch:40 [000/005 (0560/0693)]\tLoss Ss: 0.011222\n","\tEpoch:40 [000/005 (0580/0693)]\tLoss Ss: 0.011123\n","\tEpoch:40 [000/005 (0600/0693)]\tLoss Ss: 0.009614\n","\tEpoch:40 [000/005 (0620/0693)]\tLoss Ss: 0.010976\n","\tEpoch:40 [000/005 (0640/0693)]\tLoss Ss: 0.008599\n","\tEpoch:40 [000/005 (0660/0693)]\tLoss Ss: 0.018208\n","\tEpoch:40 [000/005 (0680/0693)]\tLoss Ss: 0.009337\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:40 [001/005 (0000/0755)]\tLoss Ss: 0.023101\n","\tEpoch:40 [001/005 (0020/0755)]\tLoss Ss: 0.013741\n","\tEpoch:40 [001/005 (0040/0755)]\tLoss Ss: 0.013849\n","\tEpoch:40 [001/005 (0060/0755)]\tLoss Ss: 0.016502\n","\tEpoch:40 [001/005 (0080/0755)]\tLoss Ss: 0.014821\n","\tEpoch:40 [001/005 (0100/0755)]\tLoss Ss: 0.025386\n","\tEpoch:40 [001/005 (0120/0755)]\tLoss Ss: 0.018856\n","\tEpoch:40 [001/005 (0140/0755)]\tLoss Ss: 0.014306\n","\tEpoch:40 [001/005 (0160/0755)]\tLoss Ss: 0.013799\n","\tEpoch:40 [001/005 (0180/0755)]\tLoss Ss: 0.018615\n","\tEpoch:40 [001/005 (0200/0755)]\tLoss Ss: 0.012611\n","\tEpoch:40 [001/005 (0220/0755)]\tLoss Ss: 0.018378\n","\tEpoch:40 [001/005 (0240/0755)]\tLoss Ss: 0.009310\n","\tEpoch:40 [001/005 (0260/0755)]\tLoss Ss: 0.014707\n","\tEpoch:40 [001/005 (0280/0755)]\tLoss Ss: 0.015596\n","\tEpoch:40 [001/005 (0300/0755)]\tLoss Ss: 0.023803\n","\tEpoch:40 [001/005 (0320/0755)]\tLoss Ss: 0.012095\n","\tEpoch:40 [001/005 (0340/0755)]\tLoss Ss: 0.015212\n","\tEpoch:40 [001/005 (0360/0755)]\tLoss Ss: 0.012696\n","\tEpoch:40 [001/005 (0380/0755)]\tLoss Ss: 0.012538\n","\tEpoch:40 [001/005 (0400/0755)]\tLoss Ss: 0.019318\n","\tEpoch:40 [001/005 (0420/0755)]\tLoss Ss: 0.011331\n","\tEpoch:40 [001/005 (0440/0755)]\tLoss Ss: 0.016339\n","\tEpoch:40 [001/005 (0460/0755)]\tLoss Ss: 0.010046\n","\tEpoch:40 [001/005 (0480/0755)]\tLoss Ss: 0.012664\n","\tEpoch:40 [001/005 (0500/0755)]\tLoss Ss: 0.016107\n","\tEpoch:40 [001/005 (0520/0755)]\tLoss Ss: 0.012934\n","\tEpoch:40 [001/005 (0540/0755)]\tLoss Ss: 0.016974\n","\tEpoch:40 [001/005 (0560/0755)]\tLoss Ss: 0.011087\n","\tEpoch:40 [001/005 (0580/0755)]\tLoss Ss: 0.014978\n","\tEpoch:40 [001/005 (0600/0755)]\tLoss Ss: 0.010685\n","\tEpoch:40 [001/005 (0620/0755)]\tLoss Ss: 0.015168\n","\tEpoch:40 [001/005 (0640/0755)]\tLoss Ss: 0.014871\n","\tEpoch:40 [001/005 (0660/0755)]\tLoss Ss: 0.012958\n","\tEpoch:40 [001/005 (0680/0755)]\tLoss Ss: 0.010356\n","\tEpoch:40 [001/005 (0700/0755)]\tLoss Ss: 0.017781\n","\tEpoch:40 [001/005 (0720/0755)]\tLoss Ss: 0.012305\n","\tEpoch:40 [001/005 (0740/0755)]\tLoss Ss: 0.012337\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tEpoch:40 [002/005 (0000/0614)]\tLoss Ss: 0.003851\n","\tEpoch:40 [002/005 (0020/0614)]\tLoss Ss: 0.005469\n","\tEpoch:40 [002/005 (0040/0614)]\tLoss Ss: 0.005364\n","\tEpoch:40 [002/005 (0060/0614)]\tLoss Ss: 0.012946\n","\tEpoch:40 [002/005 (0080/0614)]\tLoss Ss: 0.005680\n","\tEpoch:40 [002/005 (0100/0614)]\tLoss Ss: 0.002687\n","\tEpoch:40 [002/005 (0120/0614)]\tLoss Ss: 0.005817\n","\tEpoch:40 [002/005 (0140/0614)]\tLoss Ss: 0.005871\n","\tEpoch:40 [002/005 (0160/0614)]\tLoss Ss: 0.005459\n","\tEpoch:40 [002/005 (0180/0614)]\tLoss Ss: 0.006363\n","\tEpoch:40 [002/005 (0200/0614)]\tLoss Ss: 0.007563\n","\tEpoch:40 [002/005 (0220/0614)]\tLoss Ss: 0.006362\n","\tEpoch:40 [002/005 (0240/0614)]\tLoss Ss: 0.006591\n","\tEpoch:40 [002/005 (0260/0614)]\tLoss Ss: 0.004692\n","\tEpoch:40 [002/005 (0280/0614)]\tLoss Ss: 0.004767\n","\tEpoch:40 [002/005 (0300/0614)]\tLoss Ss: 0.005099\n","\tEpoch:40 [002/005 (0320/0614)]\tLoss Ss: 0.007334\n","\tEpoch:40 [002/005 (0340/0614)]\tLoss Ss: 0.004026\n","\tEpoch:40 [002/005 (0360/0614)]\tLoss Ss: 0.002695\n","\tEpoch:40 [002/005 (0380/0614)]\tLoss Ss: 0.005005\n","\tEpoch:40 [002/005 (0400/0614)]\tLoss Ss: 0.006061\n","\tEpoch:40 [002/005 (0420/0614)]\tLoss Ss: 0.004478\n","\tEpoch:40 [002/005 (0440/0614)]\tLoss Ss: 0.006892\n","\tEpoch:40 [002/005 (0460/0614)]\tLoss Ss: 0.004412\n","\tEpoch:40 [002/005 (0480/0614)]\tLoss Ss: 0.003647\n","\tEpoch:40 [002/005 (0500/0614)]\tLoss Ss: 0.003657\n","\tEpoch:40 [002/005 (0520/0614)]\tLoss Ss: 0.005308\n","\tEpoch:40 [002/005 (0540/0614)]\tLoss Ss: 0.005736\n","\tEpoch:40 [002/005 (0560/0614)]\tLoss Ss: 0.008059\n","\tEpoch:40 [002/005 (0580/0614)]\tLoss Ss: 0.003755\n","\tEpoch:40 [002/005 (0600/0614)]\tLoss Ss: 0.004264\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tEpoch:40 [003/005 (0000/0693)]\tLoss Ss: 0.008786\n","\tEpoch:40 [003/005 (0020/0693)]\tLoss Ss: 0.015816\n","\tEpoch:40 [003/005 (0040/0693)]\tLoss Ss: 0.012272\n","\tEpoch:40 [003/005 (0060/0693)]\tLoss Ss: 0.017717\n","\tEpoch:40 [003/005 (0080/0693)]\tLoss Ss: 0.013576\n","\tEpoch:40 [003/005 (0100/0693)]\tLoss Ss: 0.014751\n","\tEpoch:40 [003/005 (0120/0693)]\tLoss Ss: 0.011589\n","\tEpoch:40 [003/005 (0140/0693)]\tLoss Ss: 0.015349\n","\tEpoch:40 [003/005 (0160/0693)]\tLoss Ss: 0.011681\n","\tEpoch:40 [003/005 (0180/0693)]\tLoss Ss: 0.009307\n","\tEpoch:40 [003/005 (0200/0693)]\tLoss Ss: 0.015767\n","\tEpoch:40 [003/005 (0220/0693)]\tLoss Ss: 0.014096\n","\tEpoch:40 [003/005 (0240/0693)]\tLoss Ss: 0.010977\n","\tEpoch:40 [003/005 (0260/0693)]\tLoss Ss: 0.012953\n","\tEpoch:40 [003/005 (0280/0693)]\tLoss Ss: 0.010444\n","\tEpoch:40 [003/005 (0300/0693)]\tLoss Ss: 0.012691\n","\tEpoch:40 [003/005 (0320/0693)]\tLoss Ss: 0.010684\n","\tEpoch:40 [003/005 (0340/0693)]\tLoss Ss: 0.016029\n","\tEpoch:40 [003/005 (0360/0693)]\tLoss Ss: 0.010345\n","\tEpoch:40 [003/005 (0380/0693)]\tLoss Ss: 0.016965\n","\tEpoch:40 [003/005 (0400/0693)]\tLoss Ss: 0.014101\n","\tEpoch:40 [003/005 (0420/0693)]\tLoss Ss: 0.016981\n","\tEpoch:40 [003/005 (0440/0693)]\tLoss Ss: 0.009674\n","\tEpoch:40 [003/005 (0460/0693)]\tLoss Ss: 0.016856\n","\tEpoch:40 [003/005 (0480/0693)]\tLoss Ss: 0.010565\n","\tEpoch:40 [003/005 (0500/0693)]\tLoss Ss: 0.013932\n","\tEpoch:40 [003/005 (0520/0693)]\tLoss Ss: 0.010633\n","\tEpoch:40 [003/005 (0540/0693)]\tLoss Ss: 0.010974\n","\tEpoch:40 [003/005 (0560/0693)]\tLoss Ss: 0.012661\n","\tEpoch:40 [003/005 (0580/0693)]\tLoss Ss: 0.011702\n","\tEpoch:40 [003/005 (0600/0693)]\tLoss Ss: 0.012948\n","\tEpoch:40 [003/005 (0620/0693)]\tLoss Ss: 0.010283\n","\tEpoch:40 [003/005 (0640/0693)]\tLoss Ss: 0.012841\n","\tEpoch:40 [003/005 (0660/0693)]\tLoss Ss: 0.009798\n","\tEpoch:40 [003/005 (0680/0693)]\tLoss Ss: 0.015842\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:40 [004/005 (0000/0755)]\tLoss Ss: 0.014080\n","\tEpoch:40 [004/005 (0020/0755)]\tLoss Ss: 0.014123\n","\tEpoch:40 [004/005 (0040/0755)]\tLoss Ss: 0.013774\n","\tEpoch:40 [004/005 (0060/0755)]\tLoss Ss: 0.014723\n","\tEpoch:40 [004/005 (0080/0755)]\tLoss Ss: 0.009542\n","\tEpoch:40 [004/005 (0100/0755)]\tLoss Ss: 0.012965\n","\tEpoch:40 [004/005 (0120/0755)]\tLoss Ss: 0.012517\n","\tEpoch:40 [004/005 (0140/0755)]\tLoss Ss: 0.007534\n","\tEpoch:40 [004/005 (0160/0755)]\tLoss Ss: 0.018947\n","\tEpoch:40 [004/005 (0180/0755)]\tLoss Ss: 0.013523\n","\tEpoch:40 [004/005 (0200/0755)]\tLoss Ss: 0.011371\n","\tEpoch:40 [004/005 (0220/0755)]\tLoss Ss: 0.018003\n","\tEpoch:40 [004/005 (0240/0755)]\tLoss Ss: 0.015900\n","\tEpoch:40 [004/005 (0260/0755)]\tLoss Ss: 0.013543\n","\tEpoch:40 [004/005 (0280/0755)]\tLoss Ss: 0.010103\n","\tEpoch:40 [004/005 (0300/0755)]\tLoss Ss: 0.010841\n","\tEpoch:40 [004/005 (0320/0755)]\tLoss Ss: 0.009972\n","\tEpoch:40 [004/005 (0340/0755)]\tLoss Ss: 0.010136\n","\tEpoch:40 [004/005 (0360/0755)]\tLoss Ss: 0.009822\n","\tEpoch:40 [004/005 (0380/0755)]\tLoss Ss: 0.011219\n","\tEpoch:40 [004/005 (0400/0755)]\tLoss Ss: 0.010763\n","\tEpoch:40 [004/005 (0420/0755)]\tLoss Ss: 0.012463\n","\tEpoch:40 [004/005 (0440/0755)]\tLoss Ss: 0.012454\n","\tEpoch:40 [004/005 (0460/0755)]\tLoss Ss: 0.009709\n","\tEpoch:40 [004/005 (0480/0755)]\tLoss Ss: 0.013667\n","\tEpoch:40 [004/005 (0500/0755)]\tLoss Ss: 0.009431\n","\tEpoch:40 [004/005 (0520/0755)]\tLoss Ss: 0.009750\n","\tEpoch:40 [004/005 (0540/0755)]\tLoss Ss: 0.008220\n","\tEpoch:40 [004/005 (0560/0755)]\tLoss Ss: 0.007840\n","\tEpoch:40 [004/005 (0580/0755)]\tLoss Ss: 0.010001\n","\tEpoch:40 [004/005 (0600/0755)]\tLoss Ss: 0.010758\n","\tEpoch:40 [004/005 (0620/0755)]\tLoss Ss: 0.017456\n","\tEpoch:40 [004/005 (0640/0755)]\tLoss Ss: 0.015152\n","\tEpoch:40 [004/005 (0660/0755)]\tLoss Ss: 0.009987\n","\tEpoch:40 [004/005 (0680/0755)]\tLoss Ss: 0.012713\n","\tEpoch:40 [004/005 (0700/0755)]\tLoss Ss: 0.010004\n","\tEpoch:40 [004/005 (0720/0755)]\tLoss Ss: 0.010505\n","\tEpoch:40 [004/005 (0740/0755)]\tLoss Ss: 0.014674\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tEpoch:40 [005/005 (0000/0588)]\tLoss Ss: 0.006387\n","\tEpoch:40 [005/005 (0020/0588)]\tLoss Ss: 0.004157\n","\tEpoch:40 [005/005 (0040/0588)]\tLoss Ss: 0.003656\n","\tEpoch:40 [005/005 (0060/0588)]\tLoss Ss: 0.007132\n","\tEpoch:40 [005/005 (0080/0588)]\tLoss Ss: 0.004180\n","\tEpoch:40 [005/005 (0100/0588)]\tLoss Ss: 0.003532\n","\tEpoch:40 [005/005 (0120/0588)]\tLoss Ss: 0.003135\n","\tEpoch:40 [005/005 (0140/0588)]\tLoss Ss: 0.006532\n","\tEpoch:40 [005/005 (0160/0588)]\tLoss Ss: 0.006512\n","\tEpoch:40 [005/005 (0180/0588)]\tLoss Ss: 0.004336\n","\tEpoch:40 [005/005 (0200/0588)]\tLoss Ss: 0.002894\n","\tEpoch:40 [005/005 (0220/0588)]\tLoss Ss: 0.004111\n","\tEpoch:40 [005/005 (0240/0588)]\tLoss Ss: 0.003250\n","\tEpoch:40 [005/005 (0260/0588)]\tLoss Ss: 0.003508\n","\tEpoch:40 [005/005 (0280/0588)]\tLoss Ss: 0.003294\n","\tEpoch:40 [005/005 (0300/0588)]\tLoss Ss: 0.004852\n","\tEpoch:40 [005/005 (0320/0588)]\tLoss Ss: 0.005041\n","\tEpoch:40 [005/005 (0340/0588)]\tLoss Ss: 0.004460\n","\tEpoch:40 [005/005 (0360/0588)]\tLoss Ss: 0.006016\n","\tEpoch:40 [005/005 (0380/0588)]\tLoss Ss: 0.003177\n","\tEpoch:40 [005/005 (0400/0588)]\tLoss Ss: 0.003970\n","\tEpoch:40 [005/005 (0420/0588)]\tLoss Ss: 0.004181\n","\tEpoch:40 [005/005 (0440/0588)]\tLoss Ss: 0.002921\n","\tEpoch:40 [005/005 (0460/0588)]\tLoss Ss: 0.003026\n","\tEpoch:40 [005/005 (0480/0588)]\tLoss Ss: 0.003764\n","\tEpoch:40 [005/005 (0500/0588)]\tLoss Ss: 0.004821\n","\tEpoch:40 [005/005 (0520/0588)]\tLoss Ss: 0.004615\n","\tEpoch:40 [005/005 (0540/0588)]\tLoss Ss: 0.005623\n","\tEpoch:40 [005/005 (0560/0588)]\tLoss Ss: 0.003327\n","\tEpoch:40 [005/005 (0580/0588)]\tLoss Ss: 0.002628\n","Now train the rotated image\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tRotated_Epoch:40 [000/005 (0000/0693)]\tLoss Ss: 0.012740\n","\tRotated_Epoch:40 [000/005 (0020/0693)]\tLoss Ss: 0.013980\n","\tRotated_Epoch:40 [000/005 (0040/0693)]\tLoss Ss: 0.016463\n","\tRotated_Epoch:40 [000/005 (0060/0693)]\tLoss Ss: 0.012682\n","\tRotated_Epoch:40 [000/005 (0080/0693)]\tLoss Ss: 0.012292\n","\tRotated_Epoch:40 [000/005 (0100/0693)]\tLoss Ss: 0.012350\n","\tRotated_Epoch:40 [000/005 (0120/0693)]\tLoss Ss: 0.012109\n","\tRotated_Epoch:40 [000/005 (0140/0693)]\tLoss Ss: 0.014164\n","\tRotated_Epoch:40 [000/005 (0160/0693)]\tLoss Ss: 0.012176\n","\tRotated_Epoch:40 [000/005 (0180/0693)]\tLoss Ss: 0.013058\n","\tRotated_Epoch:40 [000/005 (0200/0693)]\tLoss Ss: 0.016763\n","\tRotated_Epoch:40 [000/005 (0220/0693)]\tLoss Ss: 0.014348\n","\tRotated_Epoch:40 [000/005 (0240/0693)]\tLoss Ss: 0.009190\n","\tRotated_Epoch:40 [000/005 (0260/0693)]\tLoss Ss: 0.013546\n","\tRotated_Epoch:40 [000/005 (0280/0693)]\tLoss Ss: 0.009358\n","\tRotated_Epoch:40 [000/005 (0300/0693)]\tLoss Ss: 0.013685\n","\tRotated_Epoch:40 [000/005 (0320/0693)]\tLoss Ss: 0.012405\n","\tRotated_Epoch:40 [000/005 (0340/0693)]\tLoss Ss: 0.013232\n","\tRotated_Epoch:40 [000/005 (0360/0693)]\tLoss Ss: 0.015602\n","\tRotated_Epoch:40 [000/005 (0380/0693)]\tLoss Ss: 0.014760\n","\tRotated_Epoch:40 [000/005 (0400/0693)]\tLoss Ss: 0.012054\n","\tRotated_Epoch:40 [000/005 (0420/0693)]\tLoss Ss: 0.013135\n","\tRotated_Epoch:40 [000/005 (0440/0693)]\tLoss Ss: 0.013967\n","\tRotated_Epoch:40 [000/005 (0460/0693)]\tLoss Ss: 0.015410\n","\tRotated_Epoch:40 [000/005 (0480/0693)]\tLoss Ss: 0.011230\n","\tRotated_Epoch:40 [000/005 (0500/0693)]\tLoss Ss: 0.009277\n","\tRotated_Epoch:40 [000/005 (0520/0693)]\tLoss Ss: 0.007929\n","\tRotated_Epoch:40 [000/005 (0540/0693)]\tLoss Ss: 0.014515\n","\tRotated_Epoch:40 [000/005 (0560/0693)]\tLoss Ss: 0.009608\n","\tRotated_Epoch:40 [000/005 (0580/0693)]\tLoss Ss: 0.012599\n","\tRotated_Epoch:40 [000/005 (0600/0693)]\tLoss Ss: 0.013792\n","\tRotated_Epoch:40 [000/005 (0620/0693)]\tLoss Ss: 0.011414\n","\tRotated_Epoch:40 [000/005 (0640/0693)]\tLoss Ss: 0.005204\n","\tRotated_Epoch:40 [000/005 (0660/0693)]\tLoss Ss: 0.009745\n","\tRotated_Epoch:40 [000/005 (0680/0693)]\tLoss Ss: 0.006919\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tRotated_Epoch:40 [001/005 (0000/0614)]\tLoss Ss: 0.005411\n","\tRotated_Epoch:40 [001/005 (0020/0614)]\tLoss Ss: 0.006492\n","\tRotated_Epoch:40 [001/005 (0040/0614)]\tLoss Ss: 0.005732\n","\tRotated_Epoch:40 [001/005 (0060/0614)]\tLoss Ss: 0.005792\n","\tRotated_Epoch:40 [001/005 (0080/0614)]\tLoss Ss: 0.007343\n","\tRotated_Epoch:40 [001/005 (0100/0614)]\tLoss Ss: 0.006539\n","\tRotated_Epoch:40 [001/005 (0120/0614)]\tLoss Ss: 0.006052\n","\tRotated_Epoch:40 [001/005 (0140/0614)]\tLoss Ss: 0.005293\n","\tRotated_Epoch:40 [001/005 (0160/0614)]\tLoss Ss: 0.008316\n","\tRotated_Epoch:40 [001/005 (0180/0614)]\tLoss Ss: 0.004550\n","\tRotated_Epoch:40 [001/005 (0200/0614)]\tLoss Ss: 0.003863\n","\tRotated_Epoch:40 [001/005 (0220/0614)]\tLoss Ss: 0.003392\n","\tRotated_Epoch:40 [001/005 (0240/0614)]\tLoss Ss: 0.006457\n","\tRotated_Epoch:40 [001/005 (0260/0614)]\tLoss Ss: 0.004640\n","\tRotated_Epoch:40 [001/005 (0280/0614)]\tLoss Ss: 0.006245\n","\tRotated_Epoch:40 [001/005 (0300/0614)]\tLoss Ss: 0.006801\n","\tRotated_Epoch:40 [001/005 (0320/0614)]\tLoss Ss: 0.004248\n","\tRotated_Epoch:40 [001/005 (0340/0614)]\tLoss Ss: 0.003441\n","\tRotated_Epoch:40 [001/005 (0360/0614)]\tLoss Ss: 0.006147\n","\tRotated_Epoch:40 [001/005 (0380/0614)]\tLoss Ss: 0.004180\n","\tRotated_Epoch:40 [001/005 (0400/0614)]\tLoss Ss: 0.003424\n","\tRotated_Epoch:40 [001/005 (0420/0614)]\tLoss Ss: 0.006091\n","\tRotated_Epoch:40 [001/005 (0440/0614)]\tLoss Ss: 0.006321\n","\tRotated_Epoch:40 [001/005 (0460/0614)]\tLoss Ss: 0.004523\n","\tRotated_Epoch:40 [001/005 (0480/0614)]\tLoss Ss: 0.004712\n","\tRotated_Epoch:40 [001/005 (0500/0614)]\tLoss Ss: 0.006030\n","\tRotated_Epoch:40 [001/005 (0520/0614)]\tLoss Ss: 0.006843\n","\tRotated_Epoch:40 [001/005 (0540/0614)]\tLoss Ss: 0.005255\n","\tRotated_Epoch:40 [001/005 (0560/0614)]\tLoss Ss: 0.005976\n","\tRotated_Epoch:40 [001/005 (0580/0614)]\tLoss Ss: 0.004532\n","\tRotated_Epoch:40 [001/005 (0600/0614)]\tLoss Ss: 0.003469\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:40 [002/005 (0000/0755)]\tLoss Ss: 0.016095\n","\tRotated_Epoch:40 [002/005 (0020/0755)]\tLoss Ss: 0.010250\n","\tRotated_Epoch:40 [002/005 (0040/0755)]\tLoss Ss: 0.015792\n","\tRotated_Epoch:40 [002/005 (0060/0755)]\tLoss Ss: 0.010806\n","\tRotated_Epoch:40 [002/005 (0080/0755)]\tLoss Ss: 0.010178\n","\tRotated_Epoch:40 [002/005 (0100/0755)]\tLoss Ss: 0.018827\n","\tRotated_Epoch:40 [002/005 (0120/0755)]\tLoss Ss: 0.015030\n","\tRotated_Epoch:40 [002/005 (0140/0755)]\tLoss Ss: 0.014955\n","\tRotated_Epoch:40 [002/005 (0160/0755)]\tLoss Ss: 0.011565\n","\tRotated_Epoch:40 [002/005 (0180/0755)]\tLoss Ss: 0.019143\n","\tRotated_Epoch:40 [002/005 (0200/0755)]\tLoss Ss: 0.015392\n","\tRotated_Epoch:40 [002/005 (0220/0755)]\tLoss Ss: 0.017531\n","\tRotated_Epoch:40 [002/005 (0240/0755)]\tLoss Ss: 0.020126\n","\tRotated_Epoch:40 [002/005 (0260/0755)]\tLoss Ss: 0.012627\n","\tRotated_Epoch:40 [002/005 (0280/0755)]\tLoss Ss: 0.013907\n","\tRotated_Epoch:40 [002/005 (0300/0755)]\tLoss Ss: 0.018321\n","\tRotated_Epoch:40 [002/005 (0320/0755)]\tLoss Ss: 0.009620\n","\tRotated_Epoch:40 [002/005 (0340/0755)]\tLoss Ss: 0.008860\n","\tRotated_Epoch:40 [002/005 (0360/0755)]\tLoss Ss: 0.010117\n","\tRotated_Epoch:40 [002/005 (0380/0755)]\tLoss Ss: 0.011305\n","\tRotated_Epoch:40 [002/005 (0400/0755)]\tLoss Ss: 0.013823\n","\tRotated_Epoch:40 [002/005 (0420/0755)]\tLoss Ss: 0.010684\n","\tRotated_Epoch:40 [002/005 (0440/0755)]\tLoss Ss: 0.018292\n","\tRotated_Epoch:40 [002/005 (0460/0755)]\tLoss Ss: 0.008391\n","\tRotated_Epoch:40 [002/005 (0480/0755)]\tLoss Ss: 0.019477\n","\tRotated_Epoch:40 [002/005 (0500/0755)]\tLoss Ss: 0.015491\n","\tRotated_Epoch:40 [002/005 (0520/0755)]\tLoss Ss: 0.005872\n","\tRotated_Epoch:40 [002/005 (0540/0755)]\tLoss Ss: 0.022114\n","\tRotated_Epoch:40 [002/005 (0560/0755)]\tLoss Ss: 0.012704\n","\tRotated_Epoch:40 [002/005 (0580/0755)]\tLoss Ss: 0.012663\n","\tRotated_Epoch:40 [002/005 (0600/0755)]\tLoss Ss: 0.012924\n","\tRotated_Epoch:40 [002/005 (0620/0755)]\tLoss Ss: 0.014578\n","\tRotated_Epoch:40 [002/005 (0640/0755)]\tLoss Ss: 0.019430\n","\tRotated_Epoch:40 [002/005 (0660/0755)]\tLoss Ss: 0.014953\n","\tRotated_Epoch:40 [002/005 (0680/0755)]\tLoss Ss: 0.015655\n","\tRotated_Epoch:40 [002/005 (0700/0755)]\tLoss Ss: 0.008367\n","\tRotated_Epoch:40 [002/005 (0720/0755)]\tLoss Ss: 0.014788\n","\tRotated_Epoch:40 [002/005 (0740/0755)]\tLoss Ss: 0.016986\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:40 [003/005 (0000/0755)]\tLoss Ss: 0.234103\n","\tRotated_Epoch:40 [003/005 (0020/0755)]\tLoss Ss: 0.184229\n","\tRotated_Epoch:40 [003/005 (0040/0755)]\tLoss Ss: 0.180667\n","\tRotated_Epoch:40 [003/005 (0060/0755)]\tLoss Ss: 0.109202\n","\tRotated_Epoch:40 [003/005 (0080/0755)]\tLoss Ss: 0.064941\n","\tRotated_Epoch:40 [003/005 (0100/0755)]\tLoss Ss: 0.068528\n","\tRotated_Epoch:40 [003/005 (0120/0755)]\tLoss Ss: 0.045656\n","\tRotated_Epoch:40 [003/005 (0140/0755)]\tLoss Ss: 0.076301\n","\tRotated_Epoch:40 [003/005 (0160/0755)]\tLoss Ss: 0.116466\n","\tRotated_Epoch:40 [003/005 (0180/0755)]\tLoss Ss: 0.045363\n","\tRotated_Epoch:40 [003/005 (0200/0755)]\tLoss Ss: 0.044257\n","\tRotated_Epoch:40 [003/005 (0220/0755)]\tLoss Ss: 0.043091\n","\tRotated_Epoch:40 [003/005 (0240/0755)]\tLoss Ss: 0.039329\n","\tRotated_Epoch:40 [003/005 (0260/0755)]\tLoss Ss: 0.037303\n","\tRotated_Epoch:40 [003/005 (0280/0755)]\tLoss Ss: 0.059137\n","\tRotated_Epoch:40 [003/005 (0300/0755)]\tLoss Ss: 0.038290\n","\tRotated_Epoch:40 [003/005 (0320/0755)]\tLoss Ss: 0.039132\n","\tRotated_Epoch:40 [003/005 (0340/0755)]\tLoss Ss: 0.031384\n","\tRotated_Epoch:40 [003/005 (0360/0755)]\tLoss Ss: 0.049778\n","\tRotated_Epoch:40 [003/005 (0380/0755)]\tLoss Ss: 0.034423\n","\tRotated_Epoch:40 [003/005 (0400/0755)]\tLoss Ss: 0.031865\n","\tRotated_Epoch:40 [003/005 (0420/0755)]\tLoss Ss: 0.029913\n","\tRotated_Epoch:40 [003/005 (0440/0755)]\tLoss Ss: 0.037326\n","\tRotated_Epoch:40 [003/005 (0460/0755)]\tLoss Ss: 0.029308\n","\tRotated_Epoch:40 [003/005 (0480/0755)]\tLoss Ss: 0.040920\n","\tRotated_Epoch:40 [003/005 (0500/0755)]\tLoss Ss: 0.023180\n","\tRotated_Epoch:40 [003/005 (0520/0755)]\tLoss Ss: 0.036186\n","\tRotated_Epoch:40 [003/005 (0540/0755)]\tLoss Ss: 0.034626\n","\tRotated_Epoch:40 [003/005 (0560/0755)]\tLoss Ss: 0.032219\n","\tRotated_Epoch:40 [003/005 (0580/0755)]\tLoss Ss: 0.031695\n","\tRotated_Epoch:40 [003/005 (0600/0755)]\tLoss Ss: 0.039757\n","\tRotated_Epoch:40 [003/005 (0620/0755)]\tLoss Ss: 0.034277\n","\tRotated_Epoch:40 [003/005 (0640/0755)]\tLoss Ss: 0.037292\n","\tRotated_Epoch:40 [003/005 (0660/0755)]\tLoss Ss: 0.033509\n","\tRotated_Epoch:40 [003/005 (0680/0755)]\tLoss Ss: 0.031738\n","\tRotated_Epoch:40 [003/005 (0700/0755)]\tLoss Ss: 0.038195\n","\tRotated_Epoch:40 [003/005 (0720/0755)]\tLoss Ss: 0.024476\n","\tRotated_Epoch:40 [003/005 (0740/0755)]\tLoss Ss: 0.041542\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tRotated_Epoch:40 [004/005 (0000/0693)]\tLoss Ss: 0.020123\n","\tRotated_Epoch:40 [004/005 (0020/0693)]\tLoss Ss: 0.013463\n","\tRotated_Epoch:40 [004/005 (0040/0693)]\tLoss Ss: 0.022684\n","\tRotated_Epoch:40 [004/005 (0060/0693)]\tLoss Ss: 0.020738\n","\tRotated_Epoch:40 [004/005 (0080/0693)]\tLoss Ss: 0.018451\n","\tRotated_Epoch:40 [004/005 (0100/0693)]\tLoss Ss: 0.015088\n","\tRotated_Epoch:40 [004/005 (0120/0693)]\tLoss Ss: 0.014182\n","\tRotated_Epoch:40 [004/005 (0140/0693)]\tLoss Ss: 0.016372\n","\tRotated_Epoch:40 [004/005 (0160/0693)]\tLoss Ss: 0.017885\n","\tRotated_Epoch:40 [004/005 (0180/0693)]\tLoss Ss: 0.018105\n","\tRotated_Epoch:40 [004/005 (0200/0693)]\tLoss Ss: 0.013519\n","\tRotated_Epoch:40 [004/005 (0220/0693)]\tLoss Ss: 0.015029\n","\tRotated_Epoch:40 [004/005 (0240/0693)]\tLoss Ss: 0.017148\n","\tRotated_Epoch:40 [004/005 (0260/0693)]\tLoss Ss: 0.013711\n","\tRotated_Epoch:40 [004/005 (0280/0693)]\tLoss Ss: 0.021229\n","\tRotated_Epoch:40 [004/005 (0300/0693)]\tLoss Ss: 0.012425\n","\tRotated_Epoch:40 [004/005 (0320/0693)]\tLoss Ss: 0.017867\n","\tRotated_Epoch:40 [004/005 (0340/0693)]\tLoss Ss: 0.013943\n","\tRotated_Epoch:40 [004/005 (0360/0693)]\tLoss Ss: 0.013044\n","\tRotated_Epoch:40 [004/005 (0380/0693)]\tLoss Ss: 0.014204\n","\tRotated_Epoch:40 [004/005 (0400/0693)]\tLoss Ss: 0.014043\n","\tRotated_Epoch:40 [004/005 (0420/0693)]\tLoss Ss: 0.016793\n","\tRotated_Epoch:40 [004/005 (0440/0693)]\tLoss Ss: 0.017976\n","\tRotated_Epoch:40 [004/005 (0460/0693)]\tLoss Ss: 0.016158\n","\tRotated_Epoch:40 [004/005 (0480/0693)]\tLoss Ss: 0.010202\n","\tRotated_Epoch:40 [004/005 (0500/0693)]\tLoss Ss: 0.011750\n","\tRotated_Epoch:40 [004/005 (0520/0693)]\tLoss Ss: 0.015617\n","\tRotated_Epoch:40 [004/005 (0540/0693)]\tLoss Ss: 0.009340\n","\tRotated_Epoch:40 [004/005 (0560/0693)]\tLoss Ss: 0.010354\n","\tRotated_Epoch:40 [004/005 (0580/0693)]\tLoss Ss: 0.017677\n","\tRotated_Epoch:40 [004/005 (0600/0693)]\tLoss Ss: 0.017457\n","\tRotated_Epoch:40 [004/005 (0620/0693)]\tLoss Ss: 0.014342\n","\tRotated_Epoch:40 [004/005 (0640/0693)]\tLoss Ss: 0.015536\n","\tRotated_Epoch:40 [004/005 (0660/0693)]\tLoss Ss: 0.013477\n","\tRotated_Epoch:40 [004/005 (0680/0693)]\tLoss Ss: 0.012233\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tRotated_Epoch:40 [005/005 (0000/0588)]\tLoss Ss: 0.103964\n","\tRotated_Epoch:40 [005/005 (0020/0588)]\tLoss Ss: 0.162065\n","\tRotated_Epoch:40 [005/005 (0040/0588)]\tLoss Ss: 0.053265\n","\tRotated_Epoch:40 [005/005 (0060/0588)]\tLoss Ss: 0.073794\n","\tRotated_Epoch:40 [005/005 (0080/0588)]\tLoss Ss: 0.064578\n","\tRotated_Epoch:40 [005/005 (0100/0588)]\tLoss Ss: 0.090347\n","\tRotated_Epoch:40 [005/005 (0120/0588)]\tLoss Ss: 0.045264\n","\tRotated_Epoch:40 [005/005 (0140/0588)]\tLoss Ss: 0.075827\n","\tRotated_Epoch:40 [005/005 (0160/0588)]\tLoss Ss: 0.061745\n","\tRotated_Epoch:40 [005/005 (0180/0588)]\tLoss Ss: 0.055587\n","\tRotated_Epoch:40 [005/005 (0200/0588)]\tLoss Ss: 0.051938\n","\tRotated_Epoch:40 [005/005 (0220/0588)]\tLoss Ss: 0.036923\n","\tRotated_Epoch:40 [005/005 (0240/0588)]\tLoss Ss: 0.054324\n","\tRotated_Epoch:40 [005/005 (0260/0588)]\tLoss Ss: 0.044346\n","\tRotated_Epoch:40 [005/005 (0280/0588)]\tLoss Ss: 0.052423\n","\tRotated_Epoch:40 [005/005 (0300/0588)]\tLoss Ss: 0.063251\n","\tRotated_Epoch:40 [005/005 (0320/0588)]\tLoss Ss: 0.034590\n","\tRotated_Epoch:40 [005/005 (0340/0588)]\tLoss Ss: 0.050458\n","\tRotated_Epoch:40 [005/005 (0360/0588)]\tLoss Ss: 0.058658\n","\tRotated_Epoch:40 [005/005 (0380/0588)]\tLoss Ss: 0.052878\n","\tRotated_Epoch:40 [005/005 (0400/0588)]\tLoss Ss: 0.043886\n","\tRotated_Epoch:40 [005/005 (0420/0588)]\tLoss Ss: 0.068551\n","\tRotated_Epoch:40 [005/005 (0440/0588)]\tLoss Ss: 0.065424\n","\tRotated_Epoch:40 [005/005 (0460/0588)]\tLoss Ss: 0.032727\n","\tRotated_Epoch:40 [005/005 (0480/0588)]\tLoss Ss: 0.035050\n","\tRotated_Epoch:40 [005/005 (0500/0588)]\tLoss Ss: 0.065512\n","\tRotated_Epoch:40 [005/005 (0520/0588)]\tLoss Ss: 0.063919\n","\tRotated_Epoch:40 [005/005 (0540/0588)]\tLoss Ss: 0.060518\n","\tRotated_Epoch:40 [005/005 (0560/0588)]\tLoss Ss: 0.048453\n","\tRotated_Epoch:40 [005/005 (0580/0588)]\tLoss Ss: 0.051038\n","new_1_6_d.nii\n","new_1_6_m.nii\n","new_1_7_d.nii\n","new_1_7_m.nii\n","new_2_5_d.nii\n","new_2_5_m.nii\n","new_2_6_d.nii\n","new_2_6_m.nii\n","new_2_7_d.nii\n","new_2_7_m.nii\n","new_3_d (2).nii\n","new_3_m (2).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (3).nii\n","new_3_m (3).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (7).nii\n","new_3_m (7).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (8).nii\n","new_3_m (8).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (9).nii\n","new_3_m (9).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_4_10_d_163526.nii\n","new_4_10_m_163526.nii.gz\n","new_4_11_d_084336.nii\n","new_4_11_m_084336.nii.gz\n","new_4_7_d_184422.nii\n","new_4_7_m_184422.nii.gz\n","new_4_8_d_143412.nii\n","new_4_8_m_143412.nii.gz\n","new_4_9_d_155347.nii\n","new_4_9_m_155347.nii.gz\n","\tEpoch: 40; Dice: 0.9278 +/- 0.0160; Loss: 7.8442\n","Begin Epoch 41\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:41 [000/005 (0000/0755)]\tLoss Ss: 0.024629\n","\tEpoch:41 [000/005 (0020/0755)]\tLoss Ss: 0.028094\n","\tEpoch:41 [000/005 (0040/0755)]\tLoss Ss: 0.016823\n","\tEpoch:41 [000/005 (0060/0755)]\tLoss Ss: 0.015950\n","\tEpoch:41 [000/005 (0080/0755)]\tLoss Ss: 0.013718\n","\tEpoch:41 [000/005 (0100/0755)]\tLoss Ss: 0.011765\n","\tEpoch:41 [000/005 (0120/0755)]\tLoss Ss: 0.018967\n","\tEpoch:41 [000/005 (0140/0755)]\tLoss Ss: 0.016074\n","\tEpoch:41 [000/005 (0160/0755)]\tLoss Ss: 0.018845\n","\tEpoch:41 [000/005 (0180/0755)]\tLoss Ss: 0.013060\n","\tEpoch:41 [000/005 (0200/0755)]\tLoss Ss: 0.013914\n","\tEpoch:41 [000/005 (0220/0755)]\tLoss Ss: 0.027855\n","\tEpoch:41 [000/005 (0240/0755)]\tLoss Ss: 0.019048\n","\tEpoch:41 [000/005 (0260/0755)]\tLoss Ss: 0.020159\n","\tEpoch:41 [000/005 (0280/0755)]\tLoss Ss: 0.017513\n","\tEpoch:41 [000/005 (0300/0755)]\tLoss Ss: 0.018506\n","\tEpoch:41 [000/005 (0320/0755)]\tLoss Ss: 0.017954\n","\tEpoch:41 [000/005 (0340/0755)]\tLoss Ss: 0.017103\n","\tEpoch:41 [000/005 (0360/0755)]\tLoss Ss: 0.009582\n","\tEpoch:41 [000/005 (0380/0755)]\tLoss Ss: 0.012182\n","\tEpoch:41 [000/005 (0400/0755)]\tLoss Ss: 0.011191\n","\tEpoch:41 [000/005 (0420/0755)]\tLoss Ss: 0.014581\n","\tEpoch:41 [000/005 (0440/0755)]\tLoss Ss: 0.014812\n","\tEpoch:41 [000/005 (0460/0755)]\tLoss Ss: 0.012227\n","\tEpoch:41 [000/005 (0480/0755)]\tLoss Ss: 0.012517\n","\tEpoch:41 [000/005 (0500/0755)]\tLoss Ss: 0.012066\n","\tEpoch:41 [000/005 (0520/0755)]\tLoss Ss: 0.012466\n","\tEpoch:41 [000/005 (0540/0755)]\tLoss Ss: 0.010272\n","\tEpoch:41 [000/005 (0560/0755)]\tLoss Ss: 0.016811\n","\tEpoch:41 [000/005 (0580/0755)]\tLoss Ss: 0.012202\n","\tEpoch:41 [000/005 (0600/0755)]\tLoss Ss: 0.009040\n","\tEpoch:41 [000/005 (0620/0755)]\tLoss Ss: 0.015197\n","\tEpoch:41 [000/005 (0640/0755)]\tLoss Ss: 0.012842\n","\tEpoch:41 [000/005 (0660/0755)]\tLoss Ss: 0.011761\n","\tEpoch:41 [000/005 (0680/0755)]\tLoss Ss: 0.013929\n","\tEpoch:41 [000/005 (0700/0755)]\tLoss Ss: 0.010520\n","\tEpoch:41 [000/005 (0720/0755)]\tLoss Ss: 0.008703\n","\tEpoch:41 [000/005 (0740/0755)]\tLoss Ss: 0.011679\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tEpoch:41 [001/005 (0000/0693)]\tLoss Ss: 0.017555\n","\tEpoch:41 [001/005 (0020/0693)]\tLoss Ss: 0.011055\n","\tEpoch:41 [001/005 (0040/0693)]\tLoss Ss: 0.010323\n","\tEpoch:41 [001/005 (0060/0693)]\tLoss Ss: 0.011598\n","\tEpoch:41 [001/005 (0080/0693)]\tLoss Ss: 0.014010\n","\tEpoch:41 [001/005 (0100/0693)]\tLoss Ss: 0.014069\n","\tEpoch:41 [001/005 (0120/0693)]\tLoss Ss: 0.015079\n","\tEpoch:41 [001/005 (0140/0693)]\tLoss Ss: 0.009689\n","\tEpoch:41 [001/005 (0160/0693)]\tLoss Ss: 0.017819\n","\tEpoch:41 [001/005 (0180/0693)]\tLoss Ss: 0.021508\n","\tEpoch:41 [001/005 (0200/0693)]\tLoss Ss: 0.012953\n","\tEpoch:41 [001/005 (0220/0693)]\tLoss Ss: 0.016769\n","\tEpoch:41 [001/005 (0240/0693)]\tLoss Ss: 0.009866\n","\tEpoch:41 [001/005 (0260/0693)]\tLoss Ss: 0.013913\n","\tEpoch:41 [001/005 (0280/0693)]\tLoss Ss: 0.017516\n","\tEpoch:41 [001/005 (0300/0693)]\tLoss Ss: 0.010280\n","\tEpoch:41 [001/005 (0320/0693)]\tLoss Ss: 0.013217\n","\tEpoch:41 [001/005 (0340/0693)]\tLoss Ss: 0.017943\n","\tEpoch:41 [001/005 (0360/0693)]\tLoss Ss: 0.010158\n","\tEpoch:41 [001/005 (0380/0693)]\tLoss Ss: 0.012999\n","\tEpoch:41 [001/005 (0400/0693)]\tLoss Ss: 0.008436\n","\tEpoch:41 [001/005 (0420/0693)]\tLoss Ss: 0.013184\n","\tEpoch:41 [001/005 (0440/0693)]\tLoss Ss: 0.011021\n","\tEpoch:41 [001/005 (0460/0693)]\tLoss Ss: 0.015934\n","\tEpoch:41 [001/005 (0480/0693)]\tLoss Ss: 0.012790\n","\tEpoch:41 [001/005 (0500/0693)]\tLoss Ss: 0.012541\n","\tEpoch:41 [001/005 (0520/0693)]\tLoss Ss: 0.008490\n","\tEpoch:41 [001/005 (0540/0693)]\tLoss Ss: 0.015206\n","\tEpoch:41 [001/005 (0560/0693)]\tLoss Ss: 0.012980\n","\tEpoch:41 [001/005 (0580/0693)]\tLoss Ss: 0.011648\n","\tEpoch:41 [001/005 (0600/0693)]\tLoss Ss: 0.005967\n","\tEpoch:41 [001/005 (0620/0693)]\tLoss Ss: 0.011902\n","\tEpoch:41 [001/005 (0640/0693)]\tLoss Ss: 0.013548\n","\tEpoch:41 [001/005 (0660/0693)]\tLoss Ss: 0.008321\n","\tEpoch:41 [001/005 (0680/0693)]\tLoss Ss: 0.009175\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tEpoch:41 [002/005 (0000/0693)]\tLoss Ss: 0.011139\n","\tEpoch:41 [002/005 (0020/0693)]\tLoss Ss: 0.012325\n","\tEpoch:41 [002/005 (0040/0693)]\tLoss Ss: 0.009511\n","\tEpoch:41 [002/005 (0060/0693)]\tLoss Ss: 0.016954\n","\tEpoch:41 [002/005 (0080/0693)]\tLoss Ss: 0.010202\n","\tEpoch:41 [002/005 (0100/0693)]\tLoss Ss: 0.015854\n","\tEpoch:41 [002/005 (0120/0693)]\tLoss Ss: 0.010196\n","\tEpoch:41 [002/005 (0140/0693)]\tLoss Ss: 0.013165\n","\tEpoch:41 [002/005 (0160/0693)]\tLoss Ss: 0.015530\n","\tEpoch:41 [002/005 (0180/0693)]\tLoss Ss: 0.013531\n","\tEpoch:41 [002/005 (0200/0693)]\tLoss Ss: 0.010534\n","\tEpoch:41 [002/005 (0220/0693)]\tLoss Ss: 0.010632\n","\tEpoch:41 [002/005 (0240/0693)]\tLoss Ss: 0.011096\n","\tEpoch:41 [002/005 (0260/0693)]\tLoss Ss: 0.014002\n","\tEpoch:41 [002/005 (0280/0693)]\tLoss Ss: 0.012383\n","\tEpoch:41 [002/005 (0300/0693)]\tLoss Ss: 0.008620\n","\tEpoch:41 [002/005 (0320/0693)]\tLoss Ss: 0.011446\n","\tEpoch:41 [002/005 (0340/0693)]\tLoss Ss: 0.007615\n","\tEpoch:41 [002/005 (0360/0693)]\tLoss Ss: 0.006048\n","\tEpoch:41 [002/005 (0380/0693)]\tLoss Ss: 0.011997\n","\tEpoch:41 [002/005 (0400/0693)]\tLoss Ss: 0.007904\n","\tEpoch:41 [002/005 (0420/0693)]\tLoss Ss: 0.009217\n","\tEpoch:41 [002/005 (0440/0693)]\tLoss Ss: 0.012146\n","\tEpoch:41 [002/005 (0460/0693)]\tLoss Ss: 0.010768\n","\tEpoch:41 [002/005 (0480/0693)]\tLoss Ss: 0.010813\n","\tEpoch:41 [002/005 (0500/0693)]\tLoss Ss: 0.009661\n","\tEpoch:41 [002/005 (0520/0693)]\tLoss Ss: 0.012149\n","\tEpoch:41 [002/005 (0540/0693)]\tLoss Ss: 0.010534\n","\tEpoch:41 [002/005 (0560/0693)]\tLoss Ss: 0.008613\n","\tEpoch:41 [002/005 (0580/0693)]\tLoss Ss: 0.007908\n","\tEpoch:41 [002/005 (0600/0693)]\tLoss Ss: 0.010238\n","\tEpoch:41 [002/005 (0620/0693)]\tLoss Ss: 0.014723\n","\tEpoch:41 [002/005 (0640/0693)]\tLoss Ss: 0.012861\n","\tEpoch:41 [002/005 (0660/0693)]\tLoss Ss: 0.008594\n","\tEpoch:41 [002/005 (0680/0693)]\tLoss Ss: 0.013928\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tEpoch:41 [003/005 (0000/0588)]\tLoss Ss: 0.003162\n","\tEpoch:41 [003/005 (0020/0588)]\tLoss Ss: 0.007271\n","\tEpoch:41 [003/005 (0040/0588)]\tLoss Ss: 0.006170\n","\tEpoch:41 [003/005 (0060/0588)]\tLoss Ss: 0.006747\n","\tEpoch:41 [003/005 (0080/0588)]\tLoss Ss: 0.005364\n","\tEpoch:41 [003/005 (0100/0588)]\tLoss Ss: 0.005067\n","\tEpoch:41 [003/005 (0120/0588)]\tLoss Ss: 0.005455\n","\tEpoch:41 [003/005 (0140/0588)]\tLoss Ss: 0.004565\n","\tEpoch:41 [003/005 (0160/0588)]\tLoss Ss: 0.005839\n","\tEpoch:41 [003/005 (0180/0588)]\tLoss Ss: 0.003242\n","\tEpoch:41 [003/005 (0200/0588)]\tLoss Ss: 0.006068\n","\tEpoch:41 [003/005 (0220/0588)]\tLoss Ss: 0.006713\n","\tEpoch:41 [003/005 (0240/0588)]\tLoss Ss: 0.003938\n","\tEpoch:41 [003/005 (0260/0588)]\tLoss Ss: 0.004170\n","\tEpoch:41 [003/005 (0280/0588)]\tLoss Ss: 0.005639\n","\tEpoch:41 [003/005 (0300/0588)]\tLoss Ss: 0.004546\n","\tEpoch:41 [003/005 (0320/0588)]\tLoss Ss: 0.004985\n","\tEpoch:41 [003/005 (0340/0588)]\tLoss Ss: 0.003969\n","\tEpoch:41 [003/005 (0360/0588)]\tLoss Ss: 0.005194\n","\tEpoch:41 [003/005 (0380/0588)]\tLoss Ss: 0.004655\n","\tEpoch:41 [003/005 (0400/0588)]\tLoss Ss: 0.002895\n","\tEpoch:41 [003/005 (0420/0588)]\tLoss Ss: 0.002594\n","\tEpoch:41 [003/005 (0440/0588)]\tLoss Ss: 0.004520\n","\tEpoch:41 [003/005 (0460/0588)]\tLoss Ss: 0.004072\n","\tEpoch:41 [003/005 (0480/0588)]\tLoss Ss: 0.004880\n","\tEpoch:41 [003/005 (0500/0588)]\tLoss Ss: 0.003493\n","\tEpoch:41 [003/005 (0520/0588)]\tLoss Ss: 0.003408\n","\tEpoch:41 [003/005 (0540/0588)]\tLoss Ss: 0.004846\n","\tEpoch:41 [003/005 (0560/0588)]\tLoss Ss: 0.006243\n","\tEpoch:41 [003/005 (0580/0588)]\tLoss Ss: 0.005353\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tEpoch:41 [004/005 (0000/0614)]\tLoss Ss: 0.004727\n","\tEpoch:41 [004/005 (0020/0614)]\tLoss Ss: 0.004457\n","\tEpoch:41 [004/005 (0040/0614)]\tLoss Ss: 0.005323\n","\tEpoch:41 [004/005 (0060/0614)]\tLoss Ss: 0.005756\n","\tEpoch:41 [004/005 (0080/0614)]\tLoss Ss: 0.004923\n","\tEpoch:41 [004/005 (0100/0614)]\tLoss Ss: 0.006583\n","\tEpoch:41 [004/005 (0120/0614)]\tLoss Ss: 0.003867\n","\tEpoch:41 [004/005 (0140/0614)]\tLoss Ss: 0.005005\n","\tEpoch:41 [004/005 (0160/0614)]\tLoss Ss: 0.005583\n","\tEpoch:41 [004/005 (0180/0614)]\tLoss Ss: 0.004603\n","\tEpoch:41 [004/005 (0200/0614)]\tLoss Ss: 0.006996\n","\tEpoch:41 [004/005 (0220/0614)]\tLoss Ss: 0.004174\n","\tEpoch:41 [004/005 (0240/0614)]\tLoss Ss: 0.005252\n","\tEpoch:41 [004/005 (0260/0614)]\tLoss Ss: 0.007264\n","\tEpoch:41 [004/005 (0280/0614)]\tLoss Ss: 0.004448\n","\tEpoch:41 [004/005 (0300/0614)]\tLoss Ss: 0.005528\n","\tEpoch:41 [004/005 (0320/0614)]\tLoss Ss: 0.006120\n","\tEpoch:41 [004/005 (0340/0614)]\tLoss Ss: 0.004700\n","\tEpoch:41 [004/005 (0360/0614)]\tLoss Ss: 0.005801\n","\tEpoch:41 [004/005 (0380/0614)]\tLoss Ss: 0.004668\n","\tEpoch:41 [004/005 (0400/0614)]\tLoss Ss: 0.007773\n","\tEpoch:41 [004/005 (0420/0614)]\tLoss Ss: 0.004349\n","\tEpoch:41 [004/005 (0440/0614)]\tLoss Ss: 0.007087\n","\tEpoch:41 [004/005 (0460/0614)]\tLoss Ss: 0.002375\n","\tEpoch:41 [004/005 (0480/0614)]\tLoss Ss: 0.005217\n","\tEpoch:41 [004/005 (0500/0614)]\tLoss Ss: 0.004728\n","\tEpoch:41 [004/005 (0520/0614)]\tLoss Ss: 0.004984\n","\tEpoch:41 [004/005 (0540/0614)]\tLoss Ss: 0.006118\n","\tEpoch:41 [004/005 (0560/0614)]\tLoss Ss: 0.004897\n","\tEpoch:41 [004/005 (0580/0614)]\tLoss Ss: 0.005304\n","\tEpoch:41 [004/005 (0600/0614)]\tLoss Ss: 0.004382\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:41 [005/005 (0000/0755)]\tLoss Ss: 0.013685\n","\tEpoch:41 [005/005 (0020/0755)]\tLoss Ss: 0.012428\n","\tEpoch:41 [005/005 (0040/0755)]\tLoss Ss: 0.014243\n","\tEpoch:41 [005/005 (0060/0755)]\tLoss Ss: 0.011732\n","\tEpoch:41 [005/005 (0080/0755)]\tLoss Ss: 0.013839\n","\tEpoch:41 [005/005 (0100/0755)]\tLoss Ss: 0.012336\n","\tEpoch:41 [005/005 (0120/0755)]\tLoss Ss: 0.011732\n","\tEpoch:41 [005/005 (0140/0755)]\tLoss Ss: 0.012646\n","\tEpoch:41 [005/005 (0160/0755)]\tLoss Ss: 0.016256\n","\tEpoch:41 [005/005 (0180/0755)]\tLoss Ss: 0.014246\n","\tEpoch:41 [005/005 (0200/0755)]\tLoss Ss: 0.008219\n","\tEpoch:41 [005/005 (0220/0755)]\tLoss Ss: 0.009750\n","\tEpoch:41 [005/005 (0240/0755)]\tLoss Ss: 0.015693\n","\tEpoch:41 [005/005 (0260/0755)]\tLoss Ss: 0.018465\n","\tEpoch:41 [005/005 (0280/0755)]\tLoss Ss: 0.010874\n","\tEpoch:41 [005/005 (0300/0755)]\tLoss Ss: 0.010829\n","\tEpoch:41 [005/005 (0320/0755)]\tLoss Ss: 0.014505\n","\tEpoch:41 [005/005 (0340/0755)]\tLoss Ss: 0.014223\n","\tEpoch:41 [005/005 (0360/0755)]\tLoss Ss: 0.015697\n","\tEpoch:41 [005/005 (0380/0755)]\tLoss Ss: 0.013123\n","\tEpoch:41 [005/005 (0400/0755)]\tLoss Ss: 0.014130\n","\tEpoch:41 [005/005 (0420/0755)]\tLoss Ss: 0.019536\n","\tEpoch:41 [005/005 (0440/0755)]\tLoss Ss: 0.012193\n","\tEpoch:41 [005/005 (0460/0755)]\tLoss Ss: 0.020037\n","\tEpoch:41 [005/005 (0480/0755)]\tLoss Ss: 0.020906\n","\tEpoch:41 [005/005 (0500/0755)]\tLoss Ss: 0.018597\n","\tEpoch:41 [005/005 (0520/0755)]\tLoss Ss: 0.012616\n","\tEpoch:41 [005/005 (0540/0755)]\tLoss Ss: 0.017131\n","\tEpoch:41 [005/005 (0560/0755)]\tLoss Ss: 0.012176\n","\tEpoch:41 [005/005 (0580/0755)]\tLoss Ss: 0.014608\n","\tEpoch:41 [005/005 (0600/0755)]\tLoss Ss: 0.013232\n","\tEpoch:41 [005/005 (0620/0755)]\tLoss Ss: 0.008440\n","\tEpoch:41 [005/005 (0640/0755)]\tLoss Ss: 0.011271\n","\tEpoch:41 [005/005 (0660/0755)]\tLoss Ss: 0.013614\n","\tEpoch:41 [005/005 (0680/0755)]\tLoss Ss: 0.012010\n","\tEpoch:41 [005/005 (0700/0755)]\tLoss Ss: 0.019801\n","\tEpoch:41 [005/005 (0720/0755)]\tLoss Ss: 0.015538\n","\tEpoch:41 [005/005 (0740/0755)]\tLoss Ss: 0.013101\n","Now train the rotated image\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tRotated_Epoch:41 [000/005 (0000/0693)]\tLoss Ss: 0.020136\n","\tRotated_Epoch:41 [000/005 (0020/0693)]\tLoss Ss: 0.020507\n","\tRotated_Epoch:41 [000/005 (0040/0693)]\tLoss Ss: 0.020454\n","\tRotated_Epoch:41 [000/005 (0060/0693)]\tLoss Ss: 0.009851\n","\tRotated_Epoch:41 [000/005 (0080/0693)]\tLoss Ss: 0.019041\n","\tRotated_Epoch:41 [000/005 (0100/0693)]\tLoss Ss: 0.015511\n","\tRotated_Epoch:41 [000/005 (0120/0693)]\tLoss Ss: 0.016258\n","\tRotated_Epoch:41 [000/005 (0140/0693)]\tLoss Ss: 0.015719\n","\tRotated_Epoch:41 [000/005 (0160/0693)]\tLoss Ss: 0.012939\n","\tRotated_Epoch:41 [000/005 (0180/0693)]\tLoss Ss: 0.016774\n","\tRotated_Epoch:41 [000/005 (0200/0693)]\tLoss Ss: 0.014200\n","\tRotated_Epoch:41 [000/005 (0220/0693)]\tLoss Ss: 0.015410\n","\tRotated_Epoch:41 [000/005 (0240/0693)]\tLoss Ss: 0.017081\n","\tRotated_Epoch:41 [000/005 (0260/0693)]\tLoss Ss: 0.011788\n","\tRotated_Epoch:41 [000/005 (0280/0693)]\tLoss Ss: 0.013833\n","\tRotated_Epoch:41 [000/005 (0300/0693)]\tLoss Ss: 0.011416\n","\tRotated_Epoch:41 [000/005 (0320/0693)]\tLoss Ss: 0.015195\n","\tRotated_Epoch:41 [000/005 (0340/0693)]\tLoss Ss: 0.016727\n","\tRotated_Epoch:41 [000/005 (0360/0693)]\tLoss Ss: 0.012187\n","\tRotated_Epoch:41 [000/005 (0380/0693)]\tLoss Ss: 0.016088\n","\tRotated_Epoch:41 [000/005 (0400/0693)]\tLoss Ss: 0.014643\n","\tRotated_Epoch:41 [000/005 (0420/0693)]\tLoss Ss: 0.012985\n","\tRotated_Epoch:41 [000/005 (0440/0693)]\tLoss Ss: 0.012860\n","\tRotated_Epoch:41 [000/005 (0460/0693)]\tLoss Ss: 0.015447\n","\tRotated_Epoch:41 [000/005 (0480/0693)]\tLoss Ss: 0.016750\n","\tRotated_Epoch:41 [000/005 (0500/0693)]\tLoss Ss: 0.011437\n","\tRotated_Epoch:41 [000/005 (0520/0693)]\tLoss Ss: 0.015939\n","\tRotated_Epoch:41 [000/005 (0540/0693)]\tLoss Ss: 0.009551\n","\tRotated_Epoch:41 [000/005 (0560/0693)]\tLoss Ss: 0.009079\n","\tRotated_Epoch:41 [000/005 (0580/0693)]\tLoss Ss: 0.013147\n","\tRotated_Epoch:41 [000/005 (0600/0693)]\tLoss Ss: 0.012234\n","\tRotated_Epoch:41 [000/005 (0620/0693)]\tLoss Ss: 0.012093\n","\tRotated_Epoch:41 [000/005 (0640/0693)]\tLoss Ss: 0.013762\n","\tRotated_Epoch:41 [000/005 (0660/0693)]\tLoss Ss: 0.016404\n","\tRotated_Epoch:41 [000/005 (0680/0693)]\tLoss Ss: 0.011503\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:41 [001/005 (0000/0755)]\tLoss Ss: 0.058128\n","\tRotated_Epoch:41 [001/005 (0020/0755)]\tLoss Ss: 0.039450\n","\tRotated_Epoch:41 [001/005 (0040/0755)]\tLoss Ss: 0.045019\n","\tRotated_Epoch:41 [001/005 (0060/0755)]\tLoss Ss: 0.047286\n","\tRotated_Epoch:41 [001/005 (0080/0755)]\tLoss Ss: 0.055775\n","\tRotated_Epoch:41 [001/005 (0100/0755)]\tLoss Ss: 0.035714\n","\tRotated_Epoch:41 [001/005 (0120/0755)]\tLoss Ss: 0.028013\n","\tRotated_Epoch:41 [001/005 (0140/0755)]\tLoss Ss: 0.043260\n","\tRotated_Epoch:41 [001/005 (0160/0755)]\tLoss Ss: 0.035630\n","\tRotated_Epoch:41 [001/005 (0180/0755)]\tLoss Ss: 0.044253\n","\tRotated_Epoch:41 [001/005 (0200/0755)]\tLoss Ss: 0.030425\n","\tRotated_Epoch:41 [001/005 (0220/0755)]\tLoss Ss: 0.028875\n","\tRotated_Epoch:41 [001/005 (0240/0755)]\tLoss Ss: 0.034921\n","\tRotated_Epoch:41 [001/005 (0260/0755)]\tLoss Ss: 0.028133\n","\tRotated_Epoch:41 [001/005 (0280/0755)]\tLoss Ss: 0.027008\n","\tRotated_Epoch:41 [001/005 (0300/0755)]\tLoss Ss: 0.026034\n","\tRotated_Epoch:41 [001/005 (0320/0755)]\tLoss Ss: 0.024565\n","\tRotated_Epoch:41 [001/005 (0340/0755)]\tLoss Ss: 0.030499\n","\tRotated_Epoch:41 [001/005 (0360/0755)]\tLoss Ss: 0.032934\n","\tRotated_Epoch:41 [001/005 (0380/0755)]\tLoss Ss: 0.030923\n","\tRotated_Epoch:41 [001/005 (0400/0755)]\tLoss Ss: 0.020319\n","\tRotated_Epoch:41 [001/005 (0420/0755)]\tLoss Ss: 0.018459\n","\tRotated_Epoch:41 [001/005 (0440/0755)]\tLoss Ss: 0.028669\n","\tRotated_Epoch:41 [001/005 (0460/0755)]\tLoss Ss: 0.026347\n","\tRotated_Epoch:41 [001/005 (0480/0755)]\tLoss Ss: 0.022319\n","\tRotated_Epoch:41 [001/005 (0500/0755)]\tLoss Ss: 0.027504\n","\tRotated_Epoch:41 [001/005 (0520/0755)]\tLoss Ss: 0.029060\n","\tRotated_Epoch:41 [001/005 (0540/0755)]\tLoss Ss: 0.022263\n","\tRotated_Epoch:41 [001/005 (0560/0755)]\tLoss Ss: 0.023673\n","\tRotated_Epoch:41 [001/005 (0580/0755)]\tLoss Ss: 0.037110\n","\tRotated_Epoch:41 [001/005 (0600/0755)]\tLoss Ss: 0.024762\n","\tRotated_Epoch:41 [001/005 (0620/0755)]\tLoss Ss: 0.040396\n","\tRotated_Epoch:41 [001/005 (0640/0755)]\tLoss Ss: 0.028461\n","\tRotated_Epoch:41 [001/005 (0660/0755)]\tLoss Ss: 0.015483\n","\tRotated_Epoch:41 [001/005 (0680/0755)]\tLoss Ss: 0.018238\n","\tRotated_Epoch:41 [001/005 (0700/0755)]\tLoss Ss: 0.030162\n","\tRotated_Epoch:41 [001/005 (0720/0755)]\tLoss Ss: 0.028118\n","\tRotated_Epoch:41 [001/005 (0740/0755)]\tLoss Ss: 0.029532\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:41 [002/005 (0000/0755)]\tLoss Ss: 0.125839\n","\tRotated_Epoch:41 [002/005 (0020/0755)]\tLoss Ss: 0.086099\n","\tRotated_Epoch:41 [002/005 (0040/0755)]\tLoss Ss: 0.047769\n","\tRotated_Epoch:41 [002/005 (0060/0755)]\tLoss Ss: 0.058923\n","\tRotated_Epoch:41 [002/005 (0080/0755)]\tLoss Ss: 0.020662\n","\tRotated_Epoch:41 [002/005 (0100/0755)]\tLoss Ss: 0.016337\n","\tRotated_Epoch:41 [002/005 (0120/0755)]\tLoss Ss: 0.031907\n","\tRotated_Epoch:41 [002/005 (0140/0755)]\tLoss Ss: 0.027410\n","\tRotated_Epoch:41 [002/005 (0160/0755)]\tLoss Ss: 0.017871\n","\tRotated_Epoch:41 [002/005 (0180/0755)]\tLoss Ss: 0.021767\n","\tRotated_Epoch:41 [002/005 (0200/0755)]\tLoss Ss: 0.018505\n","\tRotated_Epoch:41 [002/005 (0220/0755)]\tLoss Ss: 0.019697\n","\tRotated_Epoch:41 [002/005 (0240/0755)]\tLoss Ss: 0.020908\n","\tRotated_Epoch:41 [002/005 (0260/0755)]\tLoss Ss: 0.017953\n","\tRotated_Epoch:41 [002/005 (0280/0755)]\tLoss Ss: 0.018104\n","\tRotated_Epoch:41 [002/005 (0300/0755)]\tLoss Ss: 0.011757\n","\tRotated_Epoch:41 [002/005 (0320/0755)]\tLoss Ss: 0.022943\n","\tRotated_Epoch:41 [002/005 (0340/0755)]\tLoss Ss: 0.021696\n","\tRotated_Epoch:41 [002/005 (0360/0755)]\tLoss Ss: 0.027945\n","\tRotated_Epoch:41 [002/005 (0380/0755)]\tLoss Ss: 0.017070\n","\tRotated_Epoch:41 [002/005 (0400/0755)]\tLoss Ss: 0.017710\n","\tRotated_Epoch:41 [002/005 (0420/0755)]\tLoss Ss: 0.018723\n","\tRotated_Epoch:41 [002/005 (0440/0755)]\tLoss Ss: 0.019160\n","\tRotated_Epoch:41 [002/005 (0460/0755)]\tLoss Ss: 0.021603\n","\tRotated_Epoch:41 [002/005 (0480/0755)]\tLoss Ss: 0.016622\n","\tRotated_Epoch:41 [002/005 (0500/0755)]\tLoss Ss: 0.013659\n","\tRotated_Epoch:41 [002/005 (0520/0755)]\tLoss Ss: 0.014267\n","\tRotated_Epoch:41 [002/005 (0540/0755)]\tLoss Ss: 0.016181\n","\tRotated_Epoch:41 [002/005 (0560/0755)]\tLoss Ss: 0.016966\n","\tRotated_Epoch:41 [002/005 (0580/0755)]\tLoss Ss: 0.009954\n","\tRotated_Epoch:41 [002/005 (0600/0755)]\tLoss Ss: 0.013993\n","\tRotated_Epoch:41 [002/005 (0620/0755)]\tLoss Ss: 0.014067\n","\tRotated_Epoch:41 [002/005 (0640/0755)]\tLoss Ss: 0.010542\n","\tRotated_Epoch:41 [002/005 (0660/0755)]\tLoss Ss: 0.010059\n","\tRotated_Epoch:41 [002/005 (0680/0755)]\tLoss Ss: 0.016867\n","\tRotated_Epoch:41 [002/005 (0700/0755)]\tLoss Ss: 0.016100\n","\tRotated_Epoch:41 [002/005 (0720/0755)]\tLoss Ss: 0.015838\n","\tRotated_Epoch:41 [002/005 (0740/0755)]\tLoss Ss: 0.016265\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tRotated_Epoch:41 [003/005 (0000/0588)]\tLoss Ss: 0.139039\n","\tRotated_Epoch:41 [003/005 (0020/0588)]\tLoss Ss: 0.114562\n","\tRotated_Epoch:41 [003/005 (0040/0588)]\tLoss Ss: 0.070474\n","\tRotated_Epoch:41 [003/005 (0060/0588)]\tLoss Ss: 0.065595\n","\tRotated_Epoch:41 [003/005 (0080/0588)]\tLoss Ss: 0.061821\n","\tRotated_Epoch:41 [003/005 (0100/0588)]\tLoss Ss: 0.101750\n","\tRotated_Epoch:41 [003/005 (0120/0588)]\tLoss Ss: 0.071729\n","\tRotated_Epoch:41 [003/005 (0140/0588)]\tLoss Ss: 0.056903\n","\tRotated_Epoch:41 [003/005 (0160/0588)]\tLoss Ss: 0.060786\n","\tRotated_Epoch:41 [003/005 (0180/0588)]\tLoss Ss: 0.066603\n","\tRotated_Epoch:41 [003/005 (0200/0588)]\tLoss Ss: 0.058049\n","\tRotated_Epoch:41 [003/005 (0220/0588)]\tLoss Ss: 0.051189\n","\tRotated_Epoch:41 [003/005 (0240/0588)]\tLoss Ss: 0.053846\n","\tRotated_Epoch:41 [003/005 (0260/0588)]\tLoss Ss: 0.058611\n","\tRotated_Epoch:41 [003/005 (0280/0588)]\tLoss Ss: 0.050805\n","\tRotated_Epoch:41 [003/005 (0300/0588)]\tLoss Ss: 0.061582\n","\tRotated_Epoch:41 [003/005 (0320/0588)]\tLoss Ss: 0.038257\n","\tRotated_Epoch:41 [003/005 (0340/0588)]\tLoss Ss: 0.059671\n","\tRotated_Epoch:41 [003/005 (0360/0588)]\tLoss Ss: 0.050748\n","\tRotated_Epoch:41 [003/005 (0380/0588)]\tLoss Ss: 0.080422\n","\tRotated_Epoch:41 [003/005 (0400/0588)]\tLoss Ss: 0.051259\n","\tRotated_Epoch:41 [003/005 (0420/0588)]\tLoss Ss: 0.061344\n","\tRotated_Epoch:41 [003/005 (0440/0588)]\tLoss Ss: 0.064884\n","\tRotated_Epoch:41 [003/005 (0460/0588)]\tLoss Ss: 0.058957\n","\tRotated_Epoch:41 [003/005 (0480/0588)]\tLoss Ss: 0.065229\n","\tRotated_Epoch:41 [003/005 (0500/0588)]\tLoss Ss: 0.052809\n","\tRotated_Epoch:41 [003/005 (0520/0588)]\tLoss Ss: 0.045404\n","\tRotated_Epoch:41 [003/005 (0540/0588)]\tLoss Ss: 0.056149\n","\tRotated_Epoch:41 [003/005 (0560/0588)]\tLoss Ss: 0.051736\n","\tRotated_Epoch:41 [003/005 (0580/0588)]\tLoss Ss: 0.065753\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tRotated_Epoch:41 [004/005 (0000/0693)]\tLoss Ss: 0.016683\n","\tRotated_Epoch:41 [004/005 (0020/0693)]\tLoss Ss: 0.015277\n","\tRotated_Epoch:41 [004/005 (0040/0693)]\tLoss Ss: 0.019195\n","\tRotated_Epoch:41 [004/005 (0060/0693)]\tLoss Ss: 0.014304\n","\tRotated_Epoch:41 [004/005 (0080/0693)]\tLoss Ss: 0.016462\n","\tRotated_Epoch:41 [004/005 (0100/0693)]\tLoss Ss: 0.013744\n","\tRotated_Epoch:41 [004/005 (0120/0693)]\tLoss Ss: 0.014286\n","\tRotated_Epoch:41 [004/005 (0140/0693)]\tLoss Ss: 0.011473\n","\tRotated_Epoch:41 [004/005 (0160/0693)]\tLoss Ss: 0.017101\n","\tRotated_Epoch:41 [004/005 (0180/0693)]\tLoss Ss: 0.012917\n","\tRotated_Epoch:41 [004/005 (0200/0693)]\tLoss Ss: 0.013528\n","\tRotated_Epoch:41 [004/005 (0220/0693)]\tLoss Ss: 0.010727\n","\tRotated_Epoch:41 [004/005 (0240/0693)]\tLoss Ss: 0.010784\n","\tRotated_Epoch:41 [004/005 (0260/0693)]\tLoss Ss: 0.014021\n","\tRotated_Epoch:41 [004/005 (0280/0693)]\tLoss Ss: 0.010530\n","\tRotated_Epoch:41 [004/005 (0300/0693)]\tLoss Ss: 0.015072\n","\tRotated_Epoch:41 [004/005 (0320/0693)]\tLoss Ss: 0.011561\n","\tRotated_Epoch:41 [004/005 (0340/0693)]\tLoss Ss: 0.011073\n","\tRotated_Epoch:41 [004/005 (0360/0693)]\tLoss Ss: 0.013938\n","\tRotated_Epoch:41 [004/005 (0380/0693)]\tLoss Ss: 0.012749\n","\tRotated_Epoch:41 [004/005 (0400/0693)]\tLoss Ss: 0.011205\n","\tRotated_Epoch:41 [004/005 (0420/0693)]\tLoss Ss: 0.008070\n","\tRotated_Epoch:41 [004/005 (0440/0693)]\tLoss Ss: 0.013120\n","\tRotated_Epoch:41 [004/005 (0460/0693)]\tLoss Ss: 0.011959\n","\tRotated_Epoch:41 [004/005 (0480/0693)]\tLoss Ss: 0.013858\n","\tRotated_Epoch:41 [004/005 (0500/0693)]\tLoss Ss: 0.012609\n","\tRotated_Epoch:41 [004/005 (0520/0693)]\tLoss Ss: 0.017843\n","\tRotated_Epoch:41 [004/005 (0540/0693)]\tLoss Ss: 0.009862\n","\tRotated_Epoch:41 [004/005 (0560/0693)]\tLoss Ss: 0.014473\n","\tRotated_Epoch:41 [004/005 (0580/0693)]\tLoss Ss: 0.012880\n","\tRotated_Epoch:41 [004/005 (0600/0693)]\tLoss Ss: 0.009315\n","\tRotated_Epoch:41 [004/005 (0620/0693)]\tLoss Ss: 0.010997\n","\tRotated_Epoch:41 [004/005 (0640/0693)]\tLoss Ss: 0.010934\n","\tRotated_Epoch:41 [004/005 (0660/0693)]\tLoss Ss: 0.012899\n","\tRotated_Epoch:41 [004/005 (0680/0693)]\tLoss Ss: 0.008046\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tRotated_Epoch:41 [005/005 (0000/0614)]\tLoss Ss: 0.009409\n","\tRotated_Epoch:41 [005/005 (0020/0614)]\tLoss Ss: 0.012705\n","\tRotated_Epoch:41 [005/005 (0040/0614)]\tLoss Ss: 0.011382\n","\tRotated_Epoch:41 [005/005 (0060/0614)]\tLoss Ss: 0.006360\n","\tRotated_Epoch:41 [005/005 (0080/0614)]\tLoss Ss: 0.008375\n","\tRotated_Epoch:41 [005/005 (0100/0614)]\tLoss Ss: 0.011269\n","\tRotated_Epoch:41 [005/005 (0120/0614)]\tLoss Ss: 0.008452\n","\tRotated_Epoch:41 [005/005 (0140/0614)]\tLoss Ss: 0.004642\n","\tRotated_Epoch:41 [005/005 (0160/0614)]\tLoss Ss: 0.006514\n","\tRotated_Epoch:41 [005/005 (0180/0614)]\tLoss Ss: 0.011503\n","\tRotated_Epoch:41 [005/005 (0200/0614)]\tLoss Ss: 0.005043\n","\tRotated_Epoch:41 [005/005 (0220/0614)]\tLoss Ss: 0.007568\n","\tRotated_Epoch:41 [005/005 (0240/0614)]\tLoss Ss: 0.004429\n","\tRotated_Epoch:41 [005/005 (0260/0614)]\tLoss Ss: 0.009533\n","\tRotated_Epoch:41 [005/005 (0280/0614)]\tLoss Ss: 0.006541\n","\tRotated_Epoch:41 [005/005 (0300/0614)]\tLoss Ss: 0.004231\n","\tRotated_Epoch:41 [005/005 (0320/0614)]\tLoss Ss: 0.007519\n","\tRotated_Epoch:41 [005/005 (0340/0614)]\tLoss Ss: 0.006592\n","\tRotated_Epoch:41 [005/005 (0360/0614)]\tLoss Ss: 0.006356\n","\tRotated_Epoch:41 [005/005 (0380/0614)]\tLoss Ss: 0.005259\n","\tRotated_Epoch:41 [005/005 (0400/0614)]\tLoss Ss: 0.006775\n","\tRotated_Epoch:41 [005/005 (0420/0614)]\tLoss Ss: 0.006286\n","\tRotated_Epoch:41 [005/005 (0440/0614)]\tLoss Ss: 0.002926\n","\tRotated_Epoch:41 [005/005 (0460/0614)]\tLoss Ss: 0.003468\n","\tRotated_Epoch:41 [005/005 (0480/0614)]\tLoss Ss: 0.004231\n","\tRotated_Epoch:41 [005/005 (0500/0614)]\tLoss Ss: 0.004659\n","\tRotated_Epoch:41 [005/005 (0520/0614)]\tLoss Ss: 0.008975\n","\tRotated_Epoch:41 [005/005 (0540/0614)]\tLoss Ss: 0.006786\n","\tRotated_Epoch:41 [005/005 (0560/0614)]\tLoss Ss: 0.005478\n","\tRotated_Epoch:41 [005/005 (0580/0614)]\tLoss Ss: 0.004998\n","\tRotated_Epoch:41 [005/005 (0600/0614)]\tLoss Ss: 0.004284\n","new_1_6_d.nii\n","new_1_6_m.nii\n","new_1_7_d.nii\n","new_1_7_m.nii\n","new_2_5_d.nii\n","new_2_5_m.nii\n","new_2_6_d.nii\n","new_2_6_m.nii\n","new_2_7_d.nii\n","new_2_7_m.nii\n","new_3_d (2).nii\n","new_3_m (2).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (3).nii\n","new_3_m (3).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (7).nii\n","new_3_m (7).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (8).nii\n","new_3_m (8).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (9).nii\n","new_3_m (9).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_4_10_d_163526.nii\n","new_4_10_m_163526.nii.gz\n","new_4_11_d_084336.nii\n","new_4_11_m_084336.nii.gz\n","new_4_7_d_184422.nii\n","new_4_7_m_184422.nii.gz\n","new_4_8_d_143412.nii\n","new_4_8_m_143412.nii.gz\n","new_4_9_d_155347.nii\n","new_4_9_m_155347.nii.gz\n","\tEpoch: 41; Dice: 0.9671 +/- 0.0030; Loss: 7.5068\n","Begin Epoch 42\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tEpoch:42 [000/005 (0000/0614)]\tLoss Ss: 0.006358\n","\tEpoch:42 [000/005 (0020/0614)]\tLoss Ss: 0.007157\n","\tEpoch:42 [000/005 (0040/0614)]\tLoss Ss: 0.005904\n","\tEpoch:42 [000/005 (0060/0614)]\tLoss Ss: 0.003704\n","\tEpoch:42 [000/005 (0080/0614)]\tLoss Ss: 0.008325\n","\tEpoch:42 [000/005 (0100/0614)]\tLoss Ss: 0.005678\n","\tEpoch:42 [000/005 (0120/0614)]\tLoss Ss: 0.008687\n","\tEpoch:42 [000/005 (0140/0614)]\tLoss Ss: 0.002794\n","\tEpoch:42 [000/005 (0160/0614)]\tLoss Ss: 0.005250\n","\tEpoch:42 [000/005 (0180/0614)]\tLoss Ss: 0.004831\n","\tEpoch:42 [000/005 (0200/0614)]\tLoss Ss: 0.004237\n","\tEpoch:42 [000/005 (0220/0614)]\tLoss Ss: 0.006028\n","\tEpoch:42 [000/005 (0240/0614)]\tLoss Ss: 0.004193\n","\tEpoch:42 [000/005 (0260/0614)]\tLoss Ss: 0.005158\n","\tEpoch:42 [000/005 (0280/0614)]\tLoss Ss: 0.007104\n","\tEpoch:42 [000/005 (0300/0614)]\tLoss Ss: 0.005968\n","\tEpoch:42 [000/005 (0320/0614)]\tLoss Ss: 0.004695\n","\tEpoch:42 [000/005 (0340/0614)]\tLoss Ss: 0.003379\n","\tEpoch:42 [000/005 (0360/0614)]\tLoss Ss: 0.005008\n","\tEpoch:42 [000/005 (0380/0614)]\tLoss Ss: 0.005778\n","\tEpoch:42 [000/005 (0400/0614)]\tLoss Ss: 0.004376\n","\tEpoch:42 [000/005 (0420/0614)]\tLoss Ss: 0.006465\n","\tEpoch:42 [000/005 (0440/0614)]\tLoss Ss: 0.004804\n","\tEpoch:42 [000/005 (0460/0614)]\tLoss Ss: 0.005091\n","\tEpoch:42 [000/005 (0480/0614)]\tLoss Ss: 0.004249\n","\tEpoch:42 [000/005 (0500/0614)]\tLoss Ss: 0.005637\n","\tEpoch:42 [000/005 (0520/0614)]\tLoss Ss: 0.006719\n","\tEpoch:42 [000/005 (0540/0614)]\tLoss Ss: 0.005885\n","\tEpoch:42 [000/005 (0560/0614)]\tLoss Ss: 0.005384\n","\tEpoch:42 [000/005 (0580/0614)]\tLoss Ss: 0.004761\n","\tEpoch:42 [000/005 (0600/0614)]\tLoss Ss: 0.008511\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:42 [001/005 (0000/0755)]\tLoss Ss: 0.013021\n","\tEpoch:42 [001/005 (0020/0755)]\tLoss Ss: 0.013802\n","\tEpoch:42 [001/005 (0040/0755)]\tLoss Ss: 0.007684\n","\tEpoch:42 [001/005 (0060/0755)]\tLoss Ss: 0.010774\n","\tEpoch:42 [001/005 (0080/0755)]\tLoss Ss: 0.011239\n","\tEpoch:42 [001/005 (0100/0755)]\tLoss Ss: 0.016856\n","\tEpoch:42 [001/005 (0120/0755)]\tLoss Ss: 0.009480\n","\tEpoch:42 [001/005 (0140/0755)]\tLoss Ss: 0.009155\n","\tEpoch:42 [001/005 (0160/0755)]\tLoss Ss: 0.015596\n","\tEpoch:42 [001/005 (0180/0755)]\tLoss Ss: 0.010547\n","\tEpoch:42 [001/005 (0200/0755)]\tLoss Ss: 0.010624\n","\tEpoch:42 [001/005 (0220/0755)]\tLoss Ss: 0.008416\n","\tEpoch:42 [001/005 (0240/0755)]\tLoss Ss: 0.015163\n","\tEpoch:42 [001/005 (0260/0755)]\tLoss Ss: 0.017264\n","\tEpoch:42 [001/005 (0280/0755)]\tLoss Ss: 0.013285\n","\tEpoch:42 [001/005 (0300/0755)]\tLoss Ss: 0.012009\n","\tEpoch:42 [001/005 (0320/0755)]\tLoss Ss: 0.011580\n","\tEpoch:42 [001/005 (0340/0755)]\tLoss Ss: 0.009723\n","\tEpoch:42 [001/005 (0360/0755)]\tLoss Ss: 0.009518\n","\tEpoch:42 [001/005 (0380/0755)]\tLoss Ss: 0.010781\n","\tEpoch:42 [001/005 (0400/0755)]\tLoss Ss: 0.015038\n","\tEpoch:42 [001/005 (0420/0755)]\tLoss Ss: 0.010231\n","\tEpoch:42 [001/005 (0440/0755)]\tLoss Ss: 0.008026\n","\tEpoch:42 [001/005 (0460/0755)]\tLoss Ss: 0.023345\n","\tEpoch:42 [001/005 (0480/0755)]\tLoss Ss: 0.012748\n","\tEpoch:42 [001/005 (0500/0755)]\tLoss Ss: 0.007571\n","\tEpoch:42 [001/005 (0520/0755)]\tLoss Ss: 0.015286\n","\tEpoch:42 [001/005 (0540/0755)]\tLoss Ss: 0.007670\n","\tEpoch:42 [001/005 (0560/0755)]\tLoss Ss: 0.011285\n","\tEpoch:42 [001/005 (0580/0755)]\tLoss Ss: 0.008602\n","\tEpoch:42 [001/005 (0600/0755)]\tLoss Ss: 0.008482\n","\tEpoch:42 [001/005 (0620/0755)]\tLoss Ss: 0.005622\n","\tEpoch:42 [001/005 (0640/0755)]\tLoss Ss: 0.015135\n","\tEpoch:42 [001/005 (0660/0755)]\tLoss Ss: 0.011840\n","\tEpoch:42 [001/005 (0680/0755)]\tLoss Ss: 0.011781\n","\tEpoch:42 [001/005 (0700/0755)]\tLoss Ss: 0.012728\n","\tEpoch:42 [001/005 (0720/0755)]\tLoss Ss: 0.010517\n","\tEpoch:42 [001/005 (0740/0755)]\tLoss Ss: 0.009575\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tEpoch:42 [002/005 (0000/0693)]\tLoss Ss: 0.011113\n","\tEpoch:42 [002/005 (0020/0693)]\tLoss Ss: 0.010314\n","\tEpoch:42 [002/005 (0040/0693)]\tLoss Ss: 0.010946\n","\tEpoch:42 [002/005 (0060/0693)]\tLoss Ss: 0.008475\n","\tEpoch:42 [002/005 (0080/0693)]\tLoss Ss: 0.011593\n","\tEpoch:42 [002/005 (0100/0693)]\tLoss Ss: 0.008200\n","\tEpoch:42 [002/005 (0120/0693)]\tLoss Ss: 0.017064\n","\tEpoch:42 [002/005 (0140/0693)]\tLoss Ss: 0.009711\n","\tEpoch:42 [002/005 (0160/0693)]\tLoss Ss: 0.007573\n","\tEpoch:42 [002/005 (0180/0693)]\tLoss Ss: 0.010829\n","\tEpoch:42 [002/005 (0200/0693)]\tLoss Ss: 0.008515\n","\tEpoch:42 [002/005 (0220/0693)]\tLoss Ss: 0.012885\n","\tEpoch:42 [002/005 (0240/0693)]\tLoss Ss: 0.008576\n","\tEpoch:42 [002/005 (0260/0693)]\tLoss Ss: 0.012367\n","\tEpoch:42 [002/005 (0280/0693)]\tLoss Ss: 0.011244\n","\tEpoch:42 [002/005 (0300/0693)]\tLoss Ss: 0.014157\n","\tEpoch:42 [002/005 (0320/0693)]\tLoss Ss: 0.008547\n","\tEpoch:42 [002/005 (0340/0693)]\tLoss Ss: 0.007483\n","\tEpoch:42 [002/005 (0360/0693)]\tLoss Ss: 0.009995\n","\tEpoch:42 [002/005 (0380/0693)]\tLoss Ss: 0.012566\n","\tEpoch:42 [002/005 (0400/0693)]\tLoss Ss: 0.010538\n","\tEpoch:42 [002/005 (0420/0693)]\tLoss Ss: 0.014545\n","\tEpoch:42 [002/005 (0440/0693)]\tLoss Ss: 0.012939\n","\tEpoch:42 [002/005 (0460/0693)]\tLoss Ss: 0.008012\n","\tEpoch:42 [002/005 (0480/0693)]\tLoss Ss: 0.011234\n","\tEpoch:42 [002/005 (0500/0693)]\tLoss Ss: 0.008609\n","\tEpoch:42 [002/005 (0520/0693)]\tLoss Ss: 0.015259\n","\tEpoch:42 [002/005 (0540/0693)]\tLoss Ss: 0.011893\n","\tEpoch:42 [002/005 (0560/0693)]\tLoss Ss: 0.006187\n","\tEpoch:42 [002/005 (0580/0693)]\tLoss Ss: 0.009398\n","\tEpoch:42 [002/005 (0600/0693)]\tLoss Ss: 0.011700\n","\tEpoch:42 [002/005 (0620/0693)]\tLoss Ss: 0.010089\n","\tEpoch:42 [002/005 (0640/0693)]\tLoss Ss: 0.008794\n","\tEpoch:42 [002/005 (0660/0693)]\tLoss Ss: 0.008074\n","\tEpoch:42 [002/005 (0680/0693)]\tLoss Ss: 0.008140\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tEpoch:42 [003/005 (0000/0693)]\tLoss Ss: 0.016390\n","\tEpoch:42 [003/005 (0020/0693)]\tLoss Ss: 0.015705\n","\tEpoch:42 [003/005 (0040/0693)]\tLoss Ss: 0.018009\n","\tEpoch:42 [003/005 (0060/0693)]\tLoss Ss: 0.013129\n","\tEpoch:42 [003/005 (0080/0693)]\tLoss Ss: 0.014861\n","\tEpoch:42 [003/005 (0100/0693)]\tLoss Ss: 0.014124\n","\tEpoch:42 [003/005 (0120/0693)]\tLoss Ss: 0.016780\n","\tEpoch:42 [003/005 (0140/0693)]\tLoss Ss: 0.010325\n","\tEpoch:42 [003/005 (0160/0693)]\tLoss Ss: 0.008701\n","\tEpoch:42 [003/005 (0180/0693)]\tLoss Ss: 0.012009\n","\tEpoch:42 [003/005 (0200/0693)]\tLoss Ss: 0.011667\n","\tEpoch:42 [003/005 (0220/0693)]\tLoss Ss: 0.016053\n","\tEpoch:42 [003/005 (0240/0693)]\tLoss Ss: 0.012024\n","\tEpoch:42 [003/005 (0260/0693)]\tLoss Ss: 0.011710\n","\tEpoch:42 [003/005 (0280/0693)]\tLoss Ss: 0.011600\n","\tEpoch:42 [003/005 (0300/0693)]\tLoss Ss: 0.011025\n","\tEpoch:42 [003/005 (0320/0693)]\tLoss Ss: 0.015042\n","\tEpoch:42 [003/005 (0340/0693)]\tLoss Ss: 0.011602\n","\tEpoch:42 [003/005 (0360/0693)]\tLoss Ss: 0.014226\n","\tEpoch:42 [003/005 (0380/0693)]\tLoss Ss: 0.011503\n","\tEpoch:42 [003/005 (0400/0693)]\tLoss Ss: 0.010986\n","\tEpoch:42 [003/005 (0420/0693)]\tLoss Ss: 0.013924\n","\tEpoch:42 [003/005 (0440/0693)]\tLoss Ss: 0.014670\n","\tEpoch:42 [003/005 (0460/0693)]\tLoss Ss: 0.010058\n","\tEpoch:42 [003/005 (0480/0693)]\tLoss Ss: 0.010247\n","\tEpoch:42 [003/005 (0500/0693)]\tLoss Ss: 0.007735\n","\tEpoch:42 [003/005 (0520/0693)]\tLoss Ss: 0.010726\n","\tEpoch:42 [003/005 (0540/0693)]\tLoss Ss: 0.010881\n","\tEpoch:42 [003/005 (0560/0693)]\tLoss Ss: 0.011658\n","\tEpoch:42 [003/005 (0580/0693)]\tLoss Ss: 0.010979\n","\tEpoch:42 [003/005 (0600/0693)]\tLoss Ss: 0.013753\n","\tEpoch:42 [003/005 (0620/0693)]\tLoss Ss: 0.007928\n","\tEpoch:42 [003/005 (0640/0693)]\tLoss Ss: 0.017635\n","\tEpoch:42 [003/005 (0660/0693)]\tLoss Ss: 0.010719\n","\tEpoch:42 [003/005 (0680/0693)]\tLoss Ss: 0.009026\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:42 [004/005 (0000/0755)]\tLoss Ss: 0.011516\n","\tEpoch:42 [004/005 (0020/0755)]\tLoss Ss: 0.017303\n","\tEpoch:42 [004/005 (0040/0755)]\tLoss Ss: 0.018680\n","\tEpoch:42 [004/005 (0060/0755)]\tLoss Ss: 0.014816\n","\tEpoch:42 [004/005 (0080/0755)]\tLoss Ss: 0.023801\n","\tEpoch:42 [004/005 (0100/0755)]\tLoss Ss: 0.013617\n","\tEpoch:42 [004/005 (0120/0755)]\tLoss Ss: 0.010690\n","\tEpoch:42 [004/005 (0140/0755)]\tLoss Ss: 0.013531\n","\tEpoch:42 [004/005 (0160/0755)]\tLoss Ss: 0.017961\n","\tEpoch:42 [004/005 (0180/0755)]\tLoss Ss: 0.012357\n","\tEpoch:42 [004/005 (0200/0755)]\tLoss Ss: 0.013976\n","\tEpoch:42 [004/005 (0220/0755)]\tLoss Ss: 0.014831\n","\tEpoch:42 [004/005 (0240/0755)]\tLoss Ss: 0.010998\n","\tEpoch:42 [004/005 (0260/0755)]\tLoss Ss: 0.013985\n","\tEpoch:42 [004/005 (0280/0755)]\tLoss Ss: 0.014159\n","\tEpoch:42 [004/005 (0300/0755)]\tLoss Ss: 0.011280\n","\tEpoch:42 [004/005 (0320/0755)]\tLoss Ss: 0.013279\n","\tEpoch:42 [004/005 (0340/0755)]\tLoss Ss: 0.008500\n","\tEpoch:42 [004/005 (0360/0755)]\tLoss Ss: 0.010322\n","\tEpoch:42 [004/005 (0380/0755)]\tLoss Ss: 0.011216\n","\tEpoch:42 [004/005 (0400/0755)]\tLoss Ss: 0.018316\n","\tEpoch:42 [004/005 (0420/0755)]\tLoss Ss: 0.012960\n","\tEpoch:42 [004/005 (0440/0755)]\tLoss Ss: 0.012917\n","\tEpoch:42 [004/005 (0460/0755)]\tLoss Ss: 0.013362\n","\tEpoch:42 [004/005 (0480/0755)]\tLoss Ss: 0.010406\n","\tEpoch:42 [004/005 (0500/0755)]\tLoss Ss: 0.009502\n","\tEpoch:42 [004/005 (0520/0755)]\tLoss Ss: 0.008145\n","\tEpoch:42 [004/005 (0540/0755)]\tLoss Ss: 0.015521\n","\tEpoch:42 [004/005 (0560/0755)]\tLoss Ss: 0.012424\n","\tEpoch:42 [004/005 (0580/0755)]\tLoss Ss: 0.011738\n","\tEpoch:42 [004/005 (0600/0755)]\tLoss Ss: 0.010942\n","\tEpoch:42 [004/005 (0620/0755)]\tLoss Ss: 0.017645\n","\tEpoch:42 [004/005 (0640/0755)]\tLoss Ss: 0.012087\n","\tEpoch:42 [004/005 (0660/0755)]\tLoss Ss: 0.013611\n","\tEpoch:42 [004/005 (0680/0755)]\tLoss Ss: 0.011263\n","\tEpoch:42 [004/005 (0700/0755)]\tLoss Ss: 0.015518\n","\tEpoch:42 [004/005 (0720/0755)]\tLoss Ss: 0.016485\n","\tEpoch:42 [004/005 (0740/0755)]\tLoss Ss: 0.010496\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tEpoch:42 [005/005 (0000/0588)]\tLoss Ss: 0.005518\n","\tEpoch:42 [005/005 (0020/0588)]\tLoss Ss: 0.007301\n","\tEpoch:42 [005/005 (0040/0588)]\tLoss Ss: 0.006403\n","\tEpoch:42 [005/005 (0060/0588)]\tLoss Ss: 0.006470\n","\tEpoch:42 [005/005 (0080/0588)]\tLoss Ss: 0.005268\n","\tEpoch:42 [005/005 (0100/0588)]\tLoss Ss: 0.006803\n","\tEpoch:42 [005/005 (0120/0588)]\tLoss Ss: 0.004477\n","\tEpoch:42 [005/005 (0140/0588)]\tLoss Ss: 0.006661\n","\tEpoch:42 [005/005 (0160/0588)]\tLoss Ss: 0.006873\n","\tEpoch:42 [005/005 (0180/0588)]\tLoss Ss: 0.003769\n","\tEpoch:42 [005/005 (0200/0588)]\tLoss Ss: 0.004547\n","\tEpoch:42 [005/005 (0220/0588)]\tLoss Ss: 0.004516\n","\tEpoch:42 [005/005 (0240/0588)]\tLoss Ss: 0.004456\n","\tEpoch:42 [005/005 (0260/0588)]\tLoss Ss: 0.002592\n","\tEpoch:42 [005/005 (0280/0588)]\tLoss Ss: 0.003460\n","\tEpoch:42 [005/005 (0300/0588)]\tLoss Ss: 0.004728\n","\tEpoch:42 [005/005 (0320/0588)]\tLoss Ss: 0.002015\n","\tEpoch:42 [005/005 (0340/0588)]\tLoss Ss: 0.002963\n","\tEpoch:42 [005/005 (0360/0588)]\tLoss Ss: 0.003468\n","\tEpoch:42 [005/005 (0380/0588)]\tLoss Ss: 0.004859\n","\tEpoch:42 [005/005 (0400/0588)]\tLoss Ss: 0.004373\n","\tEpoch:42 [005/005 (0420/0588)]\tLoss Ss: 0.002786\n","\tEpoch:42 [005/005 (0440/0588)]\tLoss Ss: 0.002462\n","\tEpoch:42 [005/005 (0460/0588)]\tLoss Ss: 0.003524\n","\tEpoch:42 [005/005 (0480/0588)]\tLoss Ss: 0.005958\n","\tEpoch:42 [005/005 (0500/0588)]\tLoss Ss: 0.003379\n","\tEpoch:42 [005/005 (0520/0588)]\tLoss Ss: 0.002874\n","\tEpoch:42 [005/005 (0540/0588)]\tLoss Ss: 0.003077\n","\tEpoch:42 [005/005 (0560/0588)]\tLoss Ss: 0.005093\n","\tEpoch:42 [005/005 (0580/0588)]\tLoss Ss: 0.008136\n","Now train the rotated image\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tRotated_Epoch:42 [000/005 (0000/0693)]\tLoss Ss: 0.012772\n","\tRotated_Epoch:42 [000/005 (0020/0693)]\tLoss Ss: 0.016937\n","\tRotated_Epoch:42 [000/005 (0040/0693)]\tLoss Ss: 0.013456\n","\tRotated_Epoch:42 [000/005 (0060/0693)]\tLoss Ss: 0.013775\n","\tRotated_Epoch:42 [000/005 (0080/0693)]\tLoss Ss: 0.015201\n","\tRotated_Epoch:42 [000/005 (0100/0693)]\tLoss Ss: 0.014859\n","\tRotated_Epoch:42 [000/005 (0120/0693)]\tLoss Ss: 0.014552\n","\tRotated_Epoch:42 [000/005 (0140/0693)]\tLoss Ss: 0.021663\n","\tRotated_Epoch:42 [000/005 (0160/0693)]\tLoss Ss: 0.016467\n","\tRotated_Epoch:42 [000/005 (0180/0693)]\tLoss Ss: 0.014109\n","\tRotated_Epoch:42 [000/005 (0200/0693)]\tLoss Ss: 0.013511\n","\tRotated_Epoch:42 [000/005 (0220/0693)]\tLoss Ss: 0.014439\n","\tRotated_Epoch:42 [000/005 (0240/0693)]\tLoss Ss: 0.014414\n","\tRotated_Epoch:42 [000/005 (0260/0693)]\tLoss Ss: 0.018972\n","\tRotated_Epoch:42 [000/005 (0280/0693)]\tLoss Ss: 0.015828\n","\tRotated_Epoch:42 [000/005 (0300/0693)]\tLoss Ss: 0.014359\n","\tRotated_Epoch:42 [000/005 (0320/0693)]\tLoss Ss: 0.015531\n","\tRotated_Epoch:42 [000/005 (0340/0693)]\tLoss Ss: 0.016241\n","\tRotated_Epoch:42 [000/005 (0360/0693)]\tLoss Ss: 0.014113\n","\tRotated_Epoch:42 [000/005 (0380/0693)]\tLoss Ss: 0.011402\n","\tRotated_Epoch:42 [000/005 (0400/0693)]\tLoss Ss: 0.015635\n","\tRotated_Epoch:42 [000/005 (0420/0693)]\tLoss Ss: 0.011874\n","\tRotated_Epoch:42 [000/005 (0440/0693)]\tLoss Ss: 0.012519\n","\tRotated_Epoch:42 [000/005 (0460/0693)]\tLoss Ss: 0.013574\n","\tRotated_Epoch:42 [000/005 (0480/0693)]\tLoss Ss: 0.014753\n","\tRotated_Epoch:42 [000/005 (0500/0693)]\tLoss Ss: 0.007365\n","\tRotated_Epoch:42 [000/005 (0520/0693)]\tLoss Ss: 0.012688\n","\tRotated_Epoch:42 [000/005 (0540/0693)]\tLoss Ss: 0.009776\n","\tRotated_Epoch:42 [000/005 (0560/0693)]\tLoss Ss: 0.013194\n","\tRotated_Epoch:42 [000/005 (0580/0693)]\tLoss Ss: 0.014371\n","\tRotated_Epoch:42 [000/005 (0600/0693)]\tLoss Ss: 0.011717\n","\tRotated_Epoch:42 [000/005 (0620/0693)]\tLoss Ss: 0.013682\n","\tRotated_Epoch:42 [000/005 (0640/0693)]\tLoss Ss: 0.017595\n","\tRotated_Epoch:42 [000/005 (0660/0693)]\tLoss Ss: 0.016738\n","\tRotated_Epoch:42 [000/005 (0680/0693)]\tLoss Ss: 0.014297\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tRotated_Epoch:42 [001/005 (0000/0614)]\tLoss Ss: 0.002918\n","\tRotated_Epoch:42 [001/005 (0020/0614)]\tLoss Ss: 0.004798\n","\tRotated_Epoch:42 [001/005 (0040/0614)]\tLoss Ss: 0.002840\n","\tRotated_Epoch:42 [001/005 (0060/0614)]\tLoss Ss: 0.007407\n","\tRotated_Epoch:42 [001/005 (0080/0614)]\tLoss Ss: 0.005577\n","\tRotated_Epoch:42 [001/005 (0100/0614)]\tLoss Ss: 0.004972\n","\tRotated_Epoch:42 [001/005 (0120/0614)]\tLoss Ss: 0.004377\n","\tRotated_Epoch:42 [001/005 (0140/0614)]\tLoss Ss: 0.005030\n","\tRotated_Epoch:42 [001/005 (0160/0614)]\tLoss Ss: 0.006822\n","\tRotated_Epoch:42 [001/005 (0180/0614)]\tLoss Ss: 0.005752\n","\tRotated_Epoch:42 [001/005 (0200/0614)]\tLoss Ss: 0.006008\n","\tRotated_Epoch:42 [001/005 (0220/0614)]\tLoss Ss: 0.004557\n","\tRotated_Epoch:42 [001/005 (0240/0614)]\tLoss Ss: 0.007005\n","\tRotated_Epoch:42 [001/005 (0260/0614)]\tLoss Ss: 0.006849\n","\tRotated_Epoch:42 [001/005 (0280/0614)]\tLoss Ss: 0.004665\n","\tRotated_Epoch:42 [001/005 (0300/0614)]\tLoss Ss: 0.005865\n","\tRotated_Epoch:42 [001/005 (0320/0614)]\tLoss Ss: 0.007367\n","\tRotated_Epoch:42 [001/005 (0340/0614)]\tLoss Ss: 0.006226\n","\tRotated_Epoch:42 [001/005 (0360/0614)]\tLoss Ss: 0.005743\n","\tRotated_Epoch:42 [001/005 (0380/0614)]\tLoss Ss: 0.004872\n","\tRotated_Epoch:42 [001/005 (0400/0614)]\tLoss Ss: 0.005362\n","\tRotated_Epoch:42 [001/005 (0420/0614)]\tLoss Ss: 0.004527\n","\tRotated_Epoch:42 [001/005 (0440/0614)]\tLoss Ss: 0.005400\n","\tRotated_Epoch:42 [001/005 (0460/0614)]\tLoss Ss: 0.004655\n","\tRotated_Epoch:42 [001/005 (0480/0614)]\tLoss Ss: 0.005665\n","\tRotated_Epoch:42 [001/005 (0500/0614)]\tLoss Ss: 0.007089\n","\tRotated_Epoch:42 [001/005 (0520/0614)]\tLoss Ss: 0.007089\n","\tRotated_Epoch:42 [001/005 (0540/0614)]\tLoss Ss: 0.007238\n","\tRotated_Epoch:42 [001/005 (0560/0614)]\tLoss Ss: 0.002688\n","\tRotated_Epoch:42 [001/005 (0580/0614)]\tLoss Ss: 0.005570\n","\tRotated_Epoch:42 [001/005 (0600/0614)]\tLoss Ss: 0.005112\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tRotated_Epoch:42 [002/005 (0000/0693)]\tLoss Ss: 0.009440\n","\tRotated_Epoch:42 [002/005 (0020/0693)]\tLoss Ss: 0.012887\n","\tRotated_Epoch:42 [002/005 (0040/0693)]\tLoss Ss: 0.017556\n","\tRotated_Epoch:42 [002/005 (0060/0693)]\tLoss Ss: 0.011146\n","\tRotated_Epoch:42 [002/005 (0080/0693)]\tLoss Ss: 0.012903\n","\tRotated_Epoch:42 [002/005 (0100/0693)]\tLoss Ss: 0.012696\n","\tRotated_Epoch:42 [002/005 (0120/0693)]\tLoss Ss: 0.011610\n","\tRotated_Epoch:42 [002/005 (0140/0693)]\tLoss Ss: 0.010936\n","\tRotated_Epoch:42 [002/005 (0160/0693)]\tLoss Ss: 0.009215\n","\tRotated_Epoch:42 [002/005 (0180/0693)]\tLoss Ss: 0.010638\n","\tRotated_Epoch:42 [002/005 (0200/0693)]\tLoss Ss: 0.010861\n","\tRotated_Epoch:42 [002/005 (0220/0693)]\tLoss Ss: 0.009206\n","\tRotated_Epoch:42 [002/005 (0240/0693)]\tLoss Ss: 0.011233\n","\tRotated_Epoch:42 [002/005 (0260/0693)]\tLoss Ss: 0.018684\n","\tRotated_Epoch:42 [002/005 (0280/0693)]\tLoss Ss: 0.010166\n","\tRotated_Epoch:42 [002/005 (0300/0693)]\tLoss Ss: 0.009427\n","\tRotated_Epoch:42 [002/005 (0320/0693)]\tLoss Ss: 0.010905\n","\tRotated_Epoch:42 [002/005 (0340/0693)]\tLoss Ss: 0.015788\n","\tRotated_Epoch:42 [002/005 (0360/0693)]\tLoss Ss: 0.008861\n","\tRotated_Epoch:42 [002/005 (0380/0693)]\tLoss Ss: 0.010575\n","\tRotated_Epoch:42 [002/005 (0400/0693)]\tLoss Ss: 0.014262\n","\tRotated_Epoch:42 [002/005 (0420/0693)]\tLoss Ss: 0.010858\n","\tRotated_Epoch:42 [002/005 (0440/0693)]\tLoss Ss: 0.009064\n","\tRotated_Epoch:42 [002/005 (0460/0693)]\tLoss Ss: 0.011900\n","\tRotated_Epoch:42 [002/005 (0480/0693)]\tLoss Ss: 0.011985\n","\tRotated_Epoch:42 [002/005 (0500/0693)]\tLoss Ss: 0.011478\n","\tRotated_Epoch:42 [002/005 (0520/0693)]\tLoss Ss: 0.011313\n","\tRotated_Epoch:42 [002/005 (0540/0693)]\tLoss Ss: 0.014357\n","\tRotated_Epoch:42 [002/005 (0560/0693)]\tLoss Ss: 0.009385\n","\tRotated_Epoch:42 [002/005 (0580/0693)]\tLoss Ss: 0.011552\n","\tRotated_Epoch:42 [002/005 (0600/0693)]\tLoss Ss: 0.008169\n","\tRotated_Epoch:42 [002/005 (0620/0693)]\tLoss Ss: 0.011472\n","\tRotated_Epoch:42 [002/005 (0640/0693)]\tLoss Ss: 0.012415\n","\tRotated_Epoch:42 [002/005 (0660/0693)]\tLoss Ss: 0.013638\n","\tRotated_Epoch:42 [002/005 (0680/0693)]\tLoss Ss: 0.017576\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:42 [003/005 (0000/0755)]\tLoss Ss: 0.015259\n","\tRotated_Epoch:42 [003/005 (0020/0755)]\tLoss Ss: 0.010843\n","\tRotated_Epoch:42 [003/005 (0040/0755)]\tLoss Ss: 0.012323\n","\tRotated_Epoch:42 [003/005 (0060/0755)]\tLoss Ss: 0.011957\n","\tRotated_Epoch:42 [003/005 (0080/0755)]\tLoss Ss: 0.016487\n","\tRotated_Epoch:42 [003/005 (0100/0755)]\tLoss Ss: 0.013273\n","\tRotated_Epoch:42 [003/005 (0120/0755)]\tLoss Ss: 0.015894\n","\tRotated_Epoch:42 [003/005 (0140/0755)]\tLoss Ss: 0.007789\n","\tRotated_Epoch:42 [003/005 (0160/0755)]\tLoss Ss: 0.015750\n","\tRotated_Epoch:42 [003/005 (0180/0755)]\tLoss Ss: 0.014143\n","\tRotated_Epoch:42 [003/005 (0200/0755)]\tLoss Ss: 0.012694\n","\tRotated_Epoch:42 [003/005 (0220/0755)]\tLoss Ss: 0.015126\n","\tRotated_Epoch:42 [003/005 (0240/0755)]\tLoss Ss: 0.013772\n","\tRotated_Epoch:42 [003/005 (0260/0755)]\tLoss Ss: 0.010691\n","\tRotated_Epoch:42 [003/005 (0280/0755)]\tLoss Ss: 0.023445\n","\tRotated_Epoch:42 [003/005 (0300/0755)]\tLoss Ss: 0.012026\n","\tRotated_Epoch:42 [003/005 (0320/0755)]\tLoss Ss: 0.013415\n","\tRotated_Epoch:42 [003/005 (0340/0755)]\tLoss Ss: 0.014190\n","\tRotated_Epoch:42 [003/005 (0360/0755)]\tLoss Ss: 0.015328\n","\tRotated_Epoch:42 [003/005 (0380/0755)]\tLoss Ss: 0.010876\n","\tRotated_Epoch:42 [003/005 (0400/0755)]\tLoss Ss: 0.012959\n","\tRotated_Epoch:42 [003/005 (0420/0755)]\tLoss Ss: 0.014918\n","\tRotated_Epoch:42 [003/005 (0440/0755)]\tLoss Ss: 0.013548\n","\tRotated_Epoch:42 [003/005 (0460/0755)]\tLoss Ss: 0.012405\n","\tRotated_Epoch:42 [003/005 (0480/0755)]\tLoss Ss: 0.013446\n","\tRotated_Epoch:42 [003/005 (0500/0755)]\tLoss Ss: 0.013545\n","\tRotated_Epoch:42 [003/005 (0520/0755)]\tLoss Ss: 0.014982\n","\tRotated_Epoch:42 [003/005 (0540/0755)]\tLoss Ss: 0.007865\n","\tRotated_Epoch:42 [003/005 (0560/0755)]\tLoss Ss: 0.011875\n","\tRotated_Epoch:42 [003/005 (0580/0755)]\tLoss Ss: 0.013412\n","\tRotated_Epoch:42 [003/005 (0600/0755)]\tLoss Ss: 0.009919\n","\tRotated_Epoch:42 [003/005 (0620/0755)]\tLoss Ss: 0.013036\n","\tRotated_Epoch:42 [003/005 (0640/0755)]\tLoss Ss: 0.017752\n","\tRotated_Epoch:42 [003/005 (0660/0755)]\tLoss Ss: 0.009742\n","\tRotated_Epoch:42 [003/005 (0680/0755)]\tLoss Ss: 0.016159\n","\tRotated_Epoch:42 [003/005 (0700/0755)]\tLoss Ss: 0.015212\n","\tRotated_Epoch:42 [003/005 (0720/0755)]\tLoss Ss: 0.009665\n","\tRotated_Epoch:42 [003/005 (0740/0755)]\tLoss Ss: 0.010346\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tRotated_Epoch:42 [004/005 (0000/0588)]\tLoss Ss: 0.122631\n","\tRotated_Epoch:42 [004/005 (0020/0588)]\tLoss Ss: 0.076583\n","\tRotated_Epoch:42 [004/005 (0040/0588)]\tLoss Ss: 0.053091\n","\tRotated_Epoch:42 [004/005 (0060/0588)]\tLoss Ss: 0.060738\n","\tRotated_Epoch:42 [004/005 (0080/0588)]\tLoss Ss: 0.036079\n","\tRotated_Epoch:42 [004/005 (0100/0588)]\tLoss Ss: 0.050010\n","\tRotated_Epoch:42 [004/005 (0120/0588)]\tLoss Ss: 0.057149\n","\tRotated_Epoch:42 [004/005 (0140/0588)]\tLoss Ss: 0.053204\n","\tRotated_Epoch:42 [004/005 (0160/0588)]\tLoss Ss: 0.073318\n","\tRotated_Epoch:42 [004/005 (0180/0588)]\tLoss Ss: 0.041086\n","\tRotated_Epoch:42 [004/005 (0200/0588)]\tLoss Ss: 0.067988\n","\tRotated_Epoch:42 [004/005 (0220/0588)]\tLoss Ss: 0.055232\n","\tRotated_Epoch:42 [004/005 (0240/0588)]\tLoss Ss: 0.052188\n","\tRotated_Epoch:42 [004/005 (0260/0588)]\tLoss Ss: 0.035431\n","\tRotated_Epoch:42 [004/005 (0280/0588)]\tLoss Ss: 0.057788\n","\tRotated_Epoch:42 [004/005 (0300/0588)]\tLoss Ss: 0.048169\n","\tRotated_Epoch:42 [004/005 (0320/0588)]\tLoss Ss: 0.061071\n","\tRotated_Epoch:42 [004/005 (0340/0588)]\tLoss Ss: 0.049669\n","\tRotated_Epoch:42 [004/005 (0360/0588)]\tLoss Ss: 0.048013\n","\tRotated_Epoch:42 [004/005 (0380/0588)]\tLoss Ss: 0.061195\n","\tRotated_Epoch:42 [004/005 (0400/0588)]\tLoss Ss: 0.061121\n","\tRotated_Epoch:42 [004/005 (0420/0588)]\tLoss Ss: 0.064611\n","\tRotated_Epoch:42 [004/005 (0440/0588)]\tLoss Ss: 0.042256\n","\tRotated_Epoch:42 [004/005 (0460/0588)]\tLoss Ss: 0.062997\n","\tRotated_Epoch:42 [004/005 (0480/0588)]\tLoss Ss: 0.027377\n","\tRotated_Epoch:42 [004/005 (0500/0588)]\tLoss Ss: 0.073658\n","\tRotated_Epoch:42 [004/005 (0520/0588)]\tLoss Ss: 0.036087\n","\tRotated_Epoch:42 [004/005 (0540/0588)]\tLoss Ss: 0.044082\n","\tRotated_Epoch:42 [004/005 (0560/0588)]\tLoss Ss: 0.049622\n","\tRotated_Epoch:42 [004/005 (0580/0588)]\tLoss Ss: 0.034879\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:42 [005/005 (0000/0755)]\tLoss Ss: 0.116438\n","\tRotated_Epoch:42 [005/005 (0020/0755)]\tLoss Ss: 0.084937\n","\tRotated_Epoch:42 [005/005 (0040/0755)]\tLoss Ss: 0.065192\n","\tRotated_Epoch:42 [005/005 (0060/0755)]\tLoss Ss: 0.078814\n","\tRotated_Epoch:42 [005/005 (0080/0755)]\tLoss Ss: 0.050406\n","\tRotated_Epoch:42 [005/005 (0100/0755)]\tLoss Ss: 0.056229\n","\tRotated_Epoch:42 [005/005 (0120/0755)]\tLoss Ss: 0.064821\n","\tRotated_Epoch:42 [005/005 (0140/0755)]\tLoss Ss: 0.034406\n","\tRotated_Epoch:42 [005/005 (0160/0755)]\tLoss Ss: 0.026627\n","\tRotated_Epoch:42 [005/005 (0180/0755)]\tLoss Ss: 0.039111\n","\tRotated_Epoch:42 [005/005 (0200/0755)]\tLoss Ss: 0.038769\n","\tRotated_Epoch:42 [005/005 (0220/0755)]\tLoss Ss: 0.038947\n","\tRotated_Epoch:42 [005/005 (0240/0755)]\tLoss Ss: 0.028004\n","\tRotated_Epoch:42 [005/005 (0260/0755)]\tLoss Ss: 0.027839\n","\tRotated_Epoch:42 [005/005 (0280/0755)]\tLoss Ss: 0.043777\n","\tRotated_Epoch:42 [005/005 (0300/0755)]\tLoss Ss: 0.027832\n","\tRotated_Epoch:42 [005/005 (0320/0755)]\tLoss Ss: 0.027997\n","\tRotated_Epoch:42 [005/005 (0340/0755)]\tLoss Ss: 0.041802\n","\tRotated_Epoch:42 [005/005 (0360/0755)]\tLoss Ss: 0.025448\n","\tRotated_Epoch:42 [005/005 (0380/0755)]\tLoss Ss: 0.036967\n","\tRotated_Epoch:42 [005/005 (0400/0755)]\tLoss Ss: 0.026927\n","\tRotated_Epoch:42 [005/005 (0420/0755)]\tLoss Ss: 0.031499\n","\tRotated_Epoch:42 [005/005 (0440/0755)]\tLoss Ss: 0.030762\n","\tRotated_Epoch:42 [005/005 (0460/0755)]\tLoss Ss: 0.020665\n","\tRotated_Epoch:42 [005/005 (0480/0755)]\tLoss Ss: 0.031723\n","\tRotated_Epoch:42 [005/005 (0500/0755)]\tLoss Ss: 0.037693\n","\tRotated_Epoch:42 [005/005 (0520/0755)]\tLoss Ss: 0.017521\n","\tRotated_Epoch:42 [005/005 (0540/0755)]\tLoss Ss: 0.026390\n","\tRotated_Epoch:42 [005/005 (0560/0755)]\tLoss Ss: 0.027919\n","\tRotated_Epoch:42 [005/005 (0580/0755)]\tLoss Ss: 0.043970\n","\tRotated_Epoch:42 [005/005 (0600/0755)]\tLoss Ss: 0.025960\n","\tRotated_Epoch:42 [005/005 (0620/0755)]\tLoss Ss: 0.027538\n","\tRotated_Epoch:42 [005/005 (0640/0755)]\tLoss Ss: 0.028883\n","\tRotated_Epoch:42 [005/005 (0660/0755)]\tLoss Ss: 0.026176\n","\tRotated_Epoch:42 [005/005 (0680/0755)]\tLoss Ss: 0.031064\n","\tRotated_Epoch:42 [005/005 (0700/0755)]\tLoss Ss: 0.027557\n","\tRotated_Epoch:42 [005/005 (0720/0755)]\tLoss Ss: 0.031297\n","\tRotated_Epoch:42 [005/005 (0740/0755)]\tLoss Ss: 0.023652\n","new_1_6_d.nii\n","new_1_6_m.nii\n","new_1_7_d.nii\n","new_1_7_m.nii\n","new_2_5_d.nii\n","new_2_5_m.nii\n","new_2_6_d.nii\n","new_2_6_m.nii\n","new_2_7_d.nii\n","new_2_7_m.nii\n","new_3_d (2).nii\n","new_3_m (2).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (3).nii\n","new_3_m (3).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (7).nii\n","new_3_m (7).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (8).nii\n","new_3_m (8).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (9).nii\n","new_3_m (9).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_4_10_d_163526.nii\n","new_4_10_m_163526.nii.gz\n","new_4_11_d_084336.nii\n","new_4_11_m_084336.nii.gz\n","new_4_7_d_184422.nii\n","new_4_7_m_184422.nii.gz\n","new_4_8_d_143412.nii\n","new_4_8_m_143412.nii.gz\n","new_4_9_d_155347.nii\n","new_4_9_m_155347.nii.gz\n","\tEpoch: 42; Dice: 0.9098 +/- 0.0149; Loss: 6.7896\n","Begin Epoch 43\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tEpoch:43 [000/005 (0000/0614)]\tLoss Ss: 0.014601\n","\tEpoch:43 [000/005 (0020/0614)]\tLoss Ss: 0.012923\n","\tEpoch:43 [000/005 (0040/0614)]\tLoss Ss: 0.011463\n","\tEpoch:43 [000/005 (0060/0614)]\tLoss Ss: 0.010283\n","\tEpoch:43 [000/005 (0080/0614)]\tLoss Ss: 0.004948\n","\tEpoch:43 [000/005 (0100/0614)]\tLoss Ss: 0.006318\n","\tEpoch:43 [000/005 (0120/0614)]\tLoss Ss: 0.008070\n","\tEpoch:43 [000/005 (0140/0614)]\tLoss Ss: 0.007880\n","\tEpoch:43 [000/005 (0160/0614)]\tLoss Ss: 0.006809\n","\tEpoch:43 [000/005 (0180/0614)]\tLoss Ss: 0.005339\n","\tEpoch:43 [000/005 (0200/0614)]\tLoss Ss: 0.003992\n","\tEpoch:43 [000/005 (0220/0614)]\tLoss Ss: 0.010265\n","\tEpoch:43 [000/005 (0240/0614)]\tLoss Ss: 0.006116\n","\tEpoch:43 [000/005 (0260/0614)]\tLoss Ss: 0.005867\n","\tEpoch:43 [000/005 (0280/0614)]\tLoss Ss: 0.005318\n","\tEpoch:43 [000/005 (0300/0614)]\tLoss Ss: 0.004283\n","\tEpoch:43 [000/005 (0320/0614)]\tLoss Ss: 0.005189\n","\tEpoch:43 [000/005 (0340/0614)]\tLoss Ss: 0.006508\n","\tEpoch:43 [000/005 (0360/0614)]\tLoss Ss: 0.006163\n","\tEpoch:43 [000/005 (0380/0614)]\tLoss Ss: 0.005183\n","\tEpoch:43 [000/005 (0400/0614)]\tLoss Ss: 0.005282\n","\tEpoch:43 [000/005 (0420/0614)]\tLoss Ss: 0.005421\n","\tEpoch:43 [000/005 (0440/0614)]\tLoss Ss: 0.004115\n","\tEpoch:43 [000/005 (0460/0614)]\tLoss Ss: 0.006190\n","\tEpoch:43 [000/005 (0480/0614)]\tLoss Ss: 0.004897\n","\tEpoch:43 [000/005 (0500/0614)]\tLoss Ss: 0.006262\n","\tEpoch:43 [000/005 (0520/0614)]\tLoss Ss: 0.005952\n","\tEpoch:43 [000/005 (0540/0614)]\tLoss Ss: 0.006234\n","\tEpoch:43 [000/005 (0560/0614)]\tLoss Ss: 0.003688\n","\tEpoch:43 [000/005 (0580/0614)]\tLoss Ss: 0.005944\n","\tEpoch:43 [000/005 (0600/0614)]\tLoss Ss: 0.003831\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:43 [001/005 (0000/0755)]\tLoss Ss: 0.016746\n","\tEpoch:43 [001/005 (0020/0755)]\tLoss Ss: 0.020527\n","\tEpoch:43 [001/005 (0040/0755)]\tLoss Ss: 0.014258\n","\tEpoch:43 [001/005 (0060/0755)]\tLoss Ss: 0.024048\n","\tEpoch:43 [001/005 (0080/0755)]\tLoss Ss: 0.014846\n","\tEpoch:43 [001/005 (0100/0755)]\tLoss Ss: 0.015582\n","\tEpoch:43 [001/005 (0120/0755)]\tLoss Ss: 0.023421\n","\tEpoch:43 [001/005 (0140/0755)]\tLoss Ss: 0.017305\n","\tEpoch:43 [001/005 (0160/0755)]\tLoss Ss: 0.013669\n","\tEpoch:43 [001/005 (0180/0755)]\tLoss Ss: 0.010435\n","\tEpoch:43 [001/005 (0200/0755)]\tLoss Ss: 0.012736\n","\tEpoch:43 [001/005 (0220/0755)]\tLoss Ss: 0.019886\n","\tEpoch:43 [001/005 (0240/0755)]\tLoss Ss: 0.014210\n","\tEpoch:43 [001/005 (0260/0755)]\tLoss Ss: 0.011461\n","\tEpoch:43 [001/005 (0280/0755)]\tLoss Ss: 0.013522\n","\tEpoch:43 [001/005 (0300/0755)]\tLoss Ss: 0.014353\n","\tEpoch:43 [001/005 (0320/0755)]\tLoss Ss: 0.012707\n","\tEpoch:43 [001/005 (0340/0755)]\tLoss Ss: 0.011665\n","\tEpoch:43 [001/005 (0360/0755)]\tLoss Ss: 0.015707\n","\tEpoch:43 [001/005 (0380/0755)]\tLoss Ss: 0.018007\n","\tEpoch:43 [001/005 (0400/0755)]\tLoss Ss: 0.013120\n","\tEpoch:43 [001/005 (0420/0755)]\tLoss Ss: 0.008691\n","\tEpoch:43 [001/005 (0440/0755)]\tLoss Ss: 0.012324\n","\tEpoch:43 [001/005 (0460/0755)]\tLoss Ss: 0.014091\n","\tEpoch:43 [001/005 (0480/0755)]\tLoss Ss: 0.015283\n","\tEpoch:43 [001/005 (0500/0755)]\tLoss Ss: 0.016635\n","\tEpoch:43 [001/005 (0520/0755)]\tLoss Ss: 0.010857\n","\tEpoch:43 [001/005 (0540/0755)]\tLoss Ss: 0.015802\n","\tEpoch:43 [001/005 (0560/0755)]\tLoss Ss: 0.010878\n","\tEpoch:43 [001/005 (0580/0755)]\tLoss Ss: 0.012718\n","\tEpoch:43 [001/005 (0600/0755)]\tLoss Ss: 0.011885\n","\tEpoch:43 [001/005 (0620/0755)]\tLoss Ss: 0.011118\n","\tEpoch:43 [001/005 (0640/0755)]\tLoss Ss: 0.012839\n","\tEpoch:43 [001/005 (0660/0755)]\tLoss Ss: 0.015582\n","\tEpoch:43 [001/005 (0680/0755)]\tLoss Ss: 0.010552\n","\tEpoch:43 [001/005 (0700/0755)]\tLoss Ss: 0.011503\n","\tEpoch:43 [001/005 (0720/0755)]\tLoss Ss: 0.014970\n","\tEpoch:43 [001/005 (0740/0755)]\tLoss Ss: 0.014486\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tEpoch:43 [002/005 (0000/0693)]\tLoss Ss: 0.013419\n","\tEpoch:43 [002/005 (0020/0693)]\tLoss Ss: 0.012479\n","\tEpoch:43 [002/005 (0040/0693)]\tLoss Ss: 0.014080\n","\tEpoch:43 [002/005 (0060/0693)]\tLoss Ss: 0.011450\n","\tEpoch:43 [002/005 (0080/0693)]\tLoss Ss: 0.015103\n","\tEpoch:43 [002/005 (0100/0693)]\tLoss Ss: 0.012112\n","\tEpoch:43 [002/005 (0120/0693)]\tLoss Ss: 0.008126\n","\tEpoch:43 [002/005 (0140/0693)]\tLoss Ss: 0.010705\n","\tEpoch:43 [002/005 (0160/0693)]\tLoss Ss: 0.011657\n","\tEpoch:43 [002/005 (0180/0693)]\tLoss Ss: 0.016595\n","\tEpoch:43 [002/005 (0200/0693)]\tLoss Ss: 0.009212\n","\tEpoch:43 [002/005 (0220/0693)]\tLoss Ss: 0.014349\n","\tEpoch:43 [002/005 (0240/0693)]\tLoss Ss: 0.014582\n","\tEpoch:43 [002/005 (0260/0693)]\tLoss Ss: 0.009245\n","\tEpoch:43 [002/005 (0280/0693)]\tLoss Ss: 0.009529\n","\tEpoch:43 [002/005 (0300/0693)]\tLoss Ss: 0.014165\n","\tEpoch:43 [002/005 (0320/0693)]\tLoss Ss: 0.013154\n","\tEpoch:43 [002/005 (0340/0693)]\tLoss Ss: 0.015938\n","\tEpoch:43 [002/005 (0360/0693)]\tLoss Ss: 0.011779\n","\tEpoch:43 [002/005 (0380/0693)]\tLoss Ss: 0.007637\n","\tEpoch:43 [002/005 (0400/0693)]\tLoss Ss: 0.018273\n","\tEpoch:43 [002/005 (0420/0693)]\tLoss Ss: 0.014860\n","\tEpoch:43 [002/005 (0440/0693)]\tLoss Ss: 0.009918\n","\tEpoch:43 [002/005 (0460/0693)]\tLoss Ss: 0.008571\n","\tEpoch:43 [002/005 (0480/0693)]\tLoss Ss: 0.013841\n","\tEpoch:43 [002/005 (0500/0693)]\tLoss Ss: 0.007560\n","\tEpoch:43 [002/005 (0520/0693)]\tLoss Ss: 0.013859\n","\tEpoch:43 [002/005 (0540/0693)]\tLoss Ss: 0.011134\n","\tEpoch:43 [002/005 (0560/0693)]\tLoss Ss: 0.012486\n","\tEpoch:43 [002/005 (0580/0693)]\tLoss Ss: 0.010547\n","\tEpoch:43 [002/005 (0600/0693)]\tLoss Ss: 0.009865\n","\tEpoch:43 [002/005 (0620/0693)]\tLoss Ss: 0.009918\n","\tEpoch:43 [002/005 (0640/0693)]\tLoss Ss: 0.011837\n","\tEpoch:43 [002/005 (0660/0693)]\tLoss Ss: 0.009573\n","\tEpoch:43 [002/005 (0680/0693)]\tLoss Ss: 0.015192\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tEpoch:43 [003/005 (0000/0693)]\tLoss Ss: 0.012597\n","\tEpoch:43 [003/005 (0020/0693)]\tLoss Ss: 0.010841\n","\tEpoch:43 [003/005 (0040/0693)]\tLoss Ss: 0.012302\n","\tEpoch:43 [003/005 (0060/0693)]\tLoss Ss: 0.010707\n","\tEpoch:43 [003/005 (0080/0693)]\tLoss Ss: 0.009210\n","\tEpoch:43 [003/005 (0100/0693)]\tLoss Ss: 0.011042\n","\tEpoch:43 [003/005 (0120/0693)]\tLoss Ss: 0.015229\n","\tEpoch:43 [003/005 (0140/0693)]\tLoss Ss: 0.009756\n","\tEpoch:43 [003/005 (0160/0693)]\tLoss Ss: 0.013339\n","\tEpoch:43 [003/005 (0180/0693)]\tLoss Ss: 0.010277\n","\tEpoch:43 [003/005 (0200/0693)]\tLoss Ss: 0.014628\n","\tEpoch:43 [003/005 (0220/0693)]\tLoss Ss: 0.011922\n","\tEpoch:43 [003/005 (0240/0693)]\tLoss Ss: 0.006690\n","\tEpoch:43 [003/005 (0260/0693)]\tLoss Ss: 0.008421\n","\tEpoch:43 [003/005 (0280/0693)]\tLoss Ss: 0.015180\n","\tEpoch:43 [003/005 (0300/0693)]\tLoss Ss: 0.014305\n","\tEpoch:43 [003/005 (0320/0693)]\tLoss Ss: 0.008505\n","\tEpoch:43 [003/005 (0340/0693)]\tLoss Ss: 0.012530\n","\tEpoch:43 [003/005 (0360/0693)]\tLoss Ss: 0.009470\n","\tEpoch:43 [003/005 (0380/0693)]\tLoss Ss: 0.010508\n","\tEpoch:43 [003/005 (0400/0693)]\tLoss Ss: 0.008682\n","\tEpoch:43 [003/005 (0420/0693)]\tLoss Ss: 0.007752\n","\tEpoch:43 [003/005 (0440/0693)]\tLoss Ss: 0.009451\n","\tEpoch:43 [003/005 (0460/0693)]\tLoss Ss: 0.010660\n","\tEpoch:43 [003/005 (0480/0693)]\tLoss Ss: 0.008532\n","\tEpoch:43 [003/005 (0500/0693)]\tLoss Ss: 0.013355\n","\tEpoch:43 [003/005 (0520/0693)]\tLoss Ss: 0.012726\n","\tEpoch:43 [003/005 (0540/0693)]\tLoss Ss: 0.006992\n","\tEpoch:43 [003/005 (0560/0693)]\tLoss Ss: 0.010596\n","\tEpoch:43 [003/005 (0580/0693)]\tLoss Ss: 0.010619\n","\tEpoch:43 [003/005 (0600/0693)]\tLoss Ss: 0.007703\n","\tEpoch:43 [003/005 (0620/0693)]\tLoss Ss: 0.010786\n","\tEpoch:43 [003/005 (0640/0693)]\tLoss Ss: 0.010792\n","\tEpoch:43 [003/005 (0660/0693)]\tLoss Ss: 0.008089\n","\tEpoch:43 [003/005 (0680/0693)]\tLoss Ss: 0.010358\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:43 [004/005 (0000/0755)]\tLoss Ss: 0.011291\n","\tEpoch:43 [004/005 (0020/0755)]\tLoss Ss: 0.009568\n","\tEpoch:43 [004/005 (0040/0755)]\tLoss Ss: 0.011410\n","\tEpoch:43 [004/005 (0060/0755)]\tLoss Ss: 0.010270\n","\tEpoch:43 [004/005 (0080/0755)]\tLoss Ss: 0.014394\n","\tEpoch:43 [004/005 (0100/0755)]\tLoss Ss: 0.015401\n","\tEpoch:43 [004/005 (0120/0755)]\tLoss Ss: 0.013763\n","\tEpoch:43 [004/005 (0140/0755)]\tLoss Ss: 0.013199\n","\tEpoch:43 [004/005 (0160/0755)]\tLoss Ss: 0.009165\n","\tEpoch:43 [004/005 (0180/0755)]\tLoss Ss: 0.017981\n","\tEpoch:43 [004/005 (0200/0755)]\tLoss Ss: 0.011982\n","\tEpoch:43 [004/005 (0220/0755)]\tLoss Ss: 0.019003\n","\tEpoch:43 [004/005 (0240/0755)]\tLoss Ss: 0.007706\n","\tEpoch:43 [004/005 (0260/0755)]\tLoss Ss: 0.008372\n","\tEpoch:43 [004/005 (0280/0755)]\tLoss Ss: 0.011413\n","\tEpoch:43 [004/005 (0300/0755)]\tLoss Ss: 0.011725\n","\tEpoch:43 [004/005 (0320/0755)]\tLoss Ss: 0.009750\n","\tEpoch:43 [004/005 (0340/0755)]\tLoss Ss: 0.007890\n","\tEpoch:43 [004/005 (0360/0755)]\tLoss Ss: 0.011932\n","\tEpoch:43 [004/005 (0380/0755)]\tLoss Ss: 0.012795\n","\tEpoch:43 [004/005 (0400/0755)]\tLoss Ss: 0.009814\n","\tEpoch:43 [004/005 (0420/0755)]\tLoss Ss: 0.009788\n","\tEpoch:43 [004/005 (0440/0755)]\tLoss Ss: 0.009916\n","\tEpoch:43 [004/005 (0460/0755)]\tLoss Ss: 0.008367\n","\tEpoch:43 [004/005 (0480/0755)]\tLoss Ss: 0.008280\n","\tEpoch:43 [004/005 (0500/0755)]\tLoss Ss: 0.014140\n","\tEpoch:43 [004/005 (0520/0755)]\tLoss Ss: 0.010472\n","\tEpoch:43 [004/005 (0540/0755)]\tLoss Ss: 0.006860\n","\tEpoch:43 [004/005 (0560/0755)]\tLoss Ss: 0.011357\n","\tEpoch:43 [004/005 (0580/0755)]\tLoss Ss: 0.012791\n","\tEpoch:43 [004/005 (0600/0755)]\tLoss Ss: 0.010057\n","\tEpoch:43 [004/005 (0620/0755)]\tLoss Ss: 0.011594\n","\tEpoch:43 [004/005 (0640/0755)]\tLoss Ss: 0.012326\n","\tEpoch:43 [004/005 (0660/0755)]\tLoss Ss: 0.009535\n","\tEpoch:43 [004/005 (0680/0755)]\tLoss Ss: 0.011780\n","\tEpoch:43 [004/005 (0700/0755)]\tLoss Ss: 0.008661\n","\tEpoch:43 [004/005 (0720/0755)]\tLoss Ss: 0.014556\n","\tEpoch:43 [004/005 (0740/0755)]\tLoss Ss: 0.008709\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tEpoch:43 [005/005 (0000/0588)]\tLoss Ss: 0.003286\n","\tEpoch:43 [005/005 (0020/0588)]\tLoss Ss: 0.008072\n","\tEpoch:43 [005/005 (0040/0588)]\tLoss Ss: 0.004651\n","\tEpoch:43 [005/005 (0060/0588)]\tLoss Ss: 0.004302\n","\tEpoch:43 [005/005 (0080/0588)]\tLoss Ss: 0.004708\n","\tEpoch:43 [005/005 (0100/0588)]\tLoss Ss: 0.003046\n","\tEpoch:43 [005/005 (0120/0588)]\tLoss Ss: 0.003818\n","\tEpoch:43 [005/005 (0140/0588)]\tLoss Ss: 0.003007\n","\tEpoch:43 [005/005 (0160/0588)]\tLoss Ss: 0.005956\n","\tEpoch:43 [005/005 (0180/0588)]\tLoss Ss: 0.003289\n","\tEpoch:43 [005/005 (0200/0588)]\tLoss Ss: 0.003115\n","\tEpoch:43 [005/005 (0220/0588)]\tLoss Ss: 0.002014\n","\tEpoch:43 [005/005 (0240/0588)]\tLoss Ss: 0.003088\n","\tEpoch:43 [005/005 (0260/0588)]\tLoss Ss: 0.002805\n","\tEpoch:43 [005/005 (0280/0588)]\tLoss Ss: 0.003316\n","\tEpoch:43 [005/005 (0300/0588)]\tLoss Ss: 0.004785\n","\tEpoch:43 [005/005 (0320/0588)]\tLoss Ss: 0.003749\n","\tEpoch:43 [005/005 (0340/0588)]\tLoss Ss: 0.004672\n","\tEpoch:43 [005/005 (0360/0588)]\tLoss Ss: 0.004491\n","\tEpoch:43 [005/005 (0380/0588)]\tLoss Ss: 0.002222\n","\tEpoch:43 [005/005 (0400/0588)]\tLoss Ss: 0.004901\n","\tEpoch:43 [005/005 (0420/0588)]\tLoss Ss: 0.004780\n","\tEpoch:43 [005/005 (0440/0588)]\tLoss Ss: 0.004708\n","\tEpoch:43 [005/005 (0460/0588)]\tLoss Ss: 0.003225\n","\tEpoch:43 [005/005 (0480/0588)]\tLoss Ss: 0.005502\n","\tEpoch:43 [005/005 (0500/0588)]\tLoss Ss: 0.003055\n","\tEpoch:43 [005/005 (0520/0588)]\tLoss Ss: 0.002692\n","\tEpoch:43 [005/005 (0540/0588)]\tLoss Ss: 0.004163\n","\tEpoch:43 [005/005 (0560/0588)]\tLoss Ss: 0.004388\n","\tEpoch:43 [005/005 (0580/0588)]\tLoss Ss: 0.003581\n","Now train the rotated image\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:43 [000/005 (0000/0755)]\tLoss Ss: 0.016626\n","\tRotated_Epoch:43 [000/005 (0020/0755)]\tLoss Ss: 0.021647\n","\tRotated_Epoch:43 [000/005 (0040/0755)]\tLoss Ss: 0.021155\n","\tRotated_Epoch:43 [000/005 (0060/0755)]\tLoss Ss: 0.017285\n","\tRotated_Epoch:43 [000/005 (0080/0755)]\tLoss Ss: 0.025077\n","\tRotated_Epoch:43 [000/005 (0100/0755)]\tLoss Ss: 0.020002\n","\tRotated_Epoch:43 [000/005 (0120/0755)]\tLoss Ss: 0.019306\n","\tRotated_Epoch:43 [000/005 (0140/0755)]\tLoss Ss: 0.011897\n","\tRotated_Epoch:43 [000/005 (0160/0755)]\tLoss Ss: 0.014193\n","\tRotated_Epoch:43 [000/005 (0180/0755)]\tLoss Ss: 0.011487\n","\tRotated_Epoch:43 [000/005 (0200/0755)]\tLoss Ss: 0.015961\n","\tRotated_Epoch:43 [000/005 (0220/0755)]\tLoss Ss: 0.020093\n","\tRotated_Epoch:43 [000/005 (0240/0755)]\tLoss Ss: 0.011033\n","\tRotated_Epoch:43 [000/005 (0260/0755)]\tLoss Ss: 0.015005\n","\tRotated_Epoch:43 [000/005 (0280/0755)]\tLoss Ss: 0.014226\n","\tRotated_Epoch:43 [000/005 (0300/0755)]\tLoss Ss: 0.014304\n","\tRotated_Epoch:43 [000/005 (0320/0755)]\tLoss Ss: 0.009303\n","\tRotated_Epoch:43 [000/005 (0340/0755)]\tLoss Ss: 0.011270\n","\tRotated_Epoch:43 [000/005 (0360/0755)]\tLoss Ss: 0.014896\n","\tRotated_Epoch:43 [000/005 (0380/0755)]\tLoss Ss: 0.011661\n","\tRotated_Epoch:43 [000/005 (0400/0755)]\tLoss Ss: 0.011905\n","\tRotated_Epoch:43 [000/005 (0420/0755)]\tLoss Ss: 0.012403\n","\tRotated_Epoch:43 [000/005 (0440/0755)]\tLoss Ss: 0.008061\n","\tRotated_Epoch:43 [000/005 (0460/0755)]\tLoss Ss: 0.013846\n","\tRotated_Epoch:43 [000/005 (0480/0755)]\tLoss Ss: 0.012702\n","\tRotated_Epoch:43 [000/005 (0500/0755)]\tLoss Ss: 0.013721\n","\tRotated_Epoch:43 [000/005 (0520/0755)]\tLoss Ss: 0.010780\n","\tRotated_Epoch:43 [000/005 (0540/0755)]\tLoss Ss: 0.016067\n","\tRotated_Epoch:43 [000/005 (0560/0755)]\tLoss Ss: 0.016262\n","\tRotated_Epoch:43 [000/005 (0580/0755)]\tLoss Ss: 0.012632\n","\tRotated_Epoch:43 [000/005 (0600/0755)]\tLoss Ss: 0.010774\n","\tRotated_Epoch:43 [000/005 (0620/0755)]\tLoss Ss: 0.014840\n","\tRotated_Epoch:43 [000/005 (0640/0755)]\tLoss Ss: 0.011711\n","\tRotated_Epoch:43 [000/005 (0660/0755)]\tLoss Ss: 0.015097\n","\tRotated_Epoch:43 [000/005 (0680/0755)]\tLoss Ss: 0.011550\n","\tRotated_Epoch:43 [000/005 (0700/0755)]\tLoss Ss: 0.011676\n","\tRotated_Epoch:43 [000/005 (0720/0755)]\tLoss Ss: 0.011719\n","\tRotated_Epoch:43 [000/005 (0740/0755)]\tLoss Ss: 0.017720\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tRotated_Epoch:43 [001/005 (0000/0693)]\tLoss Ss: 0.010182\n","\tRotated_Epoch:43 [001/005 (0020/0693)]\tLoss Ss: 0.014013\n","\tRotated_Epoch:43 [001/005 (0040/0693)]\tLoss Ss: 0.010044\n","\tRotated_Epoch:43 [001/005 (0060/0693)]\tLoss Ss: 0.011206\n","\tRotated_Epoch:43 [001/005 (0080/0693)]\tLoss Ss: 0.013087\n","\tRotated_Epoch:43 [001/005 (0100/0693)]\tLoss Ss: 0.020576\n","\tRotated_Epoch:43 [001/005 (0120/0693)]\tLoss Ss: 0.013783\n","\tRotated_Epoch:43 [001/005 (0140/0693)]\tLoss Ss: 0.010478\n","\tRotated_Epoch:43 [001/005 (0160/0693)]\tLoss Ss: 0.011231\n","\tRotated_Epoch:43 [001/005 (0180/0693)]\tLoss Ss: 0.010206\n","\tRotated_Epoch:43 [001/005 (0200/0693)]\tLoss Ss: 0.012564\n","\tRotated_Epoch:43 [001/005 (0220/0693)]\tLoss Ss: 0.011367\n","\tRotated_Epoch:43 [001/005 (0240/0693)]\tLoss Ss: 0.008837\n","\tRotated_Epoch:43 [001/005 (0260/0693)]\tLoss Ss: 0.012441\n","\tRotated_Epoch:43 [001/005 (0280/0693)]\tLoss Ss: 0.014702\n","\tRotated_Epoch:43 [001/005 (0300/0693)]\tLoss Ss: 0.010970\n","\tRotated_Epoch:43 [001/005 (0320/0693)]\tLoss Ss: 0.014094\n","\tRotated_Epoch:43 [001/005 (0340/0693)]\tLoss Ss: 0.010746\n","\tRotated_Epoch:43 [001/005 (0360/0693)]\tLoss Ss: 0.012657\n","\tRotated_Epoch:43 [001/005 (0380/0693)]\tLoss Ss: 0.007394\n","\tRotated_Epoch:43 [001/005 (0400/0693)]\tLoss Ss: 0.015922\n","\tRotated_Epoch:43 [001/005 (0420/0693)]\tLoss Ss: 0.009464\n","\tRotated_Epoch:43 [001/005 (0440/0693)]\tLoss Ss: 0.007929\n","\tRotated_Epoch:43 [001/005 (0460/0693)]\tLoss Ss: 0.011885\n","\tRotated_Epoch:43 [001/005 (0480/0693)]\tLoss Ss: 0.010686\n","\tRotated_Epoch:43 [001/005 (0500/0693)]\tLoss Ss: 0.011683\n","\tRotated_Epoch:43 [001/005 (0520/0693)]\tLoss Ss: 0.012524\n","\tRotated_Epoch:43 [001/005 (0540/0693)]\tLoss Ss: 0.010512\n","\tRotated_Epoch:43 [001/005 (0560/0693)]\tLoss Ss: 0.011715\n","\tRotated_Epoch:43 [001/005 (0580/0693)]\tLoss Ss: 0.009900\n","\tRotated_Epoch:43 [001/005 (0600/0693)]\tLoss Ss: 0.010528\n","\tRotated_Epoch:43 [001/005 (0620/0693)]\tLoss Ss: 0.011609\n","\tRotated_Epoch:43 [001/005 (0640/0693)]\tLoss Ss: 0.011024\n","\tRotated_Epoch:43 [001/005 (0660/0693)]\tLoss Ss: 0.011435\n","\tRotated_Epoch:43 [001/005 (0680/0693)]\tLoss Ss: 0.015821\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tRotated_Epoch:43 [002/005 (0000/0614)]\tLoss Ss: 0.007023\n","\tRotated_Epoch:43 [002/005 (0020/0614)]\tLoss Ss: 0.005223\n","\tRotated_Epoch:43 [002/005 (0040/0614)]\tLoss Ss: 0.008160\n","\tRotated_Epoch:43 [002/005 (0060/0614)]\tLoss Ss: 0.015158\n","\tRotated_Epoch:43 [002/005 (0080/0614)]\tLoss Ss: 0.005058\n","\tRotated_Epoch:43 [002/005 (0100/0614)]\tLoss Ss: 0.005193\n","\tRotated_Epoch:43 [002/005 (0120/0614)]\tLoss Ss: 0.005614\n","\tRotated_Epoch:43 [002/005 (0140/0614)]\tLoss Ss: 0.004558\n","\tRotated_Epoch:43 [002/005 (0160/0614)]\tLoss Ss: 0.008324\n","\tRotated_Epoch:43 [002/005 (0180/0614)]\tLoss Ss: 0.005999\n","\tRotated_Epoch:43 [002/005 (0200/0614)]\tLoss Ss: 0.004491\n","\tRotated_Epoch:43 [002/005 (0220/0614)]\tLoss Ss: 0.005272\n","\tRotated_Epoch:43 [002/005 (0240/0614)]\tLoss Ss: 0.007652\n","\tRotated_Epoch:43 [002/005 (0260/0614)]\tLoss Ss: 0.004218\n","\tRotated_Epoch:43 [002/005 (0280/0614)]\tLoss Ss: 0.005163\n","\tRotated_Epoch:43 [002/005 (0300/0614)]\tLoss Ss: 0.004240\n","\tRotated_Epoch:43 [002/005 (0320/0614)]\tLoss Ss: 0.005248\n","\tRotated_Epoch:43 [002/005 (0340/0614)]\tLoss Ss: 0.006695\n","\tRotated_Epoch:43 [002/005 (0360/0614)]\tLoss Ss: 0.005568\n","\tRotated_Epoch:43 [002/005 (0380/0614)]\tLoss Ss: 0.004665\n","\tRotated_Epoch:43 [002/005 (0400/0614)]\tLoss Ss: 0.003981\n","\tRotated_Epoch:43 [002/005 (0420/0614)]\tLoss Ss: 0.004887\n","\tRotated_Epoch:43 [002/005 (0440/0614)]\tLoss Ss: 0.005982\n","\tRotated_Epoch:43 [002/005 (0460/0614)]\tLoss Ss: 0.005129\n","\tRotated_Epoch:43 [002/005 (0480/0614)]\tLoss Ss: 0.004898\n","\tRotated_Epoch:43 [002/005 (0500/0614)]\tLoss Ss: 0.007126\n","\tRotated_Epoch:43 [002/005 (0520/0614)]\tLoss Ss: 0.004462\n","\tRotated_Epoch:43 [002/005 (0540/0614)]\tLoss Ss: 0.006224\n","\tRotated_Epoch:43 [002/005 (0560/0614)]\tLoss Ss: 0.006631\n","\tRotated_Epoch:43 [002/005 (0580/0614)]\tLoss Ss: 0.007129\n","\tRotated_Epoch:43 [002/005 (0600/0614)]\tLoss Ss: 0.007261\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tRotated_Epoch:43 [003/005 (0000/0588)]\tLoss Ss: 0.281607\n","\tRotated_Epoch:43 [003/005 (0020/0588)]\tLoss Ss: 0.073143\n","\tRotated_Epoch:43 [003/005 (0040/0588)]\tLoss Ss: 0.068315\n","\tRotated_Epoch:43 [003/005 (0060/0588)]\tLoss Ss: 0.083550\n","\tRotated_Epoch:43 [003/005 (0080/0588)]\tLoss Ss: 0.042517\n","\tRotated_Epoch:43 [003/005 (0100/0588)]\tLoss Ss: 0.060510\n","\tRotated_Epoch:43 [003/005 (0120/0588)]\tLoss Ss: 0.060759\n","\tRotated_Epoch:43 [003/005 (0140/0588)]\tLoss Ss: 0.044583\n","\tRotated_Epoch:43 [003/005 (0160/0588)]\tLoss Ss: 0.055989\n","\tRotated_Epoch:43 [003/005 (0180/0588)]\tLoss Ss: 0.066097\n","\tRotated_Epoch:43 [003/005 (0200/0588)]\tLoss Ss: 0.046640\n","\tRotated_Epoch:43 [003/005 (0220/0588)]\tLoss Ss: 0.068419\n","\tRotated_Epoch:43 [003/005 (0240/0588)]\tLoss Ss: 0.036954\n","\tRotated_Epoch:43 [003/005 (0260/0588)]\tLoss Ss: 0.042703\n","\tRotated_Epoch:43 [003/005 (0280/0588)]\tLoss Ss: 0.057818\n","\tRotated_Epoch:43 [003/005 (0300/0588)]\tLoss Ss: 0.056789\n","\tRotated_Epoch:43 [003/005 (0320/0588)]\tLoss Ss: 0.055799\n","\tRotated_Epoch:43 [003/005 (0340/0588)]\tLoss Ss: 0.043312\n","\tRotated_Epoch:43 [003/005 (0360/0588)]\tLoss Ss: 0.065648\n","\tRotated_Epoch:43 [003/005 (0380/0588)]\tLoss Ss: 0.052561\n","\tRotated_Epoch:43 [003/005 (0400/0588)]\tLoss Ss: 0.054264\n","\tRotated_Epoch:43 [003/005 (0420/0588)]\tLoss Ss: 0.074040\n","\tRotated_Epoch:43 [003/005 (0440/0588)]\tLoss Ss: 0.044194\n","\tRotated_Epoch:43 [003/005 (0460/0588)]\tLoss Ss: 0.047704\n","\tRotated_Epoch:43 [003/005 (0480/0588)]\tLoss Ss: 0.059085\n","\tRotated_Epoch:43 [003/005 (0500/0588)]\tLoss Ss: 0.043285\n","\tRotated_Epoch:43 [003/005 (0520/0588)]\tLoss Ss: 0.053830\n","\tRotated_Epoch:43 [003/005 (0540/0588)]\tLoss Ss: 0.040650\n","\tRotated_Epoch:43 [003/005 (0560/0588)]\tLoss Ss: 0.047889\n","\tRotated_Epoch:43 [003/005 (0580/0588)]\tLoss Ss: 0.041539\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tRotated_Epoch:43 [004/005 (0000/0693)]\tLoss Ss: 0.086166\n","\tRotated_Epoch:43 [004/005 (0020/0693)]\tLoss Ss: 0.019748\n","\tRotated_Epoch:43 [004/005 (0040/0693)]\tLoss Ss: 0.016799\n","\tRotated_Epoch:43 [004/005 (0060/0693)]\tLoss Ss: 0.018529\n","\tRotated_Epoch:43 [004/005 (0080/0693)]\tLoss Ss: 0.021291\n","\tRotated_Epoch:43 [004/005 (0100/0693)]\tLoss Ss: 0.018244\n","\tRotated_Epoch:43 [004/005 (0120/0693)]\tLoss Ss: 0.019569\n","\tRotated_Epoch:43 [004/005 (0140/0693)]\tLoss Ss: 0.016038\n","\tRotated_Epoch:43 [004/005 (0160/0693)]\tLoss Ss: 0.012345\n","\tRotated_Epoch:43 [004/005 (0180/0693)]\tLoss Ss: 0.013392\n","\tRotated_Epoch:43 [004/005 (0200/0693)]\tLoss Ss: 0.012582\n","\tRotated_Epoch:43 [004/005 (0220/0693)]\tLoss Ss: 0.013795\n","\tRotated_Epoch:43 [004/005 (0240/0693)]\tLoss Ss: 0.015046\n","\tRotated_Epoch:43 [004/005 (0260/0693)]\tLoss Ss: 0.013530\n","\tRotated_Epoch:43 [004/005 (0280/0693)]\tLoss Ss: 0.012889\n","\tRotated_Epoch:43 [004/005 (0300/0693)]\tLoss Ss: 0.018354\n","\tRotated_Epoch:43 [004/005 (0320/0693)]\tLoss Ss: 0.018717\n","\tRotated_Epoch:43 [004/005 (0340/0693)]\tLoss Ss: 0.018954\n","\tRotated_Epoch:43 [004/005 (0360/0693)]\tLoss Ss: 0.011985\n","\tRotated_Epoch:43 [004/005 (0380/0693)]\tLoss Ss: 0.015615\n","\tRotated_Epoch:43 [004/005 (0400/0693)]\tLoss Ss: 0.018158\n","\tRotated_Epoch:43 [004/005 (0420/0693)]\tLoss Ss: 0.016848\n","\tRotated_Epoch:43 [004/005 (0440/0693)]\tLoss Ss: 0.014974\n","\tRotated_Epoch:43 [004/005 (0460/0693)]\tLoss Ss: 0.015593\n","\tRotated_Epoch:43 [004/005 (0480/0693)]\tLoss Ss: 0.014666\n","\tRotated_Epoch:43 [004/005 (0500/0693)]\tLoss Ss: 0.011738\n","\tRotated_Epoch:43 [004/005 (0520/0693)]\tLoss Ss: 0.013420\n","\tRotated_Epoch:43 [004/005 (0540/0693)]\tLoss Ss: 0.013862\n","\tRotated_Epoch:43 [004/005 (0560/0693)]\tLoss Ss: 0.013893\n","\tRotated_Epoch:43 [004/005 (0580/0693)]\tLoss Ss: 0.013694\n","\tRotated_Epoch:43 [004/005 (0600/0693)]\tLoss Ss: 0.015556\n","\tRotated_Epoch:43 [004/005 (0620/0693)]\tLoss Ss: 0.013398\n","\tRotated_Epoch:43 [004/005 (0640/0693)]\tLoss Ss: 0.018976\n","\tRotated_Epoch:43 [004/005 (0660/0693)]\tLoss Ss: 0.014336\n","\tRotated_Epoch:43 [004/005 (0680/0693)]\tLoss Ss: 0.011589\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:43 [005/005 (0000/0755)]\tLoss Ss: 0.142499\n","\tRotated_Epoch:43 [005/005 (0020/0755)]\tLoss Ss: 0.118330\n","\tRotated_Epoch:43 [005/005 (0040/0755)]\tLoss Ss: 0.107667\n","\tRotated_Epoch:43 [005/005 (0060/0755)]\tLoss Ss: 0.084570\n","\tRotated_Epoch:43 [005/005 (0080/0755)]\tLoss Ss: 0.135253\n","\tRotated_Epoch:43 [005/005 (0100/0755)]\tLoss Ss: 0.114853\n","\tRotated_Epoch:43 [005/005 (0120/0755)]\tLoss Ss: 0.051520\n","\tRotated_Epoch:43 [005/005 (0140/0755)]\tLoss Ss: 0.102793\n","\tRotated_Epoch:43 [005/005 (0160/0755)]\tLoss Ss: 0.052572\n","\tRotated_Epoch:43 [005/005 (0180/0755)]\tLoss Ss: 0.067901\n","\tRotated_Epoch:43 [005/005 (0200/0755)]\tLoss Ss: 0.078199\n","\tRotated_Epoch:43 [005/005 (0220/0755)]\tLoss Ss: 0.036486\n","\tRotated_Epoch:43 [005/005 (0240/0755)]\tLoss Ss: 0.069061\n","\tRotated_Epoch:43 [005/005 (0260/0755)]\tLoss Ss: 0.048436\n","\tRotated_Epoch:43 [005/005 (0280/0755)]\tLoss Ss: 0.046734\n","\tRotated_Epoch:43 [005/005 (0300/0755)]\tLoss Ss: 0.039830\n","\tRotated_Epoch:43 [005/005 (0320/0755)]\tLoss Ss: 0.030239\n","\tRotated_Epoch:43 [005/005 (0340/0755)]\tLoss Ss: 0.032212\n","\tRotated_Epoch:43 [005/005 (0360/0755)]\tLoss Ss: 0.047901\n","\tRotated_Epoch:43 [005/005 (0380/0755)]\tLoss Ss: 0.028236\n","\tRotated_Epoch:43 [005/005 (0400/0755)]\tLoss Ss: 0.057284\n","\tRotated_Epoch:43 [005/005 (0420/0755)]\tLoss Ss: 0.045455\n","\tRotated_Epoch:43 [005/005 (0440/0755)]\tLoss Ss: 0.040784\n","\tRotated_Epoch:43 [005/005 (0460/0755)]\tLoss Ss: 0.038012\n","\tRotated_Epoch:43 [005/005 (0480/0755)]\tLoss Ss: 0.024774\n","\tRotated_Epoch:43 [005/005 (0500/0755)]\tLoss Ss: 0.042369\n","\tRotated_Epoch:43 [005/005 (0520/0755)]\tLoss Ss: 0.035574\n","\tRotated_Epoch:43 [005/005 (0540/0755)]\tLoss Ss: 0.047491\n","\tRotated_Epoch:43 [005/005 (0560/0755)]\tLoss Ss: 0.040986\n","\tRotated_Epoch:43 [005/005 (0580/0755)]\tLoss Ss: 0.051475\n","\tRotated_Epoch:43 [005/005 (0600/0755)]\tLoss Ss: 0.037205\n","\tRotated_Epoch:43 [005/005 (0620/0755)]\tLoss Ss: 0.031596\n","\tRotated_Epoch:43 [005/005 (0640/0755)]\tLoss Ss: 0.036758\n","\tRotated_Epoch:43 [005/005 (0660/0755)]\tLoss Ss: 0.033788\n","\tRotated_Epoch:43 [005/005 (0680/0755)]\tLoss Ss: 0.038536\n","\tRotated_Epoch:43 [005/005 (0700/0755)]\tLoss Ss: 0.037398\n","\tRotated_Epoch:43 [005/005 (0720/0755)]\tLoss Ss: 0.031560\n","\tRotated_Epoch:43 [005/005 (0740/0755)]\tLoss Ss: 0.032927\n","new_1_6_d.nii\n","new_1_6_m.nii\n","new_1_7_d.nii\n","new_1_7_m.nii\n","new_2_5_d.nii\n","new_2_5_m.nii\n","new_2_6_d.nii\n","new_2_6_m.nii\n","new_2_7_d.nii\n","new_2_7_m.nii\n","new_3_d (2).nii\n","new_3_m (2).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (3).nii\n","new_3_m (3).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (7).nii\n","new_3_m (7).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (8).nii\n","new_3_m (8).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (9).nii\n","new_3_m (9).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_4_10_d_163526.nii\n","new_4_10_m_163526.nii.gz\n","new_4_11_d_084336.nii\n","new_4_11_m_084336.nii.gz\n","new_4_7_d_184422.nii\n","new_4_7_m_184422.nii.gz\n","new_4_8_d_143412.nii\n","new_4_8_m_143412.nii.gz\n","new_4_9_d_155347.nii\n","new_4_9_m_155347.nii.gz\n","\tEpoch: 43; Dice: 0.8317 +/- 0.0727; Loss: 7.8725\n","Begin Epoch 44\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:44 [000/005 (0000/0755)]\tLoss Ss: 0.067202\n","\tEpoch:44 [000/005 (0020/0755)]\tLoss Ss: 0.075457\n","\tEpoch:44 [000/005 (0040/0755)]\tLoss Ss: 0.041925\n","\tEpoch:44 [000/005 (0060/0755)]\tLoss Ss: 0.031994\n","\tEpoch:44 [000/005 (0080/0755)]\tLoss Ss: 0.034054\n","\tEpoch:44 [000/005 (0100/0755)]\tLoss Ss: 0.028947\n","\tEpoch:44 [000/005 (0120/0755)]\tLoss Ss: 0.023525\n","\tEpoch:44 [000/005 (0140/0755)]\tLoss Ss: 0.019764\n","\tEpoch:44 [000/005 (0160/0755)]\tLoss Ss: 0.021751\n","\tEpoch:44 [000/005 (0180/0755)]\tLoss Ss: 0.016707\n","\tEpoch:44 [000/005 (0200/0755)]\tLoss Ss: 0.021186\n","\tEpoch:44 [000/005 (0220/0755)]\tLoss Ss: 0.023667\n","\tEpoch:44 [000/005 (0240/0755)]\tLoss Ss: 0.014421\n","\tEpoch:44 [000/005 (0260/0755)]\tLoss Ss: 0.016407\n","\tEpoch:44 [000/005 (0280/0755)]\tLoss Ss: 0.017369\n","\tEpoch:44 [000/005 (0300/0755)]\tLoss Ss: 0.012264\n","\tEpoch:44 [000/005 (0320/0755)]\tLoss Ss: 0.014025\n","\tEpoch:44 [000/005 (0340/0755)]\tLoss Ss: 0.014682\n","\tEpoch:44 [000/005 (0360/0755)]\tLoss Ss: 0.016987\n","\tEpoch:44 [000/005 (0380/0755)]\tLoss Ss: 0.015100\n","\tEpoch:44 [000/005 (0400/0755)]\tLoss Ss: 0.022536\n","\tEpoch:44 [000/005 (0420/0755)]\tLoss Ss: 0.017194\n","\tEpoch:44 [000/005 (0440/0755)]\tLoss Ss: 0.016327\n","\tEpoch:44 [000/005 (0460/0755)]\tLoss Ss: 0.015868\n","\tEpoch:44 [000/005 (0480/0755)]\tLoss Ss: 0.022353\n","\tEpoch:44 [000/005 (0500/0755)]\tLoss Ss: 0.010969\n","\tEpoch:44 [000/005 (0520/0755)]\tLoss Ss: 0.013636\n","\tEpoch:44 [000/005 (0540/0755)]\tLoss Ss: 0.011650\n","\tEpoch:44 [000/005 (0560/0755)]\tLoss Ss: 0.017552\n","\tEpoch:44 [000/005 (0580/0755)]\tLoss Ss: 0.014409\n","\tEpoch:44 [000/005 (0600/0755)]\tLoss Ss: 0.018577\n","\tEpoch:44 [000/005 (0620/0755)]\tLoss Ss: 0.008051\n","\tEpoch:44 [000/005 (0640/0755)]\tLoss Ss: 0.013363\n","\tEpoch:44 [000/005 (0660/0755)]\tLoss Ss: 0.010899\n","\tEpoch:44 [000/005 (0680/0755)]\tLoss Ss: 0.010227\n","\tEpoch:44 [000/005 (0700/0755)]\tLoss Ss: 0.010713\n","\tEpoch:44 [000/005 (0720/0755)]\tLoss Ss: 0.009887\n","\tEpoch:44 [000/005 (0740/0755)]\tLoss Ss: 0.016208\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tEpoch:44 [001/005 (0000/0588)]\tLoss Ss: 0.017022\n","\tEpoch:44 [001/005 (0020/0588)]\tLoss Ss: 0.013200\n","\tEpoch:44 [001/005 (0040/0588)]\tLoss Ss: 0.010913\n","\tEpoch:44 [001/005 (0060/0588)]\tLoss Ss: 0.015773\n","\tEpoch:44 [001/005 (0080/0588)]\tLoss Ss: 0.009361\n","\tEpoch:44 [001/005 (0100/0588)]\tLoss Ss: 0.009287\n","\tEpoch:44 [001/005 (0120/0588)]\tLoss Ss: 0.006459\n","\tEpoch:44 [001/005 (0140/0588)]\tLoss Ss: 0.006638\n","\tEpoch:44 [001/005 (0160/0588)]\tLoss Ss: 0.005047\n","\tEpoch:44 [001/005 (0180/0588)]\tLoss Ss: 0.008357\n","\tEpoch:44 [001/005 (0200/0588)]\tLoss Ss: 0.006397\n","\tEpoch:44 [001/005 (0220/0588)]\tLoss Ss: 0.004112\n","\tEpoch:44 [001/005 (0240/0588)]\tLoss Ss: 0.004450\n","\tEpoch:44 [001/005 (0260/0588)]\tLoss Ss: 0.006248\n","\tEpoch:44 [001/005 (0280/0588)]\tLoss Ss: 0.004083\n","\tEpoch:44 [001/005 (0300/0588)]\tLoss Ss: 0.003993\n","\tEpoch:44 [001/005 (0320/0588)]\tLoss Ss: 0.005374\n","\tEpoch:44 [001/005 (0340/0588)]\tLoss Ss: 0.003590\n","\tEpoch:44 [001/005 (0360/0588)]\tLoss Ss: 0.004403\n","\tEpoch:44 [001/005 (0380/0588)]\tLoss Ss: 0.005810\n","\tEpoch:44 [001/005 (0400/0588)]\tLoss Ss: 0.002885\n","\tEpoch:44 [001/005 (0420/0588)]\tLoss Ss: 0.004475\n","\tEpoch:44 [001/005 (0440/0588)]\tLoss Ss: 0.004428\n","\tEpoch:44 [001/005 (0460/0588)]\tLoss Ss: 0.003087\n","\tEpoch:44 [001/005 (0480/0588)]\tLoss Ss: 0.007063\n","\tEpoch:44 [001/005 (0500/0588)]\tLoss Ss: 0.005389\n","\tEpoch:44 [001/005 (0520/0588)]\tLoss Ss: 0.005388\n","\tEpoch:44 [001/005 (0540/0588)]\tLoss Ss: 0.004107\n","\tEpoch:44 [001/005 (0560/0588)]\tLoss Ss: 0.003817\n","\tEpoch:44 [001/005 (0580/0588)]\tLoss Ss: 0.007538\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:44 [002/005 (0000/0755)]\tLoss Ss: 0.012341\n","\tEpoch:44 [002/005 (0020/0755)]\tLoss Ss: 0.009521\n","\tEpoch:44 [002/005 (0040/0755)]\tLoss Ss: 0.012327\n","\tEpoch:44 [002/005 (0060/0755)]\tLoss Ss: 0.008977\n","\tEpoch:44 [002/005 (0080/0755)]\tLoss Ss: 0.012815\n","\tEpoch:44 [002/005 (0100/0755)]\tLoss Ss: 0.013169\n","\tEpoch:44 [002/005 (0120/0755)]\tLoss Ss: 0.013198\n","\tEpoch:44 [002/005 (0140/0755)]\tLoss Ss: 0.010095\n","\tEpoch:44 [002/005 (0160/0755)]\tLoss Ss: 0.014525\n","\tEpoch:44 [002/005 (0180/0755)]\tLoss Ss: 0.015454\n","\tEpoch:44 [002/005 (0200/0755)]\tLoss Ss: 0.014333\n","\tEpoch:44 [002/005 (0220/0755)]\tLoss Ss: 0.015656\n","\tEpoch:44 [002/005 (0240/0755)]\tLoss Ss: 0.007621\n","\tEpoch:44 [002/005 (0260/0755)]\tLoss Ss: 0.012814\n","\tEpoch:44 [002/005 (0280/0755)]\tLoss Ss: 0.011083\n","\tEpoch:44 [002/005 (0300/0755)]\tLoss Ss: 0.010165\n","\tEpoch:44 [002/005 (0320/0755)]\tLoss Ss: 0.012126\n","\tEpoch:44 [002/005 (0340/0755)]\tLoss Ss: 0.011291\n","\tEpoch:44 [002/005 (0360/0755)]\tLoss Ss: 0.010948\n","\tEpoch:44 [002/005 (0380/0755)]\tLoss Ss: 0.010536\n","\tEpoch:44 [002/005 (0400/0755)]\tLoss Ss: 0.013272\n","\tEpoch:44 [002/005 (0420/0755)]\tLoss Ss: 0.006967\n","\tEpoch:44 [002/005 (0440/0755)]\tLoss Ss: 0.006225\n","\tEpoch:44 [002/005 (0460/0755)]\tLoss Ss: 0.011904\n","\tEpoch:44 [002/005 (0480/0755)]\tLoss Ss: 0.009317\n","\tEpoch:44 [002/005 (0500/0755)]\tLoss Ss: 0.016422\n","\tEpoch:44 [002/005 (0520/0755)]\tLoss Ss: 0.008092\n","\tEpoch:44 [002/005 (0540/0755)]\tLoss Ss: 0.013312\n","\tEpoch:44 [002/005 (0560/0755)]\tLoss Ss: 0.010843\n","\tEpoch:44 [002/005 (0580/0755)]\tLoss Ss: 0.012056\n","\tEpoch:44 [002/005 (0600/0755)]\tLoss Ss: 0.007887\n","\tEpoch:44 [002/005 (0620/0755)]\tLoss Ss: 0.012328\n","\tEpoch:44 [002/005 (0640/0755)]\tLoss Ss: 0.017829\n","\tEpoch:44 [002/005 (0660/0755)]\tLoss Ss: 0.008890\n","\tEpoch:44 [002/005 (0680/0755)]\tLoss Ss: 0.012267\n","\tEpoch:44 [002/005 (0700/0755)]\tLoss Ss: 0.013717\n","\tEpoch:44 [002/005 (0720/0755)]\tLoss Ss: 0.010588\n","\tEpoch:44 [002/005 (0740/0755)]\tLoss Ss: 0.017009\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tEpoch:44 [003/005 (0000/0693)]\tLoss Ss: 0.013113\n","\tEpoch:44 [003/005 (0020/0693)]\tLoss Ss: 0.012435\n","\tEpoch:44 [003/005 (0040/0693)]\tLoss Ss: 0.007225\n","\tEpoch:44 [003/005 (0060/0693)]\tLoss Ss: 0.010812\n","\tEpoch:44 [003/005 (0080/0693)]\tLoss Ss: 0.010738\n","\tEpoch:44 [003/005 (0100/0693)]\tLoss Ss: 0.009977\n","\tEpoch:44 [003/005 (0120/0693)]\tLoss Ss: 0.010382\n","\tEpoch:44 [003/005 (0140/0693)]\tLoss Ss: 0.012222\n","\tEpoch:44 [003/005 (0160/0693)]\tLoss Ss: 0.011032\n","\tEpoch:44 [003/005 (0180/0693)]\tLoss Ss: 0.007013\n","\tEpoch:44 [003/005 (0200/0693)]\tLoss Ss: 0.008027\n","\tEpoch:44 [003/005 (0220/0693)]\tLoss Ss: 0.015835\n","\tEpoch:44 [003/005 (0240/0693)]\tLoss Ss: 0.014180\n","\tEpoch:44 [003/005 (0260/0693)]\tLoss Ss: 0.012289\n","\tEpoch:44 [003/005 (0280/0693)]\tLoss Ss: 0.009960\n","\tEpoch:44 [003/005 (0300/0693)]\tLoss Ss: 0.010281\n","\tEpoch:44 [003/005 (0320/0693)]\tLoss Ss: 0.007946\n","\tEpoch:44 [003/005 (0340/0693)]\tLoss Ss: 0.009315\n","\tEpoch:44 [003/005 (0360/0693)]\tLoss Ss: 0.009809\n","\tEpoch:44 [003/005 (0380/0693)]\tLoss Ss: 0.015994\n","\tEpoch:44 [003/005 (0400/0693)]\tLoss Ss: 0.009654\n","\tEpoch:44 [003/005 (0420/0693)]\tLoss Ss: 0.011966\n","\tEpoch:44 [003/005 (0440/0693)]\tLoss Ss: 0.011019\n","\tEpoch:44 [003/005 (0460/0693)]\tLoss Ss: 0.017436\n","\tEpoch:44 [003/005 (0480/0693)]\tLoss Ss: 0.011268\n","\tEpoch:44 [003/005 (0500/0693)]\tLoss Ss: 0.011198\n","\tEpoch:44 [003/005 (0520/0693)]\tLoss Ss: 0.015833\n","\tEpoch:44 [003/005 (0540/0693)]\tLoss Ss: 0.010646\n","\tEpoch:44 [003/005 (0560/0693)]\tLoss Ss: 0.011687\n","\tEpoch:44 [003/005 (0580/0693)]\tLoss Ss: 0.009019\n","\tEpoch:44 [003/005 (0600/0693)]\tLoss Ss: 0.010509\n","\tEpoch:44 [003/005 (0620/0693)]\tLoss Ss: 0.008259\n","\tEpoch:44 [003/005 (0640/0693)]\tLoss Ss: 0.011428\n","\tEpoch:44 [003/005 (0660/0693)]\tLoss Ss: 0.006790\n","\tEpoch:44 [003/005 (0680/0693)]\tLoss Ss: 0.008507\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tEpoch:44 [004/005 (0000/0693)]\tLoss Ss: 0.013267\n","\tEpoch:44 [004/005 (0020/0693)]\tLoss Ss: 0.021349\n","\tEpoch:44 [004/005 (0040/0693)]\tLoss Ss: 0.016968\n","\tEpoch:44 [004/005 (0060/0693)]\tLoss Ss: 0.015699\n","\tEpoch:44 [004/005 (0080/0693)]\tLoss Ss: 0.012380\n","\tEpoch:44 [004/005 (0100/0693)]\tLoss Ss: 0.013295\n","\tEpoch:44 [004/005 (0120/0693)]\tLoss Ss: 0.008798\n","\tEpoch:44 [004/005 (0140/0693)]\tLoss Ss: 0.014303\n","\tEpoch:44 [004/005 (0160/0693)]\tLoss Ss: 0.009345\n","\tEpoch:44 [004/005 (0180/0693)]\tLoss Ss: 0.015362\n","\tEpoch:44 [004/005 (0200/0693)]\tLoss Ss: 0.010644\n","\tEpoch:44 [004/005 (0220/0693)]\tLoss Ss: 0.010223\n","\tEpoch:44 [004/005 (0240/0693)]\tLoss Ss: 0.010538\n","\tEpoch:44 [004/005 (0260/0693)]\tLoss Ss: 0.014014\n","\tEpoch:44 [004/005 (0280/0693)]\tLoss Ss: 0.012837\n","\tEpoch:44 [004/005 (0300/0693)]\tLoss Ss: 0.012980\n","\tEpoch:44 [004/005 (0320/0693)]\tLoss Ss: 0.010297\n","\tEpoch:44 [004/005 (0340/0693)]\tLoss Ss: 0.013031\n","\tEpoch:44 [004/005 (0360/0693)]\tLoss Ss: 0.011323\n","\tEpoch:44 [004/005 (0380/0693)]\tLoss Ss: 0.009955\n","\tEpoch:44 [004/005 (0400/0693)]\tLoss Ss: 0.012659\n","\tEpoch:44 [004/005 (0420/0693)]\tLoss Ss: 0.010302\n","\tEpoch:44 [004/005 (0440/0693)]\tLoss Ss: 0.018155\n","\tEpoch:44 [004/005 (0460/0693)]\tLoss Ss: 0.011339\n","\tEpoch:44 [004/005 (0480/0693)]\tLoss Ss: 0.013582\n","\tEpoch:44 [004/005 (0500/0693)]\tLoss Ss: 0.010889\n","\tEpoch:44 [004/005 (0520/0693)]\tLoss Ss: 0.012279\n","\tEpoch:44 [004/005 (0540/0693)]\tLoss Ss: 0.011325\n","\tEpoch:44 [004/005 (0560/0693)]\tLoss Ss: 0.012350\n","\tEpoch:44 [004/005 (0580/0693)]\tLoss Ss: 0.013252\n","\tEpoch:44 [004/005 (0600/0693)]\tLoss Ss: 0.012010\n","\tEpoch:44 [004/005 (0620/0693)]\tLoss Ss: 0.011446\n","\tEpoch:44 [004/005 (0640/0693)]\tLoss Ss: 0.014832\n","\tEpoch:44 [004/005 (0660/0693)]\tLoss Ss: 0.009411\n","\tEpoch:44 [004/005 (0680/0693)]\tLoss Ss: 0.009507\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tEpoch:44 [005/005 (0000/0614)]\tLoss Ss: 0.004635\n","\tEpoch:44 [005/005 (0020/0614)]\tLoss Ss: 0.006527\n","\tEpoch:44 [005/005 (0040/0614)]\tLoss Ss: 0.004798\n","\tEpoch:44 [005/005 (0060/0614)]\tLoss Ss: 0.004715\n","\tEpoch:44 [005/005 (0080/0614)]\tLoss Ss: 0.005062\n","\tEpoch:44 [005/005 (0100/0614)]\tLoss Ss: 0.004907\n","\tEpoch:44 [005/005 (0120/0614)]\tLoss Ss: 0.003890\n","\tEpoch:44 [005/005 (0140/0614)]\tLoss Ss: 0.005951\n","\tEpoch:44 [005/005 (0160/0614)]\tLoss Ss: 0.005765\n","\tEpoch:44 [005/005 (0180/0614)]\tLoss Ss: 0.005090\n","\tEpoch:44 [005/005 (0200/0614)]\tLoss Ss: 0.005783\n","\tEpoch:44 [005/005 (0220/0614)]\tLoss Ss: 0.004530\n","\tEpoch:44 [005/005 (0240/0614)]\tLoss Ss: 0.005567\n","\tEpoch:44 [005/005 (0260/0614)]\tLoss Ss: 0.003990\n","\tEpoch:44 [005/005 (0280/0614)]\tLoss Ss: 0.003561\n","\tEpoch:44 [005/005 (0300/0614)]\tLoss Ss: 0.004827\n","\tEpoch:44 [005/005 (0320/0614)]\tLoss Ss: 0.006142\n","\tEpoch:44 [005/005 (0340/0614)]\tLoss Ss: 0.002313\n","\tEpoch:44 [005/005 (0360/0614)]\tLoss Ss: 0.006261\n","\tEpoch:44 [005/005 (0380/0614)]\tLoss Ss: 0.004025\n","\tEpoch:44 [005/005 (0400/0614)]\tLoss Ss: 0.004381\n","\tEpoch:44 [005/005 (0420/0614)]\tLoss Ss: 0.004797\n","\tEpoch:44 [005/005 (0440/0614)]\tLoss Ss: 0.002906\n","\tEpoch:44 [005/005 (0460/0614)]\tLoss Ss: 0.006412\n","\tEpoch:44 [005/005 (0480/0614)]\tLoss Ss: 0.005721\n","\tEpoch:44 [005/005 (0500/0614)]\tLoss Ss: 0.005745\n","\tEpoch:44 [005/005 (0520/0614)]\tLoss Ss: 0.005171\n","\tEpoch:44 [005/005 (0540/0614)]\tLoss Ss: 0.005106\n","\tEpoch:44 [005/005 (0560/0614)]\tLoss Ss: 0.005919\n","\tEpoch:44 [005/005 (0580/0614)]\tLoss Ss: 0.005747\n","\tEpoch:44 [005/005 (0600/0614)]\tLoss Ss: 0.004094\n","Now train the rotated image\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tRotated_Epoch:44 [000/005 (0000/0693)]\tLoss Ss: 0.015302\n","\tRotated_Epoch:44 [000/005 (0020/0693)]\tLoss Ss: 0.019665\n","\tRotated_Epoch:44 [000/005 (0040/0693)]\tLoss Ss: 0.011817\n","\tRotated_Epoch:44 [000/005 (0060/0693)]\tLoss Ss: 0.014122\n","\tRotated_Epoch:44 [000/005 (0080/0693)]\tLoss Ss: 0.016822\n","\tRotated_Epoch:44 [000/005 (0100/0693)]\tLoss Ss: 0.010012\n","\tRotated_Epoch:44 [000/005 (0120/0693)]\tLoss Ss: 0.015299\n","\tRotated_Epoch:44 [000/005 (0140/0693)]\tLoss Ss: 0.017683\n","\tRotated_Epoch:44 [000/005 (0160/0693)]\tLoss Ss: 0.016308\n","\tRotated_Epoch:44 [000/005 (0180/0693)]\tLoss Ss: 0.010302\n","\tRotated_Epoch:44 [000/005 (0200/0693)]\tLoss Ss: 0.008834\n","\tRotated_Epoch:44 [000/005 (0220/0693)]\tLoss Ss: 0.011993\n","\tRotated_Epoch:44 [000/005 (0240/0693)]\tLoss Ss: 0.010982\n","\tRotated_Epoch:44 [000/005 (0260/0693)]\tLoss Ss: 0.012383\n","\tRotated_Epoch:44 [000/005 (0280/0693)]\tLoss Ss: 0.012092\n","\tRotated_Epoch:44 [000/005 (0300/0693)]\tLoss Ss: 0.011684\n","\tRotated_Epoch:44 [000/005 (0320/0693)]\tLoss Ss: 0.012129\n","\tRotated_Epoch:44 [000/005 (0340/0693)]\tLoss Ss: 0.013830\n","\tRotated_Epoch:44 [000/005 (0360/0693)]\tLoss Ss: 0.011266\n","\tRotated_Epoch:44 [000/005 (0380/0693)]\tLoss Ss: 0.011950\n","\tRotated_Epoch:44 [000/005 (0400/0693)]\tLoss Ss: 0.015397\n","\tRotated_Epoch:44 [000/005 (0420/0693)]\tLoss Ss: 0.014983\n","\tRotated_Epoch:44 [000/005 (0440/0693)]\tLoss Ss: 0.016682\n","\tRotated_Epoch:44 [000/005 (0460/0693)]\tLoss Ss: 0.011306\n","\tRotated_Epoch:44 [000/005 (0480/0693)]\tLoss Ss: 0.010001\n","\tRotated_Epoch:44 [000/005 (0500/0693)]\tLoss Ss: 0.014906\n","\tRotated_Epoch:44 [000/005 (0520/0693)]\tLoss Ss: 0.010805\n","\tRotated_Epoch:44 [000/005 (0540/0693)]\tLoss Ss: 0.008969\n","\tRotated_Epoch:44 [000/005 (0560/0693)]\tLoss Ss: 0.016010\n","\tRotated_Epoch:44 [000/005 (0580/0693)]\tLoss Ss: 0.010345\n","\tRotated_Epoch:44 [000/005 (0600/0693)]\tLoss Ss: 0.014399\n","\tRotated_Epoch:44 [000/005 (0620/0693)]\tLoss Ss: 0.011259\n","\tRotated_Epoch:44 [000/005 (0640/0693)]\tLoss Ss: 0.013690\n","\tRotated_Epoch:44 [000/005 (0660/0693)]\tLoss Ss: 0.013340\n","\tRotated_Epoch:44 [000/005 (0680/0693)]\tLoss Ss: 0.011612\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:44 [001/005 (0000/0755)]\tLoss Ss: 0.045327\n","\tRotated_Epoch:44 [001/005 (0020/0755)]\tLoss Ss: 0.037998\n","\tRotated_Epoch:44 [001/005 (0040/0755)]\tLoss Ss: 0.068537\n","\tRotated_Epoch:44 [001/005 (0060/0755)]\tLoss Ss: 0.041263\n","\tRotated_Epoch:44 [001/005 (0080/0755)]\tLoss Ss: 0.036738\n","\tRotated_Epoch:44 [001/005 (0100/0755)]\tLoss Ss: 0.049935\n","\tRotated_Epoch:44 [001/005 (0120/0755)]\tLoss Ss: 0.034230\n","\tRotated_Epoch:44 [001/005 (0140/0755)]\tLoss Ss: 0.028095\n","\tRotated_Epoch:44 [001/005 (0160/0755)]\tLoss Ss: 0.032909\n","\tRotated_Epoch:44 [001/005 (0180/0755)]\tLoss Ss: 0.023459\n","\tRotated_Epoch:44 [001/005 (0200/0755)]\tLoss Ss: 0.037989\n","\tRotated_Epoch:44 [001/005 (0220/0755)]\tLoss Ss: 0.027752\n","\tRotated_Epoch:44 [001/005 (0240/0755)]\tLoss Ss: 0.031931\n","\tRotated_Epoch:44 [001/005 (0260/0755)]\tLoss Ss: 0.038295\n","\tRotated_Epoch:44 [001/005 (0280/0755)]\tLoss Ss: 0.032226\n","\tRotated_Epoch:44 [001/005 (0300/0755)]\tLoss Ss: 0.025202\n","\tRotated_Epoch:44 [001/005 (0320/0755)]\tLoss Ss: 0.030522\n","\tRotated_Epoch:44 [001/005 (0340/0755)]\tLoss Ss: 0.030498\n","\tRotated_Epoch:44 [001/005 (0360/0755)]\tLoss Ss: 0.031360\n","\tRotated_Epoch:44 [001/005 (0380/0755)]\tLoss Ss: 0.024327\n","\tRotated_Epoch:44 [001/005 (0400/0755)]\tLoss Ss: 0.024421\n","\tRotated_Epoch:44 [001/005 (0420/0755)]\tLoss Ss: 0.029845\n","\tRotated_Epoch:44 [001/005 (0440/0755)]\tLoss Ss: 0.029160\n","\tRotated_Epoch:44 [001/005 (0460/0755)]\tLoss Ss: 0.023953\n","\tRotated_Epoch:44 [001/005 (0480/0755)]\tLoss Ss: 0.015387\n","\tRotated_Epoch:44 [001/005 (0500/0755)]\tLoss Ss: 0.025765\n","\tRotated_Epoch:44 [001/005 (0520/0755)]\tLoss Ss: 0.022998\n","\tRotated_Epoch:44 [001/005 (0540/0755)]\tLoss Ss: 0.022930\n","\tRotated_Epoch:44 [001/005 (0560/0755)]\tLoss Ss: 0.022472\n","\tRotated_Epoch:44 [001/005 (0580/0755)]\tLoss Ss: 0.016662\n","\tRotated_Epoch:44 [001/005 (0600/0755)]\tLoss Ss: 0.021287\n","\tRotated_Epoch:44 [001/005 (0620/0755)]\tLoss Ss: 0.032147\n","\tRotated_Epoch:44 [001/005 (0640/0755)]\tLoss Ss: 0.024315\n","\tRotated_Epoch:44 [001/005 (0660/0755)]\tLoss Ss: 0.025396\n","\tRotated_Epoch:44 [001/005 (0680/0755)]\tLoss Ss: 0.027923\n","\tRotated_Epoch:44 [001/005 (0700/0755)]\tLoss Ss: 0.028240\n","\tRotated_Epoch:44 [001/005 (0720/0755)]\tLoss Ss: 0.014564\n","\tRotated_Epoch:44 [001/005 (0740/0755)]\tLoss Ss: 0.022363\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tRotated_Epoch:44 [002/005 (0000/0588)]\tLoss Ss: 0.086345\n","\tRotated_Epoch:44 [002/005 (0020/0588)]\tLoss Ss: 0.065704\n","\tRotated_Epoch:44 [002/005 (0040/0588)]\tLoss Ss: 0.051342\n","\tRotated_Epoch:44 [002/005 (0060/0588)]\tLoss Ss: 0.061092\n","\tRotated_Epoch:44 [002/005 (0080/0588)]\tLoss Ss: 0.048714\n","\tRotated_Epoch:44 [002/005 (0100/0588)]\tLoss Ss: 0.033563\n","\tRotated_Epoch:44 [002/005 (0120/0588)]\tLoss Ss: 0.064741\n","\tRotated_Epoch:44 [002/005 (0140/0588)]\tLoss Ss: 0.052339\n","\tRotated_Epoch:44 [002/005 (0160/0588)]\tLoss Ss: 0.053393\n","\tRotated_Epoch:44 [002/005 (0180/0588)]\tLoss Ss: 0.073051\n","\tRotated_Epoch:44 [002/005 (0200/0588)]\tLoss Ss: 0.053040\n","\tRotated_Epoch:44 [002/005 (0220/0588)]\tLoss Ss: 0.044083\n","\tRotated_Epoch:44 [002/005 (0240/0588)]\tLoss Ss: 0.058004\n","\tRotated_Epoch:44 [002/005 (0260/0588)]\tLoss Ss: 0.064017\n","\tRotated_Epoch:44 [002/005 (0280/0588)]\tLoss Ss: 0.031952\n","\tRotated_Epoch:44 [002/005 (0300/0588)]\tLoss Ss: 0.051423\n","\tRotated_Epoch:44 [002/005 (0320/0588)]\tLoss Ss: 0.039295\n","\tRotated_Epoch:44 [002/005 (0340/0588)]\tLoss Ss: 0.058516\n","\tRotated_Epoch:44 [002/005 (0360/0588)]\tLoss Ss: 0.042761\n","\tRotated_Epoch:44 [002/005 (0380/0588)]\tLoss Ss: 0.046033\n","\tRotated_Epoch:44 [002/005 (0400/0588)]\tLoss Ss: 0.065895\n","\tRotated_Epoch:44 [002/005 (0420/0588)]\tLoss Ss: 0.060074\n","\tRotated_Epoch:44 [002/005 (0440/0588)]\tLoss Ss: 0.046683\n","\tRotated_Epoch:44 [002/005 (0460/0588)]\tLoss Ss: 0.043359\n","\tRotated_Epoch:44 [002/005 (0480/0588)]\tLoss Ss: 0.046167\n","\tRotated_Epoch:44 [002/005 (0500/0588)]\tLoss Ss: 0.059015\n","\tRotated_Epoch:44 [002/005 (0520/0588)]\tLoss Ss: 0.042816\n","\tRotated_Epoch:44 [002/005 (0540/0588)]\tLoss Ss: 0.048271\n","\tRotated_Epoch:44 [002/005 (0560/0588)]\tLoss Ss: 0.050727\n","\tRotated_Epoch:44 [002/005 (0580/0588)]\tLoss Ss: 0.094273\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tRotated_Epoch:44 [003/005 (0000/0693)]\tLoss Ss: 0.016509\n","\tRotated_Epoch:44 [003/005 (0020/0693)]\tLoss Ss: 0.015972\n","\tRotated_Epoch:44 [003/005 (0040/0693)]\tLoss Ss: 0.012303\n","\tRotated_Epoch:44 [003/005 (0060/0693)]\tLoss Ss: 0.016313\n","\tRotated_Epoch:44 [003/005 (0080/0693)]\tLoss Ss: 0.014805\n","\tRotated_Epoch:44 [003/005 (0100/0693)]\tLoss Ss: 0.017974\n","\tRotated_Epoch:44 [003/005 (0120/0693)]\tLoss Ss: 0.010436\n","\tRotated_Epoch:44 [003/005 (0140/0693)]\tLoss Ss: 0.013385\n","\tRotated_Epoch:44 [003/005 (0160/0693)]\tLoss Ss: 0.011362\n","\tRotated_Epoch:44 [003/005 (0180/0693)]\tLoss Ss: 0.011217\n","\tRotated_Epoch:44 [003/005 (0200/0693)]\tLoss Ss: 0.014858\n","\tRotated_Epoch:44 [003/005 (0220/0693)]\tLoss Ss: 0.014991\n","\tRotated_Epoch:44 [003/005 (0240/0693)]\tLoss Ss: 0.014603\n","\tRotated_Epoch:44 [003/005 (0260/0693)]\tLoss Ss: 0.020960\n","\tRotated_Epoch:44 [003/005 (0280/0693)]\tLoss Ss: 0.015704\n","\tRotated_Epoch:44 [003/005 (0300/0693)]\tLoss Ss: 0.013264\n","\tRotated_Epoch:44 [003/005 (0320/0693)]\tLoss Ss: 0.011931\n","\tRotated_Epoch:44 [003/005 (0340/0693)]\tLoss Ss: 0.017145\n","\tRotated_Epoch:44 [003/005 (0360/0693)]\tLoss Ss: 0.014676\n","\tRotated_Epoch:44 [003/005 (0380/0693)]\tLoss Ss: 0.012485\n","\tRotated_Epoch:44 [003/005 (0400/0693)]\tLoss Ss: 0.013964\n","\tRotated_Epoch:44 [003/005 (0420/0693)]\tLoss Ss: 0.011186\n","\tRotated_Epoch:44 [003/005 (0440/0693)]\tLoss Ss: 0.013024\n","\tRotated_Epoch:44 [003/005 (0460/0693)]\tLoss Ss: 0.014833\n","\tRotated_Epoch:44 [003/005 (0480/0693)]\tLoss Ss: 0.012394\n","\tRotated_Epoch:44 [003/005 (0500/0693)]\tLoss Ss: 0.014452\n","\tRotated_Epoch:44 [003/005 (0520/0693)]\tLoss Ss: 0.015536\n","\tRotated_Epoch:44 [003/005 (0540/0693)]\tLoss Ss: 0.016053\n","\tRotated_Epoch:44 [003/005 (0560/0693)]\tLoss Ss: 0.015351\n","\tRotated_Epoch:44 [003/005 (0580/0693)]\tLoss Ss: 0.015526\n","\tRotated_Epoch:44 [003/005 (0600/0693)]\tLoss Ss: 0.011352\n","\tRotated_Epoch:44 [003/005 (0620/0693)]\tLoss Ss: 0.012987\n","\tRotated_Epoch:44 [003/005 (0640/0693)]\tLoss Ss: 0.008745\n","\tRotated_Epoch:44 [003/005 (0660/0693)]\tLoss Ss: 0.008294\n","\tRotated_Epoch:44 [003/005 (0680/0693)]\tLoss Ss: 0.012238\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tRotated_Epoch:44 [004/005 (0000/0614)]\tLoss Ss: 0.031521\n","\tRotated_Epoch:44 [004/005 (0020/0614)]\tLoss Ss: 0.030556\n","\tRotated_Epoch:44 [004/005 (0040/0614)]\tLoss Ss: 0.026760\n","\tRotated_Epoch:44 [004/005 (0060/0614)]\tLoss Ss: 0.013430\n","\tRotated_Epoch:44 [004/005 (0080/0614)]\tLoss Ss: 0.013471\n","\tRotated_Epoch:44 [004/005 (0100/0614)]\tLoss Ss: 0.009611\n","\tRotated_Epoch:44 [004/005 (0120/0614)]\tLoss Ss: 0.005976\n","\tRotated_Epoch:44 [004/005 (0140/0614)]\tLoss Ss: 0.004955\n","\tRotated_Epoch:44 [004/005 (0160/0614)]\tLoss Ss: 0.005774\n","\tRotated_Epoch:44 [004/005 (0180/0614)]\tLoss Ss: 0.008911\n","\tRotated_Epoch:44 [004/005 (0200/0614)]\tLoss Ss: 0.010349\n","\tRotated_Epoch:44 [004/005 (0220/0614)]\tLoss Ss: 0.004916\n","\tRotated_Epoch:44 [004/005 (0240/0614)]\tLoss Ss: 0.003298\n","\tRotated_Epoch:44 [004/005 (0260/0614)]\tLoss Ss: 0.004991\n","\tRotated_Epoch:44 [004/005 (0280/0614)]\tLoss Ss: 0.003636\n","\tRotated_Epoch:44 [004/005 (0300/0614)]\tLoss Ss: 0.005498\n","\tRotated_Epoch:44 [004/005 (0320/0614)]\tLoss Ss: 0.005820\n","\tRotated_Epoch:44 [004/005 (0340/0614)]\tLoss Ss: 0.006605\n","\tRotated_Epoch:44 [004/005 (0360/0614)]\tLoss Ss: 0.003947\n","\tRotated_Epoch:44 [004/005 (0380/0614)]\tLoss Ss: 0.005757\n","\tRotated_Epoch:44 [004/005 (0400/0614)]\tLoss Ss: 0.007817\n","\tRotated_Epoch:44 [004/005 (0420/0614)]\tLoss Ss: 0.005221\n","\tRotated_Epoch:44 [004/005 (0440/0614)]\tLoss Ss: 0.004265\n","\tRotated_Epoch:44 [004/005 (0460/0614)]\tLoss Ss: 0.006916\n","\tRotated_Epoch:44 [004/005 (0480/0614)]\tLoss Ss: 0.007167\n","\tRotated_Epoch:44 [004/005 (0500/0614)]\tLoss Ss: 0.005793\n","\tRotated_Epoch:44 [004/005 (0520/0614)]\tLoss Ss: 0.007048\n","\tRotated_Epoch:44 [004/005 (0540/0614)]\tLoss Ss: 0.008625\n","\tRotated_Epoch:44 [004/005 (0560/0614)]\tLoss Ss: 0.004754\n","\tRotated_Epoch:44 [004/005 (0580/0614)]\tLoss Ss: 0.005736\n","\tRotated_Epoch:44 [004/005 (0600/0614)]\tLoss Ss: 0.004182\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:44 [005/005 (0000/0755)]\tLoss Ss: 0.056591\n","\tRotated_Epoch:44 [005/005 (0020/0755)]\tLoss Ss: 0.047408\n","\tRotated_Epoch:44 [005/005 (0040/0755)]\tLoss Ss: 0.048948\n","\tRotated_Epoch:44 [005/005 (0060/0755)]\tLoss Ss: 0.024566\n","\tRotated_Epoch:44 [005/005 (0080/0755)]\tLoss Ss: 0.027167\n","\tRotated_Epoch:44 [005/005 (0100/0755)]\tLoss Ss: 0.020495\n","\tRotated_Epoch:44 [005/005 (0120/0755)]\tLoss Ss: 0.023706\n","\tRotated_Epoch:44 [005/005 (0140/0755)]\tLoss Ss: 0.013440\n","\tRotated_Epoch:44 [005/005 (0160/0755)]\tLoss Ss: 0.031363\n","\tRotated_Epoch:44 [005/005 (0180/0755)]\tLoss Ss: 0.022809\n","\tRotated_Epoch:44 [005/005 (0200/0755)]\tLoss Ss: 0.019862\n","\tRotated_Epoch:44 [005/005 (0220/0755)]\tLoss Ss: 0.012431\n","\tRotated_Epoch:44 [005/005 (0240/0755)]\tLoss Ss: 0.012563\n","\tRotated_Epoch:44 [005/005 (0260/0755)]\tLoss Ss: 0.014474\n","\tRotated_Epoch:44 [005/005 (0280/0755)]\tLoss Ss: 0.018366\n","\tRotated_Epoch:44 [005/005 (0300/0755)]\tLoss Ss: 0.024716\n","\tRotated_Epoch:44 [005/005 (0320/0755)]\tLoss Ss: 0.014261\n","\tRotated_Epoch:44 [005/005 (0340/0755)]\tLoss Ss: 0.016964\n","\tRotated_Epoch:44 [005/005 (0360/0755)]\tLoss Ss: 0.012204\n","\tRotated_Epoch:44 [005/005 (0380/0755)]\tLoss Ss: 0.016107\n","\tRotated_Epoch:44 [005/005 (0400/0755)]\tLoss Ss: 0.011693\n","\tRotated_Epoch:44 [005/005 (0420/0755)]\tLoss Ss: 0.011181\n","\tRotated_Epoch:44 [005/005 (0440/0755)]\tLoss Ss: 0.017176\n","\tRotated_Epoch:44 [005/005 (0460/0755)]\tLoss Ss: 0.015186\n","\tRotated_Epoch:44 [005/005 (0480/0755)]\tLoss Ss: 0.009915\n","\tRotated_Epoch:44 [005/005 (0500/0755)]\tLoss Ss: 0.012087\n","\tRotated_Epoch:44 [005/005 (0520/0755)]\tLoss Ss: 0.014951\n","\tRotated_Epoch:44 [005/005 (0540/0755)]\tLoss Ss: 0.015501\n","\tRotated_Epoch:44 [005/005 (0560/0755)]\tLoss Ss: 0.009438\n","\tRotated_Epoch:44 [005/005 (0580/0755)]\tLoss Ss: 0.016254\n","\tRotated_Epoch:44 [005/005 (0600/0755)]\tLoss Ss: 0.010898\n","\tRotated_Epoch:44 [005/005 (0620/0755)]\tLoss Ss: 0.015525\n","\tRotated_Epoch:44 [005/005 (0640/0755)]\tLoss Ss: 0.018692\n","\tRotated_Epoch:44 [005/005 (0660/0755)]\tLoss Ss: 0.014249\n","\tRotated_Epoch:44 [005/005 (0680/0755)]\tLoss Ss: 0.012512\n","\tRotated_Epoch:44 [005/005 (0700/0755)]\tLoss Ss: 0.009492\n","\tRotated_Epoch:44 [005/005 (0720/0755)]\tLoss Ss: 0.009887\n","\tRotated_Epoch:44 [005/005 (0740/0755)]\tLoss Ss: 0.011603\n","new_1_6_d.nii\n","new_1_6_m.nii\n","new_1_7_d.nii\n","new_1_7_m.nii\n","new_2_5_d.nii\n","new_2_5_m.nii\n","new_2_6_d.nii\n","new_2_6_m.nii\n","new_2_7_d.nii\n","new_2_7_m.nii\n","new_3_d (2).nii\n","new_3_m (2).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (3).nii\n","new_3_m (3).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (7).nii\n","new_3_m (7).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (8).nii\n","new_3_m (8).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (9).nii\n","new_3_m (9).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_4_10_d_163526.nii\n","new_4_10_m_163526.nii.gz\n","new_4_11_d_084336.nii\n","new_4_11_m_084336.nii.gz\n","new_4_7_d_184422.nii\n","new_4_7_m_184422.nii.gz\n","new_4_8_d_143412.nii\n","new_4_8_m_143412.nii.gz\n","new_4_9_d_155347.nii\n","new_4_9_m_155347.nii.gz\n","\tEpoch: 44; Dice: 0.9638 +/- 0.0072; Loss: 7.1207\n","Begin Epoch 45\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:45 [000/005 (0000/0755)]\tLoss Ss: 0.011851\n","\tEpoch:45 [000/005 (0020/0755)]\tLoss Ss: 0.009844\n","\tEpoch:45 [000/005 (0040/0755)]\tLoss Ss: 0.017533\n","\tEpoch:45 [000/005 (0060/0755)]\tLoss Ss: 0.016259\n","\tEpoch:45 [000/005 (0080/0755)]\tLoss Ss: 0.011652\n","\tEpoch:45 [000/005 (0100/0755)]\tLoss Ss: 0.010392\n","\tEpoch:45 [000/005 (0120/0755)]\tLoss Ss: 0.008831\n","\tEpoch:45 [000/005 (0140/0755)]\tLoss Ss: 0.017474\n","\tEpoch:45 [000/005 (0160/0755)]\tLoss Ss: 0.012073\n","\tEpoch:45 [000/005 (0180/0755)]\tLoss Ss: 0.011682\n","\tEpoch:45 [000/005 (0200/0755)]\tLoss Ss: 0.009817\n","\tEpoch:45 [000/005 (0220/0755)]\tLoss Ss: 0.010997\n","\tEpoch:45 [000/005 (0240/0755)]\tLoss Ss: 0.014099\n","\tEpoch:45 [000/005 (0260/0755)]\tLoss Ss: 0.008114\n","\tEpoch:45 [000/005 (0280/0755)]\tLoss Ss: 0.011951\n","\tEpoch:45 [000/005 (0300/0755)]\tLoss Ss: 0.007721\n","\tEpoch:45 [000/005 (0320/0755)]\tLoss Ss: 0.013292\n","\tEpoch:45 [000/005 (0340/0755)]\tLoss Ss: 0.008387\n","\tEpoch:45 [000/005 (0360/0755)]\tLoss Ss: 0.016794\n","\tEpoch:45 [000/005 (0380/0755)]\tLoss Ss: 0.013881\n","\tEpoch:45 [000/005 (0400/0755)]\tLoss Ss: 0.013419\n","\tEpoch:45 [000/005 (0420/0755)]\tLoss Ss: 0.013704\n","\tEpoch:45 [000/005 (0440/0755)]\tLoss Ss: 0.009937\n","\tEpoch:45 [000/005 (0460/0755)]\tLoss Ss: 0.009624\n","\tEpoch:45 [000/005 (0480/0755)]\tLoss Ss: 0.010574\n","\tEpoch:45 [000/005 (0500/0755)]\tLoss Ss: 0.011037\n","\tEpoch:45 [000/005 (0520/0755)]\tLoss Ss: 0.011655\n","\tEpoch:45 [000/005 (0540/0755)]\tLoss Ss: 0.007515\n","\tEpoch:45 [000/005 (0560/0755)]\tLoss Ss: 0.009539\n","\tEpoch:45 [000/005 (0580/0755)]\tLoss Ss: 0.005071\n","\tEpoch:45 [000/005 (0600/0755)]\tLoss Ss: 0.009168\n","\tEpoch:45 [000/005 (0620/0755)]\tLoss Ss: 0.008444\n","\tEpoch:45 [000/005 (0640/0755)]\tLoss Ss: 0.010082\n","\tEpoch:45 [000/005 (0660/0755)]\tLoss Ss: 0.017347\n","\tEpoch:45 [000/005 (0680/0755)]\tLoss Ss: 0.009076\n","\tEpoch:45 [000/005 (0700/0755)]\tLoss Ss: 0.007291\n","\tEpoch:45 [000/005 (0720/0755)]\tLoss Ss: 0.007002\n","\tEpoch:45 [000/005 (0740/0755)]\tLoss Ss: 0.016498\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:45 [001/005 (0000/0755)]\tLoss Ss: 0.024490\n","\tEpoch:45 [001/005 (0020/0755)]\tLoss Ss: 0.020117\n","\tEpoch:45 [001/005 (0040/0755)]\tLoss Ss: 0.018599\n","\tEpoch:45 [001/005 (0060/0755)]\tLoss Ss: 0.019143\n","\tEpoch:45 [001/005 (0080/0755)]\tLoss Ss: 0.012688\n","\tEpoch:45 [001/005 (0100/0755)]\tLoss Ss: 0.015113\n","\tEpoch:45 [001/005 (0120/0755)]\tLoss Ss: 0.014981\n","\tEpoch:45 [001/005 (0140/0755)]\tLoss Ss: 0.012120\n","\tEpoch:45 [001/005 (0160/0755)]\tLoss Ss: 0.018464\n","\tEpoch:45 [001/005 (0180/0755)]\tLoss Ss: 0.012159\n","\tEpoch:45 [001/005 (0200/0755)]\tLoss Ss: 0.014661\n","\tEpoch:45 [001/005 (0220/0755)]\tLoss Ss: 0.014994\n","\tEpoch:45 [001/005 (0240/0755)]\tLoss Ss: 0.018534\n","\tEpoch:45 [001/005 (0260/0755)]\tLoss Ss: 0.011647\n","\tEpoch:45 [001/005 (0280/0755)]\tLoss Ss: 0.015082\n","\tEpoch:45 [001/005 (0300/0755)]\tLoss Ss: 0.013737\n","\tEpoch:45 [001/005 (0320/0755)]\tLoss Ss: 0.012988\n","\tEpoch:45 [001/005 (0340/0755)]\tLoss Ss: 0.009241\n","\tEpoch:45 [001/005 (0360/0755)]\tLoss Ss: 0.012920\n","\tEpoch:45 [001/005 (0380/0755)]\tLoss Ss: 0.010873\n","\tEpoch:45 [001/005 (0400/0755)]\tLoss Ss: 0.013182\n","\tEpoch:45 [001/005 (0420/0755)]\tLoss Ss: 0.015168\n","\tEpoch:45 [001/005 (0440/0755)]\tLoss Ss: 0.012072\n","\tEpoch:45 [001/005 (0460/0755)]\tLoss Ss: 0.010445\n","\tEpoch:45 [001/005 (0480/0755)]\tLoss Ss: 0.011600\n","\tEpoch:45 [001/005 (0500/0755)]\tLoss Ss: 0.010387\n","\tEpoch:45 [001/005 (0520/0755)]\tLoss Ss: 0.007590\n","\tEpoch:45 [001/005 (0540/0755)]\tLoss Ss: 0.013114\n","\tEpoch:45 [001/005 (0560/0755)]\tLoss Ss: 0.010016\n","\tEpoch:45 [001/005 (0580/0755)]\tLoss Ss: 0.019686\n","\tEpoch:45 [001/005 (0600/0755)]\tLoss Ss: 0.006768\n","\tEpoch:45 [001/005 (0620/0755)]\tLoss Ss: 0.009573\n","\tEpoch:45 [001/005 (0640/0755)]\tLoss Ss: 0.011230\n","\tEpoch:45 [001/005 (0660/0755)]\tLoss Ss: 0.013509\n","\tEpoch:45 [001/005 (0680/0755)]\tLoss Ss: 0.014298\n","\tEpoch:45 [001/005 (0700/0755)]\tLoss Ss: 0.016519\n","\tEpoch:45 [001/005 (0720/0755)]\tLoss Ss: 0.011481\n","\tEpoch:45 [001/005 (0740/0755)]\tLoss Ss: 0.011614\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tEpoch:45 [002/005 (0000/0614)]\tLoss Ss: 0.005660\n","\tEpoch:45 [002/005 (0020/0614)]\tLoss Ss: 0.002895\n","\tEpoch:45 [002/005 (0040/0614)]\tLoss Ss: 0.006209\n","\tEpoch:45 [002/005 (0060/0614)]\tLoss Ss: 0.004657\n","\tEpoch:45 [002/005 (0080/0614)]\tLoss Ss: 0.004696\n","\tEpoch:45 [002/005 (0100/0614)]\tLoss Ss: 0.003324\n","\tEpoch:45 [002/005 (0120/0614)]\tLoss Ss: 0.005765\n","\tEpoch:45 [002/005 (0140/0614)]\tLoss Ss: 0.007775\n","\tEpoch:45 [002/005 (0160/0614)]\tLoss Ss: 0.004903\n","\tEpoch:45 [002/005 (0180/0614)]\tLoss Ss: 0.005018\n","\tEpoch:45 [002/005 (0200/0614)]\tLoss Ss: 0.004314\n","\tEpoch:45 [002/005 (0220/0614)]\tLoss Ss: 0.004834\n","\tEpoch:45 [002/005 (0240/0614)]\tLoss Ss: 0.005122\n","\tEpoch:45 [002/005 (0260/0614)]\tLoss Ss: 0.005064\n","\tEpoch:45 [002/005 (0280/0614)]\tLoss Ss: 0.004649\n","\tEpoch:45 [002/005 (0300/0614)]\tLoss Ss: 0.003942\n","\tEpoch:45 [002/005 (0320/0614)]\tLoss Ss: 0.003948\n","\tEpoch:45 [002/005 (0340/0614)]\tLoss Ss: 0.007672\n","\tEpoch:45 [002/005 (0360/0614)]\tLoss Ss: 0.007986\n","\tEpoch:45 [002/005 (0380/0614)]\tLoss Ss: 0.007042\n","\tEpoch:45 [002/005 (0400/0614)]\tLoss Ss: 0.006002\n","\tEpoch:45 [002/005 (0420/0614)]\tLoss Ss: 0.002637\n","\tEpoch:45 [002/005 (0440/0614)]\tLoss Ss: 0.006191\n","\tEpoch:45 [002/005 (0460/0614)]\tLoss Ss: 0.004231\n","\tEpoch:45 [002/005 (0480/0614)]\tLoss Ss: 0.004473\n","\tEpoch:45 [002/005 (0500/0614)]\tLoss Ss: 0.005738\n","\tEpoch:45 [002/005 (0520/0614)]\tLoss Ss: 0.005197\n","\tEpoch:45 [002/005 (0540/0614)]\tLoss Ss: 0.004429\n","\tEpoch:45 [002/005 (0560/0614)]\tLoss Ss: 0.005841\n","\tEpoch:45 [002/005 (0580/0614)]\tLoss Ss: 0.004428\n","\tEpoch:45 [002/005 (0600/0614)]\tLoss Ss: 0.007719\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tEpoch:45 [003/005 (0000/0693)]\tLoss Ss: 0.012124\n","\tEpoch:45 [003/005 (0020/0693)]\tLoss Ss: 0.009831\n","\tEpoch:45 [003/005 (0040/0693)]\tLoss Ss: 0.010312\n","\tEpoch:45 [003/005 (0060/0693)]\tLoss Ss: 0.010869\n","\tEpoch:45 [003/005 (0080/0693)]\tLoss Ss: 0.009934\n","\tEpoch:45 [003/005 (0100/0693)]\tLoss Ss: 0.014371\n","\tEpoch:45 [003/005 (0120/0693)]\tLoss Ss: 0.015127\n","\tEpoch:45 [003/005 (0140/0693)]\tLoss Ss: 0.015812\n","\tEpoch:45 [003/005 (0160/0693)]\tLoss Ss: 0.010843\n","\tEpoch:45 [003/005 (0180/0693)]\tLoss Ss: 0.009261\n","\tEpoch:45 [003/005 (0200/0693)]\tLoss Ss: 0.015116\n","\tEpoch:45 [003/005 (0220/0693)]\tLoss Ss: 0.019116\n","\tEpoch:45 [003/005 (0240/0693)]\tLoss Ss: 0.009609\n","\tEpoch:45 [003/005 (0260/0693)]\tLoss Ss: 0.013718\n","\tEpoch:45 [003/005 (0280/0693)]\tLoss Ss: 0.015472\n","\tEpoch:45 [003/005 (0300/0693)]\tLoss Ss: 0.010877\n","\tEpoch:45 [003/005 (0320/0693)]\tLoss Ss: 0.009545\n","\tEpoch:45 [003/005 (0340/0693)]\tLoss Ss: 0.011772\n","\tEpoch:45 [003/005 (0360/0693)]\tLoss Ss: 0.012747\n","\tEpoch:45 [003/005 (0380/0693)]\tLoss Ss: 0.009297\n","\tEpoch:45 [003/005 (0400/0693)]\tLoss Ss: 0.011512\n","\tEpoch:45 [003/005 (0420/0693)]\tLoss Ss: 0.010662\n","\tEpoch:45 [003/005 (0440/0693)]\tLoss Ss: 0.011767\n","\tEpoch:45 [003/005 (0460/0693)]\tLoss Ss: 0.014897\n","\tEpoch:45 [003/005 (0480/0693)]\tLoss Ss: 0.009272\n","\tEpoch:45 [003/005 (0500/0693)]\tLoss Ss: 0.010169\n","\tEpoch:45 [003/005 (0520/0693)]\tLoss Ss: 0.007889\n","\tEpoch:45 [003/005 (0540/0693)]\tLoss Ss: 0.013915\n","\tEpoch:45 [003/005 (0560/0693)]\tLoss Ss: 0.009218\n","\tEpoch:45 [003/005 (0580/0693)]\tLoss Ss: 0.011656\n","\tEpoch:45 [003/005 (0600/0693)]\tLoss Ss: 0.012862\n","\tEpoch:45 [003/005 (0620/0693)]\tLoss Ss: 0.014390\n","\tEpoch:45 [003/005 (0640/0693)]\tLoss Ss: 0.011990\n","\tEpoch:45 [003/005 (0660/0693)]\tLoss Ss: 0.011718\n","\tEpoch:45 [003/005 (0680/0693)]\tLoss Ss: 0.009679\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tEpoch:45 [004/005 (0000/0693)]\tLoss Ss: 0.014083\n","\tEpoch:45 [004/005 (0020/0693)]\tLoss Ss: 0.011908\n","\tEpoch:45 [004/005 (0040/0693)]\tLoss Ss: 0.006420\n","\tEpoch:45 [004/005 (0060/0693)]\tLoss Ss: 0.013678\n","\tEpoch:45 [004/005 (0080/0693)]\tLoss Ss: 0.007229\n","\tEpoch:45 [004/005 (0100/0693)]\tLoss Ss: 0.009753\n","\tEpoch:45 [004/005 (0120/0693)]\tLoss Ss: 0.011809\n","\tEpoch:45 [004/005 (0140/0693)]\tLoss Ss: 0.013313\n","\tEpoch:45 [004/005 (0160/0693)]\tLoss Ss: 0.008757\n","\tEpoch:45 [004/005 (0180/0693)]\tLoss Ss: 0.007149\n","\tEpoch:45 [004/005 (0200/0693)]\tLoss Ss: 0.016095\n","\tEpoch:45 [004/005 (0220/0693)]\tLoss Ss: 0.015080\n","\tEpoch:45 [004/005 (0240/0693)]\tLoss Ss: 0.010841\n","\tEpoch:45 [004/005 (0260/0693)]\tLoss Ss: 0.005901\n","\tEpoch:45 [004/005 (0280/0693)]\tLoss Ss: 0.011492\n","\tEpoch:45 [004/005 (0300/0693)]\tLoss Ss: 0.013116\n","\tEpoch:45 [004/005 (0320/0693)]\tLoss Ss: 0.011399\n","\tEpoch:45 [004/005 (0340/0693)]\tLoss Ss: 0.012536\n","\tEpoch:45 [004/005 (0360/0693)]\tLoss Ss: 0.009279\n","\tEpoch:45 [004/005 (0380/0693)]\tLoss Ss: 0.009168\n","\tEpoch:45 [004/005 (0400/0693)]\tLoss Ss: 0.012121\n","\tEpoch:45 [004/005 (0420/0693)]\tLoss Ss: 0.011682\n","\tEpoch:45 [004/005 (0440/0693)]\tLoss Ss: 0.008368\n","\tEpoch:45 [004/005 (0460/0693)]\tLoss Ss: 0.010062\n","\tEpoch:45 [004/005 (0480/0693)]\tLoss Ss: 0.012431\n","\tEpoch:45 [004/005 (0500/0693)]\tLoss Ss: 0.008850\n","\tEpoch:45 [004/005 (0520/0693)]\tLoss Ss: 0.010057\n","\tEpoch:45 [004/005 (0540/0693)]\tLoss Ss: 0.010638\n","\tEpoch:45 [004/005 (0560/0693)]\tLoss Ss: 0.008851\n","\tEpoch:45 [004/005 (0580/0693)]\tLoss Ss: 0.010737\n","\tEpoch:45 [004/005 (0600/0693)]\tLoss Ss: 0.008880\n","\tEpoch:45 [004/005 (0620/0693)]\tLoss Ss: 0.009702\n","\tEpoch:45 [004/005 (0640/0693)]\tLoss Ss: 0.009706\n","\tEpoch:45 [004/005 (0660/0693)]\tLoss Ss: 0.009559\n","\tEpoch:45 [004/005 (0680/0693)]\tLoss Ss: 0.011511\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tEpoch:45 [005/005 (0000/0588)]\tLoss Ss: 0.006365\n","\tEpoch:45 [005/005 (0020/0588)]\tLoss Ss: 0.003296\n","\tEpoch:45 [005/005 (0040/0588)]\tLoss Ss: 0.003126\n","\tEpoch:45 [005/005 (0060/0588)]\tLoss Ss: 0.003289\n","\tEpoch:45 [005/005 (0080/0588)]\tLoss Ss: 0.003652\n","\tEpoch:45 [005/005 (0100/0588)]\tLoss Ss: 0.004441\n","\tEpoch:45 [005/005 (0120/0588)]\tLoss Ss: 0.003174\n","\tEpoch:45 [005/005 (0140/0588)]\tLoss Ss: 0.005114\n","\tEpoch:45 [005/005 (0160/0588)]\tLoss Ss: 0.003411\n","\tEpoch:45 [005/005 (0180/0588)]\tLoss Ss: 0.005298\n","\tEpoch:45 [005/005 (0200/0588)]\tLoss Ss: 0.002967\n","\tEpoch:45 [005/005 (0220/0588)]\tLoss Ss: 0.004276\n","\tEpoch:45 [005/005 (0240/0588)]\tLoss Ss: 0.002580\n","\tEpoch:45 [005/005 (0260/0588)]\tLoss Ss: 0.003970\n","\tEpoch:45 [005/005 (0280/0588)]\tLoss Ss: 0.004178\n","\tEpoch:45 [005/005 (0300/0588)]\tLoss Ss: 0.003280\n","\tEpoch:45 [005/005 (0320/0588)]\tLoss Ss: 0.004855\n","\tEpoch:45 [005/005 (0340/0588)]\tLoss Ss: 0.005078\n","\tEpoch:45 [005/005 (0360/0588)]\tLoss Ss: 0.002313\n","\tEpoch:45 [005/005 (0380/0588)]\tLoss Ss: 0.002432\n","\tEpoch:45 [005/005 (0400/0588)]\tLoss Ss: 0.003141\n","\tEpoch:45 [005/005 (0420/0588)]\tLoss Ss: 0.004589\n","\tEpoch:45 [005/005 (0440/0588)]\tLoss Ss: 0.003720\n","\tEpoch:45 [005/005 (0460/0588)]\tLoss Ss: 0.004249\n","\tEpoch:45 [005/005 (0480/0588)]\tLoss Ss: 0.004580\n","\tEpoch:45 [005/005 (0500/0588)]\tLoss Ss: 0.002453\n","\tEpoch:45 [005/005 (0520/0588)]\tLoss Ss: 0.004794\n","\tEpoch:45 [005/005 (0540/0588)]\tLoss Ss: 0.003994\n","\tEpoch:45 [005/005 (0560/0588)]\tLoss Ss: 0.004229\n","\tEpoch:45 [005/005 (0580/0588)]\tLoss Ss: 0.005471\n","Now train the rotated image\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tRotated_Epoch:45 [000/005 (0000/0693)]\tLoss Ss: 0.018033\n","\tRotated_Epoch:45 [000/005 (0020/0693)]\tLoss Ss: 0.009590\n","\tRotated_Epoch:45 [000/005 (0040/0693)]\tLoss Ss: 0.011635\n","\tRotated_Epoch:45 [000/005 (0060/0693)]\tLoss Ss: 0.016879\n","\tRotated_Epoch:45 [000/005 (0080/0693)]\tLoss Ss: 0.013460\n","\tRotated_Epoch:45 [000/005 (0100/0693)]\tLoss Ss: 0.010085\n","\tRotated_Epoch:45 [000/005 (0120/0693)]\tLoss Ss: 0.008766\n","\tRotated_Epoch:45 [000/005 (0140/0693)]\tLoss Ss: 0.015802\n","\tRotated_Epoch:45 [000/005 (0160/0693)]\tLoss Ss: 0.011104\n","\tRotated_Epoch:45 [000/005 (0180/0693)]\tLoss Ss: 0.009750\n","\tRotated_Epoch:45 [000/005 (0200/0693)]\tLoss Ss: 0.015928\n","\tRotated_Epoch:45 [000/005 (0220/0693)]\tLoss Ss: 0.012755\n","\tRotated_Epoch:45 [000/005 (0240/0693)]\tLoss Ss: 0.011349\n","\tRotated_Epoch:45 [000/005 (0260/0693)]\tLoss Ss: 0.014823\n","\tRotated_Epoch:45 [000/005 (0280/0693)]\tLoss Ss: 0.013618\n","\tRotated_Epoch:45 [000/005 (0300/0693)]\tLoss Ss: 0.008076\n","\tRotated_Epoch:45 [000/005 (0320/0693)]\tLoss Ss: 0.010093\n","\tRotated_Epoch:45 [000/005 (0340/0693)]\tLoss Ss: 0.011877\n","\tRotated_Epoch:45 [000/005 (0360/0693)]\tLoss Ss: 0.011355\n","\tRotated_Epoch:45 [000/005 (0380/0693)]\tLoss Ss: 0.011692\n","\tRotated_Epoch:45 [000/005 (0400/0693)]\tLoss Ss: 0.010385\n","\tRotated_Epoch:45 [000/005 (0420/0693)]\tLoss Ss: 0.009473\n","\tRotated_Epoch:45 [000/005 (0440/0693)]\tLoss Ss: 0.010293\n","\tRotated_Epoch:45 [000/005 (0460/0693)]\tLoss Ss: 0.009496\n","\tRotated_Epoch:45 [000/005 (0480/0693)]\tLoss Ss: 0.012850\n","\tRotated_Epoch:45 [000/005 (0500/0693)]\tLoss Ss: 0.009184\n","\tRotated_Epoch:45 [000/005 (0520/0693)]\tLoss Ss: 0.011134\n","\tRotated_Epoch:45 [000/005 (0540/0693)]\tLoss Ss: 0.011608\n","\tRotated_Epoch:45 [000/005 (0560/0693)]\tLoss Ss: 0.009909\n","\tRotated_Epoch:45 [000/005 (0580/0693)]\tLoss Ss: 0.009113\n","\tRotated_Epoch:45 [000/005 (0600/0693)]\tLoss Ss: 0.010694\n","\tRotated_Epoch:45 [000/005 (0620/0693)]\tLoss Ss: 0.014657\n","\tRotated_Epoch:45 [000/005 (0640/0693)]\tLoss Ss: 0.010494\n","\tRotated_Epoch:45 [000/005 (0660/0693)]\tLoss Ss: 0.012523\n","\tRotated_Epoch:45 [000/005 (0680/0693)]\tLoss Ss: 0.012243\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tRotated_Epoch:45 [001/005 (0000/0614)]\tLoss Ss: 0.006054\n","\tRotated_Epoch:45 [001/005 (0020/0614)]\tLoss Ss: 0.004002\n","\tRotated_Epoch:45 [001/005 (0040/0614)]\tLoss Ss: 0.005789\n","\tRotated_Epoch:45 [001/005 (0060/0614)]\tLoss Ss: 0.004106\n","\tRotated_Epoch:45 [001/005 (0080/0614)]\tLoss Ss: 0.006464\n","\tRotated_Epoch:45 [001/005 (0100/0614)]\tLoss Ss: 0.003574\n","\tRotated_Epoch:45 [001/005 (0120/0614)]\tLoss Ss: 0.008830\n","\tRotated_Epoch:45 [001/005 (0140/0614)]\tLoss Ss: 0.003365\n","\tRotated_Epoch:45 [001/005 (0160/0614)]\tLoss Ss: 0.001667\n","\tRotated_Epoch:45 [001/005 (0180/0614)]\tLoss Ss: 0.004532\n","\tRotated_Epoch:45 [001/005 (0200/0614)]\tLoss Ss: 0.005806\n","\tRotated_Epoch:45 [001/005 (0220/0614)]\tLoss Ss: 0.004203\n","\tRotated_Epoch:45 [001/005 (0240/0614)]\tLoss Ss: 0.006977\n","\tRotated_Epoch:45 [001/005 (0260/0614)]\tLoss Ss: 0.003931\n","\tRotated_Epoch:45 [001/005 (0280/0614)]\tLoss Ss: 0.005004\n","\tRotated_Epoch:45 [001/005 (0300/0614)]\tLoss Ss: 0.008495\n","\tRotated_Epoch:45 [001/005 (0320/0614)]\tLoss Ss: 0.005983\n","\tRotated_Epoch:45 [001/005 (0340/0614)]\tLoss Ss: 0.007573\n","\tRotated_Epoch:45 [001/005 (0360/0614)]\tLoss Ss: 0.006740\n","\tRotated_Epoch:45 [001/005 (0380/0614)]\tLoss Ss: 0.004414\n","\tRotated_Epoch:45 [001/005 (0400/0614)]\tLoss Ss: 0.007068\n","\tRotated_Epoch:45 [001/005 (0420/0614)]\tLoss Ss: 0.005670\n","\tRotated_Epoch:45 [001/005 (0440/0614)]\tLoss Ss: 0.005428\n","\tRotated_Epoch:45 [001/005 (0460/0614)]\tLoss Ss: 0.003382\n","\tRotated_Epoch:45 [001/005 (0480/0614)]\tLoss Ss: 0.004146\n","\tRotated_Epoch:45 [001/005 (0500/0614)]\tLoss Ss: 0.003556\n","\tRotated_Epoch:45 [001/005 (0520/0614)]\tLoss Ss: 0.003027\n","\tRotated_Epoch:45 [001/005 (0540/0614)]\tLoss Ss: 0.005693\n","\tRotated_Epoch:45 [001/005 (0560/0614)]\tLoss Ss: 0.006386\n","\tRotated_Epoch:45 [001/005 (0580/0614)]\tLoss Ss: 0.005733\n","\tRotated_Epoch:45 [001/005 (0600/0614)]\tLoss Ss: 0.004561\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tRotated_Epoch:45 [002/005 (0000/0588)]\tLoss Ss: 0.056555\n","\tRotated_Epoch:45 [002/005 (0020/0588)]\tLoss Ss: 0.061652\n","\tRotated_Epoch:45 [002/005 (0040/0588)]\tLoss Ss: 0.055575\n","\tRotated_Epoch:45 [002/005 (0060/0588)]\tLoss Ss: 0.056232\n","\tRotated_Epoch:45 [002/005 (0080/0588)]\tLoss Ss: 0.068600\n","\tRotated_Epoch:45 [002/005 (0100/0588)]\tLoss Ss: 0.078143\n","\tRotated_Epoch:45 [002/005 (0120/0588)]\tLoss Ss: 0.061008\n","\tRotated_Epoch:45 [002/005 (0140/0588)]\tLoss Ss: 0.047197\n","\tRotated_Epoch:45 [002/005 (0160/0588)]\tLoss Ss: 0.047394\n","\tRotated_Epoch:45 [002/005 (0180/0588)]\tLoss Ss: 0.053386\n","\tRotated_Epoch:45 [002/005 (0200/0588)]\tLoss Ss: 0.029377\n","\tRotated_Epoch:45 [002/005 (0220/0588)]\tLoss Ss: 0.040517\n","\tRotated_Epoch:45 [002/005 (0240/0588)]\tLoss Ss: 0.052242\n","\tRotated_Epoch:45 [002/005 (0260/0588)]\tLoss Ss: 0.052018\n","\tRotated_Epoch:45 [002/005 (0280/0588)]\tLoss Ss: 0.065756\n","\tRotated_Epoch:45 [002/005 (0300/0588)]\tLoss Ss: 0.049862\n","\tRotated_Epoch:45 [002/005 (0320/0588)]\tLoss Ss: 0.048114\n","\tRotated_Epoch:45 [002/005 (0340/0588)]\tLoss Ss: 0.043989\n","\tRotated_Epoch:45 [002/005 (0360/0588)]\tLoss Ss: 0.061348\n","\tRotated_Epoch:45 [002/005 (0380/0588)]\tLoss Ss: 0.066384\n","\tRotated_Epoch:45 [002/005 (0400/0588)]\tLoss Ss: 0.055731\n","\tRotated_Epoch:45 [002/005 (0420/0588)]\tLoss Ss: 0.042528\n","\tRotated_Epoch:45 [002/005 (0440/0588)]\tLoss Ss: 0.025331\n","\tRotated_Epoch:45 [002/005 (0460/0588)]\tLoss Ss: 0.066063\n","\tRotated_Epoch:45 [002/005 (0480/0588)]\tLoss Ss: 0.065405\n","\tRotated_Epoch:45 [002/005 (0500/0588)]\tLoss Ss: 0.036398\n","\tRotated_Epoch:45 [002/005 (0520/0588)]\tLoss Ss: 0.060609\n","\tRotated_Epoch:45 [002/005 (0540/0588)]\tLoss Ss: 0.055703\n","\tRotated_Epoch:45 [002/005 (0560/0588)]\tLoss Ss: 0.039427\n","\tRotated_Epoch:45 [002/005 (0580/0588)]\tLoss Ss: 0.028037\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tRotated_Epoch:45 [003/005 (0000/0693)]\tLoss Ss: 0.015565\n","\tRotated_Epoch:45 [003/005 (0020/0693)]\tLoss Ss: 0.013956\n","\tRotated_Epoch:45 [003/005 (0040/0693)]\tLoss Ss: 0.015633\n","\tRotated_Epoch:45 [003/005 (0060/0693)]\tLoss Ss: 0.014931\n","\tRotated_Epoch:45 [003/005 (0080/0693)]\tLoss Ss: 0.016553\n","\tRotated_Epoch:45 [003/005 (0100/0693)]\tLoss Ss: 0.015052\n","\tRotated_Epoch:45 [003/005 (0120/0693)]\tLoss Ss: 0.012190\n","\tRotated_Epoch:45 [003/005 (0140/0693)]\tLoss Ss: 0.013956\n","\tRotated_Epoch:45 [003/005 (0160/0693)]\tLoss Ss: 0.013695\n","\tRotated_Epoch:45 [003/005 (0180/0693)]\tLoss Ss: 0.012779\n","\tRotated_Epoch:45 [003/005 (0200/0693)]\tLoss Ss: 0.011730\n","\tRotated_Epoch:45 [003/005 (0220/0693)]\tLoss Ss: 0.011898\n","\tRotated_Epoch:45 [003/005 (0240/0693)]\tLoss Ss: 0.010492\n","\tRotated_Epoch:45 [003/005 (0260/0693)]\tLoss Ss: 0.011874\n","\tRotated_Epoch:45 [003/005 (0280/0693)]\tLoss Ss: 0.011899\n","\tRotated_Epoch:45 [003/005 (0300/0693)]\tLoss Ss: 0.014058\n","\tRotated_Epoch:45 [003/005 (0320/0693)]\tLoss Ss: 0.016032\n","\tRotated_Epoch:45 [003/005 (0340/0693)]\tLoss Ss: 0.017001\n","\tRotated_Epoch:45 [003/005 (0360/0693)]\tLoss Ss: 0.011475\n","\tRotated_Epoch:45 [003/005 (0380/0693)]\tLoss Ss: 0.011557\n","\tRotated_Epoch:45 [003/005 (0400/0693)]\tLoss Ss: 0.013511\n","\tRotated_Epoch:45 [003/005 (0420/0693)]\tLoss Ss: 0.013788\n","\tRotated_Epoch:45 [003/005 (0440/0693)]\tLoss Ss: 0.014904\n","\tRotated_Epoch:45 [003/005 (0460/0693)]\tLoss Ss: 0.012024\n","\tRotated_Epoch:45 [003/005 (0480/0693)]\tLoss Ss: 0.010983\n","\tRotated_Epoch:45 [003/005 (0500/0693)]\tLoss Ss: 0.015162\n","\tRotated_Epoch:45 [003/005 (0520/0693)]\tLoss Ss: 0.009032\n","\tRotated_Epoch:45 [003/005 (0540/0693)]\tLoss Ss: 0.012232\n","\tRotated_Epoch:45 [003/005 (0560/0693)]\tLoss Ss: 0.016655\n","\tRotated_Epoch:45 [003/005 (0580/0693)]\tLoss Ss: 0.012889\n","\tRotated_Epoch:45 [003/005 (0600/0693)]\tLoss Ss: 0.015119\n","\tRotated_Epoch:45 [003/005 (0620/0693)]\tLoss Ss: 0.013211\n","\tRotated_Epoch:45 [003/005 (0640/0693)]\tLoss Ss: 0.013553\n","\tRotated_Epoch:45 [003/005 (0660/0693)]\tLoss Ss: 0.012020\n","\tRotated_Epoch:45 [003/005 (0680/0693)]\tLoss Ss: 0.011142\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:45 [004/005 (0000/0755)]\tLoss Ss: 0.016364\n","\tRotated_Epoch:45 [004/005 (0020/0755)]\tLoss Ss: 0.011474\n","\tRotated_Epoch:45 [004/005 (0040/0755)]\tLoss Ss: 0.014911\n","\tRotated_Epoch:45 [004/005 (0060/0755)]\tLoss Ss: 0.016107\n","\tRotated_Epoch:45 [004/005 (0080/0755)]\tLoss Ss: 0.014243\n","\tRotated_Epoch:45 [004/005 (0100/0755)]\tLoss Ss: 0.015850\n","\tRotated_Epoch:45 [004/005 (0120/0755)]\tLoss Ss: 0.018480\n","\tRotated_Epoch:45 [004/005 (0140/0755)]\tLoss Ss: 0.015063\n","\tRotated_Epoch:45 [004/005 (0160/0755)]\tLoss Ss: 0.011291\n","\tRotated_Epoch:45 [004/005 (0180/0755)]\tLoss Ss: 0.019454\n","\tRotated_Epoch:45 [004/005 (0200/0755)]\tLoss Ss: 0.021224\n","\tRotated_Epoch:45 [004/005 (0220/0755)]\tLoss Ss: 0.015925\n","\tRotated_Epoch:45 [004/005 (0240/0755)]\tLoss Ss: 0.018268\n","\tRotated_Epoch:45 [004/005 (0260/0755)]\tLoss Ss: 0.016020\n","\tRotated_Epoch:45 [004/005 (0280/0755)]\tLoss Ss: 0.012125\n","\tRotated_Epoch:45 [004/005 (0300/0755)]\tLoss Ss: 0.014867\n","\tRotated_Epoch:45 [004/005 (0320/0755)]\tLoss Ss: 0.016167\n","\tRotated_Epoch:45 [004/005 (0340/0755)]\tLoss Ss: 0.015624\n","\tRotated_Epoch:45 [004/005 (0360/0755)]\tLoss Ss: 0.016244\n","\tRotated_Epoch:45 [004/005 (0380/0755)]\tLoss Ss: 0.011770\n","\tRotated_Epoch:45 [004/005 (0400/0755)]\tLoss Ss: 0.009822\n","\tRotated_Epoch:45 [004/005 (0420/0755)]\tLoss Ss: 0.012805\n","\tRotated_Epoch:45 [004/005 (0440/0755)]\tLoss Ss: 0.012716\n","\tRotated_Epoch:45 [004/005 (0460/0755)]\tLoss Ss: 0.008544\n","\tRotated_Epoch:45 [004/005 (0480/0755)]\tLoss Ss: 0.010071\n","\tRotated_Epoch:45 [004/005 (0500/0755)]\tLoss Ss: 0.007269\n","\tRotated_Epoch:45 [004/005 (0520/0755)]\tLoss Ss: 0.010868\n","\tRotated_Epoch:45 [004/005 (0540/0755)]\tLoss Ss: 0.009195\n","\tRotated_Epoch:45 [004/005 (0560/0755)]\tLoss Ss: 0.015148\n","\tRotated_Epoch:45 [004/005 (0580/0755)]\tLoss Ss: 0.011743\n","\tRotated_Epoch:45 [004/005 (0600/0755)]\tLoss Ss: 0.014769\n","\tRotated_Epoch:45 [004/005 (0620/0755)]\tLoss Ss: 0.013103\n","\tRotated_Epoch:45 [004/005 (0640/0755)]\tLoss Ss: 0.013552\n","\tRotated_Epoch:45 [004/005 (0660/0755)]\tLoss Ss: 0.018631\n","\tRotated_Epoch:45 [004/005 (0680/0755)]\tLoss Ss: 0.008888\n","\tRotated_Epoch:45 [004/005 (0700/0755)]\tLoss Ss: 0.011988\n","\tRotated_Epoch:45 [004/005 (0720/0755)]\tLoss Ss: 0.013967\n","\tRotated_Epoch:45 [004/005 (0740/0755)]\tLoss Ss: 0.009243\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:45 [005/005 (0000/0755)]\tLoss Ss: 0.316675\n","\tRotated_Epoch:45 [005/005 (0020/0755)]\tLoss Ss: 0.196778\n","\tRotated_Epoch:45 [005/005 (0040/0755)]\tLoss Ss: 0.140041\n","\tRotated_Epoch:45 [005/005 (0060/0755)]\tLoss Ss: 0.129307\n","\tRotated_Epoch:45 [005/005 (0080/0755)]\tLoss Ss: 0.119904\n","\tRotated_Epoch:45 [005/005 (0100/0755)]\tLoss Ss: 0.136230\n","\tRotated_Epoch:45 [005/005 (0120/0755)]\tLoss Ss: 0.117920\n","\tRotated_Epoch:45 [005/005 (0140/0755)]\tLoss Ss: 0.087079\n","\tRotated_Epoch:45 [005/005 (0160/0755)]\tLoss Ss: 0.080581\n","\tRotated_Epoch:45 [005/005 (0180/0755)]\tLoss Ss: 0.061809\n","\tRotated_Epoch:45 [005/005 (0200/0755)]\tLoss Ss: 0.053784\n","\tRotated_Epoch:45 [005/005 (0220/0755)]\tLoss Ss: 0.078033\n","\tRotated_Epoch:45 [005/005 (0240/0755)]\tLoss Ss: 0.070176\n","\tRotated_Epoch:45 [005/005 (0260/0755)]\tLoss Ss: 0.098381\n","\tRotated_Epoch:45 [005/005 (0280/0755)]\tLoss Ss: 0.040628\n","\tRotated_Epoch:45 [005/005 (0300/0755)]\tLoss Ss: 0.059050\n","\tRotated_Epoch:45 [005/005 (0320/0755)]\tLoss Ss: 0.052197\n","\tRotated_Epoch:45 [005/005 (0340/0755)]\tLoss Ss: 0.066531\n","\tRotated_Epoch:45 [005/005 (0360/0755)]\tLoss Ss: 0.056748\n","\tRotated_Epoch:45 [005/005 (0380/0755)]\tLoss Ss: 0.058877\n","\tRotated_Epoch:45 [005/005 (0400/0755)]\tLoss Ss: 0.080410\n","\tRotated_Epoch:45 [005/005 (0420/0755)]\tLoss Ss: 0.053207\n","\tRotated_Epoch:45 [005/005 (0440/0755)]\tLoss Ss: 0.054857\n","\tRotated_Epoch:45 [005/005 (0460/0755)]\tLoss Ss: 0.056042\n","\tRotated_Epoch:45 [005/005 (0480/0755)]\tLoss Ss: 0.065135\n","\tRotated_Epoch:45 [005/005 (0500/0755)]\tLoss Ss: 0.056646\n","\tRotated_Epoch:45 [005/005 (0520/0755)]\tLoss Ss: 0.046775\n","\tRotated_Epoch:45 [005/005 (0540/0755)]\tLoss Ss: 0.049663\n","\tRotated_Epoch:45 [005/005 (0560/0755)]\tLoss Ss: 0.037135\n","\tRotated_Epoch:45 [005/005 (0580/0755)]\tLoss Ss: 0.041607\n","\tRotated_Epoch:45 [005/005 (0600/0755)]\tLoss Ss: 0.044681\n","\tRotated_Epoch:45 [005/005 (0620/0755)]\tLoss Ss: 0.049285\n","\tRotated_Epoch:45 [005/005 (0640/0755)]\tLoss Ss: 0.033986\n","\tRotated_Epoch:45 [005/005 (0660/0755)]\tLoss Ss: 0.036001\n","\tRotated_Epoch:45 [005/005 (0680/0755)]\tLoss Ss: 0.040488\n","\tRotated_Epoch:45 [005/005 (0700/0755)]\tLoss Ss: 0.038203\n","\tRotated_Epoch:45 [005/005 (0720/0755)]\tLoss Ss: 0.039738\n","\tRotated_Epoch:45 [005/005 (0740/0755)]\tLoss Ss: 0.038534\n","new_1_6_d.nii\n","new_1_6_m.nii\n","new_1_7_d.nii\n","new_1_7_m.nii\n","new_2_5_d.nii\n","new_2_5_m.nii\n","new_2_6_d.nii\n","new_2_6_m.nii\n","new_2_7_d.nii\n","new_2_7_m.nii\n","new_3_d (2).nii\n","new_3_m (2).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (3).nii\n","new_3_m (3).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (7).nii\n","new_3_m (7).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (8).nii\n","new_3_m (8).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (9).nii\n","new_3_m (9).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_4_10_d_163526.nii\n","new_4_10_m_163526.nii.gz\n","new_4_11_d_084336.nii\n","new_4_11_m_084336.nii.gz\n","new_4_7_d_184422.nii\n","new_4_7_m_184422.nii.gz\n","new_4_8_d_143412.nii\n","new_4_8_m_143412.nii.gz\n","new_4_9_d_155347.nii\n","new_4_9_m_155347.nii.gz\n","\tEpoch: 45; Dice: 0.8752 +/- 0.0635; Loss: 8.0395\n","Begin Epoch 46\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tEpoch:46 [000/005 (0000/0588)]\tLoss Ss: 0.090677\n","\tEpoch:46 [000/005 (0020/0588)]\tLoss Ss: 0.040836\n","\tEpoch:46 [000/005 (0040/0588)]\tLoss Ss: 0.014021\n","\tEpoch:46 [000/005 (0060/0588)]\tLoss Ss: 0.019093\n","\tEpoch:46 [000/005 (0080/0588)]\tLoss Ss: 0.011288\n","\tEpoch:46 [000/005 (0100/0588)]\tLoss Ss: 0.009871\n","\tEpoch:46 [000/005 (0120/0588)]\tLoss Ss: 0.011358\n","\tEpoch:46 [000/005 (0140/0588)]\tLoss Ss: 0.009946\n","\tEpoch:46 [000/005 (0160/0588)]\tLoss Ss: 0.008299\n","\tEpoch:46 [000/005 (0180/0588)]\tLoss Ss: 0.012510\n","\tEpoch:46 [000/005 (0200/0588)]\tLoss Ss: 0.007612\n","\tEpoch:46 [000/005 (0220/0588)]\tLoss Ss: 0.008659\n","\tEpoch:46 [000/005 (0240/0588)]\tLoss Ss: 0.006543\n","\tEpoch:46 [000/005 (0260/0588)]\tLoss Ss: 0.005683\n","\tEpoch:46 [000/005 (0280/0588)]\tLoss Ss: 0.007647\n","\tEpoch:46 [000/005 (0300/0588)]\tLoss Ss: 0.005452\n","\tEpoch:46 [000/005 (0320/0588)]\tLoss Ss: 0.008632\n","\tEpoch:46 [000/005 (0340/0588)]\tLoss Ss: 0.006791\n","\tEpoch:46 [000/005 (0360/0588)]\tLoss Ss: 0.005704\n","\tEpoch:46 [000/005 (0380/0588)]\tLoss Ss: 0.004780\n","\tEpoch:46 [000/005 (0400/0588)]\tLoss Ss: 0.007763\n","\tEpoch:46 [000/005 (0420/0588)]\tLoss Ss: 0.006694\n","\tEpoch:46 [000/005 (0440/0588)]\tLoss Ss: 0.005478\n","\tEpoch:46 [000/005 (0460/0588)]\tLoss Ss: 0.005236\n","\tEpoch:46 [000/005 (0480/0588)]\tLoss Ss: 0.006095\n","\tEpoch:46 [000/005 (0500/0588)]\tLoss Ss: 0.006044\n","\tEpoch:46 [000/005 (0520/0588)]\tLoss Ss: 0.007047\n","\tEpoch:46 [000/005 (0540/0588)]\tLoss Ss: 0.004540\n","\tEpoch:46 [000/005 (0560/0588)]\tLoss Ss: 0.004994\n","\tEpoch:46 [000/005 (0580/0588)]\tLoss Ss: 0.004874\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:46 [001/005 (0000/0755)]\tLoss Ss: 0.028229\n","\tEpoch:46 [001/005 (0020/0755)]\tLoss Ss: 0.024156\n","\tEpoch:46 [001/005 (0040/0755)]\tLoss Ss: 0.028002\n","\tEpoch:46 [001/005 (0060/0755)]\tLoss Ss: 0.022802\n","\tEpoch:46 [001/005 (0080/0755)]\tLoss Ss: 0.021440\n","\tEpoch:46 [001/005 (0100/0755)]\tLoss Ss: 0.019788\n","\tEpoch:46 [001/005 (0120/0755)]\tLoss Ss: 0.020108\n","\tEpoch:46 [001/005 (0140/0755)]\tLoss Ss: 0.013863\n","\tEpoch:46 [001/005 (0160/0755)]\tLoss Ss: 0.014177\n","\tEpoch:46 [001/005 (0180/0755)]\tLoss Ss: 0.019267\n","\tEpoch:46 [001/005 (0200/0755)]\tLoss Ss: 0.018729\n","\tEpoch:46 [001/005 (0220/0755)]\tLoss Ss: 0.011309\n","\tEpoch:46 [001/005 (0240/0755)]\tLoss Ss: 0.015786\n","\tEpoch:46 [001/005 (0260/0755)]\tLoss Ss: 0.028026\n","\tEpoch:46 [001/005 (0280/0755)]\tLoss Ss: 0.016963\n","\tEpoch:46 [001/005 (0300/0755)]\tLoss Ss: 0.024422\n","\tEpoch:46 [001/005 (0320/0755)]\tLoss Ss: 0.013589\n","\tEpoch:46 [001/005 (0340/0755)]\tLoss Ss: 0.013723\n","\tEpoch:46 [001/005 (0360/0755)]\tLoss Ss: 0.030365\n","\tEpoch:46 [001/005 (0380/0755)]\tLoss Ss: 0.016541\n","\tEpoch:46 [001/005 (0400/0755)]\tLoss Ss: 0.017553\n","\tEpoch:46 [001/005 (0420/0755)]\tLoss Ss: 0.016277\n","\tEpoch:46 [001/005 (0440/0755)]\tLoss Ss: 0.017179\n","\tEpoch:46 [001/005 (0460/0755)]\tLoss Ss: 0.015980\n","\tEpoch:46 [001/005 (0480/0755)]\tLoss Ss: 0.012339\n","\tEpoch:46 [001/005 (0500/0755)]\tLoss Ss: 0.015759\n","\tEpoch:46 [001/005 (0520/0755)]\tLoss Ss: 0.013202\n","\tEpoch:46 [001/005 (0540/0755)]\tLoss Ss: 0.013238\n","\tEpoch:46 [001/005 (0560/0755)]\tLoss Ss: 0.014044\n","\tEpoch:46 [001/005 (0580/0755)]\tLoss Ss: 0.018959\n","\tEpoch:46 [001/005 (0600/0755)]\tLoss Ss: 0.010461\n","\tEpoch:46 [001/005 (0620/0755)]\tLoss Ss: 0.013703\n","\tEpoch:46 [001/005 (0640/0755)]\tLoss Ss: 0.014634\n","\tEpoch:46 [001/005 (0660/0755)]\tLoss Ss: 0.010585\n","\tEpoch:46 [001/005 (0680/0755)]\tLoss Ss: 0.013919\n","\tEpoch:46 [001/005 (0700/0755)]\tLoss Ss: 0.011376\n","\tEpoch:46 [001/005 (0720/0755)]\tLoss Ss: 0.011223\n","\tEpoch:46 [001/005 (0740/0755)]\tLoss Ss: 0.014135\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tEpoch:46 [002/005 (0000/0693)]\tLoss Ss: 0.011525\n","\tEpoch:46 [002/005 (0020/0693)]\tLoss Ss: 0.014282\n","\tEpoch:46 [002/005 (0040/0693)]\tLoss Ss: 0.014796\n","\tEpoch:46 [002/005 (0060/0693)]\tLoss Ss: 0.015777\n","\tEpoch:46 [002/005 (0080/0693)]\tLoss Ss: 0.012389\n","\tEpoch:46 [002/005 (0100/0693)]\tLoss Ss: 0.013813\n","\tEpoch:46 [002/005 (0120/0693)]\tLoss Ss: 0.010503\n","\tEpoch:46 [002/005 (0140/0693)]\tLoss Ss: 0.011794\n","\tEpoch:46 [002/005 (0160/0693)]\tLoss Ss: 0.012006\n","\tEpoch:46 [002/005 (0180/0693)]\tLoss Ss: 0.012203\n","\tEpoch:46 [002/005 (0200/0693)]\tLoss Ss: 0.010790\n","\tEpoch:46 [002/005 (0220/0693)]\tLoss Ss: 0.012030\n","\tEpoch:46 [002/005 (0240/0693)]\tLoss Ss: 0.010818\n","\tEpoch:46 [002/005 (0260/0693)]\tLoss Ss: 0.008878\n","\tEpoch:46 [002/005 (0280/0693)]\tLoss Ss: 0.010131\n","\tEpoch:46 [002/005 (0300/0693)]\tLoss Ss: 0.008654\n","\tEpoch:46 [002/005 (0320/0693)]\tLoss Ss: 0.013374\n","\tEpoch:46 [002/005 (0340/0693)]\tLoss Ss: 0.011933\n","\tEpoch:46 [002/005 (0360/0693)]\tLoss Ss: 0.011559\n","\tEpoch:46 [002/005 (0380/0693)]\tLoss Ss: 0.008389\n","\tEpoch:46 [002/005 (0400/0693)]\tLoss Ss: 0.010543\n","\tEpoch:46 [002/005 (0420/0693)]\tLoss Ss: 0.007488\n","\tEpoch:46 [002/005 (0440/0693)]\tLoss Ss: 0.009725\n","\tEpoch:46 [002/005 (0460/0693)]\tLoss Ss: 0.010424\n","\tEpoch:46 [002/005 (0480/0693)]\tLoss Ss: 0.013455\n","\tEpoch:46 [002/005 (0500/0693)]\tLoss Ss: 0.012421\n","\tEpoch:46 [002/005 (0520/0693)]\tLoss Ss: 0.011479\n","\tEpoch:46 [002/005 (0540/0693)]\tLoss Ss: 0.011731\n","\tEpoch:46 [002/005 (0560/0693)]\tLoss Ss: 0.010866\n","\tEpoch:46 [002/005 (0580/0693)]\tLoss Ss: 0.011642\n","\tEpoch:46 [002/005 (0600/0693)]\tLoss Ss: 0.011888\n","\tEpoch:46 [002/005 (0620/0693)]\tLoss Ss: 0.009040\n","\tEpoch:46 [002/005 (0640/0693)]\tLoss Ss: 0.011291\n","\tEpoch:46 [002/005 (0660/0693)]\tLoss Ss: 0.009683\n","\tEpoch:46 [002/005 (0680/0693)]\tLoss Ss: 0.014665\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:46 [003/005 (0000/0755)]\tLoss Ss: 0.017458\n","\tEpoch:46 [003/005 (0020/0755)]\tLoss Ss: 0.013914\n","\tEpoch:46 [003/005 (0040/0755)]\tLoss Ss: 0.012436\n","\tEpoch:46 [003/005 (0060/0755)]\tLoss Ss: 0.013377\n","\tEpoch:46 [003/005 (0080/0755)]\tLoss Ss: 0.017971\n","\tEpoch:46 [003/005 (0100/0755)]\tLoss Ss: 0.015114\n","\tEpoch:46 [003/005 (0120/0755)]\tLoss Ss: 0.006065\n","\tEpoch:46 [003/005 (0140/0755)]\tLoss Ss: 0.009177\n","\tEpoch:46 [003/005 (0160/0755)]\tLoss Ss: 0.007882\n","\tEpoch:46 [003/005 (0180/0755)]\tLoss Ss: 0.012599\n","\tEpoch:46 [003/005 (0200/0755)]\tLoss Ss: 0.009962\n","\tEpoch:46 [003/005 (0220/0755)]\tLoss Ss: 0.013120\n","\tEpoch:46 [003/005 (0240/0755)]\tLoss Ss: 0.010735\n","\tEpoch:46 [003/005 (0260/0755)]\tLoss Ss: 0.014500\n","\tEpoch:46 [003/005 (0280/0755)]\tLoss Ss: 0.013341\n","\tEpoch:46 [003/005 (0300/0755)]\tLoss Ss: 0.010429\n","\tEpoch:46 [003/005 (0320/0755)]\tLoss Ss: 0.016756\n","\tEpoch:46 [003/005 (0340/0755)]\tLoss Ss: 0.013136\n","\tEpoch:46 [003/005 (0360/0755)]\tLoss Ss: 0.009102\n","\tEpoch:46 [003/005 (0380/0755)]\tLoss Ss: 0.010006\n","\tEpoch:46 [003/005 (0400/0755)]\tLoss Ss: 0.010884\n","\tEpoch:46 [003/005 (0420/0755)]\tLoss Ss: 0.011025\n","\tEpoch:46 [003/005 (0440/0755)]\tLoss Ss: 0.006452\n","\tEpoch:46 [003/005 (0460/0755)]\tLoss Ss: 0.017866\n","\tEpoch:46 [003/005 (0480/0755)]\tLoss Ss: 0.009993\n","\tEpoch:46 [003/005 (0500/0755)]\tLoss Ss: 0.014496\n","\tEpoch:46 [003/005 (0520/0755)]\tLoss Ss: 0.008620\n","\tEpoch:46 [003/005 (0540/0755)]\tLoss Ss: 0.012226\n","\tEpoch:46 [003/005 (0560/0755)]\tLoss Ss: 0.010869\n","\tEpoch:46 [003/005 (0580/0755)]\tLoss Ss: 0.007990\n","\tEpoch:46 [003/005 (0600/0755)]\tLoss Ss: 0.011889\n","\tEpoch:46 [003/005 (0620/0755)]\tLoss Ss: 0.010380\n","\tEpoch:46 [003/005 (0640/0755)]\tLoss Ss: 0.012047\n","\tEpoch:46 [003/005 (0660/0755)]\tLoss Ss: 0.008149\n","\tEpoch:46 [003/005 (0680/0755)]\tLoss Ss: 0.014833\n","\tEpoch:46 [003/005 (0700/0755)]\tLoss Ss: 0.013522\n","\tEpoch:46 [003/005 (0720/0755)]\tLoss Ss: 0.006832\n","\tEpoch:46 [003/005 (0740/0755)]\tLoss Ss: 0.014615\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tEpoch:46 [004/005 (0000/0614)]\tLoss Ss: 0.005893\n","\tEpoch:46 [004/005 (0020/0614)]\tLoss Ss: 0.008824\n","\tEpoch:46 [004/005 (0040/0614)]\tLoss Ss: 0.009650\n","\tEpoch:46 [004/005 (0060/0614)]\tLoss Ss: 0.006225\n","\tEpoch:46 [004/005 (0080/0614)]\tLoss Ss: 0.005043\n","\tEpoch:46 [004/005 (0100/0614)]\tLoss Ss: 0.004679\n","\tEpoch:46 [004/005 (0120/0614)]\tLoss Ss: 0.006016\n","\tEpoch:46 [004/005 (0140/0614)]\tLoss Ss: 0.003892\n","\tEpoch:46 [004/005 (0160/0614)]\tLoss Ss: 0.005142\n","\tEpoch:46 [004/005 (0180/0614)]\tLoss Ss: 0.004144\n","\tEpoch:46 [004/005 (0200/0614)]\tLoss Ss: 0.004662\n","\tEpoch:46 [004/005 (0220/0614)]\tLoss Ss: 0.005430\n","\tEpoch:46 [004/005 (0240/0614)]\tLoss Ss: 0.004401\n","\tEpoch:46 [004/005 (0260/0614)]\tLoss Ss: 0.004933\n","\tEpoch:46 [004/005 (0280/0614)]\tLoss Ss: 0.008081\n","\tEpoch:46 [004/005 (0300/0614)]\tLoss Ss: 0.005318\n","\tEpoch:46 [004/005 (0320/0614)]\tLoss Ss: 0.004351\n","\tEpoch:46 [004/005 (0340/0614)]\tLoss Ss: 0.004130\n","\tEpoch:46 [004/005 (0360/0614)]\tLoss Ss: 0.004865\n","\tEpoch:46 [004/005 (0380/0614)]\tLoss Ss: 0.004366\n","\tEpoch:46 [004/005 (0400/0614)]\tLoss Ss: 0.006078\n","\tEpoch:46 [004/005 (0420/0614)]\tLoss Ss: 0.004353\n","\tEpoch:46 [004/005 (0440/0614)]\tLoss Ss: 0.004799\n","\tEpoch:46 [004/005 (0460/0614)]\tLoss Ss: 0.005347\n","\tEpoch:46 [004/005 (0480/0614)]\tLoss Ss: 0.003822\n","\tEpoch:46 [004/005 (0500/0614)]\tLoss Ss: 0.005479\n","\tEpoch:46 [004/005 (0520/0614)]\tLoss Ss: 0.004323\n","\tEpoch:46 [004/005 (0540/0614)]\tLoss Ss: 0.006466\n","\tEpoch:46 [004/005 (0560/0614)]\tLoss Ss: 0.004108\n","\tEpoch:46 [004/005 (0580/0614)]\tLoss Ss: 0.004066\n","\tEpoch:46 [004/005 (0600/0614)]\tLoss Ss: 0.003520\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tEpoch:46 [005/005 (0000/0693)]\tLoss Ss: 0.016632\n","\tEpoch:46 [005/005 (0020/0693)]\tLoss Ss: 0.012370\n","\tEpoch:46 [005/005 (0040/0693)]\tLoss Ss: 0.015926\n","\tEpoch:46 [005/005 (0060/0693)]\tLoss Ss: 0.019178\n","\tEpoch:46 [005/005 (0080/0693)]\tLoss Ss: 0.013624\n","\tEpoch:46 [005/005 (0100/0693)]\tLoss Ss: 0.016144\n","\tEpoch:46 [005/005 (0120/0693)]\tLoss Ss: 0.007951\n","\tEpoch:46 [005/005 (0140/0693)]\tLoss Ss: 0.015714\n","\tEpoch:46 [005/005 (0160/0693)]\tLoss Ss: 0.014306\n","\tEpoch:46 [005/005 (0180/0693)]\tLoss Ss: 0.012572\n","\tEpoch:46 [005/005 (0200/0693)]\tLoss Ss: 0.013547\n","\tEpoch:46 [005/005 (0220/0693)]\tLoss Ss: 0.012949\n","\tEpoch:46 [005/005 (0240/0693)]\tLoss Ss: 0.010905\n","\tEpoch:46 [005/005 (0260/0693)]\tLoss Ss: 0.011266\n","\tEpoch:46 [005/005 (0280/0693)]\tLoss Ss: 0.012249\n","\tEpoch:46 [005/005 (0300/0693)]\tLoss Ss: 0.012860\n","\tEpoch:46 [005/005 (0320/0693)]\tLoss Ss: 0.012454\n","\tEpoch:46 [005/005 (0340/0693)]\tLoss Ss: 0.009029\n","\tEpoch:46 [005/005 (0360/0693)]\tLoss Ss: 0.012794\n","\tEpoch:46 [005/005 (0380/0693)]\tLoss Ss: 0.015765\n","\tEpoch:46 [005/005 (0400/0693)]\tLoss Ss: 0.010345\n","\tEpoch:46 [005/005 (0420/0693)]\tLoss Ss: 0.009171\n","\tEpoch:46 [005/005 (0440/0693)]\tLoss Ss: 0.013095\n","\tEpoch:46 [005/005 (0460/0693)]\tLoss Ss: 0.011741\n","\tEpoch:46 [005/005 (0480/0693)]\tLoss Ss: 0.016299\n","\tEpoch:46 [005/005 (0500/0693)]\tLoss Ss: 0.009767\n","\tEpoch:46 [005/005 (0520/0693)]\tLoss Ss: 0.010875\n","\tEpoch:46 [005/005 (0540/0693)]\tLoss Ss: 0.014368\n","\tEpoch:46 [005/005 (0560/0693)]\tLoss Ss: 0.010109\n","\tEpoch:46 [005/005 (0580/0693)]\tLoss Ss: 0.012352\n","\tEpoch:46 [005/005 (0600/0693)]\tLoss Ss: 0.009723\n","\tEpoch:46 [005/005 (0620/0693)]\tLoss Ss: 0.011455\n","\tEpoch:46 [005/005 (0640/0693)]\tLoss Ss: 0.007748\n","\tEpoch:46 [005/005 (0660/0693)]\tLoss Ss: 0.012844\n","\tEpoch:46 [005/005 (0680/0693)]\tLoss Ss: 0.011512\n","Now train the rotated image\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tRotated_Epoch:46 [000/005 (0000/0588)]\tLoss Ss: 0.178796\n","\tRotated_Epoch:46 [000/005 (0020/0588)]\tLoss Ss: 0.100943\n","\tRotated_Epoch:46 [000/005 (0040/0588)]\tLoss Ss: 0.078509\n","\tRotated_Epoch:46 [000/005 (0060/0588)]\tLoss Ss: 0.096922\n","\tRotated_Epoch:46 [000/005 (0080/0588)]\tLoss Ss: 0.069309\n","\tRotated_Epoch:46 [000/005 (0100/0588)]\tLoss Ss: 0.069342\n","\tRotated_Epoch:46 [000/005 (0120/0588)]\tLoss Ss: 0.048039\n","\tRotated_Epoch:46 [000/005 (0140/0588)]\tLoss Ss: 0.073229\n","\tRotated_Epoch:46 [000/005 (0160/0588)]\tLoss Ss: 0.056441\n","\tRotated_Epoch:46 [000/005 (0180/0588)]\tLoss Ss: 0.099375\n","\tRotated_Epoch:46 [000/005 (0200/0588)]\tLoss Ss: 0.048098\n","\tRotated_Epoch:46 [000/005 (0220/0588)]\tLoss Ss: 0.059330\n","\tRotated_Epoch:46 [000/005 (0240/0588)]\tLoss Ss: 0.057187\n","\tRotated_Epoch:46 [000/005 (0260/0588)]\tLoss Ss: 0.068052\n","\tRotated_Epoch:46 [000/005 (0280/0588)]\tLoss Ss: 0.047225\n","\tRotated_Epoch:46 [000/005 (0300/0588)]\tLoss Ss: 0.067575\n","\tRotated_Epoch:46 [000/005 (0320/0588)]\tLoss Ss: 0.057883\n","\tRotated_Epoch:46 [000/005 (0340/0588)]\tLoss Ss: 0.082371\n","\tRotated_Epoch:46 [000/005 (0360/0588)]\tLoss Ss: 0.068393\n","\tRotated_Epoch:46 [000/005 (0380/0588)]\tLoss Ss: 0.065041\n","\tRotated_Epoch:46 [000/005 (0400/0588)]\tLoss Ss: 0.065391\n","\tRotated_Epoch:46 [000/005 (0420/0588)]\tLoss Ss: 0.056416\n","\tRotated_Epoch:46 [000/005 (0440/0588)]\tLoss Ss: 0.057880\n","\tRotated_Epoch:46 [000/005 (0460/0588)]\tLoss Ss: 0.047901\n","\tRotated_Epoch:46 [000/005 (0480/0588)]\tLoss Ss: 0.068646\n","\tRotated_Epoch:46 [000/005 (0500/0588)]\tLoss Ss: 0.058645\n","\tRotated_Epoch:46 [000/005 (0520/0588)]\tLoss Ss: 0.061112\n","\tRotated_Epoch:46 [000/005 (0540/0588)]\tLoss Ss: 0.046266\n","\tRotated_Epoch:46 [000/005 (0560/0588)]\tLoss Ss: 0.061655\n","\tRotated_Epoch:46 [000/005 (0580/0588)]\tLoss Ss: 0.073594\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:46 [001/005 (0000/0755)]\tLoss Ss: 0.052337\n","\tRotated_Epoch:46 [001/005 (0020/0755)]\tLoss Ss: 0.045079\n","\tRotated_Epoch:46 [001/005 (0040/0755)]\tLoss Ss: 0.045052\n","\tRotated_Epoch:46 [001/005 (0060/0755)]\tLoss Ss: 0.056738\n","\tRotated_Epoch:46 [001/005 (0080/0755)]\tLoss Ss: 0.044944\n","\tRotated_Epoch:46 [001/005 (0100/0755)]\tLoss Ss: 0.064120\n","\tRotated_Epoch:46 [001/005 (0120/0755)]\tLoss Ss: 0.037819\n","\tRotated_Epoch:46 [001/005 (0140/0755)]\tLoss Ss: 0.023742\n","\tRotated_Epoch:46 [001/005 (0160/0755)]\tLoss Ss: 0.041633\n","\tRotated_Epoch:46 [001/005 (0180/0755)]\tLoss Ss: 0.035053\n","\tRotated_Epoch:46 [001/005 (0200/0755)]\tLoss Ss: 0.040079\n","\tRotated_Epoch:46 [001/005 (0220/0755)]\tLoss Ss: 0.036064\n","\tRotated_Epoch:46 [001/005 (0240/0755)]\tLoss Ss: 0.049702\n","\tRotated_Epoch:46 [001/005 (0260/0755)]\tLoss Ss: 0.043362\n","\tRotated_Epoch:46 [001/005 (0280/0755)]\tLoss Ss: 0.036448\n","\tRotated_Epoch:46 [001/005 (0300/0755)]\tLoss Ss: 0.026521\n","\tRotated_Epoch:46 [001/005 (0320/0755)]\tLoss Ss: 0.030881\n","\tRotated_Epoch:46 [001/005 (0340/0755)]\tLoss Ss: 0.036623\n","\tRotated_Epoch:46 [001/005 (0360/0755)]\tLoss Ss: 0.031234\n","\tRotated_Epoch:46 [001/005 (0380/0755)]\tLoss Ss: 0.031868\n","\tRotated_Epoch:46 [001/005 (0400/0755)]\tLoss Ss: 0.016825\n","\tRotated_Epoch:46 [001/005 (0420/0755)]\tLoss Ss: 0.023946\n","\tRotated_Epoch:46 [001/005 (0440/0755)]\tLoss Ss: 0.035085\n","\tRotated_Epoch:46 [001/005 (0460/0755)]\tLoss Ss: 0.028999\n","\tRotated_Epoch:46 [001/005 (0480/0755)]\tLoss Ss: 0.038510\n","\tRotated_Epoch:46 [001/005 (0500/0755)]\tLoss Ss: 0.018167\n","\tRotated_Epoch:46 [001/005 (0520/0755)]\tLoss Ss: 0.019860\n","\tRotated_Epoch:46 [001/005 (0540/0755)]\tLoss Ss: 0.023247\n","\tRotated_Epoch:46 [001/005 (0560/0755)]\tLoss Ss: 0.024517\n","\tRotated_Epoch:46 [001/005 (0580/0755)]\tLoss Ss: 0.029762\n","\tRotated_Epoch:46 [001/005 (0600/0755)]\tLoss Ss: 0.015712\n","\tRotated_Epoch:46 [001/005 (0620/0755)]\tLoss Ss: 0.038408\n","\tRotated_Epoch:46 [001/005 (0640/0755)]\tLoss Ss: 0.023905\n","\tRotated_Epoch:46 [001/005 (0660/0755)]\tLoss Ss: 0.030033\n","\tRotated_Epoch:46 [001/005 (0680/0755)]\tLoss Ss: 0.021357\n","\tRotated_Epoch:46 [001/005 (0700/0755)]\tLoss Ss: 0.030629\n","\tRotated_Epoch:46 [001/005 (0720/0755)]\tLoss Ss: 0.028799\n","\tRotated_Epoch:46 [001/005 (0740/0755)]\tLoss Ss: 0.032532\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tRotated_Epoch:46 [002/005 (0000/0614)]\tLoss Ss: 0.076538\n","\tRotated_Epoch:46 [002/005 (0020/0614)]\tLoss Ss: 0.042982\n","\tRotated_Epoch:46 [002/005 (0040/0614)]\tLoss Ss: 0.023248\n","\tRotated_Epoch:46 [002/005 (0060/0614)]\tLoss Ss: 0.012233\n","\tRotated_Epoch:46 [002/005 (0080/0614)]\tLoss Ss: 0.012763\n","\tRotated_Epoch:46 [002/005 (0100/0614)]\tLoss Ss: 0.008276\n","\tRotated_Epoch:46 [002/005 (0120/0614)]\tLoss Ss: 0.011008\n","\tRotated_Epoch:46 [002/005 (0140/0614)]\tLoss Ss: 0.010654\n","\tRotated_Epoch:46 [002/005 (0160/0614)]\tLoss Ss: 0.010077\n","\tRotated_Epoch:46 [002/005 (0180/0614)]\tLoss Ss: 0.008106\n","\tRotated_Epoch:46 [002/005 (0200/0614)]\tLoss Ss: 0.008709\n","\tRotated_Epoch:46 [002/005 (0220/0614)]\tLoss Ss: 0.006636\n","\tRotated_Epoch:46 [002/005 (0240/0614)]\tLoss Ss: 0.005190\n","\tRotated_Epoch:46 [002/005 (0260/0614)]\tLoss Ss: 0.007099\n","\tRotated_Epoch:46 [002/005 (0280/0614)]\tLoss Ss: 0.009534\n","\tRotated_Epoch:46 [002/005 (0300/0614)]\tLoss Ss: 0.006793\n","\tRotated_Epoch:46 [002/005 (0320/0614)]\tLoss Ss: 0.006948\n","\tRotated_Epoch:46 [002/005 (0340/0614)]\tLoss Ss: 0.008128\n","\tRotated_Epoch:46 [002/005 (0360/0614)]\tLoss Ss: 0.006543\n","\tRotated_Epoch:46 [002/005 (0380/0614)]\tLoss Ss: 0.006384\n","\tRotated_Epoch:46 [002/005 (0400/0614)]\tLoss Ss: 0.011239\n","\tRotated_Epoch:46 [002/005 (0420/0614)]\tLoss Ss: 0.006660\n","\tRotated_Epoch:46 [002/005 (0440/0614)]\tLoss Ss: 0.007126\n","\tRotated_Epoch:46 [002/005 (0460/0614)]\tLoss Ss: 0.009176\n","\tRotated_Epoch:46 [002/005 (0480/0614)]\tLoss Ss: 0.007074\n","\tRotated_Epoch:46 [002/005 (0500/0614)]\tLoss Ss: 0.006155\n","\tRotated_Epoch:46 [002/005 (0520/0614)]\tLoss Ss: 0.009232\n","\tRotated_Epoch:46 [002/005 (0540/0614)]\tLoss Ss: 0.007705\n","\tRotated_Epoch:46 [002/005 (0560/0614)]\tLoss Ss: 0.007140\n","\tRotated_Epoch:46 [002/005 (0580/0614)]\tLoss Ss: 0.005861\n","\tRotated_Epoch:46 [002/005 (0600/0614)]\tLoss Ss: 0.007156\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tRotated_Epoch:46 [003/005 (0000/0693)]\tLoss Ss: 0.016451\n","\tRotated_Epoch:46 [003/005 (0020/0693)]\tLoss Ss: 0.014073\n","\tRotated_Epoch:46 [003/005 (0040/0693)]\tLoss Ss: 0.012097\n","\tRotated_Epoch:46 [003/005 (0060/0693)]\tLoss Ss: 0.015079\n","\tRotated_Epoch:46 [003/005 (0080/0693)]\tLoss Ss: 0.016390\n","\tRotated_Epoch:46 [003/005 (0100/0693)]\tLoss Ss: 0.019989\n","\tRotated_Epoch:46 [003/005 (0120/0693)]\tLoss Ss: 0.011167\n","\tRotated_Epoch:46 [003/005 (0140/0693)]\tLoss Ss: 0.015037\n","\tRotated_Epoch:46 [003/005 (0160/0693)]\tLoss Ss: 0.016231\n","\tRotated_Epoch:46 [003/005 (0180/0693)]\tLoss Ss: 0.014701\n","\tRotated_Epoch:46 [003/005 (0200/0693)]\tLoss Ss: 0.019446\n","\tRotated_Epoch:46 [003/005 (0220/0693)]\tLoss Ss: 0.012440\n","\tRotated_Epoch:46 [003/005 (0240/0693)]\tLoss Ss: 0.011179\n","\tRotated_Epoch:46 [003/005 (0260/0693)]\tLoss Ss: 0.015255\n","\tRotated_Epoch:46 [003/005 (0280/0693)]\tLoss Ss: 0.014758\n","\tRotated_Epoch:46 [003/005 (0300/0693)]\tLoss Ss: 0.015332\n","\tRotated_Epoch:46 [003/005 (0320/0693)]\tLoss Ss: 0.015873\n","\tRotated_Epoch:46 [003/005 (0340/0693)]\tLoss Ss: 0.012178\n","\tRotated_Epoch:46 [003/005 (0360/0693)]\tLoss Ss: 0.017751\n","\tRotated_Epoch:46 [003/005 (0380/0693)]\tLoss Ss: 0.018384\n","\tRotated_Epoch:46 [003/005 (0400/0693)]\tLoss Ss: 0.014989\n","\tRotated_Epoch:46 [003/005 (0420/0693)]\tLoss Ss: 0.011770\n","\tRotated_Epoch:46 [003/005 (0440/0693)]\tLoss Ss: 0.017398\n","\tRotated_Epoch:46 [003/005 (0460/0693)]\tLoss Ss: 0.014619\n","\tRotated_Epoch:46 [003/005 (0480/0693)]\tLoss Ss: 0.009173\n","\tRotated_Epoch:46 [003/005 (0500/0693)]\tLoss Ss: 0.009322\n","\tRotated_Epoch:46 [003/005 (0520/0693)]\tLoss Ss: 0.012960\n","\tRotated_Epoch:46 [003/005 (0540/0693)]\tLoss Ss: 0.010158\n","\tRotated_Epoch:46 [003/005 (0560/0693)]\tLoss Ss: 0.013632\n","\tRotated_Epoch:46 [003/005 (0580/0693)]\tLoss Ss: 0.009453\n","\tRotated_Epoch:46 [003/005 (0600/0693)]\tLoss Ss: 0.017976\n","\tRotated_Epoch:46 [003/005 (0620/0693)]\tLoss Ss: 0.012352\n","\tRotated_Epoch:46 [003/005 (0640/0693)]\tLoss Ss: 0.013104\n","\tRotated_Epoch:46 [003/005 (0660/0693)]\tLoss Ss: 0.012289\n","\tRotated_Epoch:46 [003/005 (0680/0693)]\tLoss Ss: 0.018034\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tRotated_Epoch:46 [004/005 (0000/0693)]\tLoss Ss: 0.016336\n","\tRotated_Epoch:46 [004/005 (0020/0693)]\tLoss Ss: 0.013736\n","\tRotated_Epoch:46 [004/005 (0040/0693)]\tLoss Ss: 0.015180\n","\tRotated_Epoch:46 [004/005 (0060/0693)]\tLoss Ss: 0.010082\n","\tRotated_Epoch:46 [004/005 (0080/0693)]\tLoss Ss: 0.013901\n","\tRotated_Epoch:46 [004/005 (0100/0693)]\tLoss Ss: 0.013231\n","\tRotated_Epoch:46 [004/005 (0120/0693)]\tLoss Ss: 0.013876\n","\tRotated_Epoch:46 [004/005 (0140/0693)]\tLoss Ss: 0.011364\n","\tRotated_Epoch:46 [004/005 (0160/0693)]\tLoss Ss: 0.010183\n","\tRotated_Epoch:46 [004/005 (0180/0693)]\tLoss Ss: 0.012683\n","\tRotated_Epoch:46 [004/005 (0200/0693)]\tLoss Ss: 0.013926\n","\tRotated_Epoch:46 [004/005 (0220/0693)]\tLoss Ss: 0.013782\n","\tRotated_Epoch:46 [004/005 (0240/0693)]\tLoss Ss: 0.012185\n","\tRotated_Epoch:46 [004/005 (0260/0693)]\tLoss Ss: 0.015359\n","\tRotated_Epoch:46 [004/005 (0280/0693)]\tLoss Ss: 0.015974\n","\tRotated_Epoch:46 [004/005 (0300/0693)]\tLoss Ss: 0.015753\n","\tRotated_Epoch:46 [004/005 (0320/0693)]\tLoss Ss: 0.010154\n","\tRotated_Epoch:46 [004/005 (0340/0693)]\tLoss Ss: 0.012360\n","\tRotated_Epoch:46 [004/005 (0360/0693)]\tLoss Ss: 0.012386\n","\tRotated_Epoch:46 [004/005 (0380/0693)]\tLoss Ss: 0.011226\n","\tRotated_Epoch:46 [004/005 (0400/0693)]\tLoss Ss: 0.010419\n","\tRotated_Epoch:46 [004/005 (0420/0693)]\tLoss Ss: 0.012711\n","\tRotated_Epoch:46 [004/005 (0440/0693)]\tLoss Ss: 0.013012\n","\tRotated_Epoch:46 [004/005 (0460/0693)]\tLoss Ss: 0.015893\n","\tRotated_Epoch:46 [004/005 (0480/0693)]\tLoss Ss: 0.009127\n","\tRotated_Epoch:46 [004/005 (0500/0693)]\tLoss Ss: 0.009574\n","\tRotated_Epoch:46 [004/005 (0520/0693)]\tLoss Ss: 0.013023\n","\tRotated_Epoch:46 [004/005 (0540/0693)]\tLoss Ss: 0.013993\n","\tRotated_Epoch:46 [004/005 (0560/0693)]\tLoss Ss: 0.012526\n","\tRotated_Epoch:46 [004/005 (0580/0693)]\tLoss Ss: 0.010258\n","\tRotated_Epoch:46 [004/005 (0600/0693)]\tLoss Ss: 0.008716\n","\tRotated_Epoch:46 [004/005 (0620/0693)]\tLoss Ss: 0.010312\n","\tRotated_Epoch:46 [004/005 (0640/0693)]\tLoss Ss: 0.010356\n","\tRotated_Epoch:46 [004/005 (0660/0693)]\tLoss Ss: 0.018541\n","\tRotated_Epoch:46 [004/005 (0680/0693)]\tLoss Ss: 0.009654\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:46 [005/005 (0000/0755)]\tLoss Ss: 0.058194\n","\tRotated_Epoch:46 [005/005 (0020/0755)]\tLoss Ss: 0.049705\n","\tRotated_Epoch:46 [005/005 (0040/0755)]\tLoss Ss: 0.051842\n","\tRotated_Epoch:46 [005/005 (0060/0755)]\tLoss Ss: 0.030935\n","\tRotated_Epoch:46 [005/005 (0080/0755)]\tLoss Ss: 0.030745\n","\tRotated_Epoch:46 [005/005 (0100/0755)]\tLoss Ss: 0.016800\n","\tRotated_Epoch:46 [005/005 (0120/0755)]\tLoss Ss: 0.017376\n","\tRotated_Epoch:46 [005/005 (0140/0755)]\tLoss Ss: 0.020682\n","\tRotated_Epoch:46 [005/005 (0160/0755)]\tLoss Ss: 0.019567\n","\tRotated_Epoch:46 [005/005 (0180/0755)]\tLoss Ss: 0.019839\n","\tRotated_Epoch:46 [005/005 (0200/0755)]\tLoss Ss: 0.013354\n","\tRotated_Epoch:46 [005/005 (0220/0755)]\tLoss Ss: 0.015916\n","\tRotated_Epoch:46 [005/005 (0240/0755)]\tLoss Ss: 0.018915\n","\tRotated_Epoch:46 [005/005 (0260/0755)]\tLoss Ss: 0.014936\n","\tRotated_Epoch:46 [005/005 (0280/0755)]\tLoss Ss: 0.012263\n","\tRotated_Epoch:46 [005/005 (0300/0755)]\tLoss Ss: 0.013526\n","\tRotated_Epoch:46 [005/005 (0320/0755)]\tLoss Ss: 0.011990\n","\tRotated_Epoch:46 [005/005 (0340/0755)]\tLoss Ss: 0.011191\n","\tRotated_Epoch:46 [005/005 (0360/0755)]\tLoss Ss: 0.016843\n","\tRotated_Epoch:46 [005/005 (0380/0755)]\tLoss Ss: 0.027502\n","\tRotated_Epoch:46 [005/005 (0400/0755)]\tLoss Ss: 0.014558\n","\tRotated_Epoch:46 [005/005 (0420/0755)]\tLoss Ss: 0.009913\n","\tRotated_Epoch:46 [005/005 (0440/0755)]\tLoss Ss: 0.015944\n","\tRotated_Epoch:46 [005/005 (0460/0755)]\tLoss Ss: 0.013888\n","\tRotated_Epoch:46 [005/005 (0480/0755)]\tLoss Ss: 0.017204\n","\tRotated_Epoch:46 [005/005 (0500/0755)]\tLoss Ss: 0.015873\n","\tRotated_Epoch:46 [005/005 (0520/0755)]\tLoss Ss: 0.012484\n","\tRotated_Epoch:46 [005/005 (0540/0755)]\tLoss Ss: 0.011855\n","\tRotated_Epoch:46 [005/005 (0560/0755)]\tLoss Ss: 0.016101\n","\tRotated_Epoch:46 [005/005 (0580/0755)]\tLoss Ss: 0.014377\n","\tRotated_Epoch:46 [005/005 (0600/0755)]\tLoss Ss: 0.010512\n","\tRotated_Epoch:46 [005/005 (0620/0755)]\tLoss Ss: 0.010288\n","\tRotated_Epoch:46 [005/005 (0640/0755)]\tLoss Ss: 0.011509\n","\tRotated_Epoch:46 [005/005 (0660/0755)]\tLoss Ss: 0.011967\n","\tRotated_Epoch:46 [005/005 (0680/0755)]\tLoss Ss: 0.014745\n","\tRotated_Epoch:46 [005/005 (0700/0755)]\tLoss Ss: 0.015209\n","\tRotated_Epoch:46 [005/005 (0720/0755)]\tLoss Ss: 0.009953\n","\tRotated_Epoch:46 [005/005 (0740/0755)]\tLoss Ss: 0.012352\n","new_1_6_d.nii\n","new_1_6_m.nii\n","new_1_7_d.nii\n","new_1_7_m.nii\n","new_2_5_d.nii\n","new_2_5_m.nii\n","new_2_6_d.nii\n","new_2_6_m.nii\n","new_2_7_d.nii\n","new_2_7_m.nii\n","new_3_d (2).nii\n","new_3_m (2).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (3).nii\n","new_3_m (3).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (7).nii\n","new_3_m (7).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (8).nii\n","new_3_m (8).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (9).nii\n","new_3_m (9).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_4_10_d_163526.nii\n","new_4_10_m_163526.nii.gz\n","new_4_11_d_084336.nii\n","new_4_11_m_084336.nii.gz\n","new_4_7_d_184422.nii\n","new_4_7_m_184422.nii.gz\n","new_4_8_d_143412.nii\n","new_4_8_m_143412.nii.gz\n","new_4_9_d_155347.nii\n","new_4_9_m_155347.nii.gz\n","\tEpoch: 46; Dice: 0.9594 +/- 0.0107; Loss: 7.8690\n","Begin Epoch 47\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:47 [000/005 (0000/0755)]\tLoss Ss: 0.023227\n","\tEpoch:47 [000/005 (0020/0755)]\tLoss Ss: 0.019359\n","\tEpoch:47 [000/005 (0040/0755)]\tLoss Ss: 0.017026\n","\tEpoch:47 [000/005 (0060/0755)]\tLoss Ss: 0.022373\n","\tEpoch:47 [000/005 (0080/0755)]\tLoss Ss: 0.018905\n","\tEpoch:47 [000/005 (0100/0755)]\tLoss Ss: 0.019364\n","\tEpoch:47 [000/005 (0120/0755)]\tLoss Ss: 0.011463\n","\tEpoch:47 [000/005 (0140/0755)]\tLoss Ss: 0.016042\n","\tEpoch:47 [000/005 (0160/0755)]\tLoss Ss: 0.011233\n","\tEpoch:47 [000/005 (0180/0755)]\tLoss Ss: 0.012563\n","\tEpoch:47 [000/005 (0200/0755)]\tLoss Ss: 0.016224\n","\tEpoch:47 [000/005 (0220/0755)]\tLoss Ss: 0.018958\n","\tEpoch:47 [000/005 (0240/0755)]\tLoss Ss: 0.008198\n","\tEpoch:47 [000/005 (0260/0755)]\tLoss Ss: 0.022233\n","\tEpoch:47 [000/005 (0280/0755)]\tLoss Ss: 0.013467\n","\tEpoch:47 [000/005 (0300/0755)]\tLoss Ss: 0.013369\n","\tEpoch:47 [000/005 (0320/0755)]\tLoss Ss: 0.010094\n","\tEpoch:47 [000/005 (0340/0755)]\tLoss Ss: 0.014057\n","\tEpoch:47 [000/005 (0360/0755)]\tLoss Ss: 0.008811\n","\tEpoch:47 [000/005 (0380/0755)]\tLoss Ss: 0.018038\n","\tEpoch:47 [000/005 (0400/0755)]\tLoss Ss: 0.013655\n","\tEpoch:47 [000/005 (0420/0755)]\tLoss Ss: 0.016364\n","\tEpoch:47 [000/005 (0440/0755)]\tLoss Ss: 0.013046\n","\tEpoch:47 [000/005 (0460/0755)]\tLoss Ss: 0.016485\n","\tEpoch:47 [000/005 (0480/0755)]\tLoss Ss: 0.006678\n","\tEpoch:47 [000/005 (0500/0755)]\tLoss Ss: 0.014297\n","\tEpoch:47 [000/005 (0520/0755)]\tLoss Ss: 0.013978\n","\tEpoch:47 [000/005 (0540/0755)]\tLoss Ss: 0.014816\n","\tEpoch:47 [000/005 (0560/0755)]\tLoss Ss: 0.009753\n","\tEpoch:47 [000/005 (0580/0755)]\tLoss Ss: 0.010710\n","\tEpoch:47 [000/005 (0600/0755)]\tLoss Ss: 0.010143\n","\tEpoch:47 [000/005 (0620/0755)]\tLoss Ss: 0.012783\n","\tEpoch:47 [000/005 (0640/0755)]\tLoss Ss: 0.012236\n","\tEpoch:47 [000/005 (0660/0755)]\tLoss Ss: 0.018981\n","\tEpoch:47 [000/005 (0680/0755)]\tLoss Ss: 0.011485\n","\tEpoch:47 [000/005 (0700/0755)]\tLoss Ss: 0.017017\n","\tEpoch:47 [000/005 (0720/0755)]\tLoss Ss: 0.013699\n","\tEpoch:47 [000/005 (0740/0755)]\tLoss Ss: 0.010031\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tEpoch:47 [001/005 (0000/0693)]\tLoss Ss: 0.014608\n","\tEpoch:47 [001/005 (0020/0693)]\tLoss Ss: 0.012416\n","\tEpoch:47 [001/005 (0040/0693)]\tLoss Ss: 0.015726\n","\tEpoch:47 [001/005 (0060/0693)]\tLoss Ss: 0.011549\n","\tEpoch:47 [001/005 (0080/0693)]\tLoss Ss: 0.007791\n","\tEpoch:47 [001/005 (0100/0693)]\tLoss Ss: 0.009119\n","\tEpoch:47 [001/005 (0120/0693)]\tLoss Ss: 0.014945\n","\tEpoch:47 [001/005 (0140/0693)]\tLoss Ss: 0.012055\n","\tEpoch:47 [001/005 (0160/0693)]\tLoss Ss: 0.008359\n","\tEpoch:47 [001/005 (0180/0693)]\tLoss Ss: 0.013727\n","\tEpoch:47 [001/005 (0200/0693)]\tLoss Ss: 0.011122\n","\tEpoch:47 [001/005 (0220/0693)]\tLoss Ss: 0.011989\n","\tEpoch:47 [001/005 (0240/0693)]\tLoss Ss: 0.018915\n","\tEpoch:47 [001/005 (0260/0693)]\tLoss Ss: 0.010052\n","\tEpoch:47 [001/005 (0280/0693)]\tLoss Ss: 0.009818\n","\tEpoch:47 [001/005 (0300/0693)]\tLoss Ss: 0.008130\n","\tEpoch:47 [001/005 (0320/0693)]\tLoss Ss: 0.013938\n","\tEpoch:47 [001/005 (0340/0693)]\tLoss Ss: 0.009818\n","\tEpoch:47 [001/005 (0360/0693)]\tLoss Ss: 0.015297\n","\tEpoch:47 [001/005 (0380/0693)]\tLoss Ss: 0.014054\n","\tEpoch:47 [001/005 (0400/0693)]\tLoss Ss: 0.010840\n","\tEpoch:47 [001/005 (0420/0693)]\tLoss Ss: 0.012549\n","\tEpoch:47 [001/005 (0440/0693)]\tLoss Ss: 0.010079\n","\tEpoch:47 [001/005 (0460/0693)]\tLoss Ss: 0.014983\n","\tEpoch:47 [001/005 (0480/0693)]\tLoss Ss: 0.015482\n","\tEpoch:47 [001/005 (0500/0693)]\tLoss Ss: 0.013191\n","\tEpoch:47 [001/005 (0520/0693)]\tLoss Ss: 0.010024\n","\tEpoch:47 [001/005 (0540/0693)]\tLoss Ss: 0.012313\n","\tEpoch:47 [001/005 (0560/0693)]\tLoss Ss: 0.012555\n","\tEpoch:47 [001/005 (0580/0693)]\tLoss Ss: 0.013632\n","\tEpoch:47 [001/005 (0600/0693)]\tLoss Ss: 0.012443\n","\tEpoch:47 [001/005 (0620/0693)]\tLoss Ss: 0.008773\n","\tEpoch:47 [001/005 (0640/0693)]\tLoss Ss: 0.011683\n","\tEpoch:47 [001/005 (0660/0693)]\tLoss Ss: 0.008051\n","\tEpoch:47 [001/005 (0680/0693)]\tLoss Ss: 0.016820\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tEpoch:47 [002/005 (0000/0588)]\tLoss Ss: 0.005765\n","\tEpoch:47 [002/005 (0020/0588)]\tLoss Ss: 0.006580\n","\tEpoch:47 [002/005 (0040/0588)]\tLoss Ss: 0.007235\n","\tEpoch:47 [002/005 (0060/0588)]\tLoss Ss: 0.005656\n","\tEpoch:47 [002/005 (0080/0588)]\tLoss Ss: 0.006030\n","\tEpoch:47 [002/005 (0100/0588)]\tLoss Ss: 0.003580\n","\tEpoch:47 [002/005 (0120/0588)]\tLoss Ss: 0.005045\n","\tEpoch:47 [002/005 (0140/0588)]\tLoss Ss: 0.002520\n","\tEpoch:47 [002/005 (0160/0588)]\tLoss Ss: 0.002986\n","\tEpoch:47 [002/005 (0180/0588)]\tLoss Ss: 0.004451\n","\tEpoch:47 [002/005 (0200/0588)]\tLoss Ss: 0.002361\n","\tEpoch:47 [002/005 (0220/0588)]\tLoss Ss: 0.005494\n","\tEpoch:47 [002/005 (0240/0588)]\tLoss Ss: 0.005733\n","\tEpoch:47 [002/005 (0260/0588)]\tLoss Ss: 0.004415\n","\tEpoch:47 [002/005 (0280/0588)]\tLoss Ss: 0.004381\n","\tEpoch:47 [002/005 (0300/0588)]\tLoss Ss: 0.004146\n","\tEpoch:47 [002/005 (0320/0588)]\tLoss Ss: 0.003758\n","\tEpoch:47 [002/005 (0340/0588)]\tLoss Ss: 0.004224\n","\tEpoch:47 [002/005 (0360/0588)]\tLoss Ss: 0.001875\n","\tEpoch:47 [002/005 (0380/0588)]\tLoss Ss: 0.003139\n","\tEpoch:47 [002/005 (0400/0588)]\tLoss Ss: 0.003860\n","\tEpoch:47 [002/005 (0420/0588)]\tLoss Ss: 0.004494\n","\tEpoch:47 [002/005 (0440/0588)]\tLoss Ss: 0.003125\n","\tEpoch:47 [002/005 (0460/0588)]\tLoss Ss: 0.004792\n","\tEpoch:47 [002/005 (0480/0588)]\tLoss Ss: 0.002556\n","\tEpoch:47 [002/005 (0500/0588)]\tLoss Ss: 0.004314\n","\tEpoch:47 [002/005 (0520/0588)]\tLoss Ss: 0.003330\n","\tEpoch:47 [002/005 (0540/0588)]\tLoss Ss: 0.004132\n","\tEpoch:47 [002/005 (0560/0588)]\tLoss Ss: 0.003619\n","\tEpoch:47 [002/005 (0580/0588)]\tLoss Ss: 0.003680\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:47 [003/005 (0000/0755)]\tLoss Ss: 0.008737\n","\tEpoch:47 [003/005 (0020/0755)]\tLoss Ss: 0.018574\n","\tEpoch:47 [003/005 (0040/0755)]\tLoss Ss: 0.007759\n","\tEpoch:47 [003/005 (0060/0755)]\tLoss Ss: 0.012231\n","\tEpoch:47 [003/005 (0080/0755)]\tLoss Ss: 0.010506\n","\tEpoch:47 [003/005 (0100/0755)]\tLoss Ss: 0.011844\n","\tEpoch:47 [003/005 (0120/0755)]\tLoss Ss: 0.007557\n","\tEpoch:47 [003/005 (0140/0755)]\tLoss Ss: 0.012034\n","\tEpoch:47 [003/005 (0160/0755)]\tLoss Ss: 0.011886\n","\tEpoch:47 [003/005 (0180/0755)]\tLoss Ss: 0.013840\n","\tEpoch:47 [003/005 (0200/0755)]\tLoss Ss: 0.011819\n","\tEpoch:47 [003/005 (0220/0755)]\tLoss Ss: 0.008775\n","\tEpoch:47 [003/005 (0240/0755)]\tLoss Ss: 0.011009\n","\tEpoch:47 [003/005 (0260/0755)]\tLoss Ss: 0.009800\n","\tEpoch:47 [003/005 (0280/0755)]\tLoss Ss: 0.011364\n","\tEpoch:47 [003/005 (0300/0755)]\tLoss Ss: 0.014480\n","\tEpoch:47 [003/005 (0320/0755)]\tLoss Ss: 0.011349\n","\tEpoch:47 [003/005 (0340/0755)]\tLoss Ss: 0.012584\n","\tEpoch:47 [003/005 (0360/0755)]\tLoss Ss: 0.007783\n","\tEpoch:47 [003/005 (0380/0755)]\tLoss Ss: 0.009932\n","\tEpoch:47 [003/005 (0400/0755)]\tLoss Ss: 0.015114\n","\tEpoch:47 [003/005 (0420/0755)]\tLoss Ss: 0.009305\n","\tEpoch:47 [003/005 (0440/0755)]\tLoss Ss: 0.011662\n","\tEpoch:47 [003/005 (0460/0755)]\tLoss Ss: 0.007014\n","\tEpoch:47 [003/005 (0480/0755)]\tLoss Ss: 0.015522\n","\tEpoch:47 [003/005 (0500/0755)]\tLoss Ss: 0.007379\n","\tEpoch:47 [003/005 (0520/0755)]\tLoss Ss: 0.012164\n","\tEpoch:47 [003/005 (0540/0755)]\tLoss Ss: 0.010800\n","\tEpoch:47 [003/005 (0560/0755)]\tLoss Ss: 0.011979\n","\tEpoch:47 [003/005 (0580/0755)]\tLoss Ss: 0.007481\n","\tEpoch:47 [003/005 (0600/0755)]\tLoss Ss: 0.013320\n","\tEpoch:47 [003/005 (0620/0755)]\tLoss Ss: 0.011892\n","\tEpoch:47 [003/005 (0640/0755)]\tLoss Ss: 0.010994\n","\tEpoch:47 [003/005 (0660/0755)]\tLoss Ss: 0.013310\n","\tEpoch:47 [003/005 (0680/0755)]\tLoss Ss: 0.012534\n","\tEpoch:47 [003/005 (0700/0755)]\tLoss Ss: 0.008910\n","\tEpoch:47 [003/005 (0720/0755)]\tLoss Ss: 0.013244\n","\tEpoch:47 [003/005 (0740/0755)]\tLoss Ss: 0.006675\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tEpoch:47 [004/005 (0000/0693)]\tLoss Ss: 0.010850\n","\tEpoch:47 [004/005 (0020/0693)]\tLoss Ss: 0.007074\n","\tEpoch:47 [004/005 (0040/0693)]\tLoss Ss: 0.006019\n","\tEpoch:47 [004/005 (0060/0693)]\tLoss Ss: 0.011989\n","\tEpoch:47 [004/005 (0080/0693)]\tLoss Ss: 0.012666\n","\tEpoch:47 [004/005 (0100/0693)]\tLoss Ss: 0.012503\n","\tEpoch:47 [004/005 (0120/0693)]\tLoss Ss: 0.009767\n","\tEpoch:47 [004/005 (0140/0693)]\tLoss Ss: 0.010187\n","\tEpoch:47 [004/005 (0160/0693)]\tLoss Ss: 0.011807\n","\tEpoch:47 [004/005 (0180/0693)]\tLoss Ss: 0.010944\n","\tEpoch:47 [004/005 (0200/0693)]\tLoss Ss: 0.015112\n","\tEpoch:47 [004/005 (0220/0693)]\tLoss Ss: 0.011762\n","\tEpoch:47 [004/005 (0240/0693)]\tLoss Ss: 0.010282\n","\tEpoch:47 [004/005 (0260/0693)]\tLoss Ss: 0.011751\n","\tEpoch:47 [004/005 (0280/0693)]\tLoss Ss: 0.010959\n","\tEpoch:47 [004/005 (0300/0693)]\tLoss Ss: 0.011148\n","\tEpoch:47 [004/005 (0320/0693)]\tLoss Ss: 0.009662\n","\tEpoch:47 [004/005 (0340/0693)]\tLoss Ss: 0.010334\n","\tEpoch:47 [004/005 (0360/0693)]\tLoss Ss: 0.009529\n","\tEpoch:47 [004/005 (0380/0693)]\tLoss Ss: 0.008295\n","\tEpoch:47 [004/005 (0400/0693)]\tLoss Ss: 0.011530\n","\tEpoch:47 [004/005 (0420/0693)]\tLoss Ss: 0.008296\n","\tEpoch:47 [004/005 (0440/0693)]\tLoss Ss: 0.010293\n","\tEpoch:47 [004/005 (0460/0693)]\tLoss Ss: 0.009453\n","\tEpoch:47 [004/005 (0480/0693)]\tLoss Ss: 0.010232\n","\tEpoch:47 [004/005 (0500/0693)]\tLoss Ss: 0.012525\n","\tEpoch:47 [004/005 (0520/0693)]\tLoss Ss: 0.008173\n","\tEpoch:47 [004/005 (0540/0693)]\tLoss Ss: 0.011109\n","\tEpoch:47 [004/005 (0560/0693)]\tLoss Ss: 0.009286\n","\tEpoch:47 [004/005 (0580/0693)]\tLoss Ss: 0.013522\n","\tEpoch:47 [004/005 (0600/0693)]\tLoss Ss: 0.009155\n","\tEpoch:47 [004/005 (0620/0693)]\tLoss Ss: 0.007377\n","\tEpoch:47 [004/005 (0640/0693)]\tLoss Ss: 0.011274\n","\tEpoch:47 [004/005 (0660/0693)]\tLoss Ss: 0.008415\n","\tEpoch:47 [004/005 (0680/0693)]\tLoss Ss: 0.006335\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tEpoch:47 [005/005 (0000/0614)]\tLoss Ss: 0.004446\n","\tEpoch:47 [005/005 (0020/0614)]\tLoss Ss: 0.007044\n","\tEpoch:47 [005/005 (0040/0614)]\tLoss Ss: 0.005874\n","\tEpoch:47 [005/005 (0060/0614)]\tLoss Ss: 0.003889\n","\tEpoch:47 [005/005 (0080/0614)]\tLoss Ss: 0.003865\n","\tEpoch:47 [005/005 (0100/0614)]\tLoss Ss: 0.004996\n","\tEpoch:47 [005/005 (0120/0614)]\tLoss Ss: 0.004271\n","\tEpoch:47 [005/005 (0140/0614)]\tLoss Ss: 0.005426\n","\tEpoch:47 [005/005 (0160/0614)]\tLoss Ss: 0.004717\n","\tEpoch:47 [005/005 (0180/0614)]\tLoss Ss: 0.002676\n","\tEpoch:47 [005/005 (0200/0614)]\tLoss Ss: 0.004031\n","\tEpoch:47 [005/005 (0220/0614)]\tLoss Ss: 0.003641\n","\tEpoch:47 [005/005 (0240/0614)]\tLoss Ss: 0.003775\n","\tEpoch:47 [005/005 (0260/0614)]\tLoss Ss: 0.005106\n","\tEpoch:47 [005/005 (0280/0614)]\tLoss Ss: 0.003620\n","\tEpoch:47 [005/005 (0300/0614)]\tLoss Ss: 0.003637\n","\tEpoch:47 [005/005 (0320/0614)]\tLoss Ss: 0.006706\n","\tEpoch:47 [005/005 (0340/0614)]\tLoss Ss: 0.003782\n","\tEpoch:47 [005/005 (0360/0614)]\tLoss Ss: 0.005442\n","\tEpoch:47 [005/005 (0380/0614)]\tLoss Ss: 0.004383\n","\tEpoch:47 [005/005 (0400/0614)]\tLoss Ss: 0.005234\n","\tEpoch:47 [005/005 (0420/0614)]\tLoss Ss: 0.005970\n","\tEpoch:47 [005/005 (0440/0614)]\tLoss Ss: 0.004521\n","\tEpoch:47 [005/005 (0460/0614)]\tLoss Ss: 0.005372\n","\tEpoch:47 [005/005 (0480/0614)]\tLoss Ss: 0.005915\n","\tEpoch:47 [005/005 (0500/0614)]\tLoss Ss: 0.006301\n","\tEpoch:47 [005/005 (0520/0614)]\tLoss Ss: 0.003527\n","\tEpoch:47 [005/005 (0540/0614)]\tLoss Ss: 0.003837\n","\tEpoch:47 [005/005 (0560/0614)]\tLoss Ss: 0.004976\n","\tEpoch:47 [005/005 (0580/0614)]\tLoss Ss: 0.004934\n","\tEpoch:47 [005/005 (0600/0614)]\tLoss Ss: 0.005586\n","Now train the rotated image\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tRotated_Epoch:47 [000/005 (0000/0693)]\tLoss Ss: 0.017207\n","\tRotated_Epoch:47 [000/005 (0020/0693)]\tLoss Ss: 0.015025\n","\tRotated_Epoch:47 [000/005 (0040/0693)]\tLoss Ss: 0.011483\n","\tRotated_Epoch:47 [000/005 (0060/0693)]\tLoss Ss: 0.015798\n","\tRotated_Epoch:47 [000/005 (0080/0693)]\tLoss Ss: 0.008599\n","\tRotated_Epoch:47 [000/005 (0100/0693)]\tLoss Ss: 0.013545\n","\tRotated_Epoch:47 [000/005 (0120/0693)]\tLoss Ss: 0.012861\n","\tRotated_Epoch:47 [000/005 (0140/0693)]\tLoss Ss: 0.008593\n","\tRotated_Epoch:47 [000/005 (0160/0693)]\tLoss Ss: 0.010800\n","\tRotated_Epoch:47 [000/005 (0180/0693)]\tLoss Ss: 0.008978\n","\tRotated_Epoch:47 [000/005 (0200/0693)]\tLoss Ss: 0.015085\n","\tRotated_Epoch:47 [000/005 (0220/0693)]\tLoss Ss: 0.007752\n","\tRotated_Epoch:47 [000/005 (0240/0693)]\tLoss Ss: 0.017431\n","\tRotated_Epoch:47 [000/005 (0260/0693)]\tLoss Ss: 0.010811\n","\tRotated_Epoch:47 [000/005 (0280/0693)]\tLoss Ss: 0.012288\n","\tRotated_Epoch:47 [000/005 (0300/0693)]\tLoss Ss: 0.012210\n","\tRotated_Epoch:47 [000/005 (0320/0693)]\tLoss Ss: 0.014485\n","\tRotated_Epoch:47 [000/005 (0340/0693)]\tLoss Ss: 0.009694\n","\tRotated_Epoch:47 [000/005 (0360/0693)]\tLoss Ss: 0.015632\n","\tRotated_Epoch:47 [000/005 (0380/0693)]\tLoss Ss: 0.009568\n","\tRotated_Epoch:47 [000/005 (0400/0693)]\tLoss Ss: 0.014065\n","\tRotated_Epoch:47 [000/005 (0420/0693)]\tLoss Ss: 0.011637\n","\tRotated_Epoch:47 [000/005 (0440/0693)]\tLoss Ss: 0.012573\n","\tRotated_Epoch:47 [000/005 (0460/0693)]\tLoss Ss: 0.013267\n","\tRotated_Epoch:47 [000/005 (0480/0693)]\tLoss Ss: 0.009256\n","\tRotated_Epoch:47 [000/005 (0500/0693)]\tLoss Ss: 0.009252\n","\tRotated_Epoch:47 [000/005 (0520/0693)]\tLoss Ss: 0.013920\n","\tRotated_Epoch:47 [000/005 (0540/0693)]\tLoss Ss: 0.013845\n","\tRotated_Epoch:47 [000/005 (0560/0693)]\tLoss Ss: 0.006038\n","\tRotated_Epoch:47 [000/005 (0580/0693)]\tLoss Ss: 0.009071\n","\tRotated_Epoch:47 [000/005 (0600/0693)]\tLoss Ss: 0.012516\n","\tRotated_Epoch:47 [000/005 (0620/0693)]\tLoss Ss: 0.010857\n","\tRotated_Epoch:47 [000/005 (0640/0693)]\tLoss Ss: 0.012181\n","\tRotated_Epoch:47 [000/005 (0660/0693)]\tLoss Ss: 0.013100\n","\tRotated_Epoch:47 [000/005 (0680/0693)]\tLoss Ss: 0.013381\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tRotated_Epoch:47 [001/005 (0000/0614)]\tLoss Ss: 0.004582\n","\tRotated_Epoch:47 [001/005 (0020/0614)]\tLoss Ss: 0.004525\n","\tRotated_Epoch:47 [001/005 (0040/0614)]\tLoss Ss: 0.005573\n","\tRotated_Epoch:47 [001/005 (0060/0614)]\tLoss Ss: 0.007800\n","\tRotated_Epoch:47 [001/005 (0080/0614)]\tLoss Ss: 0.006465\n","\tRotated_Epoch:47 [001/005 (0100/0614)]\tLoss Ss: 0.004330\n","\tRotated_Epoch:47 [001/005 (0120/0614)]\tLoss Ss: 0.004057\n","\tRotated_Epoch:47 [001/005 (0140/0614)]\tLoss Ss: 0.006137\n","\tRotated_Epoch:47 [001/005 (0160/0614)]\tLoss Ss: 0.005500\n","\tRotated_Epoch:47 [001/005 (0180/0614)]\tLoss Ss: 0.005961\n","\tRotated_Epoch:47 [001/005 (0200/0614)]\tLoss Ss: 0.006287\n","\tRotated_Epoch:47 [001/005 (0220/0614)]\tLoss Ss: 0.004866\n","\tRotated_Epoch:47 [001/005 (0240/0614)]\tLoss Ss: 0.004286\n","\tRotated_Epoch:47 [001/005 (0260/0614)]\tLoss Ss: 0.004575\n","\tRotated_Epoch:47 [001/005 (0280/0614)]\tLoss Ss: 0.004236\n","\tRotated_Epoch:47 [001/005 (0300/0614)]\tLoss Ss: 0.005564\n","\tRotated_Epoch:47 [001/005 (0320/0614)]\tLoss Ss: 0.005557\n","\tRotated_Epoch:47 [001/005 (0340/0614)]\tLoss Ss: 0.003853\n","\tRotated_Epoch:47 [001/005 (0360/0614)]\tLoss Ss: 0.005584\n","\tRotated_Epoch:47 [001/005 (0380/0614)]\tLoss Ss: 0.004009\n","\tRotated_Epoch:47 [001/005 (0400/0614)]\tLoss Ss: 0.003622\n","\tRotated_Epoch:47 [001/005 (0420/0614)]\tLoss Ss: 0.007109\n","\tRotated_Epoch:47 [001/005 (0440/0614)]\tLoss Ss: 0.007548\n","\tRotated_Epoch:47 [001/005 (0460/0614)]\tLoss Ss: 0.004299\n","\tRotated_Epoch:47 [001/005 (0480/0614)]\tLoss Ss: 0.004508\n","\tRotated_Epoch:47 [001/005 (0500/0614)]\tLoss Ss: 0.002566\n","\tRotated_Epoch:47 [001/005 (0520/0614)]\tLoss Ss: 0.005888\n","\tRotated_Epoch:47 [001/005 (0540/0614)]\tLoss Ss: 0.006466\n","\tRotated_Epoch:47 [001/005 (0560/0614)]\tLoss Ss: 0.007033\n","\tRotated_Epoch:47 [001/005 (0580/0614)]\tLoss Ss: 0.004138\n","\tRotated_Epoch:47 [001/005 (0600/0614)]\tLoss Ss: 0.005568\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tRotated_Epoch:47 [002/005 (0000/0588)]\tLoss Ss: 0.235811\n","\tRotated_Epoch:47 [002/005 (0020/0588)]\tLoss Ss: 0.122747\n","\tRotated_Epoch:47 [002/005 (0040/0588)]\tLoss Ss: 0.067255\n","\tRotated_Epoch:47 [002/005 (0060/0588)]\tLoss Ss: 0.043116\n","\tRotated_Epoch:47 [002/005 (0080/0588)]\tLoss Ss: 0.065174\n","\tRotated_Epoch:47 [002/005 (0100/0588)]\tLoss Ss: 0.045069\n","\tRotated_Epoch:47 [002/005 (0120/0588)]\tLoss Ss: 0.049580\n","\tRotated_Epoch:47 [002/005 (0140/0588)]\tLoss Ss: 0.057047\n","\tRotated_Epoch:47 [002/005 (0160/0588)]\tLoss Ss: 0.052379\n","\tRotated_Epoch:47 [002/005 (0180/0588)]\tLoss Ss: 0.064773\n","\tRotated_Epoch:47 [002/005 (0200/0588)]\tLoss Ss: 0.055451\n","\tRotated_Epoch:47 [002/005 (0220/0588)]\tLoss Ss: 0.075446\n","\tRotated_Epoch:47 [002/005 (0240/0588)]\tLoss Ss: 0.043996\n","\tRotated_Epoch:47 [002/005 (0260/0588)]\tLoss Ss: 0.058244\n","\tRotated_Epoch:47 [002/005 (0280/0588)]\tLoss Ss: 0.065948\n","\tRotated_Epoch:47 [002/005 (0300/0588)]\tLoss Ss: 0.068787\n","\tRotated_Epoch:47 [002/005 (0320/0588)]\tLoss Ss: 0.065790\n","\tRotated_Epoch:47 [002/005 (0340/0588)]\tLoss Ss: 0.044451\n","\tRotated_Epoch:47 [002/005 (0360/0588)]\tLoss Ss: 0.052703\n","\tRotated_Epoch:47 [002/005 (0380/0588)]\tLoss Ss: 0.075942\n","\tRotated_Epoch:47 [002/005 (0400/0588)]\tLoss Ss: 0.065236\n","\tRotated_Epoch:47 [002/005 (0420/0588)]\tLoss Ss: 0.050716\n","\tRotated_Epoch:47 [002/005 (0440/0588)]\tLoss Ss: 0.049938\n","\tRotated_Epoch:47 [002/005 (0460/0588)]\tLoss Ss: 0.056898\n","\tRotated_Epoch:47 [002/005 (0480/0588)]\tLoss Ss: 0.061317\n","\tRotated_Epoch:47 [002/005 (0500/0588)]\tLoss Ss: 0.035407\n","\tRotated_Epoch:47 [002/005 (0520/0588)]\tLoss Ss: 0.059593\n","\tRotated_Epoch:47 [002/005 (0540/0588)]\tLoss Ss: 0.039916\n","\tRotated_Epoch:47 [002/005 (0560/0588)]\tLoss Ss: 0.037328\n","\tRotated_Epoch:47 [002/005 (0580/0588)]\tLoss Ss: 0.068433\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:47 [003/005 (0000/0755)]\tLoss Ss: 0.036777\n","\tRotated_Epoch:47 [003/005 (0020/0755)]\tLoss Ss: 0.036444\n","\tRotated_Epoch:47 [003/005 (0040/0755)]\tLoss Ss: 0.015830\n","\tRotated_Epoch:47 [003/005 (0060/0755)]\tLoss Ss: 0.029059\n","\tRotated_Epoch:47 [003/005 (0080/0755)]\tLoss Ss: 0.012453\n","\tRotated_Epoch:47 [003/005 (0100/0755)]\tLoss Ss: 0.020610\n","\tRotated_Epoch:47 [003/005 (0120/0755)]\tLoss Ss: 0.028179\n","\tRotated_Epoch:47 [003/005 (0140/0755)]\tLoss Ss: 0.019550\n","\tRotated_Epoch:47 [003/005 (0160/0755)]\tLoss Ss: 0.011188\n","\tRotated_Epoch:47 [003/005 (0180/0755)]\tLoss Ss: 0.012602\n","\tRotated_Epoch:47 [003/005 (0200/0755)]\tLoss Ss: 0.016248\n","\tRotated_Epoch:47 [003/005 (0220/0755)]\tLoss Ss: 0.016897\n","\tRotated_Epoch:47 [003/005 (0240/0755)]\tLoss Ss: 0.015667\n","\tRotated_Epoch:47 [003/005 (0260/0755)]\tLoss Ss: 0.016537\n","\tRotated_Epoch:47 [003/005 (0280/0755)]\tLoss Ss: 0.013234\n","\tRotated_Epoch:47 [003/005 (0300/0755)]\tLoss Ss: 0.019339\n","\tRotated_Epoch:47 [003/005 (0320/0755)]\tLoss Ss: 0.013816\n","\tRotated_Epoch:47 [003/005 (0340/0755)]\tLoss Ss: 0.012145\n","\tRotated_Epoch:47 [003/005 (0360/0755)]\tLoss Ss: 0.014226\n","\tRotated_Epoch:47 [003/005 (0380/0755)]\tLoss Ss: 0.011292\n","\tRotated_Epoch:47 [003/005 (0400/0755)]\tLoss Ss: 0.013103\n","\tRotated_Epoch:47 [003/005 (0420/0755)]\tLoss Ss: 0.017452\n","\tRotated_Epoch:47 [003/005 (0440/0755)]\tLoss Ss: 0.022136\n","\tRotated_Epoch:47 [003/005 (0460/0755)]\tLoss Ss: 0.013469\n","\tRotated_Epoch:47 [003/005 (0480/0755)]\tLoss Ss: 0.013129\n","\tRotated_Epoch:47 [003/005 (0500/0755)]\tLoss Ss: 0.016466\n","\tRotated_Epoch:47 [003/005 (0520/0755)]\tLoss Ss: 0.012743\n","\tRotated_Epoch:47 [003/005 (0540/0755)]\tLoss Ss: 0.013078\n","\tRotated_Epoch:47 [003/005 (0560/0755)]\tLoss Ss: 0.011885\n","\tRotated_Epoch:47 [003/005 (0580/0755)]\tLoss Ss: 0.011213\n","\tRotated_Epoch:47 [003/005 (0600/0755)]\tLoss Ss: 0.016832\n","\tRotated_Epoch:47 [003/005 (0620/0755)]\tLoss Ss: 0.011041\n","\tRotated_Epoch:47 [003/005 (0640/0755)]\tLoss Ss: 0.011872\n","\tRotated_Epoch:47 [003/005 (0660/0755)]\tLoss Ss: 0.012493\n","\tRotated_Epoch:47 [003/005 (0680/0755)]\tLoss Ss: 0.014170\n","\tRotated_Epoch:47 [003/005 (0700/0755)]\tLoss Ss: 0.009048\n","\tRotated_Epoch:47 [003/005 (0720/0755)]\tLoss Ss: 0.014495\n","\tRotated_Epoch:47 [003/005 (0740/0755)]\tLoss Ss: 0.009056\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:47 [004/005 (0000/0755)]\tLoss Ss: 0.175876\n","\tRotated_Epoch:47 [004/005 (0020/0755)]\tLoss Ss: 0.101868\n","\tRotated_Epoch:47 [004/005 (0040/0755)]\tLoss Ss: 0.070558\n","\tRotated_Epoch:47 [004/005 (0060/0755)]\tLoss Ss: 0.044743\n","\tRotated_Epoch:47 [004/005 (0080/0755)]\tLoss Ss: 0.047315\n","\tRotated_Epoch:47 [004/005 (0100/0755)]\tLoss Ss: 0.057663\n","\tRotated_Epoch:47 [004/005 (0120/0755)]\tLoss Ss: 0.057834\n","\tRotated_Epoch:47 [004/005 (0140/0755)]\tLoss Ss: 0.036092\n","\tRotated_Epoch:47 [004/005 (0160/0755)]\tLoss Ss: 0.035898\n","\tRotated_Epoch:47 [004/005 (0180/0755)]\tLoss Ss: 0.043405\n","\tRotated_Epoch:47 [004/005 (0200/0755)]\tLoss Ss: 0.033956\n","\tRotated_Epoch:47 [004/005 (0220/0755)]\tLoss Ss: 0.034213\n","\tRotated_Epoch:47 [004/005 (0240/0755)]\tLoss Ss: 0.026652\n","\tRotated_Epoch:47 [004/005 (0260/0755)]\tLoss Ss: 0.028833\n","\tRotated_Epoch:47 [004/005 (0280/0755)]\tLoss Ss: 0.033465\n","\tRotated_Epoch:47 [004/005 (0300/0755)]\tLoss Ss: 0.032540\n","\tRotated_Epoch:47 [004/005 (0320/0755)]\tLoss Ss: 0.026556\n","\tRotated_Epoch:47 [004/005 (0340/0755)]\tLoss Ss: 0.027714\n","\tRotated_Epoch:47 [004/005 (0360/0755)]\tLoss Ss: 0.035457\n","\tRotated_Epoch:47 [004/005 (0380/0755)]\tLoss Ss: 0.037410\n","\tRotated_Epoch:47 [004/005 (0400/0755)]\tLoss Ss: 0.028573\n","\tRotated_Epoch:47 [004/005 (0420/0755)]\tLoss Ss: 0.026406\n","\tRotated_Epoch:47 [004/005 (0440/0755)]\tLoss Ss: 0.027250\n","\tRotated_Epoch:47 [004/005 (0460/0755)]\tLoss Ss: 0.034629\n","\tRotated_Epoch:47 [004/005 (0480/0755)]\tLoss Ss: 0.023806\n","\tRotated_Epoch:47 [004/005 (0500/0755)]\tLoss Ss: 0.025326\n","\tRotated_Epoch:47 [004/005 (0520/0755)]\tLoss Ss: 0.017925\n","\tRotated_Epoch:47 [004/005 (0540/0755)]\tLoss Ss: 0.024234\n","\tRotated_Epoch:47 [004/005 (0560/0755)]\tLoss Ss: 0.030477\n","\tRotated_Epoch:47 [004/005 (0580/0755)]\tLoss Ss: 0.025878\n","\tRotated_Epoch:47 [004/005 (0600/0755)]\tLoss Ss: 0.016035\n","\tRotated_Epoch:47 [004/005 (0620/0755)]\tLoss Ss: 0.027759\n","\tRotated_Epoch:47 [004/005 (0640/0755)]\tLoss Ss: 0.017043\n","\tRotated_Epoch:47 [004/005 (0660/0755)]\tLoss Ss: 0.015241\n","\tRotated_Epoch:47 [004/005 (0680/0755)]\tLoss Ss: 0.021177\n","\tRotated_Epoch:47 [004/005 (0700/0755)]\tLoss Ss: 0.027254\n","\tRotated_Epoch:47 [004/005 (0720/0755)]\tLoss Ss: 0.031870\n","\tRotated_Epoch:47 [004/005 (0740/0755)]\tLoss Ss: 0.023753\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tRotated_Epoch:47 [005/005 (0000/0693)]\tLoss Ss: 0.016504\n","\tRotated_Epoch:47 [005/005 (0020/0693)]\tLoss Ss: 0.017157\n","\tRotated_Epoch:47 [005/005 (0040/0693)]\tLoss Ss: 0.020344\n","\tRotated_Epoch:47 [005/005 (0060/0693)]\tLoss Ss: 0.009765\n","\tRotated_Epoch:47 [005/005 (0080/0693)]\tLoss Ss: 0.016079\n","\tRotated_Epoch:47 [005/005 (0100/0693)]\tLoss Ss: 0.014026\n","\tRotated_Epoch:47 [005/005 (0120/0693)]\tLoss Ss: 0.016426\n","\tRotated_Epoch:47 [005/005 (0140/0693)]\tLoss Ss: 0.017098\n","\tRotated_Epoch:47 [005/005 (0160/0693)]\tLoss Ss: 0.014958\n","\tRotated_Epoch:47 [005/005 (0180/0693)]\tLoss Ss: 0.013123\n","\tRotated_Epoch:47 [005/005 (0200/0693)]\tLoss Ss: 0.017941\n","\tRotated_Epoch:47 [005/005 (0220/0693)]\tLoss Ss: 0.012211\n","\tRotated_Epoch:47 [005/005 (0240/0693)]\tLoss Ss: 0.018066\n","\tRotated_Epoch:47 [005/005 (0260/0693)]\tLoss Ss: 0.012530\n","\tRotated_Epoch:47 [005/005 (0280/0693)]\tLoss Ss: 0.012436\n","\tRotated_Epoch:47 [005/005 (0300/0693)]\tLoss Ss: 0.011692\n","\tRotated_Epoch:47 [005/005 (0320/0693)]\tLoss Ss: 0.012472\n","\tRotated_Epoch:47 [005/005 (0340/0693)]\tLoss Ss: 0.015649\n","\tRotated_Epoch:47 [005/005 (0360/0693)]\tLoss Ss: 0.011894\n","\tRotated_Epoch:47 [005/005 (0380/0693)]\tLoss Ss: 0.012708\n","\tRotated_Epoch:47 [005/005 (0400/0693)]\tLoss Ss: 0.011204\n","\tRotated_Epoch:47 [005/005 (0420/0693)]\tLoss Ss: 0.014912\n","\tRotated_Epoch:47 [005/005 (0440/0693)]\tLoss Ss: 0.013981\n","\tRotated_Epoch:47 [005/005 (0460/0693)]\tLoss Ss: 0.009870\n","\tRotated_Epoch:47 [005/005 (0480/0693)]\tLoss Ss: 0.011030\n","\tRotated_Epoch:47 [005/005 (0500/0693)]\tLoss Ss: 0.015614\n","\tRotated_Epoch:47 [005/005 (0520/0693)]\tLoss Ss: 0.014153\n","\tRotated_Epoch:47 [005/005 (0540/0693)]\tLoss Ss: 0.017051\n","\tRotated_Epoch:47 [005/005 (0560/0693)]\tLoss Ss: 0.014302\n","\tRotated_Epoch:47 [005/005 (0580/0693)]\tLoss Ss: 0.012191\n","\tRotated_Epoch:47 [005/005 (0600/0693)]\tLoss Ss: 0.012048\n","\tRotated_Epoch:47 [005/005 (0620/0693)]\tLoss Ss: 0.011997\n","\tRotated_Epoch:47 [005/005 (0640/0693)]\tLoss Ss: 0.011155\n","\tRotated_Epoch:47 [005/005 (0660/0693)]\tLoss Ss: 0.020484\n","\tRotated_Epoch:47 [005/005 (0680/0693)]\tLoss Ss: 0.012218\n","new_1_6_d.nii\n","new_1_6_m.nii\n","new_1_7_d.nii\n","new_1_7_m.nii\n","new_2_5_d.nii\n","new_2_5_m.nii\n","new_2_6_d.nii\n","new_2_6_m.nii\n","new_2_7_d.nii\n","new_2_7_m.nii\n","new_3_d (2).nii\n","new_3_m (2).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (3).nii\n","new_3_m (3).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (7).nii\n","new_3_m (7).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (8).nii\n","new_3_m (8).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (9).nii\n","new_3_m (9).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_4_10_d_163526.nii\n","new_4_10_m_163526.nii.gz\n","new_4_11_d_084336.nii\n","new_4_11_m_084336.nii.gz\n","new_4_7_d_184422.nii\n","new_4_7_m_184422.nii.gz\n","new_4_8_d_143412.nii\n","new_4_8_m_143412.nii.gz\n","new_4_9_d_155347.nii\n","new_4_9_m_155347.nii.gz\n","\tEpoch: 47; Dice: 0.9414 +/- 0.0123; Loss: 7.0991\n","Begin Epoch 48\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tEpoch:48 [000/005 (0000/0614)]\tLoss Ss: 0.005722\n","\tEpoch:48 [000/005 (0020/0614)]\tLoss Ss: 0.007361\n","\tEpoch:48 [000/005 (0040/0614)]\tLoss Ss: 0.009149\n","\tEpoch:48 [000/005 (0060/0614)]\tLoss Ss: 0.005884\n","\tEpoch:48 [000/005 (0080/0614)]\tLoss Ss: 0.004090\n","\tEpoch:48 [000/005 (0100/0614)]\tLoss Ss: 0.005316\n","\tEpoch:48 [000/005 (0120/0614)]\tLoss Ss: 0.005328\n","\tEpoch:48 [000/005 (0140/0614)]\tLoss Ss: 0.007158\n","\tEpoch:48 [000/005 (0160/0614)]\tLoss Ss: 0.006452\n","\tEpoch:48 [000/005 (0180/0614)]\tLoss Ss: 0.011206\n","\tEpoch:48 [000/005 (0200/0614)]\tLoss Ss: 0.006312\n","\tEpoch:48 [000/005 (0220/0614)]\tLoss Ss: 0.006438\n","\tEpoch:48 [000/005 (0240/0614)]\tLoss Ss: 0.006897\n","\tEpoch:48 [000/005 (0260/0614)]\tLoss Ss: 0.004662\n","\tEpoch:48 [000/005 (0280/0614)]\tLoss Ss: 0.004064\n","\tEpoch:48 [000/005 (0300/0614)]\tLoss Ss: 0.006016\n","\tEpoch:48 [000/005 (0320/0614)]\tLoss Ss: 0.004924\n","\tEpoch:48 [000/005 (0340/0614)]\tLoss Ss: 0.007175\n","\tEpoch:48 [000/005 (0360/0614)]\tLoss Ss: 0.008075\n","\tEpoch:48 [000/005 (0380/0614)]\tLoss Ss: 0.002654\n","\tEpoch:48 [000/005 (0400/0614)]\tLoss Ss: 0.004531\n","\tEpoch:48 [000/005 (0420/0614)]\tLoss Ss: 0.005466\n","\tEpoch:48 [000/005 (0440/0614)]\tLoss Ss: 0.005450\n","\tEpoch:48 [000/005 (0460/0614)]\tLoss Ss: 0.005760\n","\tEpoch:48 [000/005 (0480/0614)]\tLoss Ss: 0.004225\n","\tEpoch:48 [000/005 (0500/0614)]\tLoss Ss: 0.006758\n","\tEpoch:48 [000/005 (0520/0614)]\tLoss Ss: 0.005216\n","\tEpoch:48 [000/005 (0540/0614)]\tLoss Ss: 0.004426\n","\tEpoch:48 [000/005 (0560/0614)]\tLoss Ss: 0.003881\n","\tEpoch:48 [000/005 (0580/0614)]\tLoss Ss: 0.005904\n","\tEpoch:48 [000/005 (0600/0614)]\tLoss Ss: 0.005498\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tEpoch:48 [001/005 (0000/0693)]\tLoss Ss: 0.012044\n","\tEpoch:48 [001/005 (0020/0693)]\tLoss Ss: 0.011419\n","\tEpoch:48 [001/005 (0040/0693)]\tLoss Ss: 0.014230\n","\tEpoch:48 [001/005 (0060/0693)]\tLoss Ss: 0.009724\n","\tEpoch:48 [001/005 (0080/0693)]\tLoss Ss: 0.014603\n","\tEpoch:48 [001/005 (0100/0693)]\tLoss Ss: 0.011856\n","\tEpoch:48 [001/005 (0120/0693)]\tLoss Ss: 0.012845\n","\tEpoch:48 [001/005 (0140/0693)]\tLoss Ss: 0.009310\n","\tEpoch:48 [001/005 (0160/0693)]\tLoss Ss: 0.018531\n","\tEpoch:48 [001/005 (0180/0693)]\tLoss Ss: 0.011018\n","\tEpoch:48 [001/005 (0200/0693)]\tLoss Ss: 0.013173\n","\tEpoch:48 [001/005 (0220/0693)]\tLoss Ss: 0.010960\n","\tEpoch:48 [001/005 (0240/0693)]\tLoss Ss: 0.012563\n","\tEpoch:48 [001/005 (0260/0693)]\tLoss Ss: 0.013581\n","\tEpoch:48 [001/005 (0280/0693)]\tLoss Ss: 0.011510\n","\tEpoch:48 [001/005 (0300/0693)]\tLoss Ss: 0.013651\n","\tEpoch:48 [001/005 (0320/0693)]\tLoss Ss: 0.008751\n","\tEpoch:48 [001/005 (0340/0693)]\tLoss Ss: 0.011814\n","\tEpoch:48 [001/005 (0360/0693)]\tLoss Ss: 0.009734\n","\tEpoch:48 [001/005 (0380/0693)]\tLoss Ss: 0.012382\n","\tEpoch:48 [001/005 (0400/0693)]\tLoss Ss: 0.014725\n","\tEpoch:48 [001/005 (0420/0693)]\tLoss Ss: 0.008505\n","\tEpoch:48 [001/005 (0440/0693)]\tLoss Ss: 0.012189\n","\tEpoch:48 [001/005 (0460/0693)]\tLoss Ss: 0.010881\n","\tEpoch:48 [001/005 (0480/0693)]\tLoss Ss: 0.013294\n","\tEpoch:48 [001/005 (0500/0693)]\tLoss Ss: 0.014084\n","\tEpoch:48 [001/005 (0520/0693)]\tLoss Ss: 0.010027\n","\tEpoch:48 [001/005 (0540/0693)]\tLoss Ss: 0.011920\n","\tEpoch:48 [001/005 (0560/0693)]\tLoss Ss: 0.011971\n","\tEpoch:48 [001/005 (0580/0693)]\tLoss Ss: 0.010158\n","\tEpoch:48 [001/005 (0600/0693)]\tLoss Ss: 0.014031\n","\tEpoch:48 [001/005 (0620/0693)]\tLoss Ss: 0.012716\n","\tEpoch:48 [001/005 (0640/0693)]\tLoss Ss: 0.013319\n","\tEpoch:48 [001/005 (0660/0693)]\tLoss Ss: 0.008172\n","\tEpoch:48 [001/005 (0680/0693)]\tLoss Ss: 0.009419\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tEpoch:48 [002/005 (0000/0588)]\tLoss Ss: 0.006923\n","\tEpoch:48 [002/005 (0020/0588)]\tLoss Ss: 0.011476\n","\tEpoch:48 [002/005 (0040/0588)]\tLoss Ss: 0.009000\n","\tEpoch:48 [002/005 (0060/0588)]\tLoss Ss: 0.007493\n","\tEpoch:48 [002/005 (0080/0588)]\tLoss Ss: 0.004939\n","\tEpoch:48 [002/005 (0100/0588)]\tLoss Ss: 0.006610\n","\tEpoch:48 [002/005 (0120/0588)]\tLoss Ss: 0.006493\n","\tEpoch:48 [002/005 (0140/0588)]\tLoss Ss: 0.005317\n","\tEpoch:48 [002/005 (0160/0588)]\tLoss Ss: 0.005777\n","\tEpoch:48 [002/005 (0180/0588)]\tLoss Ss: 0.003591\n","\tEpoch:48 [002/005 (0200/0588)]\tLoss Ss: 0.004480\n","\tEpoch:48 [002/005 (0220/0588)]\tLoss Ss: 0.005101\n","\tEpoch:48 [002/005 (0240/0588)]\tLoss Ss: 0.002809\n","\tEpoch:48 [002/005 (0260/0588)]\tLoss Ss: 0.004764\n","\tEpoch:48 [002/005 (0280/0588)]\tLoss Ss: 0.003705\n","\tEpoch:48 [002/005 (0300/0588)]\tLoss Ss: 0.004070\n","\tEpoch:48 [002/005 (0320/0588)]\tLoss Ss: 0.002529\n","\tEpoch:48 [002/005 (0340/0588)]\tLoss Ss: 0.003330\n","\tEpoch:48 [002/005 (0360/0588)]\tLoss Ss: 0.003332\n","\tEpoch:48 [002/005 (0380/0588)]\tLoss Ss: 0.005758\n","\tEpoch:48 [002/005 (0400/0588)]\tLoss Ss: 0.003987\n","\tEpoch:48 [002/005 (0420/0588)]\tLoss Ss: 0.004292\n","\tEpoch:48 [002/005 (0440/0588)]\tLoss Ss: 0.003573\n","\tEpoch:48 [002/005 (0460/0588)]\tLoss Ss: 0.003233\n","\tEpoch:48 [002/005 (0480/0588)]\tLoss Ss: 0.003673\n","\tEpoch:48 [002/005 (0500/0588)]\tLoss Ss: 0.004372\n","\tEpoch:48 [002/005 (0520/0588)]\tLoss Ss: 0.003843\n","\tEpoch:48 [002/005 (0540/0588)]\tLoss Ss: 0.004448\n","\tEpoch:48 [002/005 (0560/0588)]\tLoss Ss: 0.003607\n","\tEpoch:48 [002/005 (0580/0588)]\tLoss Ss: 0.006248\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:48 [003/005 (0000/0755)]\tLoss Ss: 0.024168\n","\tEpoch:48 [003/005 (0020/0755)]\tLoss Ss: 0.016042\n","\tEpoch:48 [003/005 (0040/0755)]\tLoss Ss: 0.015525\n","\tEpoch:48 [003/005 (0060/0755)]\tLoss Ss: 0.020216\n","\tEpoch:48 [003/005 (0080/0755)]\tLoss Ss: 0.010995\n","\tEpoch:48 [003/005 (0100/0755)]\tLoss Ss: 0.013302\n","\tEpoch:48 [003/005 (0120/0755)]\tLoss Ss: 0.016181\n","\tEpoch:48 [003/005 (0140/0755)]\tLoss Ss: 0.011298\n","\tEpoch:48 [003/005 (0160/0755)]\tLoss Ss: 0.016333\n","\tEpoch:48 [003/005 (0180/0755)]\tLoss Ss: 0.016240\n","\tEpoch:48 [003/005 (0200/0755)]\tLoss Ss: 0.016029\n","\tEpoch:48 [003/005 (0220/0755)]\tLoss Ss: 0.010174\n","\tEpoch:48 [003/005 (0240/0755)]\tLoss Ss: 0.010590\n","\tEpoch:48 [003/005 (0260/0755)]\tLoss Ss: 0.013800\n","\tEpoch:48 [003/005 (0280/0755)]\tLoss Ss: 0.009974\n","\tEpoch:48 [003/005 (0300/0755)]\tLoss Ss: 0.012702\n","\tEpoch:48 [003/005 (0320/0755)]\tLoss Ss: 0.012492\n","\tEpoch:48 [003/005 (0340/0755)]\tLoss Ss: 0.016257\n","\tEpoch:48 [003/005 (0360/0755)]\tLoss Ss: 0.012730\n","\tEpoch:48 [003/005 (0380/0755)]\tLoss Ss: 0.012218\n","\tEpoch:48 [003/005 (0400/0755)]\tLoss Ss: 0.013789\n","\tEpoch:48 [003/005 (0420/0755)]\tLoss Ss: 0.010063\n","\tEpoch:48 [003/005 (0440/0755)]\tLoss Ss: 0.013066\n","\tEpoch:48 [003/005 (0460/0755)]\tLoss Ss: 0.014378\n","\tEpoch:48 [003/005 (0480/0755)]\tLoss Ss: 0.009713\n","\tEpoch:48 [003/005 (0500/0755)]\tLoss Ss: 0.016265\n","\tEpoch:48 [003/005 (0520/0755)]\tLoss Ss: 0.010825\n","\tEpoch:48 [003/005 (0540/0755)]\tLoss Ss: 0.013026\n","\tEpoch:48 [003/005 (0560/0755)]\tLoss Ss: 0.013259\n","\tEpoch:48 [003/005 (0580/0755)]\tLoss Ss: 0.013009\n","\tEpoch:48 [003/005 (0600/0755)]\tLoss Ss: 0.009441\n","\tEpoch:48 [003/005 (0620/0755)]\tLoss Ss: 0.015810\n","\tEpoch:48 [003/005 (0640/0755)]\tLoss Ss: 0.010467\n","\tEpoch:48 [003/005 (0660/0755)]\tLoss Ss: 0.009762\n","\tEpoch:48 [003/005 (0680/0755)]\tLoss Ss: 0.008825\n","\tEpoch:48 [003/005 (0700/0755)]\tLoss Ss: 0.010589\n","\tEpoch:48 [003/005 (0720/0755)]\tLoss Ss: 0.016673\n","\tEpoch:48 [003/005 (0740/0755)]\tLoss Ss: 0.013764\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:48 [004/005 (0000/0755)]\tLoss Ss: 0.011305\n","\tEpoch:48 [004/005 (0020/0755)]\tLoss Ss: 0.010359\n","\tEpoch:48 [004/005 (0040/0755)]\tLoss Ss: 0.011626\n","\tEpoch:48 [004/005 (0060/0755)]\tLoss Ss: 0.013192\n","\tEpoch:48 [004/005 (0080/0755)]\tLoss Ss: 0.013185\n","\tEpoch:48 [004/005 (0100/0755)]\tLoss Ss: 0.014395\n","\tEpoch:48 [004/005 (0120/0755)]\tLoss Ss: 0.013237\n","\tEpoch:48 [004/005 (0140/0755)]\tLoss Ss: 0.009860\n","\tEpoch:48 [004/005 (0160/0755)]\tLoss Ss: 0.010085\n","\tEpoch:48 [004/005 (0180/0755)]\tLoss Ss: 0.015840\n","\tEpoch:48 [004/005 (0200/0755)]\tLoss Ss: 0.010616\n","\tEpoch:48 [004/005 (0220/0755)]\tLoss Ss: 0.007222\n","\tEpoch:48 [004/005 (0240/0755)]\tLoss Ss: 0.013798\n","\tEpoch:48 [004/005 (0260/0755)]\tLoss Ss: 0.010295\n","\tEpoch:48 [004/005 (0280/0755)]\tLoss Ss: 0.009246\n","\tEpoch:48 [004/005 (0300/0755)]\tLoss Ss: 0.013352\n","\tEpoch:48 [004/005 (0320/0755)]\tLoss Ss: 0.009273\n","\tEpoch:48 [004/005 (0340/0755)]\tLoss Ss: 0.012889\n","\tEpoch:48 [004/005 (0360/0755)]\tLoss Ss: 0.011450\n","\tEpoch:48 [004/005 (0380/0755)]\tLoss Ss: 0.010227\n","\tEpoch:48 [004/005 (0400/0755)]\tLoss Ss: 0.009892\n","\tEpoch:48 [004/005 (0420/0755)]\tLoss Ss: 0.008215\n","\tEpoch:48 [004/005 (0440/0755)]\tLoss Ss: 0.012305\n","\tEpoch:48 [004/005 (0460/0755)]\tLoss Ss: 0.010171\n","\tEpoch:48 [004/005 (0480/0755)]\tLoss Ss: 0.011660\n","\tEpoch:48 [004/005 (0500/0755)]\tLoss Ss: 0.012973\n","\tEpoch:48 [004/005 (0520/0755)]\tLoss Ss: 0.012532\n","\tEpoch:48 [004/005 (0540/0755)]\tLoss Ss: 0.011463\n","\tEpoch:48 [004/005 (0560/0755)]\tLoss Ss: 0.010321\n","\tEpoch:48 [004/005 (0580/0755)]\tLoss Ss: 0.007192\n","\tEpoch:48 [004/005 (0600/0755)]\tLoss Ss: 0.012365\n","\tEpoch:48 [004/005 (0620/0755)]\tLoss Ss: 0.010251\n","\tEpoch:48 [004/005 (0640/0755)]\tLoss Ss: 0.008362\n","\tEpoch:48 [004/005 (0660/0755)]\tLoss Ss: 0.010429\n","\tEpoch:48 [004/005 (0680/0755)]\tLoss Ss: 0.011234\n","\tEpoch:48 [004/005 (0700/0755)]\tLoss Ss: 0.009233\n","\tEpoch:48 [004/005 (0720/0755)]\tLoss Ss: 0.010908\n","\tEpoch:48 [004/005 (0740/0755)]\tLoss Ss: 0.011331\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tEpoch:48 [005/005 (0000/0693)]\tLoss Ss: 0.012417\n","\tEpoch:48 [005/005 (0020/0693)]\tLoss Ss: 0.011519\n","\tEpoch:48 [005/005 (0040/0693)]\tLoss Ss: 0.011563\n","\tEpoch:48 [005/005 (0060/0693)]\tLoss Ss: 0.010644\n","\tEpoch:48 [005/005 (0080/0693)]\tLoss Ss: 0.008824\n","\tEpoch:48 [005/005 (0100/0693)]\tLoss Ss: 0.009576\n","\tEpoch:48 [005/005 (0120/0693)]\tLoss Ss: 0.006858\n","\tEpoch:48 [005/005 (0140/0693)]\tLoss Ss: 0.012781\n","\tEpoch:48 [005/005 (0160/0693)]\tLoss Ss: 0.010303\n","\tEpoch:48 [005/005 (0180/0693)]\tLoss Ss: 0.009857\n","\tEpoch:48 [005/005 (0200/0693)]\tLoss Ss: 0.009546\n","\tEpoch:48 [005/005 (0220/0693)]\tLoss Ss: 0.009509\n","\tEpoch:48 [005/005 (0240/0693)]\tLoss Ss: 0.009375\n","\tEpoch:48 [005/005 (0260/0693)]\tLoss Ss: 0.008961\n","\tEpoch:48 [005/005 (0280/0693)]\tLoss Ss: 0.012963\n","\tEpoch:48 [005/005 (0300/0693)]\tLoss Ss: 0.008626\n","\tEpoch:48 [005/005 (0320/0693)]\tLoss Ss: 0.014853\n","\tEpoch:48 [005/005 (0340/0693)]\tLoss Ss: 0.009492\n","\tEpoch:48 [005/005 (0360/0693)]\tLoss Ss: 0.014598\n","\tEpoch:48 [005/005 (0380/0693)]\tLoss Ss: 0.011767\n","\tEpoch:48 [005/005 (0400/0693)]\tLoss Ss: 0.011355\n","\tEpoch:48 [005/005 (0420/0693)]\tLoss Ss: 0.006087\n","\tEpoch:48 [005/005 (0440/0693)]\tLoss Ss: 0.009625\n","\tEpoch:48 [005/005 (0460/0693)]\tLoss Ss: 0.006882\n","\tEpoch:48 [005/005 (0480/0693)]\tLoss Ss: 0.010890\n","\tEpoch:48 [005/005 (0500/0693)]\tLoss Ss: 0.011310\n","\tEpoch:48 [005/005 (0520/0693)]\tLoss Ss: 0.013384\n","\tEpoch:48 [005/005 (0540/0693)]\tLoss Ss: 0.008621\n","\tEpoch:48 [005/005 (0560/0693)]\tLoss Ss: 0.005836\n","\tEpoch:48 [005/005 (0580/0693)]\tLoss Ss: 0.007740\n","\tEpoch:48 [005/005 (0600/0693)]\tLoss Ss: 0.007659\n","\tEpoch:48 [005/005 (0620/0693)]\tLoss Ss: 0.010881\n","\tEpoch:48 [005/005 (0640/0693)]\tLoss Ss: 0.010594\n","\tEpoch:48 [005/005 (0660/0693)]\tLoss Ss: 0.009259\n","\tEpoch:48 [005/005 (0680/0693)]\tLoss Ss: 0.011562\n","Now train the rotated image\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tRotated_Epoch:48 [000/005 (0000/0693)]\tLoss Ss: 0.014995\n","\tRotated_Epoch:48 [000/005 (0020/0693)]\tLoss Ss: 0.007472\n","\tRotated_Epoch:48 [000/005 (0040/0693)]\tLoss Ss: 0.014918\n","\tRotated_Epoch:48 [000/005 (0060/0693)]\tLoss Ss: 0.009443\n","\tRotated_Epoch:48 [000/005 (0080/0693)]\tLoss Ss: 0.014106\n","\tRotated_Epoch:48 [000/005 (0100/0693)]\tLoss Ss: 0.008582\n","\tRotated_Epoch:48 [000/005 (0120/0693)]\tLoss Ss: 0.015867\n","\tRotated_Epoch:48 [000/005 (0140/0693)]\tLoss Ss: 0.012753\n","\tRotated_Epoch:48 [000/005 (0160/0693)]\tLoss Ss: 0.014017\n","\tRotated_Epoch:48 [000/005 (0180/0693)]\tLoss Ss: 0.016631\n","\tRotated_Epoch:48 [000/005 (0200/0693)]\tLoss Ss: 0.011556\n","\tRotated_Epoch:48 [000/005 (0220/0693)]\tLoss Ss: 0.009783\n","\tRotated_Epoch:48 [000/005 (0240/0693)]\tLoss Ss: 0.013470\n","\tRotated_Epoch:48 [000/005 (0260/0693)]\tLoss Ss: 0.008437\n","\tRotated_Epoch:48 [000/005 (0280/0693)]\tLoss Ss: 0.013355\n","\tRotated_Epoch:48 [000/005 (0300/0693)]\tLoss Ss: 0.019729\n","\tRotated_Epoch:48 [000/005 (0320/0693)]\tLoss Ss: 0.007486\n","\tRotated_Epoch:48 [000/005 (0340/0693)]\tLoss Ss: 0.013998\n","\tRotated_Epoch:48 [000/005 (0360/0693)]\tLoss Ss: 0.017795\n","\tRotated_Epoch:48 [000/005 (0380/0693)]\tLoss Ss: 0.014015\n","\tRotated_Epoch:48 [000/005 (0400/0693)]\tLoss Ss: 0.007632\n","\tRotated_Epoch:48 [000/005 (0420/0693)]\tLoss Ss: 0.018030\n","\tRotated_Epoch:48 [000/005 (0440/0693)]\tLoss Ss: 0.011582\n","\tRotated_Epoch:48 [000/005 (0460/0693)]\tLoss Ss: 0.013137\n","\tRotated_Epoch:48 [000/005 (0480/0693)]\tLoss Ss: 0.015978\n","\tRotated_Epoch:48 [000/005 (0500/0693)]\tLoss Ss: 0.008777\n","\tRotated_Epoch:48 [000/005 (0520/0693)]\tLoss Ss: 0.016110\n","\tRotated_Epoch:48 [000/005 (0540/0693)]\tLoss Ss: 0.010721\n","\tRotated_Epoch:48 [000/005 (0560/0693)]\tLoss Ss: 0.014111\n","\tRotated_Epoch:48 [000/005 (0580/0693)]\tLoss Ss: 0.026513\n","\tRotated_Epoch:48 [000/005 (0600/0693)]\tLoss Ss: 0.013784\n","\tRotated_Epoch:48 [000/005 (0620/0693)]\tLoss Ss: 0.013409\n","\tRotated_Epoch:48 [000/005 (0640/0693)]\tLoss Ss: 0.014331\n","\tRotated_Epoch:48 [000/005 (0660/0693)]\tLoss Ss: 0.013060\n","\tRotated_Epoch:48 [000/005 (0680/0693)]\tLoss Ss: 0.008874\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tRotated_Epoch:48 [001/005 (0000/0588)]\tLoss Ss: 0.076460\n","\tRotated_Epoch:48 [001/005 (0020/0588)]\tLoss Ss: 0.063579\n","\tRotated_Epoch:48 [001/005 (0040/0588)]\tLoss Ss: 0.079155\n","\tRotated_Epoch:48 [001/005 (0060/0588)]\tLoss Ss: 0.053064\n","\tRotated_Epoch:48 [001/005 (0080/0588)]\tLoss Ss: 0.064647\n","\tRotated_Epoch:48 [001/005 (0100/0588)]\tLoss Ss: 0.066016\n","\tRotated_Epoch:48 [001/005 (0120/0588)]\tLoss Ss: 0.042763\n","\tRotated_Epoch:48 [001/005 (0140/0588)]\tLoss Ss: 0.074582\n","\tRotated_Epoch:48 [001/005 (0160/0588)]\tLoss Ss: 0.057045\n","\tRotated_Epoch:48 [001/005 (0180/0588)]\tLoss Ss: 0.048704\n","\tRotated_Epoch:48 [001/005 (0200/0588)]\tLoss Ss: 0.043747\n","\tRotated_Epoch:48 [001/005 (0220/0588)]\tLoss Ss: 0.067334\n","\tRotated_Epoch:48 [001/005 (0240/0588)]\tLoss Ss: 0.070150\n","\tRotated_Epoch:48 [001/005 (0260/0588)]\tLoss Ss: 0.039735\n","\tRotated_Epoch:48 [001/005 (0280/0588)]\tLoss Ss: 0.065960\n","\tRotated_Epoch:48 [001/005 (0300/0588)]\tLoss Ss: 0.051164\n","\tRotated_Epoch:48 [001/005 (0320/0588)]\tLoss Ss: 0.037697\n","\tRotated_Epoch:48 [001/005 (0340/0588)]\tLoss Ss: 0.065295\n","\tRotated_Epoch:48 [001/005 (0360/0588)]\tLoss Ss: 0.053407\n","\tRotated_Epoch:48 [001/005 (0380/0588)]\tLoss Ss: 0.060739\n","\tRotated_Epoch:48 [001/005 (0400/0588)]\tLoss Ss: 0.048456\n","\tRotated_Epoch:48 [001/005 (0420/0588)]\tLoss Ss: 0.058907\n","\tRotated_Epoch:48 [001/005 (0440/0588)]\tLoss Ss: 0.050470\n","\tRotated_Epoch:48 [001/005 (0460/0588)]\tLoss Ss: 0.053833\n","\tRotated_Epoch:48 [001/005 (0480/0588)]\tLoss Ss: 0.048209\n","\tRotated_Epoch:48 [001/005 (0500/0588)]\tLoss Ss: 0.046625\n","\tRotated_Epoch:48 [001/005 (0520/0588)]\tLoss Ss: 0.041303\n","\tRotated_Epoch:48 [001/005 (0540/0588)]\tLoss Ss: 0.063722\n","\tRotated_Epoch:48 [001/005 (0560/0588)]\tLoss Ss: 0.049782\n","\tRotated_Epoch:48 [001/005 (0580/0588)]\tLoss Ss: 0.055642\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tRotated_Epoch:48 [002/005 (0000/0614)]\tLoss Ss: 0.011404\n","\tRotated_Epoch:48 [002/005 (0020/0614)]\tLoss Ss: 0.009601\n","\tRotated_Epoch:48 [002/005 (0040/0614)]\tLoss Ss: 0.009388\n","\tRotated_Epoch:48 [002/005 (0060/0614)]\tLoss Ss: 0.006795\n","\tRotated_Epoch:48 [002/005 (0080/0614)]\tLoss Ss: 0.007609\n","\tRotated_Epoch:48 [002/005 (0100/0614)]\tLoss Ss: 0.005837\n","\tRotated_Epoch:48 [002/005 (0120/0614)]\tLoss Ss: 0.005011\n","\tRotated_Epoch:48 [002/005 (0140/0614)]\tLoss Ss: 0.009205\n","\tRotated_Epoch:48 [002/005 (0160/0614)]\tLoss Ss: 0.006323\n","\tRotated_Epoch:48 [002/005 (0180/0614)]\tLoss Ss: 0.008421\n","\tRotated_Epoch:48 [002/005 (0200/0614)]\tLoss Ss: 0.006232\n","\tRotated_Epoch:48 [002/005 (0220/0614)]\tLoss Ss: 0.005721\n","\tRotated_Epoch:48 [002/005 (0240/0614)]\tLoss Ss: 0.006778\n","\tRotated_Epoch:48 [002/005 (0260/0614)]\tLoss Ss: 0.005356\n","\tRotated_Epoch:48 [002/005 (0280/0614)]\tLoss Ss: 0.003156\n","\tRotated_Epoch:48 [002/005 (0300/0614)]\tLoss Ss: 0.006661\n","\tRotated_Epoch:48 [002/005 (0320/0614)]\tLoss Ss: 0.006697\n","\tRotated_Epoch:48 [002/005 (0340/0614)]\tLoss Ss: 0.007177\n","\tRotated_Epoch:48 [002/005 (0360/0614)]\tLoss Ss: 0.006386\n","\tRotated_Epoch:48 [002/005 (0380/0614)]\tLoss Ss: 0.006861\n","\tRotated_Epoch:48 [002/005 (0400/0614)]\tLoss Ss: 0.009951\n","\tRotated_Epoch:48 [002/005 (0420/0614)]\tLoss Ss: 0.005918\n","\tRotated_Epoch:48 [002/005 (0440/0614)]\tLoss Ss: 0.003726\n","\tRotated_Epoch:48 [002/005 (0460/0614)]\tLoss Ss: 0.004355\n","\tRotated_Epoch:48 [002/005 (0480/0614)]\tLoss Ss: 0.004048\n","\tRotated_Epoch:48 [002/005 (0500/0614)]\tLoss Ss: 0.004552\n","\tRotated_Epoch:48 [002/005 (0520/0614)]\tLoss Ss: 0.007046\n","\tRotated_Epoch:48 [002/005 (0540/0614)]\tLoss Ss: 0.006829\n","\tRotated_Epoch:48 [002/005 (0560/0614)]\tLoss Ss: 0.004968\n","\tRotated_Epoch:48 [002/005 (0580/0614)]\tLoss Ss: 0.004998\n","\tRotated_Epoch:48 [002/005 (0600/0614)]\tLoss Ss: 0.004558\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:48 [003/005 (0000/0755)]\tLoss Ss: 0.024257\n","\tRotated_Epoch:48 [003/005 (0020/0755)]\tLoss Ss: 0.020481\n","\tRotated_Epoch:48 [003/005 (0040/0755)]\tLoss Ss: 0.044698\n","\tRotated_Epoch:48 [003/005 (0060/0755)]\tLoss Ss: 0.032117\n","\tRotated_Epoch:48 [003/005 (0080/0755)]\tLoss Ss: 0.028875\n","\tRotated_Epoch:48 [003/005 (0100/0755)]\tLoss Ss: 0.030789\n","\tRotated_Epoch:48 [003/005 (0120/0755)]\tLoss Ss: 0.025062\n","\tRotated_Epoch:48 [003/005 (0140/0755)]\tLoss Ss: 0.024015\n","\tRotated_Epoch:48 [003/005 (0160/0755)]\tLoss Ss: 0.023362\n","\tRotated_Epoch:48 [003/005 (0180/0755)]\tLoss Ss: 0.028370\n","\tRotated_Epoch:48 [003/005 (0200/0755)]\tLoss Ss: 0.029531\n","\tRotated_Epoch:48 [003/005 (0220/0755)]\tLoss Ss: 0.029043\n","\tRotated_Epoch:48 [003/005 (0240/0755)]\tLoss Ss: 0.018154\n","\tRotated_Epoch:48 [003/005 (0260/0755)]\tLoss Ss: 0.025534\n","\tRotated_Epoch:48 [003/005 (0280/0755)]\tLoss Ss: 0.022110\n","\tRotated_Epoch:48 [003/005 (0300/0755)]\tLoss Ss: 0.022288\n","\tRotated_Epoch:48 [003/005 (0320/0755)]\tLoss Ss: 0.020537\n","\tRotated_Epoch:48 [003/005 (0340/0755)]\tLoss Ss: 0.020988\n","\tRotated_Epoch:48 [003/005 (0360/0755)]\tLoss Ss: 0.024351\n","\tRotated_Epoch:48 [003/005 (0380/0755)]\tLoss Ss: 0.023256\n","\tRotated_Epoch:48 [003/005 (0400/0755)]\tLoss Ss: 0.023768\n","\tRotated_Epoch:48 [003/005 (0420/0755)]\tLoss Ss: 0.018847\n","\tRotated_Epoch:48 [003/005 (0440/0755)]\tLoss Ss: 0.017392\n","\tRotated_Epoch:48 [003/005 (0460/0755)]\tLoss Ss: 0.022604\n","\tRotated_Epoch:48 [003/005 (0480/0755)]\tLoss Ss: 0.019258\n","\tRotated_Epoch:48 [003/005 (0500/0755)]\tLoss Ss: 0.012310\n","\tRotated_Epoch:48 [003/005 (0520/0755)]\tLoss Ss: 0.033446\n","\tRotated_Epoch:48 [003/005 (0540/0755)]\tLoss Ss: 0.020411\n","\tRotated_Epoch:48 [003/005 (0560/0755)]\tLoss Ss: 0.026413\n","\tRotated_Epoch:48 [003/005 (0580/0755)]\tLoss Ss: 0.012041\n","\tRotated_Epoch:48 [003/005 (0600/0755)]\tLoss Ss: 0.025763\n","\tRotated_Epoch:48 [003/005 (0620/0755)]\tLoss Ss: 0.016110\n","\tRotated_Epoch:48 [003/005 (0640/0755)]\tLoss Ss: 0.029063\n","\tRotated_Epoch:48 [003/005 (0660/0755)]\tLoss Ss: 0.012706\n","\tRotated_Epoch:48 [003/005 (0680/0755)]\tLoss Ss: 0.025456\n","\tRotated_Epoch:48 [003/005 (0700/0755)]\tLoss Ss: 0.020686\n","\tRotated_Epoch:48 [003/005 (0720/0755)]\tLoss Ss: 0.019903\n","\tRotated_Epoch:48 [003/005 (0740/0755)]\tLoss Ss: 0.015344\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:48 [004/005 (0000/0755)]\tLoss Ss: 0.069936\n","\tRotated_Epoch:48 [004/005 (0020/0755)]\tLoss Ss: 0.095205\n","\tRotated_Epoch:48 [004/005 (0040/0755)]\tLoss Ss: 0.046543\n","\tRotated_Epoch:48 [004/005 (0060/0755)]\tLoss Ss: 0.023324\n","\tRotated_Epoch:48 [004/005 (0080/0755)]\tLoss Ss: 0.031253\n","\tRotated_Epoch:48 [004/005 (0100/0755)]\tLoss Ss: 0.030530\n","\tRotated_Epoch:48 [004/005 (0120/0755)]\tLoss Ss: 0.021754\n","\tRotated_Epoch:48 [004/005 (0140/0755)]\tLoss Ss: 0.017662\n","\tRotated_Epoch:48 [004/005 (0160/0755)]\tLoss Ss: 0.020784\n","\tRotated_Epoch:48 [004/005 (0180/0755)]\tLoss Ss: 0.028718\n","\tRotated_Epoch:48 [004/005 (0200/0755)]\tLoss Ss: 0.012818\n","\tRotated_Epoch:48 [004/005 (0220/0755)]\tLoss Ss: 0.016289\n","\tRotated_Epoch:48 [004/005 (0240/0755)]\tLoss Ss: 0.016485\n","\tRotated_Epoch:48 [004/005 (0260/0755)]\tLoss Ss: 0.019904\n","\tRotated_Epoch:48 [004/005 (0280/0755)]\tLoss Ss: 0.013540\n","\tRotated_Epoch:48 [004/005 (0300/0755)]\tLoss Ss: 0.016051\n","\tRotated_Epoch:48 [004/005 (0320/0755)]\tLoss Ss: 0.013979\n","\tRotated_Epoch:48 [004/005 (0340/0755)]\tLoss Ss: 0.012324\n","\tRotated_Epoch:48 [004/005 (0360/0755)]\tLoss Ss: 0.018569\n","\tRotated_Epoch:48 [004/005 (0380/0755)]\tLoss Ss: 0.014398\n","\tRotated_Epoch:48 [004/005 (0400/0755)]\tLoss Ss: 0.016110\n","\tRotated_Epoch:48 [004/005 (0420/0755)]\tLoss Ss: 0.016214\n","\tRotated_Epoch:48 [004/005 (0440/0755)]\tLoss Ss: 0.011544\n","\tRotated_Epoch:48 [004/005 (0460/0755)]\tLoss Ss: 0.016025\n","\tRotated_Epoch:48 [004/005 (0480/0755)]\tLoss Ss: 0.014775\n","\tRotated_Epoch:48 [004/005 (0500/0755)]\tLoss Ss: 0.009537\n","\tRotated_Epoch:48 [004/005 (0520/0755)]\tLoss Ss: 0.014399\n","\tRotated_Epoch:48 [004/005 (0540/0755)]\tLoss Ss: 0.012706\n","\tRotated_Epoch:48 [004/005 (0560/0755)]\tLoss Ss: 0.010136\n","\tRotated_Epoch:48 [004/005 (0580/0755)]\tLoss Ss: 0.017017\n","\tRotated_Epoch:48 [004/005 (0600/0755)]\tLoss Ss: 0.014228\n","\tRotated_Epoch:48 [004/005 (0620/0755)]\tLoss Ss: 0.013006\n","\tRotated_Epoch:48 [004/005 (0640/0755)]\tLoss Ss: 0.015298\n","\tRotated_Epoch:48 [004/005 (0660/0755)]\tLoss Ss: 0.013997\n","\tRotated_Epoch:48 [004/005 (0680/0755)]\tLoss Ss: 0.011391\n","\tRotated_Epoch:48 [004/005 (0700/0755)]\tLoss Ss: 0.015753\n","\tRotated_Epoch:48 [004/005 (0720/0755)]\tLoss Ss: 0.011658\n","\tRotated_Epoch:48 [004/005 (0740/0755)]\tLoss Ss: 0.009106\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tRotated_Epoch:48 [005/005 (0000/0693)]\tLoss Ss: 0.015155\n","\tRotated_Epoch:48 [005/005 (0020/0693)]\tLoss Ss: 0.010815\n","\tRotated_Epoch:48 [005/005 (0040/0693)]\tLoss Ss: 0.011538\n","\tRotated_Epoch:48 [005/005 (0060/0693)]\tLoss Ss: 0.017549\n","\tRotated_Epoch:48 [005/005 (0080/0693)]\tLoss Ss: 0.010533\n","\tRotated_Epoch:48 [005/005 (0100/0693)]\tLoss Ss: 0.014767\n","\tRotated_Epoch:48 [005/005 (0120/0693)]\tLoss Ss: 0.012000\n","\tRotated_Epoch:48 [005/005 (0140/0693)]\tLoss Ss: 0.010608\n","\tRotated_Epoch:48 [005/005 (0160/0693)]\tLoss Ss: 0.011006\n","\tRotated_Epoch:48 [005/005 (0180/0693)]\tLoss Ss: 0.010712\n","\tRotated_Epoch:48 [005/005 (0200/0693)]\tLoss Ss: 0.014966\n","\tRotated_Epoch:48 [005/005 (0220/0693)]\tLoss Ss: 0.015484\n","\tRotated_Epoch:48 [005/005 (0240/0693)]\tLoss Ss: 0.014006\n","\tRotated_Epoch:48 [005/005 (0260/0693)]\tLoss Ss: 0.011158\n","\tRotated_Epoch:48 [005/005 (0280/0693)]\tLoss Ss: 0.011923\n","\tRotated_Epoch:48 [005/005 (0300/0693)]\tLoss Ss: 0.011117\n","\tRotated_Epoch:48 [005/005 (0320/0693)]\tLoss Ss: 0.013707\n","\tRotated_Epoch:48 [005/005 (0340/0693)]\tLoss Ss: 0.011182\n","\tRotated_Epoch:48 [005/005 (0360/0693)]\tLoss Ss: 0.010328\n","\tRotated_Epoch:48 [005/005 (0380/0693)]\tLoss Ss: 0.013558\n","\tRotated_Epoch:48 [005/005 (0400/0693)]\tLoss Ss: 0.008931\n","\tRotated_Epoch:48 [005/005 (0420/0693)]\tLoss Ss: 0.011147\n","\tRotated_Epoch:48 [005/005 (0440/0693)]\tLoss Ss: 0.011040\n","\tRotated_Epoch:48 [005/005 (0460/0693)]\tLoss Ss: 0.009253\n","\tRotated_Epoch:48 [005/005 (0480/0693)]\tLoss Ss: 0.010822\n","\tRotated_Epoch:48 [005/005 (0500/0693)]\tLoss Ss: 0.012361\n","\tRotated_Epoch:48 [005/005 (0520/0693)]\tLoss Ss: 0.009994\n","\tRotated_Epoch:48 [005/005 (0540/0693)]\tLoss Ss: 0.010922\n","\tRotated_Epoch:48 [005/005 (0560/0693)]\tLoss Ss: 0.010831\n","\tRotated_Epoch:48 [005/005 (0580/0693)]\tLoss Ss: 0.012895\n","\tRotated_Epoch:48 [005/005 (0600/0693)]\tLoss Ss: 0.012979\n","\tRotated_Epoch:48 [005/005 (0620/0693)]\tLoss Ss: 0.011069\n","\tRotated_Epoch:48 [005/005 (0640/0693)]\tLoss Ss: 0.010379\n","\tRotated_Epoch:48 [005/005 (0660/0693)]\tLoss Ss: 0.011987\n","\tRotated_Epoch:48 [005/005 (0680/0693)]\tLoss Ss: 0.009425\n","new_1_6_d.nii\n","new_1_6_m.nii\n","new_1_7_d.nii\n","new_1_7_m.nii\n","new_2_5_d.nii\n","new_2_5_m.nii\n","new_2_6_d.nii\n","new_2_6_m.nii\n","new_2_7_d.nii\n","new_2_7_m.nii\n","new_3_d (2).nii\n","new_3_m (2).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (3).nii\n","new_3_m (3).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (7).nii\n","new_3_m (7).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (8).nii\n","new_3_m (8).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (9).nii\n","new_3_m (9).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_4_10_d_163526.nii\n","new_4_10_m_163526.nii.gz\n","new_4_11_d_084336.nii\n","new_4_11_m_084336.nii.gz\n","new_4_7_d_184422.nii\n","new_4_7_m_184422.nii.gz\n","new_4_8_d_143412.nii\n","new_4_8_m_143412.nii.gz\n","new_4_9_d_155347.nii\n","new_4_9_m_155347.nii.gz\n","\tEpoch: 48; Dice: 0.9681 +/- 0.0034; Loss: 6.4906\n","Begin Epoch 49\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:49 [000/005 (0000/0755)]\tLoss Ss: 0.007481\n","\tEpoch:49 [000/005 (0020/0755)]\tLoss Ss: 0.012217\n","\tEpoch:49 [000/005 (0040/0755)]\tLoss Ss: 0.012895\n","\tEpoch:49 [000/005 (0060/0755)]\tLoss Ss: 0.018969\n","\tEpoch:49 [000/005 (0080/0755)]\tLoss Ss: 0.012534\n","\tEpoch:49 [000/005 (0100/0755)]\tLoss Ss: 0.012489\n","\tEpoch:49 [000/005 (0120/0755)]\tLoss Ss: 0.006681\n","\tEpoch:49 [000/005 (0140/0755)]\tLoss Ss: 0.008821\n","\tEpoch:49 [000/005 (0160/0755)]\tLoss Ss: 0.011852\n","\tEpoch:49 [000/005 (0180/0755)]\tLoss Ss: 0.012923\n","\tEpoch:49 [000/005 (0200/0755)]\tLoss Ss: 0.008464\n","\tEpoch:49 [000/005 (0220/0755)]\tLoss Ss: 0.008326\n","\tEpoch:49 [000/005 (0240/0755)]\tLoss Ss: 0.014691\n","\tEpoch:49 [000/005 (0260/0755)]\tLoss Ss: 0.012920\n","\tEpoch:49 [000/005 (0280/0755)]\tLoss Ss: 0.008609\n","\tEpoch:49 [000/005 (0300/0755)]\tLoss Ss: 0.014055\n","\tEpoch:49 [000/005 (0320/0755)]\tLoss Ss: 0.013130\n","\tEpoch:49 [000/005 (0340/0755)]\tLoss Ss: 0.009017\n","\tEpoch:49 [000/005 (0360/0755)]\tLoss Ss: 0.011098\n","\tEpoch:49 [000/005 (0380/0755)]\tLoss Ss: 0.009447\n","\tEpoch:49 [000/005 (0400/0755)]\tLoss Ss: 0.009487\n","\tEpoch:49 [000/005 (0420/0755)]\tLoss Ss: 0.010300\n","\tEpoch:49 [000/005 (0440/0755)]\tLoss Ss: 0.010283\n","\tEpoch:49 [000/005 (0460/0755)]\tLoss Ss: 0.013307\n","\tEpoch:49 [000/005 (0480/0755)]\tLoss Ss: 0.015522\n","\tEpoch:49 [000/005 (0500/0755)]\tLoss Ss: 0.008589\n","\tEpoch:49 [000/005 (0520/0755)]\tLoss Ss: 0.011929\n","\tEpoch:49 [000/005 (0540/0755)]\tLoss Ss: 0.010661\n","\tEpoch:49 [000/005 (0560/0755)]\tLoss Ss: 0.007191\n","\tEpoch:49 [000/005 (0580/0755)]\tLoss Ss: 0.007992\n","\tEpoch:49 [000/005 (0600/0755)]\tLoss Ss: 0.009853\n","\tEpoch:49 [000/005 (0620/0755)]\tLoss Ss: 0.009786\n","\tEpoch:49 [000/005 (0640/0755)]\tLoss Ss: 0.009592\n","\tEpoch:49 [000/005 (0660/0755)]\tLoss Ss: 0.011501\n","\tEpoch:49 [000/005 (0680/0755)]\tLoss Ss: 0.009682\n","\tEpoch:49 [000/005 (0700/0755)]\tLoss Ss: 0.009369\n","\tEpoch:49 [000/005 (0720/0755)]\tLoss Ss: 0.012545\n","\tEpoch:49 [000/005 (0740/0755)]\tLoss Ss: 0.009840\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tEpoch:49 [001/005 (0000/0588)]\tLoss Ss: 0.006539\n","\tEpoch:49 [001/005 (0020/0588)]\tLoss Ss: 0.004326\n","\tEpoch:49 [001/005 (0040/0588)]\tLoss Ss: 0.003615\n","\tEpoch:49 [001/005 (0060/0588)]\tLoss Ss: 0.003420\n","\tEpoch:49 [001/005 (0080/0588)]\tLoss Ss: 0.004258\n","\tEpoch:49 [001/005 (0100/0588)]\tLoss Ss: 0.003869\n","\tEpoch:49 [001/005 (0120/0588)]\tLoss Ss: 0.002810\n","\tEpoch:49 [001/005 (0140/0588)]\tLoss Ss: 0.004658\n","\tEpoch:49 [001/005 (0160/0588)]\tLoss Ss: 0.004617\n","\tEpoch:49 [001/005 (0180/0588)]\tLoss Ss: 0.003981\n","\tEpoch:49 [001/005 (0200/0588)]\tLoss Ss: 0.003749\n","\tEpoch:49 [001/005 (0220/0588)]\tLoss Ss: 0.003941\n","\tEpoch:49 [001/005 (0240/0588)]\tLoss Ss: 0.003358\n","\tEpoch:49 [001/005 (0260/0588)]\tLoss Ss: 0.006815\n","\tEpoch:49 [001/005 (0280/0588)]\tLoss Ss: 0.003154\n","\tEpoch:49 [001/005 (0300/0588)]\tLoss Ss: 0.004885\n","\tEpoch:49 [001/005 (0320/0588)]\tLoss Ss: 0.003313\n","\tEpoch:49 [001/005 (0340/0588)]\tLoss Ss: 0.004597\n","\tEpoch:49 [001/005 (0360/0588)]\tLoss Ss: 0.004247\n","\tEpoch:49 [001/005 (0380/0588)]\tLoss Ss: 0.005171\n","\tEpoch:49 [001/005 (0400/0588)]\tLoss Ss: 0.004075\n","\tEpoch:49 [001/005 (0420/0588)]\tLoss Ss: 0.004960\n","\tEpoch:49 [001/005 (0440/0588)]\tLoss Ss: 0.002584\n","\tEpoch:49 [001/005 (0460/0588)]\tLoss Ss: 0.003893\n","\tEpoch:49 [001/005 (0480/0588)]\tLoss Ss: 0.002464\n","\tEpoch:49 [001/005 (0500/0588)]\tLoss Ss: 0.001930\n","\tEpoch:49 [001/005 (0520/0588)]\tLoss Ss: 0.004608\n","\tEpoch:49 [001/005 (0540/0588)]\tLoss Ss: 0.003542\n","\tEpoch:49 [001/005 (0560/0588)]\tLoss Ss: 0.006116\n","\tEpoch:49 [001/005 (0580/0588)]\tLoss Ss: 0.002406\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tEpoch:49 [002/005 (0000/0614)]\tLoss Ss: 0.004964\n","\tEpoch:49 [002/005 (0020/0614)]\tLoss Ss: 0.004943\n","\tEpoch:49 [002/005 (0040/0614)]\tLoss Ss: 0.005240\n","\tEpoch:49 [002/005 (0060/0614)]\tLoss Ss: 0.002960\n","\tEpoch:49 [002/005 (0080/0614)]\tLoss Ss: 0.005661\n","\tEpoch:49 [002/005 (0100/0614)]\tLoss Ss: 0.004166\n","\tEpoch:49 [002/005 (0120/0614)]\tLoss Ss: 0.007201\n","\tEpoch:49 [002/005 (0140/0614)]\tLoss Ss: 0.005170\n","\tEpoch:49 [002/005 (0160/0614)]\tLoss Ss: 0.003114\n","\tEpoch:49 [002/005 (0180/0614)]\tLoss Ss: 0.005780\n","\tEpoch:49 [002/005 (0200/0614)]\tLoss Ss: 0.004462\n","\tEpoch:49 [002/005 (0220/0614)]\tLoss Ss: 0.005530\n","\tEpoch:49 [002/005 (0240/0614)]\tLoss Ss: 0.007161\n","\tEpoch:49 [002/005 (0260/0614)]\tLoss Ss: 0.005419\n","\tEpoch:49 [002/005 (0280/0614)]\tLoss Ss: 0.004980\n","\tEpoch:49 [002/005 (0300/0614)]\tLoss Ss: 0.005078\n","\tEpoch:49 [002/005 (0320/0614)]\tLoss Ss: 0.003618\n","\tEpoch:49 [002/005 (0340/0614)]\tLoss Ss: 0.004497\n","\tEpoch:49 [002/005 (0360/0614)]\tLoss Ss: 0.004643\n","\tEpoch:49 [002/005 (0380/0614)]\tLoss Ss: 0.003781\n","\tEpoch:49 [002/005 (0400/0614)]\tLoss Ss: 0.004119\n","\tEpoch:49 [002/005 (0420/0614)]\tLoss Ss: 0.003828\n","\tEpoch:49 [002/005 (0440/0614)]\tLoss Ss: 0.006466\n","\tEpoch:49 [002/005 (0460/0614)]\tLoss Ss: 0.004285\n","\tEpoch:49 [002/005 (0480/0614)]\tLoss Ss: 0.004117\n","\tEpoch:49 [002/005 (0500/0614)]\tLoss Ss: 0.003709\n","\tEpoch:49 [002/005 (0520/0614)]\tLoss Ss: 0.003515\n","\tEpoch:49 [002/005 (0540/0614)]\tLoss Ss: 0.003053\n","\tEpoch:49 [002/005 (0560/0614)]\tLoss Ss: 0.007108\n","\tEpoch:49 [002/005 (0580/0614)]\tLoss Ss: 0.004529\n","\tEpoch:49 [002/005 (0600/0614)]\tLoss Ss: 0.008071\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tEpoch:49 [003/005 (0000/0755)]\tLoss Ss: 0.015984\n","\tEpoch:49 [003/005 (0020/0755)]\tLoss Ss: 0.014046\n","\tEpoch:49 [003/005 (0040/0755)]\tLoss Ss: 0.017644\n","\tEpoch:49 [003/005 (0060/0755)]\tLoss Ss: 0.013057\n","\tEpoch:49 [003/005 (0080/0755)]\tLoss Ss: 0.013275\n","\tEpoch:49 [003/005 (0100/0755)]\tLoss Ss: 0.010568\n","\tEpoch:49 [003/005 (0120/0755)]\tLoss Ss: 0.015607\n","\tEpoch:49 [003/005 (0140/0755)]\tLoss Ss: 0.009360\n","\tEpoch:49 [003/005 (0160/0755)]\tLoss Ss: 0.010163\n","\tEpoch:49 [003/005 (0180/0755)]\tLoss Ss: 0.010598\n","\tEpoch:49 [003/005 (0200/0755)]\tLoss Ss: 0.016138\n","\tEpoch:49 [003/005 (0220/0755)]\tLoss Ss: 0.013732\n","\tEpoch:49 [003/005 (0240/0755)]\tLoss Ss: 0.015566\n","\tEpoch:49 [003/005 (0260/0755)]\tLoss Ss: 0.013273\n","\tEpoch:49 [003/005 (0280/0755)]\tLoss Ss: 0.010926\n","\tEpoch:49 [003/005 (0300/0755)]\tLoss Ss: 0.014022\n","\tEpoch:49 [003/005 (0320/0755)]\tLoss Ss: 0.018572\n","\tEpoch:49 [003/005 (0340/0755)]\tLoss Ss: 0.010538\n","\tEpoch:49 [003/005 (0360/0755)]\tLoss Ss: 0.011865\n","\tEpoch:49 [003/005 (0380/0755)]\tLoss Ss: 0.010021\n","\tEpoch:49 [003/005 (0400/0755)]\tLoss Ss: 0.010476\n","\tEpoch:49 [003/005 (0420/0755)]\tLoss Ss: 0.012912\n","\tEpoch:49 [003/005 (0440/0755)]\tLoss Ss: 0.012837\n","\tEpoch:49 [003/005 (0460/0755)]\tLoss Ss: 0.016115\n","\tEpoch:49 [003/005 (0480/0755)]\tLoss Ss: 0.007926\n","\tEpoch:49 [003/005 (0500/0755)]\tLoss Ss: 0.011299\n","\tEpoch:49 [003/005 (0520/0755)]\tLoss Ss: 0.013291\n","\tEpoch:49 [003/005 (0540/0755)]\tLoss Ss: 0.011654\n","\tEpoch:49 [003/005 (0560/0755)]\tLoss Ss: 0.013872\n","\tEpoch:49 [003/005 (0580/0755)]\tLoss Ss: 0.010419\n","\tEpoch:49 [003/005 (0600/0755)]\tLoss Ss: 0.013122\n","\tEpoch:49 [003/005 (0620/0755)]\tLoss Ss: 0.012129\n","\tEpoch:49 [003/005 (0640/0755)]\tLoss Ss: 0.009752\n","\tEpoch:49 [003/005 (0660/0755)]\tLoss Ss: 0.013475\n","\tEpoch:49 [003/005 (0680/0755)]\tLoss Ss: 0.011673\n","\tEpoch:49 [003/005 (0700/0755)]\tLoss Ss: 0.010164\n","\tEpoch:49 [003/005 (0720/0755)]\tLoss Ss: 0.010782\n","\tEpoch:49 [003/005 (0740/0755)]\tLoss Ss: 0.015391\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tEpoch:49 [004/005 (0000/0693)]\tLoss Ss: 0.009660\n","\tEpoch:49 [004/005 (0020/0693)]\tLoss Ss: 0.012625\n","\tEpoch:49 [004/005 (0040/0693)]\tLoss Ss: 0.010045\n","\tEpoch:49 [004/005 (0060/0693)]\tLoss Ss: 0.011908\n","\tEpoch:49 [004/005 (0080/0693)]\tLoss Ss: 0.011857\n","\tEpoch:49 [004/005 (0100/0693)]\tLoss Ss: 0.014987\n","\tEpoch:49 [004/005 (0120/0693)]\tLoss Ss: 0.009436\n","\tEpoch:49 [004/005 (0140/0693)]\tLoss Ss: 0.008630\n","\tEpoch:49 [004/005 (0160/0693)]\tLoss Ss: 0.011159\n","\tEpoch:49 [004/005 (0180/0693)]\tLoss Ss: 0.011317\n","\tEpoch:49 [004/005 (0200/0693)]\tLoss Ss: 0.009523\n","\tEpoch:49 [004/005 (0220/0693)]\tLoss Ss: 0.009728\n","\tEpoch:49 [004/005 (0240/0693)]\tLoss Ss: 0.011744\n","\tEpoch:49 [004/005 (0260/0693)]\tLoss Ss: 0.008598\n","\tEpoch:49 [004/005 (0280/0693)]\tLoss Ss: 0.006652\n","\tEpoch:49 [004/005 (0300/0693)]\tLoss Ss: 0.008837\n","\tEpoch:49 [004/005 (0320/0693)]\tLoss Ss: 0.009234\n","\tEpoch:49 [004/005 (0340/0693)]\tLoss Ss: 0.010745\n","\tEpoch:49 [004/005 (0360/0693)]\tLoss Ss: 0.007784\n","\tEpoch:49 [004/005 (0380/0693)]\tLoss Ss: 0.012378\n","\tEpoch:49 [004/005 (0400/0693)]\tLoss Ss: 0.009954\n","\tEpoch:49 [004/005 (0420/0693)]\tLoss Ss: 0.014151\n","\tEpoch:49 [004/005 (0440/0693)]\tLoss Ss: 0.009421\n","\tEpoch:49 [004/005 (0460/0693)]\tLoss Ss: 0.009441\n","\tEpoch:49 [004/005 (0480/0693)]\tLoss Ss: 0.007735\n","\tEpoch:49 [004/005 (0500/0693)]\tLoss Ss: 0.011055\n","\tEpoch:49 [004/005 (0520/0693)]\tLoss Ss: 0.006868\n","\tEpoch:49 [004/005 (0540/0693)]\tLoss Ss: 0.010073\n","\tEpoch:49 [004/005 (0560/0693)]\tLoss Ss: 0.011655\n","\tEpoch:49 [004/005 (0580/0693)]\tLoss Ss: 0.010018\n","\tEpoch:49 [004/005 (0600/0693)]\tLoss Ss: 0.010806\n","\tEpoch:49 [004/005 (0620/0693)]\tLoss Ss: 0.008343\n","\tEpoch:49 [004/005 (0640/0693)]\tLoss Ss: 0.008563\n","\tEpoch:49 [004/005 (0660/0693)]\tLoss Ss: 0.007175\n","\tEpoch:49 [004/005 (0680/0693)]\tLoss Ss: 0.012173\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tEpoch:49 [005/005 (0000/0693)]\tLoss Ss: 0.013879\n","\tEpoch:49 [005/005 (0020/0693)]\tLoss Ss: 0.010179\n","\tEpoch:49 [005/005 (0040/0693)]\tLoss Ss: 0.012731\n","\tEpoch:49 [005/005 (0060/0693)]\tLoss Ss: 0.014063\n","\tEpoch:49 [005/005 (0080/0693)]\tLoss Ss: 0.010710\n","\tEpoch:49 [005/005 (0100/0693)]\tLoss Ss: 0.014388\n","\tEpoch:49 [005/005 (0120/0693)]\tLoss Ss: 0.015021\n","\tEpoch:49 [005/005 (0140/0693)]\tLoss Ss: 0.013368\n","\tEpoch:49 [005/005 (0160/0693)]\tLoss Ss: 0.011932\n","\tEpoch:49 [005/005 (0180/0693)]\tLoss Ss: 0.012877\n","\tEpoch:49 [005/005 (0200/0693)]\tLoss Ss: 0.010863\n","\tEpoch:49 [005/005 (0220/0693)]\tLoss Ss: 0.015836\n","\tEpoch:49 [005/005 (0240/0693)]\tLoss Ss: 0.012228\n","\tEpoch:49 [005/005 (0260/0693)]\tLoss Ss: 0.008397\n","\tEpoch:49 [005/005 (0280/0693)]\tLoss Ss: 0.014296\n","\tEpoch:49 [005/005 (0300/0693)]\tLoss Ss: 0.014630\n","\tEpoch:49 [005/005 (0320/0693)]\tLoss Ss: 0.011612\n","\tEpoch:49 [005/005 (0340/0693)]\tLoss Ss: 0.013064\n","\tEpoch:49 [005/005 (0360/0693)]\tLoss Ss: 0.011138\n","\tEpoch:49 [005/005 (0380/0693)]\tLoss Ss: 0.009498\n","\tEpoch:49 [005/005 (0400/0693)]\tLoss Ss: 0.009588\n","\tEpoch:49 [005/005 (0420/0693)]\tLoss Ss: 0.012669\n","\tEpoch:49 [005/005 (0440/0693)]\tLoss Ss: 0.011342\n","\tEpoch:49 [005/005 (0460/0693)]\tLoss Ss: 0.013472\n","\tEpoch:49 [005/005 (0480/0693)]\tLoss Ss: 0.007193\n","\tEpoch:49 [005/005 (0500/0693)]\tLoss Ss: 0.009325\n","\tEpoch:49 [005/005 (0520/0693)]\tLoss Ss: 0.009712\n","\tEpoch:49 [005/005 (0540/0693)]\tLoss Ss: 0.013214\n","\tEpoch:49 [005/005 (0560/0693)]\tLoss Ss: 0.010706\n","\tEpoch:49 [005/005 (0580/0693)]\tLoss Ss: 0.008166\n","\tEpoch:49 [005/005 (0600/0693)]\tLoss Ss: 0.009858\n","\tEpoch:49 [005/005 (0620/0693)]\tLoss Ss: 0.010849\n","\tEpoch:49 [005/005 (0640/0693)]\tLoss Ss: 0.010590\n","\tEpoch:49 [005/005 (0660/0693)]\tLoss Ss: 0.009126\n","\tEpoch:49 [005/005 (0680/0693)]\tLoss Ss: 0.010237\n","Now train the rotated image\n","new_3_d (1).nii\n","new_3_m (1).nii.gz\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:49 [000/005 (0000/0755)]\tLoss Ss: 0.065964\n","\tRotated_Epoch:49 [000/005 (0020/0755)]\tLoss Ss: 0.095320\n","\tRotated_Epoch:49 [000/005 (0040/0755)]\tLoss Ss: 0.102612\n","\tRotated_Epoch:49 [000/005 (0060/0755)]\tLoss Ss: 0.053401\n","\tRotated_Epoch:49 [000/005 (0080/0755)]\tLoss Ss: 0.037722\n","\tRotated_Epoch:49 [000/005 (0100/0755)]\tLoss Ss: 0.062432\n","\tRotated_Epoch:49 [000/005 (0120/0755)]\tLoss Ss: 0.038771\n","\tRotated_Epoch:49 [000/005 (0140/0755)]\tLoss Ss: 0.019244\n","\tRotated_Epoch:49 [000/005 (0160/0755)]\tLoss Ss: 0.040291\n","\tRotated_Epoch:49 [000/005 (0180/0755)]\tLoss Ss: 0.026305\n","\tRotated_Epoch:49 [000/005 (0200/0755)]\tLoss Ss: 0.037109\n","\tRotated_Epoch:49 [000/005 (0220/0755)]\tLoss Ss: 0.024852\n","\tRotated_Epoch:49 [000/005 (0240/0755)]\tLoss Ss: 0.031485\n","\tRotated_Epoch:49 [000/005 (0260/0755)]\tLoss Ss: 0.028269\n","\tRotated_Epoch:49 [000/005 (0280/0755)]\tLoss Ss: 0.022714\n","\tRotated_Epoch:49 [000/005 (0300/0755)]\tLoss Ss: 0.024202\n","\tRotated_Epoch:49 [000/005 (0320/0755)]\tLoss Ss: 0.023552\n","\tRotated_Epoch:49 [000/005 (0340/0755)]\tLoss Ss: 0.026331\n","\tRotated_Epoch:49 [000/005 (0360/0755)]\tLoss Ss: 0.027785\n","\tRotated_Epoch:49 [000/005 (0380/0755)]\tLoss Ss: 0.026172\n","\tRotated_Epoch:49 [000/005 (0400/0755)]\tLoss Ss: 0.019067\n","\tRotated_Epoch:49 [000/005 (0420/0755)]\tLoss Ss: 0.018309\n","\tRotated_Epoch:49 [000/005 (0440/0755)]\tLoss Ss: 0.025205\n","\tRotated_Epoch:49 [000/005 (0460/0755)]\tLoss Ss: 0.026298\n","\tRotated_Epoch:49 [000/005 (0480/0755)]\tLoss Ss: 0.026600\n","\tRotated_Epoch:49 [000/005 (0500/0755)]\tLoss Ss: 0.019085\n","\tRotated_Epoch:49 [000/005 (0520/0755)]\tLoss Ss: 0.027096\n","\tRotated_Epoch:49 [000/005 (0540/0755)]\tLoss Ss: 0.017672\n","\tRotated_Epoch:49 [000/005 (0560/0755)]\tLoss Ss: 0.021654\n","\tRotated_Epoch:49 [000/005 (0580/0755)]\tLoss Ss: 0.020649\n","\tRotated_Epoch:49 [000/005 (0600/0755)]\tLoss Ss: 0.021812\n","\tRotated_Epoch:49 [000/005 (0620/0755)]\tLoss Ss: 0.019328\n","\tRotated_Epoch:49 [000/005 (0640/0755)]\tLoss Ss: 0.018011\n","\tRotated_Epoch:49 [000/005 (0660/0755)]\tLoss Ss: 0.023004\n","\tRotated_Epoch:49 [000/005 (0680/0755)]\tLoss Ss: 0.017735\n","\tRotated_Epoch:49 [000/005 (0700/0755)]\tLoss Ss: 0.016339\n","\tRotated_Epoch:49 [000/005 (0720/0755)]\tLoss Ss: 0.020599\n","\tRotated_Epoch:49 [000/005 (0740/0755)]\tLoss Ss: 0.033707\n","new_2_1_d.nii\n","new_2_1_m.nii\n","\tRotated_Epoch:49 [001/005 (0000/0693)]\tLoss Ss: 0.018774\n","\tRotated_Epoch:49 [001/005 (0020/0693)]\tLoss Ss: 0.014125\n","\tRotated_Epoch:49 [001/005 (0040/0693)]\tLoss Ss: 0.015001\n","\tRotated_Epoch:49 [001/005 (0060/0693)]\tLoss Ss: 0.016215\n","\tRotated_Epoch:49 [001/005 (0080/0693)]\tLoss Ss: 0.016971\n","\tRotated_Epoch:49 [001/005 (0100/0693)]\tLoss Ss: 0.007306\n","\tRotated_Epoch:49 [001/005 (0120/0693)]\tLoss Ss: 0.014630\n","\tRotated_Epoch:49 [001/005 (0140/0693)]\tLoss Ss: 0.015613\n","\tRotated_Epoch:49 [001/005 (0160/0693)]\tLoss Ss: 0.012005\n","\tRotated_Epoch:49 [001/005 (0180/0693)]\tLoss Ss: 0.012945\n","\tRotated_Epoch:49 [001/005 (0200/0693)]\tLoss Ss: 0.013680\n","\tRotated_Epoch:49 [001/005 (0220/0693)]\tLoss Ss: 0.011878\n","\tRotated_Epoch:49 [001/005 (0240/0693)]\tLoss Ss: 0.014180\n","\tRotated_Epoch:49 [001/005 (0260/0693)]\tLoss Ss: 0.010209\n","\tRotated_Epoch:49 [001/005 (0280/0693)]\tLoss Ss: 0.010756\n","\tRotated_Epoch:49 [001/005 (0300/0693)]\tLoss Ss: 0.011041\n","\tRotated_Epoch:49 [001/005 (0320/0693)]\tLoss Ss: 0.014819\n","\tRotated_Epoch:49 [001/005 (0340/0693)]\tLoss Ss: 0.015799\n","\tRotated_Epoch:49 [001/005 (0360/0693)]\tLoss Ss: 0.012675\n","\tRotated_Epoch:49 [001/005 (0380/0693)]\tLoss Ss: 0.013283\n","\tRotated_Epoch:49 [001/005 (0400/0693)]\tLoss Ss: 0.010812\n","\tRotated_Epoch:49 [001/005 (0420/0693)]\tLoss Ss: 0.012775\n","\tRotated_Epoch:49 [001/005 (0440/0693)]\tLoss Ss: 0.012346\n","\tRotated_Epoch:49 [001/005 (0460/0693)]\tLoss Ss: 0.014313\n","\tRotated_Epoch:49 [001/005 (0480/0693)]\tLoss Ss: 0.013988\n","\tRotated_Epoch:49 [001/005 (0500/0693)]\tLoss Ss: 0.014938\n","\tRotated_Epoch:49 [001/005 (0520/0693)]\tLoss Ss: 0.012400\n","\tRotated_Epoch:49 [001/005 (0540/0693)]\tLoss Ss: 0.013026\n","\tRotated_Epoch:49 [001/005 (0560/0693)]\tLoss Ss: 0.012509\n","\tRotated_Epoch:49 [001/005 (0580/0693)]\tLoss Ss: 0.015255\n","\tRotated_Epoch:49 [001/005 (0600/0693)]\tLoss Ss: 0.013154\n","\tRotated_Epoch:49 [001/005 (0620/0693)]\tLoss Ss: 0.012278\n","\tRotated_Epoch:49 [001/005 (0640/0693)]\tLoss Ss: 0.011831\n","\tRotated_Epoch:49 [001/005 (0660/0693)]\tLoss Ss: 0.011652\n","\tRotated_Epoch:49 [001/005 (0680/0693)]\tLoss Ss: 0.010738\n","new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["\tRotated_Epoch:49 [002/005 (0000/0755)]\tLoss Ss: 0.042758\n","\tRotated_Epoch:49 [002/005 (0020/0755)]\tLoss Ss: 0.040563\n","\tRotated_Epoch:49 [002/005 (0040/0755)]\tLoss Ss: 0.037585\n","\tRotated_Epoch:49 [002/005 (0060/0755)]\tLoss Ss: 0.024117\n","\tRotated_Epoch:49 [002/005 (0080/0755)]\tLoss Ss: 0.015487\n","\tRotated_Epoch:49 [002/005 (0100/0755)]\tLoss Ss: 0.028764\n","\tRotated_Epoch:49 [002/005 (0120/0755)]\tLoss Ss: 0.016963\n","\tRotated_Epoch:49 [002/005 (0140/0755)]\tLoss Ss: 0.023147\n","\tRotated_Epoch:49 [002/005 (0160/0755)]\tLoss Ss: 0.019954\n","\tRotated_Epoch:49 [002/005 (0180/0755)]\tLoss Ss: 0.014812\n","\tRotated_Epoch:49 [002/005 (0200/0755)]\tLoss Ss: 0.011216\n","\tRotated_Epoch:49 [002/005 (0220/0755)]\tLoss Ss: 0.013036\n","\tRotated_Epoch:49 [002/005 (0240/0755)]\tLoss Ss: 0.013500\n","\tRotated_Epoch:49 [002/005 (0260/0755)]\tLoss Ss: 0.011976\n","\tRotated_Epoch:49 [002/005 (0280/0755)]\tLoss Ss: 0.012137\n","\tRotated_Epoch:49 [002/005 (0300/0755)]\tLoss Ss: 0.019762\n","\tRotated_Epoch:49 [002/005 (0320/0755)]\tLoss Ss: 0.010902\n","\tRotated_Epoch:49 [002/005 (0340/0755)]\tLoss Ss: 0.013321\n","\tRotated_Epoch:49 [002/005 (0360/0755)]\tLoss Ss: 0.011908\n","\tRotated_Epoch:49 [002/005 (0380/0755)]\tLoss Ss: 0.013432\n","\tRotated_Epoch:49 [002/005 (0400/0755)]\tLoss Ss: 0.017669\n","\tRotated_Epoch:49 [002/005 (0420/0755)]\tLoss Ss: 0.010007\n","\tRotated_Epoch:49 [002/005 (0440/0755)]\tLoss Ss: 0.013069\n","\tRotated_Epoch:49 [002/005 (0460/0755)]\tLoss Ss: 0.010119\n","\tRotated_Epoch:49 [002/005 (0480/0755)]\tLoss Ss: 0.009298\n","\tRotated_Epoch:49 [002/005 (0500/0755)]\tLoss Ss: 0.013021\n","\tRotated_Epoch:49 [002/005 (0520/0755)]\tLoss Ss: 0.014349\n","\tRotated_Epoch:49 [002/005 (0540/0755)]\tLoss Ss: 0.007411\n","\tRotated_Epoch:49 [002/005 (0560/0755)]\tLoss Ss: 0.010441\n","\tRotated_Epoch:49 [002/005 (0580/0755)]\tLoss Ss: 0.017570\n","\tRotated_Epoch:49 [002/005 (0600/0755)]\tLoss Ss: 0.011168\n","\tRotated_Epoch:49 [002/005 (0620/0755)]\tLoss Ss: 0.006613\n","\tRotated_Epoch:49 [002/005 (0640/0755)]\tLoss Ss: 0.015726\n","\tRotated_Epoch:49 [002/005 (0660/0755)]\tLoss Ss: 0.010628\n","\tRotated_Epoch:49 [002/005 (0680/0755)]\tLoss Ss: 0.011227\n","\tRotated_Epoch:49 [002/005 (0700/0755)]\tLoss Ss: 0.005668\n","\tRotated_Epoch:49 [002/005 (0720/0755)]\tLoss Ss: 0.015965\n","\tRotated_Epoch:49 [002/005 (0740/0755)]\tLoss Ss: 0.013904\n","new_4_6_d_163930.nii\n","new_4_6_m_163930.nii.gz\n","\tRotated_Epoch:49 [003/005 (0000/0614)]\tLoss Ss: 0.009058\n","\tRotated_Epoch:49 [003/005 (0020/0614)]\tLoss Ss: 0.006124\n","\tRotated_Epoch:49 [003/005 (0040/0614)]\tLoss Ss: 0.007873\n","\tRotated_Epoch:49 [003/005 (0060/0614)]\tLoss Ss: 0.004544\n","\tRotated_Epoch:49 [003/005 (0080/0614)]\tLoss Ss: 0.005341\n","\tRotated_Epoch:49 [003/005 (0100/0614)]\tLoss Ss: 0.004610\n","\tRotated_Epoch:49 [003/005 (0120/0614)]\tLoss Ss: 0.007462\n","\tRotated_Epoch:49 [003/005 (0140/0614)]\tLoss Ss: 0.005917\n","\tRotated_Epoch:49 [003/005 (0160/0614)]\tLoss Ss: 0.007198\n","\tRotated_Epoch:49 [003/005 (0180/0614)]\tLoss Ss: 0.006741\n","\tRotated_Epoch:49 [003/005 (0200/0614)]\tLoss Ss: 0.005493\n","\tRotated_Epoch:49 [003/005 (0220/0614)]\tLoss Ss: 0.003627\n","\tRotated_Epoch:49 [003/005 (0240/0614)]\tLoss Ss: 0.004635\n","\tRotated_Epoch:49 [003/005 (0260/0614)]\tLoss Ss: 0.005943\n","\tRotated_Epoch:49 [003/005 (0280/0614)]\tLoss Ss: 0.008729\n","\tRotated_Epoch:49 [003/005 (0300/0614)]\tLoss Ss: 0.003980\n","\tRotated_Epoch:49 [003/005 (0320/0614)]\tLoss Ss: 0.004583\n","\tRotated_Epoch:49 [003/005 (0340/0614)]\tLoss Ss: 0.004474\n","\tRotated_Epoch:49 [003/005 (0360/0614)]\tLoss Ss: 0.005796\n","\tRotated_Epoch:49 [003/005 (0380/0614)]\tLoss Ss: 0.006361\n","\tRotated_Epoch:49 [003/005 (0400/0614)]\tLoss Ss: 0.005846\n","\tRotated_Epoch:49 [003/005 (0420/0614)]\tLoss Ss: 0.006070\n","\tRotated_Epoch:49 [003/005 (0440/0614)]\tLoss Ss: 0.005334\n","\tRotated_Epoch:49 [003/005 (0460/0614)]\tLoss Ss: 0.007121\n","\tRotated_Epoch:49 [003/005 (0480/0614)]\tLoss Ss: 0.006171\n","\tRotated_Epoch:49 [003/005 (0500/0614)]\tLoss Ss: 0.004828\n","\tRotated_Epoch:49 [003/005 (0520/0614)]\tLoss Ss: 0.002618\n","\tRotated_Epoch:49 [003/005 (0540/0614)]\tLoss Ss: 0.003315\n","\tRotated_Epoch:49 [003/005 (0560/0614)]\tLoss Ss: 0.004442\n","\tRotated_Epoch:49 [003/005 (0580/0614)]\tLoss Ss: 0.006330\n","\tRotated_Epoch:49 [003/005 (0600/0614)]\tLoss Ss: 0.009478\n","new_1_1_d.nii\n","new_1_1_m.nii\n","\tRotated_Epoch:49 [004/005 (0000/0693)]\tLoss Ss: 0.007773\n","\tRotated_Epoch:49 [004/005 (0020/0693)]\tLoss Ss: 0.014405\n","\tRotated_Epoch:49 [004/005 (0040/0693)]\tLoss Ss: 0.015130\n","\tRotated_Epoch:49 [004/005 (0060/0693)]\tLoss Ss: 0.010081\n","\tRotated_Epoch:49 [004/005 (0080/0693)]\tLoss Ss: 0.009310\n","\tRotated_Epoch:49 [004/005 (0100/0693)]\tLoss Ss: 0.013991\n","\tRotated_Epoch:49 [004/005 (0120/0693)]\tLoss Ss: 0.010870\n","\tRotated_Epoch:49 [004/005 (0140/0693)]\tLoss Ss: 0.011678\n","\tRotated_Epoch:49 [004/005 (0160/0693)]\tLoss Ss: 0.010628\n","\tRotated_Epoch:49 [004/005 (0180/0693)]\tLoss Ss: 0.010085\n","\tRotated_Epoch:49 [004/005 (0200/0693)]\tLoss Ss: 0.012458\n","\tRotated_Epoch:49 [004/005 (0220/0693)]\tLoss Ss: 0.012241\n","\tRotated_Epoch:49 [004/005 (0240/0693)]\tLoss Ss: 0.008886\n","\tRotated_Epoch:49 [004/005 (0260/0693)]\tLoss Ss: 0.009632\n","\tRotated_Epoch:49 [004/005 (0280/0693)]\tLoss Ss: 0.012643\n","\tRotated_Epoch:49 [004/005 (0300/0693)]\tLoss Ss: 0.010764\n","\tRotated_Epoch:49 [004/005 (0320/0693)]\tLoss Ss: 0.011994\n","\tRotated_Epoch:49 [004/005 (0340/0693)]\tLoss Ss: 0.012346\n","\tRotated_Epoch:49 [004/005 (0360/0693)]\tLoss Ss: 0.011772\n","\tRotated_Epoch:49 [004/005 (0380/0693)]\tLoss Ss: 0.013262\n","\tRotated_Epoch:49 [004/005 (0400/0693)]\tLoss Ss: 0.009829\n","\tRotated_Epoch:49 [004/005 (0420/0693)]\tLoss Ss: 0.013019\n","\tRotated_Epoch:49 [004/005 (0440/0693)]\tLoss Ss: 0.006784\n","\tRotated_Epoch:49 [004/005 (0460/0693)]\tLoss Ss: 0.011788\n","\tRotated_Epoch:49 [004/005 (0480/0693)]\tLoss Ss: 0.010321\n","\tRotated_Epoch:49 [004/005 (0500/0693)]\tLoss Ss: 0.012531\n","\tRotated_Epoch:49 [004/005 (0520/0693)]\tLoss Ss: 0.012111\n","\tRotated_Epoch:49 [004/005 (0540/0693)]\tLoss Ss: 0.011951\n","\tRotated_Epoch:49 [004/005 (0560/0693)]\tLoss Ss: 0.008886\n","\tRotated_Epoch:49 [004/005 (0580/0693)]\tLoss Ss: 0.009272\n","\tRotated_Epoch:49 [004/005 (0600/0693)]\tLoss Ss: 0.013764\n","\tRotated_Epoch:49 [004/005 (0620/0693)]\tLoss Ss: 0.009665\n","\tRotated_Epoch:49 [004/005 (0640/0693)]\tLoss Ss: 0.011649\n","\tRotated_Epoch:49 [004/005 (0660/0693)]\tLoss Ss: 0.012090\n","\tRotated_Epoch:49 [004/005 (0680/0693)]\tLoss Ss: 0.012392\n","new_4_12_d_183819.nii\n","new_4_12_m_183819.nii.gz\n","\tRotated_Epoch:49 [005/005 (0000/0588)]\tLoss Ss: 0.078074\n","\tRotated_Epoch:49 [005/005 (0020/0588)]\tLoss Ss: 0.064118\n","\tRotated_Epoch:49 [005/005 (0040/0588)]\tLoss Ss: 0.073339\n","\tRotated_Epoch:49 [005/005 (0060/0588)]\tLoss Ss: 0.054424\n","\tRotated_Epoch:49 [005/005 (0080/0588)]\tLoss Ss: 0.080739\n","\tRotated_Epoch:49 [005/005 (0100/0588)]\tLoss Ss: 0.046377\n","\tRotated_Epoch:49 [005/005 (0120/0588)]\tLoss Ss: 0.058588\n","\tRotated_Epoch:49 [005/005 (0140/0588)]\tLoss Ss: 0.057700\n","\tRotated_Epoch:49 [005/005 (0160/0588)]\tLoss Ss: 0.077832\n","\tRotated_Epoch:49 [005/005 (0180/0588)]\tLoss Ss: 0.038136\n","\tRotated_Epoch:49 [005/005 (0200/0588)]\tLoss Ss: 0.055785\n","\tRotated_Epoch:49 [005/005 (0220/0588)]\tLoss Ss: 0.075408\n","\tRotated_Epoch:49 [005/005 (0240/0588)]\tLoss Ss: 0.036245\n","\tRotated_Epoch:49 [005/005 (0260/0588)]\tLoss Ss: 0.050410\n","\tRotated_Epoch:49 [005/005 (0280/0588)]\tLoss Ss: 0.061520\n","\tRotated_Epoch:49 [005/005 (0300/0588)]\tLoss Ss: 0.062406\n","\tRotated_Epoch:49 [005/005 (0320/0588)]\tLoss Ss: 0.053202\n","\tRotated_Epoch:49 [005/005 (0340/0588)]\tLoss Ss: 0.051529\n","\tRotated_Epoch:49 [005/005 (0360/0588)]\tLoss Ss: 0.031530\n","\tRotated_Epoch:49 [005/005 (0380/0588)]\tLoss Ss: 0.049550\n","\tRotated_Epoch:49 [005/005 (0400/0588)]\tLoss Ss: 0.069119\n","\tRotated_Epoch:49 [005/005 (0420/0588)]\tLoss Ss: 0.042362\n","\tRotated_Epoch:49 [005/005 (0440/0588)]\tLoss Ss: 0.052427\n","\tRotated_Epoch:49 [005/005 (0460/0588)]\tLoss Ss: 0.058964\n","\tRotated_Epoch:49 [005/005 (0480/0588)]\tLoss Ss: 0.059300\n","\tRotated_Epoch:49 [005/005 (0500/0588)]\tLoss Ss: 0.058029\n","\tRotated_Epoch:49 [005/005 (0520/0588)]\tLoss Ss: 0.048875\n","\tRotated_Epoch:49 [005/005 (0540/0588)]\tLoss Ss: 0.043490\n","\tRotated_Epoch:49 [005/005 (0560/0588)]\tLoss Ss: 0.034668\n","\tRotated_Epoch:49 [005/005 (0580/0588)]\tLoss Ss: 0.056635\n","new_1_6_d.nii\n","new_1_6_m.nii\n","new_1_7_d.nii\n","new_1_7_m.nii\n","new_2_5_d.nii\n","new_2_5_m.nii\n","new_2_6_d.nii\n","new_2_6_m.nii\n","new_2_7_d.nii\n","new_2_7_m.nii\n","new_3_d (2).nii\n","new_3_m (2).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (3).nii\n","new_3_m (3).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (7).nii\n","new_3_m (7).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (8).nii\n","new_3_m (8).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (9).nii\n","new_3_m (9).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_4_10_d_163526.nii\n","new_4_10_m_163526.nii.gz\n","new_4_11_d_084336.nii\n","new_4_11_m_084336.nii.gz\n","new_4_7_d_184422.nii\n","new_4_7_m_184422.nii.gz\n","new_4_8_d_143412.nii\n","new_4_8_m_143412.nii.gz\n","new_4_9_d_155347.nii\n","new_4_9_m_155347.nii.gz\n","\tEpoch: 49; Dice: 0.9661 +/- 0.0051; Loss: 6.4670\n"]}],"source":["model_train = Train(args=args)\n","Dict_DL = model_train.train()"]},{"cell_type":"markdown","metadata":{"id":"w6SP1ELrbLPR"},"source":["## 2) Test part for the model"]},{"cell_type":"markdown","metadata":{"id":"EMaagD5QbLPR"},"source":["### *Notice*: Here we need to reset the parameters"]},{"cell_type":"markdown","metadata":{"id":"daxq1BFAbLPS"},"source":["### Part 1: Reset the parameters\n","|Parameters|Meaning|Default value|Whether the input is required|\n","| :----: | :------: | :----: | :----:|\n","|test_t1w|Test T1w Directory|None|Yes|\n","|test_msk|Test Mask Directory|None|Yes|\n","|out_dir|Output Directory|None|Yes|\n","|test_model|Test Model Directory or Filename|None|Yes|\n","|conv_block|Num of UNet Blocks|5|Optional|\n","|input_slice|Num of Slices for Model Input|3|Optional|\n","|kernel_root|Num of the Root of Kernel|16|Optional|\n","|rescale_dim|Dimension to Rescale|256|Optional|"]},{"cell_type":"markdown","metadata":{"id":"6nb0328bbLPT"},"source":["#### Some parameters that can be changed individually\n","|Parameters|Function to Change it|\n","| :----: | :------: |\n","|test_t1w|change_test_t1w|\n","|test_msk|change_test_msk|\n","|out_dir|change_out_dir|\n","|test_model|change_test_model|"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"8tiu0gq7bLPT","executionInfo":{"status":"ok","timestamp":1647737693541,"user_tz":-480,"elapsed":444,"user":{"displayName":"胡浩宇","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06402011815971394657"}}},"outputs":[],"source":["args_test = Args_test(test_t1w=\"/content/drive/MyDrive/Test/Test_data_2\"\n","           , test_msk=\"/content/drive/MyDrive/Test/Test_msk\"\n","           , out_dir=\"/content/drive/MyDrive/Train_Val_Test_model_3/Train_data.zip (Unzipped Files)/Out_Dir/data_noba_model\"\n","           , test_model=\"/content/drive/MyDrive/Train_Val_Test_model_3/Train_data.zip (Unzipped Files)/Out_Dir/model_noba_model/model-49-epoch.model\")"]},{"cell_type":"markdown","metadata":{"id":"J47OBV8TbLPU"},"source":["### Part 2: Check the Orientation & Resample the Image"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":246112,"status":"ok","timestamp":1647737025517,"user":{"displayName":"胡浩宇","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06402011815971394657"},"user_tz":-480},"id":"WIA-frSCbLPV","outputId":"b043843b-820b-4f35-d5dc-f44491b0b137","scrolled":true},"outputs":[{"output_type":"stream","name":"stdout","text":["image : new_3_d (10).nii\n","image : new_3_d (11).nii\n","image : new_3_d (12).nii\n","image : new_3_d (13).nii\n","image : new_3_d (14).nii\n","image : new_3_d (15).nii\n","image : new_3_d (17).nii\n","image : new_3_d (16).nii\n","image : new_3_d (18).nii\n","image : new_3_d (19).nii\n","image : new_3_d (2).nii\n","image : new_3_d (20).nii\n","image : new_3_d (21).nii\n","image : new_3_d (22).nii\n","image : new_3_d (23).nii\n","image : new_3_d (24).nii\n","image : new_3_d (25).nii\n","image : new_3_d (26).nii\n","image : new_3_d (27).nii\n","image : new_3_d (28).nii\n","image : new_3_d (29).nii\n","image : new_3_d (3).nii\n","image : new_3_d (4).nii\n","image : new_3_d (5).nii\n","image : new_3_d (6).nii\n","image : new_4_1_d_191046.nii\n","image : new_4_2_d_222342.nii\n","image : new_4_3_d_084336.nii\n","image : new_4_4_d_112405.nii\n","image : new_4_5_d_140141.nii\n","image : new_1_10_d.nii\n","image : new_1_12_d.nii\n","image : new_1_2_d.nii\n","image : new_1_3_d.nii\n","image : new_1_4_d.nii\n","image : new_1_5_d.nii\n","image : new_2_11_d.nii\n","image : new_2_10_d.nii\n","image : new_2_4_d.nii\n","image : new_2_3_d.nii\n","image : new_2_2_d.nii\n","image : new_2_9_d.nii\n","image : new_2_8_d.nii\n","mask : new_3_m (10).nii\n","mask : new_3_m (11).nii\n","mask : new_3_m (12).nii\n","mask : new_3_m (13).nii\n","mask : new_3_m (14).nii\n","mask : new_3_m (15).nii\n","mask : new_3_m (16).nii\n","mask : new_3_m (17).nii\n","mask : new_3_m (18).nii\n","mask : new_3_m (19).nii\n","mask : new_3_m (2).nii\n","mask : new_3_m (20).nii\n","mask : new_3_m (21).nii\n","mask : new_3_m (22).nii\n","mask : new_3_m (23).nii\n","mask : new_3_m (24).nii\n","mask : new_3_m (25).nii\n","mask : new_3_m (26).nii\n","mask : new_3_m (27).nii\n","mask : new_3_m (28).nii\n","mask : new_3_m (29).nii\n","mask : new_3_m (3).nii\n","mask : new_4_1_m_191046.nii.gz\n","mask : new_3_m (4).nii\n","mask : new_3_m (5).nii\n","mask : new_3_m (6).nii\n","mask : new_4_2_m_222342.nii.gz\n","mask : new_4_4_m_112405.nii.gz\n","mask : new_4_3_m_084336.nii.gz\n","mask : new_4_5_m_140141.nii.gz\n","mask : new_1_10_m.nii\n","mask : new_1_12_m.nii\n","mask : new_1_2_m.nii\n","mask : new_1_3_m.nii\n","mask : new_1_5_m.nii\n","mask : new_1_4_m.nii\n","mask : new_2_10_m.nii\n","mask : new_2_3_m.nii\n","mask : new_2_2_m.nii\n","mask : new_2_11_m.nii\n","mask : new_2_8_m.nii\n","mask : new_2_4_m.nii\n","mask : new_2_9_m.nii\n"]}],"source":["ort_test = orientation(t1_path = args_test.test_t1w)\n","if ort_test.check() == False:\n","    args_test.test_t1w = ort_test.orient()\n","\n","ort_test.path = args_test.test_msk\n","if ort_test.check() == False:\n","    args_test.test_msk = ort_test.orient(mode='mask')"]},{"cell_type":"markdown","metadata":{"id":"hofqKlQCbLPV"},"source":["### Part 3: Test the Model"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":710961,"status":"ok","timestamp":1647738406485,"user":{"displayName":"胡浩宇","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06402011815971394657"},"user_tz":-480},"id":"wmjeDLHibLPW","outputId":"5313f60c-269f-4aa3-c85a-2cbb9eae2fca","scrolled":true},"outputs":[{"output_type":"stream","name":"stdout","text":["===================================Testing Model====================================\n","new_1_10_d.nii\n","new_1_10_m.nii\n","new_1_12_d.nii\n","new_1_12_m.nii\n","new_1_2_d.nii\n","new_1_2_m.nii\n","new_1_3_d.nii\n","new_1_3_m.nii\n","new_1_4_d.nii\n","new_1_4_m.nii\n","new_1_5_d.nii\n","new_1_5_m.nii\n","new_2_10_d.nii\n","new_2_10_m.nii\n","new_2_11_d.nii\n","new_2_11_m.nii\n","new_2_2_d.nii\n","new_2_2_m.nii\n","new_2_3_d.nii\n","new_2_3_m.nii\n","new_2_4_d.nii\n","new_2_4_m.nii\n","new_2_8_d.nii\n","new_2_8_m.nii\n","new_2_9_d.nii\n","new_2_9_m.nii\n","new_3_d (10).nii\n","new_3_m (10).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (11).nii\n","new_3_m (11).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (12).nii\n","new_3_m (12).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (13).nii\n","new_3_m (13).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (14).nii\n","new_3_m (14).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (15).nii\n","new_3_m (15).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (16).nii\n","new_3_m (16).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (17).nii\n","new_3_m (17).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (18).nii\n","new_3_m (18).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (19).nii\n","new_3_m (19).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (2).nii\n","new_3_m (2).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (20).nii\n","new_3_m (20).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (21).nii\n","new_3_m (21).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (22).nii\n","new_3_m (22).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (23).nii\n","new_3_m (23).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (24).nii\n","new_3_m (24).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (25).nii\n","new_3_m (25).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (26).nii\n","new_3_m (26).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (27).nii\n","new_3_m (27).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (28).nii\n","new_3_m (28).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (29).nii\n","new_3_m (29).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (3).nii\n","new_3_m (3).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (4).nii\n","new_3_m (4).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (5).nii\n","new_3_m (5).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_3_d (6).nii\n","new_3_m (6).nii\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n","  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"]},{"output_type":"stream","name":"stdout","text":["new_4_1_d_191046.nii\n","new_4_1_m_191046.nii.gz\n","new_4_2_d_222342.nii\n","new_4_2_m_222342.nii.gz\n","new_4_3_d_084336.nii\n","new_4_3_m_084336.nii.gz\n","new_4_4_d_112405.nii\n","new_4_4_m_112405.nii.gz\n","new_4_5_d_140141.nii\n","new_4_5_m_140141.nii.gz\n","\t0.9555 +/- 0.0102\n"]}],"source":["model_test = Test(args=args_test)\n","dice_dict = model_test.test()"]},{"cell_type":"markdown","metadata":{"id":"yeWvx-jybLPX"},"source":["## 3) Plot our results"]},{"cell_type":"markdown","metadata":{"id":"DHaYnNGsbLPX"},"source":["#### plot the training dice change"]},{"cell_type":"code","source":["Dict_DL = pickle.load(open(args.out_dir+\"/DiceAndLoss.pkl\", 'rb'))"],"metadata":{"id":"CDjbIGnWrThR","executionInfo":{"status":"ok","timestamp":1647736737469,"user_tz":-480,"elapsed":461,"user":{"displayName":"胡浩宇","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06402011815971394657"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TI1SzWwjbLPY"},"source":["#### plot the training loss change"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":446},"executionInfo":{"elapsed":1233,"status":"ok","timestamp":1647736741377,"user":{"displayName":"胡浩宇","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06402011815971394657"},"user_tz":-480},"id":"PRvdJXvZbLPX","outputId":"2786a52c-b7be-4183-b9fd-5164b0002138"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x504 with 1 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAA30AAAG+CAYAAAAnaW2KAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOydd3gUxd/AP3Mld0lIT0iBUELvVVBEOooIAhasqK9YsYFixYKI7Yci2EFFUYqCEqwoRYr03ltCTUjv9ZLL3b5/zKWRuxQSijif57ln77bN7Ozc7nzrCE3TUCgUCoVCoVAoFArF5YnuYldAoVAoFAqFQqFQKBTnDyX0KRQKhUKhUCgUCsVljBL6FAqFQqFQKBQKheIyRgl9CoVCoVAoFAqFQnEZo4Q+hUKhUCgUCoVCobiMUUKfQqFQKBQKhUKhUFzGKKFPoVAoLnOEEN8IIaY6vl8jhDhyMcq+lBFC5AghIi5i+QeEEP3qel+FQqFQKAAMF7sCCoVCobhwaJr2D9DqYtfjUkPTtHrncpwQoglwAjBqmlZUi/LbnY99FQqFQqEAZelTKBQKheK8IoS47BSsQqLGEAqFQvEvQT2wFQqF4jJDCNFFCLFTCJEthPgBMJfZ1k8IEVvmd7gQYokQIlkIkSqE+LjMtvuFEIeEEOlCiL+EEI0rKbO3EGKjECJDCBEjhLivzGY/IcTvjvpsEUI0K3PcTMf+WUKIHUKIa8psmyyEWCSE+NZx7AEhRPcy27sKIXY5ti0WQvxQ1pVUCDFMCLHbUaeNQoiOldRfE0I0d3z/Rgjxias6n8U6xzLD4SJ6lRDiPiHEBiHEB0KIVGCyEKKZEOJvRxunCCHmCyF8y5R/UggxqJrXXZN9K22js9qguN4fCyEyhRCHhRADy2xfI4R4UwixAcgDIoQQvYQQ2xz7bxNC9Cqzv78Q4mshRJyjDy2tzr0RQjwvhDjjqPOR4joIIXoIIbY7+kqiEGK6q/upUCgUivIooU+hUCguI4QQbsBS4DvAH1gM3OxiXz3wG3AKaAI0AL53bBsBvATcBAQB/wALXZynMbAM+Mixb2dgd5ldbgdeB/yAaODNMtu2Ofb3BxYAi4UQ5jLbb3TUyRf4Bfi4zHVGAt84jl0IjCpTpy7AHOBhIACYBfwihDA5uwYnVFbnsvRxLH01Taunadomx++ewHEg2HGsAN4GwoA2QDgwuZLynV53Tfatqo1c0BM4BgQCrwFLhBD+ZbaPAR4CvIBs4HfgQ2QbTwd+F0IEOPb9DvAA2gH1gQ8c9XJ5b4QQrYDHgSs0TfMCrgNOOs43E5ipaZo30AxYVMW1KBQKhcKBEvoUCoXi8uJKwAjM0DTNqmnaj0jByhk9kELIs5qm5WqaZtE0bb1j2yPA25qmHXLEqr0FdHZh7bsTWKlp2kJHmamappUV+iI1TdvqOM98pJAHgKZp8xz7F2ma9j5gonzM4XpN0/7QNM2GFCI6lblOA/Cho8wlwNYyxz0EzNI0bYumaTZN0+YCBY7jqoPLOleTOE3TPnJcV76madGapq3QNK1A07RkpIDUt5LjXV13Tfatqo2ckURp3/kBOALcUGb7N5qmHXC0y7VAlKZp3zmucyFwGBguhAgFrgce0TQt3XG+tY5zVHZvbMg+0FYIYdQ07aSmacccx1mB5kKIQE3TcjRN21zFtSgUCoXCgRL6FAqF4vIiDDijaZpWZt0pF/uGA6dcJCBpDMx0uN9lAGlIa1UDF+c55mR9MQllvucBJUlThBAThXQhzXSU44O0Mrk61ixkjJyz64w5q/7PFNffce5wx3HVwWWdq0nZuiCECBZCfO9wW8wC5lH+Oqsqv/i6a7JvVW3kDGd9p2yblT0+jIp96xSyj4QDaZqmpTspw+W90TQtGhiPtIImOdqsuPyxQEvgsMOVdFgV16JQKBQKB0roUygUisuLeKCBEEKUWdfIxb4xQCMXwkQM8LCmab5lPu6apm10sa+rmDeXCBm/9xwwGvDTNM0XyEQKl1Xh7DrDz6rTm2fV38NhjapLtGquf8uxroPDPfFuqnedtaGqNnKGs74TV+Z32euKQwpwZWkEnEG2v3/ZuMUyVHpvNE1boGlab8e5NeBdx/ooTdPuQLqKvgv8KITwrOJ6FAqFQoES+hQKheJyYxNQBDwphDAKIW5CunE6YytSMHhHCOEphDALIa52bPsceFEI0Q5ACOEjhLjVxXnmA4OEEKOFEAYhRIAQojrukF6OuiYDBiHEq4B3ta5SXqcNeNxR5gjKX+cXwCNCiJ5C4imEuEEI4VXN81eXZMAOVDXHnxeQA2QKIRoAz9ZxPZxRVRs5oz6lfedWZPzhHy72/QNoKYS403H+24C2wG+apsUj4zw/FUL4Oc5XHP/o8t4IIVoJIQY4Yi8tQD6yfRFC3C2ECNI0zQ5kOM5lP5eGUSgUiv8aSuhTKBSKywhN0wqRyVfuQ7pk3gYscbGvDRgONAdOA7GO/dE0LRJpTfne4Y64Hxmj5ew8p4GhwDOOMndTeQxaMX8BfwJHkW6BFqp2Pywus/g6xyIFgLuRSWkKHNu3Aw8ik5qkI5Ox3Fedc9cETdPykIlaNjhcFV3FDL4OdEVaMn/HxT2p47pV2kYu2AK0AFKQ13WLpmmpLs6fCgxD3vdUpNV2mKZpKY5dxiDj8A4jYwXHO46r7N6YgHcc5ScghdAXHduGAAeEEDnIpC63a5qWX63GUCgUiv84orzrvkKhUCgU/06EEFuAzzVN+/pi1+VSpbI2EnKajQccrpUKhUKhuIxQlj6FQqFQ/CsRQvQVQoQ4XAvvBToiLYcKB6qNFAqFQgEylbNCoVAoFP9GWiHnavNEzol3iyOWTFGKaiOFQqFQKPdOhUKhUCgUCoVCobicUe6dCoVCoVAoFAqFQnEZc1m4dwYGBmpNmjS52NVQKBQKhUKhUCgUiovCjh07UjRNC3K27bIQ+po0acL27dsvdjUUCoVCoVAoFAqF4qIghDjlapty71QoFAqFQqFQKBSKyxgl9CkUCoVCoVAoFArFZYwS+hQKhUKhUCgUCoXiMkYJfQqFQqFQKBQKhUJxGXNBhT4hxBwhRJIQYr+L7UII8aEQIloIsVcI0fVC1k+hUCgUCoVCoVAoLjcutKXvG2BIJduvB1o4Pg8Bn12AOikUCoVCoVAoFArFZcsFFfo0TVsHpFWyywjgW02yGfAVQoRemNopFAqFQqFQKBQKxeXHpRbT1wCIKfM71rGuAkKIh4QQ24UQ25OTky9I5RQKhUKhUCgUCoXi38alJvRVG03TZmua1l3TtO5BQU4nnlcoFAqFQqFQKBSK/zyXmtB3Bggv87uhY51CoVAoFAqFQqFQKM6BS03o+wW4x5HF80ogU9O0+ItdKYVCoVAoFAqFQqH4t2K4kIUJIRYC/YBAIUQs8BpgBNA07XPgD2AoEA3kAf93IeunUCgUCoVCoVAoFJcbF1To0zTtjiq2a8BjF6g6CoVCoVAoFAqFQnHZc6m5dyoUCoVCccmRlGVh9KxNJGVbLnZVFJc5qq8pLhQXs69d7H5em/Ivdt3PFSX0KRSKi8a/9cH5X+bfOkg412M1TSPLYmXq7wfZdjKNGSuOXrCy/+v8W/tabflwVRTbTqbx4cqoczr+v9rf/qv9pTbUtq/9W8uubfkXu+7nipAelf9uunfvrm3fvv1iV0OhuCgkZVl4fOEuPr6zC/W9zBe7OjXi5ch9zN96mrt6NGLqqA41Pv7ffO3/Vmp7zy5W2Wcfa7HaSMkpIDnb8Sn7vczv2PR8p+cTwMA2wQR7mwjxNhPsbSbYx1zy28fdiBCi1vWG2vXz2v5HLmbZF7PdLkY/b/XyMgqK7BXW6wTc3qMRgZ5uBNQzEVDPjQBPE4H15G9fdyM6nbiodS/mYj6TX1yyl++3xfzrnk0XA1d9zWTQcWTq9RelbDeDjqPnuezi8q+zr+M5wyLCRApxWiD/KxrN71zDc9e1Qq8T6ISQS51AJ0Av5PeXluyjyF5RbroQ7VZdhBA7NE3r7nSbEvoUin83Ly3Zy8KtMdzV89/xstE0jVYv/0mhreJDX68TTBnRjnomA15mA/VMxjLfDXiaDLgZSh0U/m0v2kuBmgzK7HaNuMx8opNyeGDu9ov2snM1SNALwV1XNqLIrmG3a+WWNk3DZtNYfjABJ9V2ib+nG0H1TAR5yY+nm55dMRkcTczGatMw6AQNfN0J83MnI89KYpaFtNzCCucxGXRO6wyynz97XSuMeh1ueoGbQYdRLz9uBh1ujqVcJ/jin+Ms25fATV0b8OaoDpiN+mpfT23/I9U9XtM0bI62L7JrFNnsTP39ID/tPMPIzmFMGNRK3hO7HZsdbHa5v7N1//fNVqy2ijfNzaBj3+RrMRmqd/01uXZN08gttNF1ygqnz6YL0c+Tsiws+PI9bsn4mjCRQjyBfKq7k82eA8m0WEnLLXTal/U6gb+nGynZBTjr6hdyQHoxnsktJy1jiLaO29wXMy1Ez7MJNn7Iv5W/dH0uyLPJmQBxIcquDUlZFt747SC/7Y0v6TOD2tTnrZs6nHdhPSnLwmMLdrLtZDogFWgaUrnRq1kg13cI4bp2IQTWM9VZmak5Baw5kszfR5LwOPITrzObODeNifUDeC8plbBCwQvWB/jF3rvKc92oW19yv+MJZFXYwwy588lLRvGshD6Fohr8m6xGZzLy6fu/1U4H4Ua94MDrQ8oJR+eL6rRZQqaFPbEZ7I3NYE9MJntjM8iyFJ1zmZUNpi8lbdv5pK4tGEU2O6fT8ohOyiEqKYdjxcvkHPIKbSXHuukFhY7BuNmo47p2IUy6oc0FGSS8vHQ/yw8mlqzTCzC76THqdRgcmlmDQzNbdmm3a3TPXsnj9oVyUEYg35jvwdjlNpoGeJYId0FeJvw93TDqK/5vJkXuY8HW07jpdRTa7BUGtAVFNpKyCkjMspCYVUBCloWkLAsnU3MJPP4zj9oWlBsQVmdgURX+nm4lypBiBYm32UA9s/z9+drj2Jw8Hww6wYtD22Cx2iiw2igossvvjqXFaqegyMaao8m4Gh4E1jNRZLdjs2lY7XaKbFqFZ1HZgVFdXjfI/7m3u7xeb3cjPu5GvM1GvN0NeJuNzFrn/Nr1QjCicxhZFitZliKyLUVkW6xk5VvJKSgqEagu1qAuY/N83JaNx0OUKhEKhQm3UR9Dx9HY7BoZeYWk5haSklNAak4hqTkFpOQUkppbwJn0fPadySQ9zwpc2P/oxbAcHUnI5pPV0Yh9i3jV7SvuaBhAgkFPaJGNhWdSSej5Fm2HPHBeygYpSKz64WOGnX6nnADRqEhH/vUf4NPjrvNWdl0wY/rUEgVDnBbITG7n6WdeJtTH/byW+09UMvd+tRU7sn8U2uwMaRdCRJAnf+xL4ERKLjoBPZr6M7RDKNe1CyHYu2b9V9M0DsZnsfpwEqsOJ7E7JgNNgyAvE8sZh6koiZENQkv6S+SZeMxeDbDctgitMBe7oR5FRk9sBg/5QYfNrmE+/BPeK57BpBWUlFX2P3opoIQ+haIa1EZDeb4FRovVxpYTaaw7mszao8lEJ+UA8qVutUnteLF3j10DL5OBvq2CGNw2mH6t6uPjbqzzOkHFNsvIK2RvrBTs9sRmsicmg6Rs+XDU6wStQ7zo2NCXTg19WBeVzLL9CeUG0i8ObUNOQZH8WOQy21L8Ww7MsguKSM4qwDsqkgcK5100bdvFcHvTNI2cgiJeWbqfn3fHMaxjKE8MbIFd07Dbwa5paJpc2jQNTdOwa9Jid/dXW5xaTwRgdNyDYkJ9zDSvX4/m9evRor5XyfeV339Er1OflghPGxuNY/TYZ2p07efKbbM2seVEGm56HVZ7RcHLJXsXURj5OG61eEl/+ek73Jw+B19rEhnG+vzkdz8PjHvhnMs2jPyYwrY3U1Bkx2qzU+hYWm12xzoNq81OYpaFE39/zU3pcwhF9vN5nveS1+ombJpGTrHgUlAqwBT/Z5wJPc4wGXSYjXrMRh0mQ+lSJyAuI59Uh3VJLyDM153OjXypZzJi1AsMOh0GvRSuDQ7h26AXND7zOwOj3iTWaC8ZCDe06tja/jUyWoxCLwR6Heh1OvQ6HAK7Dp0ODI51s9cdZ/mBRAx6QZFNo1fzAIa0CyHLUkRWvpXMfKsU3vKLHEtryTZnyjCAeiY9vh5ueJmNeJkNUmh0fC9e1yFtOd32vnbBB3V2u0bamy0JtCURbTSWtFtzqxXMvnDXjxDaEQyVWz8mRe5jwZbTJdabuy+QB0hSloV75mzlcEJ2ybqmAZ68fmM7rmkZWOLqXBfsicng49XRrDiYiKebno3mp3jDx8oaD3cKdDpMdjv98/J5KlHwdqvFTLyuFU0DPeus/LTcQmavO863m06yXDyGvy61ggDh7t0QMWF/nZVZ1+RuX4D49alyCoY8zY0PzI/zyBMvEFCHVray7DiVzt1fbmG4bj0vui2q8EzVNI0jidn8sS+BZfviiUrKQQjo3tiPIe1Dub59CGG+Uig9+z2aX2hjQ3QKfx9JYvXhJOIzZXxlp4Y+DGgdzIDW9WkXWg/dG/5MDAqo0F+mJadB6xvg8G/lK+3TCCbsk9/fDAVrXsX/qE84XCL3Wwl9/zH+TRarS4HK4ihu6trQ4Vaox8NNatU93PRyaTJQz7F+1tpj/Lwnjjt7NOLNOog90TSNY8m5rD2azLqjyWw+nkpBkR03g46eTf3p2zKIvi2D+GbjyXIWiNu6hzOwTTArDyay6nAiKTmFGHSCnhH+DGoTzKA2wYT7e1Radm3arCwRgZ50CvelY0MfOjb0pV2Ydzm3tHMeSEOdDORrS13El93WPZzHBzQnPddKWl4h6bmFpOUWkp531tKxPTm7oOqT1wBPNz1dG/vSNsynRLhrFuSJl9mJkmDvIgoiHy83GC4QJkwXqM1fmfIqj9jmEyZSK+8vdjsk7IWM05AZA6vfgsKcivsZ3KHjreDuJz9N+kDDblBUCClHStcf/h1+fRKs+eWPHfgKNOwBuUmQmwwR/cGvMcRshb/fgNwUSD4MmpP/idBBYCvwCAAPf7nsMxF8GkLaCUg9Jtef3kzh8sk17ueapmGx2pkUuY/IXWdKBKcbO4cxaWgbTAY9JqMOk0FX6WC8KgunSz5oT15WbIWBsId3w2oPjB7+bjtBXmbu7NGIBVtPk5xtYdYYp+OYCteeb7UxKXI/S3edwWjQYa1J3ae3hawzFdef50Hd1xtOcO/yzlh0omK7FY/TdEYI6QDd74euY+Q6TYMy97Dsc/WMPYCl/mN5fPxL563exew8nc5Nn24EpLeJ1SYVkXYNWod4Mbp7OCO7NMDf0+2czq9pGltOpPHJ6mj+iUrBx93Ifb2a8H+9GrH6swjeDvAjX1dqpTfb7byYms7LWXMoLLJze49wnhzYolZjovTcQr745zhzN54kz2pjeMcwZh7px7NOBIj/JaeR/1IKHm4XdGa0apPxVkt8CxMrrD+jBfJQwDcsfOhKvJ29B2rBofgsbpu1iVvdNvGy/XNEUZlnqtEdhn9Y4bkWlZjNsv0JLNufwKH4LAA6h/sytEMIB+Oy+Hl3HN2b+OFpMrDpmBwnebrp6dMyiP6t69OvVVD5e77/JyL/fII3A/woKNNf9JrGdYVwbc+nCbXZCBMmfGw2hDVPKlqucFiMJ/uQJ5z9R4HJGXXaXueKEvr+Y6g4p5qRkJHPTZ9tJM6hFdIJ8HY34utupLDITk5BEbmFtmprzkFaT65o6k+Apxt+nm5y6eFGQD03/M/6bjLoS+7ZNc0DaeDnwbqjyZzJkA/EiCBP+rYMok/LIK5sGoC7W6ngVNnAyGbX2B2TwcpDiaw8mEiUwzrYOsSLwW2DGdw2mPZhPrz68/4K/aWgyEZchoWYtDxi0vOIScsnNj2PmPR8YtPySHUSwxTqY2Zklwb0bh5I+wY+lVsX9y6qOJAufui3HQFFFigqcCwLwTdcPngzYyHlKPz0IOSlVDzvBdC2uRJ4BdCugTc2u9Ta2xwxTsUfu6aVaB6rQgjw83DDz8NY0l/8Pd1wM+jYcSq9JL7MqBd0Cvfllq4N8fVwQyek1USnAyGky2NxELoQgq/WH2fVoSQ5KLNrNXtGXKTBMEDR7h+wRj6OexmtNHo3aD1MCmaZMdCgG/R7QQp9U+uD3VruHBU0swD1QiA/DWyFMHgKXP0UpB2HD7vUvJK3zIH2N0PsdvjrJfAMKtEYOy27zXDIS4O8VPm5/y8IaAYbP4LlL1de93rBMH4/GCofQJ+r4FRMlYoZuw0yToG7P7j7SoF3xatwepNrTXrLIVC/NdRvC0GtIbAlGJ0MxPcuglVT5H/epyEMfLVGyoVqKZVyU+S99w6D7ESYO0w+X5ygIRDnaVB3NDGbYR+tZ6PpSd7xt1VsN4sJrn8XzmyH2B3QbiT0eBBykuDjK2Tfb9hdPk+3fgFlBtMWTBhGfoSh823npe4AOQVFDJ35D0lZFkZ0DuPeXk1ZsPU0cRn5DGhdn8XbY9gTm4lRLxjcNphbu4VzTYtADE5cqc9G0zTWHEnmk9XRbD+VTmA9Ew9e05S7rmxMPV0R/DyOvlmbSdNXjPP0t2v8eMsOPvo7igVbTmPU63jgmqY81CfCuWLLBZl5Vr5cf5yvN5wkt7CIoR1CGT+wBS1iFhO55mXeCvDDcpbA+UiKlUjzV3x17xU1dk083xxLzqHpxw3QOdH3aAhaFC6gSyNfvr2/Z7nxRm04kZLLrZ9vwqATrDc9iSE7tuJOVbxLTqTksmx/PNP+POI0dlUvBHPv70GPpv7lw1ssWVIBF94D7DaumXcFGZrVyRnK425wJ8QzhDDPMEI8Qwj1DCVs/UdE6gvYY3KjsOx/tNBDWfouFErok7gajBr1gp2vDK7RQ+6/xPTlR/jw72gEMlGAM422pmkUFNnJK7SR63A/zCssIjY9n3mbT7E7JqMkyUNDP3ciAj3JLbSRVsZyU5NkEjoBb4xsT58WQRUsc+fKiZRcVh1KZMXBRLadTKu0PkJQLp7HqJfJKxr6eRDuL5cbj6WwMTq15pp0gA/ay4H62bj7QX56xfWPbYWgVrDpEzmgdlBxMC3Ou7atODX3ydQ8QN6rAE83mtf3wt1Nj14nHO5rMq5MLyiJLysssrM7JoOY9HxsdtlfOjTw4c6ejWga6Imfpxv+Hm54uxvRO3sjUwvrC7UQAnZ+C7884WLj+W/zgmltMeU6EThBur75hkOrG6D/i3Jd1ErwDATfRjCrT+VWJ00rVT64eUBBNhxbLfthfjqsfM11xe76UZbjGQSe9SsKYedi8cpNkYJnXiosvN2FVlkDvQlC2ssBf1hX6HR7OYsPUDvByZlixmCGZoNAb4CUKEiNBlsBjJoNnW6D+D2w7AUiMw/ztq9HBcvLS9lWRun9ITUK7I64XqGDpw+DVzCc2gTZ8VKQXPuuc6VQdervSqnU9V5wqwcJ++QnOw66/R8MnyEF2MX3wfE1UJBV4ZRxBML4/SWuZXVFQZGNkZ9sJCnLwrNtvmV69s6z2k3jpcY3MmrAWxUPzoiBdf+TgmDyIedWZSDLFIL3i0fqtN5lmbh4D0t2xrJiUCLN9k532t8OJ2SxeHsskbvOkJZbSLC3iZu7NuTW7uElrpdlvU4CPU38eSCBT1ZHcyAuiwa+7jzcN4LR3cNLPUb+mgSbPuad5t2YX5RUrv+b7XYmpecw8pZF0KgnJ1NyeX/FUX7dE4e/pxuP92/OXVc2qjQpUGa+la/Wn+Dr9SfILihiaIcQnhrYklbB9WDN27D2Xfo2aUyaqPgy9RZm0qPfwNts5Kv7utMuzKcOW7x2jP1mG2+cuJ0wkeLUTfG3gct5YuEurmkRxBf3dKt24iRXxGfmc8tnm8i32lj08JU0/zQcnIpt1XuXJGVZeClyH2uOJFNk1zAZdAxp7yR2VdPgwBL48yX5vJmwn3wBN0beSEJeQrlzmoWRp654mi71u5CQk0BcbhzxufEk5CYQnxNPXG4caZY0p/Wp9D96EVBC33+EpCwLU/84xG974pwO6JsEeNAuzIe2Yd60b+BDuzBvp9mRLqZ76IUue+muM4z/YTf31dvKU2LhObkaVmcQbrdrZOZbSXUIgKk5cnk6LY+/9idwKjUXmwZueh1D2gfz8rC21bv+cxzUpecW8vPuM8xed7zEwglSeLkyIoAWwfWkgOfnTri/B8He5gpCyDkJEDYr7PsRlj7iYgch3eYMZmnZ05vk9xaDpSUhK0667S26h7zcioHYFyKOYl9sJsM/Xg+UBqHXRPCqjdAGtbfeVJu8NNAbweQFByIh8hFpeT2bC2Dp0yb7Is51kLB3ERM3TGKN2a3UemIpZNrVb1ZPgHCloKjOdTvKXm2WWmGj3c6VBVZev+J5TB1uxaw3Y9QZXbtYftCeiW55FS0/uUDnO+HMLojbBZ4BMN4Rc7L6bWnpsVpg59zy96xYcGozXAqVuSlyWa++dBu05kulSm4KHP1LCnTO8I+QFrrAFnLZtK90bXXQd35P0oryKh5m8GDtXVuk9T7tGCQdhJRo6PucHLBHPgJ7FrpuT5MPNL1GCoo6PQg9eIXAdW/K7Rs/lufdu8i5S69wDF4DW8r4uJAO0PhqaNC1dB+HwBhNUclgOEIz8HzhWI6FDOWHh69ymuznXHl72SFmrT3Ol2O68ca2a50KEG46Nz4Z9Ak9QnqgEy7KLsiGt50Ppu2aYP+DJ+nY0LfO6l3Mb3vjeHzBLj7pEM0NJ9+uUlAvLLLz9+FEFm2PZc2RJOwa9Gjiz63dG7L9ZDqLdsRwVdMAErMtHEvOpWmgJ4/2a8bIzg0qJifLSyPq8FLuPjQLvd1Ggc1CoRAITWOwVzPePxMj3xm3fgOthgDy+f3un4dZH51CQz93nrm2JSM6NSAlp6Bk7GE26vl6/Um+XH+cbEsR17ULZvyglrQJ9QZbEfw+QSrCutzNnKZd+GDXzHLVMtvtTDKG07L/D4z9djuZ+VY+uqMLA9sE13n715R/opIZ89VWvuxynF7RbzIy2K/0HZqUjscweb++33qaF5bs4/r2IXx0R5dqWWWdkZZbyOhZm0jItLDwwSvp0NDH9TMVpMv8lWg8ebAAACAASURBVI9Ir4lKqPI9mhINf0yE46shtBPc8AFag65MXDuRFadW0KV+Fw6kHqDAVoBJZ6J/o/5M6zut0jItRRYG/TiIzILMCtv8zf6svW1tle1xIahM6Ls0nY0V50R9bzNeJgN2TVofNOCmLg24oWMoB85kcSAui71nMvh9X3zJMcHeJtqF+dA+zJu2YVIQnLX2WMmkkxfaPbTshJfnu+ztJ9N47se9PBOym8dzZyEcLys/ayIPpM+AvRHVGhSm5BRwV8/G5QbhZ6PTCfwcrp5nk5Vv5URqbokA4W02Vl/gK6vNzoyRv6HKevt5unHf1U2JSsop9+C8vn1Itdu9rKAxdWT7yncujjspzIHfnwGdoVTTXxafhnBNJYlBvMPk59qpvLphEml6HZoQpOp1TAoM5K5GT3AexJ8S7HaNV37ej5tex6gupW5Mzu65K6rTXyqjRu1+LuSmwuZPYMts6e7Y91loN0oK7GdZT4r0ZgwDX637OpxFki6QYHtyxQ0+Das8dq4un1WeHhQ5rCAFOh0rPNx5KXM3YzO6EeETUXmSiYGvVhACmmOQ612QXZjNrqRdLEj8m00e7tgdA3GrTsc/7iYG7J8B+2cAIBCY9CZMBhMmvQmz3ozJIJdZIQHEFIDdUb8CnY41Hh5EtrmRUQOmysLsNsgpE5sTvweiV1Zwb5UVyIclD1FBMOh2HwyfKV1mD/0q3TVdCXwIeHKXy2vPs+bR0Lc5aSl7y613E3om9HRYYg1uUL+N/JRl2Adw5TiYdY3zkxdkyphHzSavW7NJ4buY05vg9GbnAh9IS9ikeCmMuKLjaPJshYzb9Q4JOngsNJhIt7b07fgYixfs4t1lh3l5WFvXx9eAzcdTmb3uOHf0aMSgBlay19h4w8uAtYzFTi/06ISOB5c/SLhXODe3uJkRzUcQ6B5Y/mQmL/l/cDKYThSBPL1oD7890btG031URVxGPi8t2SdjrJJmlxf4QP5eNaXc+8jNoGNI+1CGtA8lMcvCTztjmfbnEbaeLLWibDyeCshMsyuf7lte4Ri7AzbMgJu+IE0HT5xYhIfRgznXzeHhFQ8TnxuPJgTd294OQwbD/Fvgp7Hw1F7wDKBDQx/mPdCTf6KSeffPw0z4YQ+z150gxNvEtpNpPDh3OydT88jMtzK4bTDjB7Uob6WzFUgrcZ9nof8kDq97Hh06DDoDhXbpft61XmNGhl0HYd78/NjVjJ27nQe/3c7LN7Tl/65uUqdJbWpCkc3O1N8O0cjfg2tuHsek39eTln5AvkMNel5r15dpjnt1e49G5BQUMfX3Q7ywZB//u7ljubkgq0O2xcq9c7YSk5bHt/f3kAIfyGfn0nHln1EGd2gzDM7shKgVpUJf5hnwaVDh3JW+R9OOw2dXSYXx0PdkDKxOz+d7PmP5qeVM6DaB21vdzsifR5KQm0CAewCv93q9yusxG8w80+0Z3t76NvllXKjNejMTuk2oUdtcLJSl7zLjoW+3s/JgIjd1a4DZaHBqBcjMt3IwLosDcZkccCyjk3JcuvtdzHm4zlfZp1PzGPnpBnzcjazUPYY+q+b+5XXFOVtu3msFOQkV19eg3ufdapQVB5s/k9aIe3+Vgl/yUYjf7TqmrwqBtchexGe7P2PO/i9LBvIAerset+zRrB/3wnmbruKHbad5/qd9TB/diZu6Vi1w/KvISYZNH8HWL8GaJ+OG+jwHwWUGuA7LspYZS7wWwLpGj3L72InntVpJWRamvvM6H5hmo9fKKAoq6S92zc7muM38cOQH/o75u9Lz+5n86BrclW7B3ega3JVWfq0w6MrrQ/N2zWOkQwgItUNklxfw6HJ3yfYMSwY7knawI3EH2xO2cyT9CHYXrnYAHgYPHuv8GAW2Aiw2CwVFBRTYCir83hK/hSKtonKkSq1yUQFMDca5CxVy0OUR6EgkEyDdYM8eWJ2DhfNk5kkmrJnAsYxjRPhGEJsdS4FDePQweLD2trWYDdVQaNXGuloHx09cO5E1MWukFUDo6Z+dxbTr5/DavkDmbjrF53d3Y0j7kKrrUQmZ+VaGzvwHN4OO35/sjYebAVtBLlcu7keBrQANrcQCMbX3VFaeWsnio4vZkbgDgzDQv1F/bml5C1eGXllq/XNipWyOgUPd3+T61SE81CeCl4a2qbxi1cRm17jzi83sP5PJH09dQ+OPGnCuLnuJmfk8s3gvm46lODxeBNd3CK3ornf4d/hxLNSrj/WepTy4dQr7kvfx9ZCv6RjUkej0aCaunYhBZyDNksZvo37DozixU+NeFcq12zVavrzM5dRHUW8OLV2RlyaVIqZ60opuNLM3eS93/XEX97e/n2UnlpGQK9/HPUN78sW1XxQXQl6RnQk/7OavA4mMubIxrw1ve86Ws9owb/MpXl66n8/v7kq+cb0UXsoIXmah5yX/HowaNrtk3fQVR/lwVRT39WrCa8PbVltgtVht3DNnKztPpTP7nm4MaF3GypmbImPEhU56IpT1UrLbpdLG7A3xe2FWH2g2AK4YCy2uk67lrkg7Lr0QADZ/LpWVXrLcFadW8PSapxkeMZw3e7+JEEL2l3UTea/PezT3a17tdiz3fKimlfBCoix9/yFeGtqG5QcT6dE0gNHdw53u4+Nu5KpmAVzVLKBkncVqY+OxVGasPMreWGm6LjvHz/nmn+f688ZvB/l1r7RClvXRrmuyLFbGzt2Gza7x1b3d0X/iIlYo04kgeB6Y1em41IbujmVq8cOvrL1K02TsTMwWmSihx0MylseZwFe23raiyh+QnEerUeJBmZBi32KpjW83Cgpz5QszqKX8ANFrpjDR3cp7+Uaa93Pummqz2zicfpjtCdvZmrCVnYk7ybFW1OTbdDZyPX9h4dY7ubdXk7q7FgcZeYW8++cRrmjix6guFTWP/woqcwf+9Uk4skxqWPs8K5NtnE3H0eAdhvh9IrPMz7M8yZvbNO28aq7XHk3mF3tv3gzbh9cZ6VbrypU5zZLG0uilLD6ymNicWPzN/vQO6822xG0lwgdIzeyjnR7Fz+zHjkQprK06vQoAT6Mnnet3pntwd7oFd6NdQDtezdpNmpsZzVZAqtHEC+lbGXoykB0JO9ieuJ3ojGgATHoTHYM68nDHh+kW3I2TWSd5f/v7FbTCL/Z8kZHNR1Z57ZFRkRW0ygZhYHzX8ZUfaDC5tPrgE165Nb0Yh4WzgmLGhYVzxakVvLLhFYw6I58P/pzOQZ1LNOn+Zn9SLal8se8LnujiKjb03Muuy+MjoyJZF7uupL8UaDbWeHoSuepZXrpvI7tjMnj2xz20CfWiccC5TwPw2s/7Sciy8NOjvfDIPAZ+TViXuBWLzYKfyY+MgowSC4RJb+KGiBu4IeIGjmceZ8nRJfx87GdWnFpBg3oNuKXlLYxsPpLAs62UwUFEdn2JNl3HcFfePr745zjXtg2mexP/c653MbPXHWfLiTSm3dJRtoOjv1WIEdMb4djfMruti+dEsI87jQM82HCs1GXey2QoL/Bt/hz+fAEadEW7/Xum7vuEHYk7eOead+gY1BGA5n7NWTpyKbuTdjNm2RjmHpzLo50eLRX4ds2DhP1w3Vug06HTCTa+MIA3fjvInwcSsNo052OPjNMw72bpFnz7fDDKDNvTtk0j0D2Qhzs+zPCI4UxcN5HeYb2Ze3AuW+O30iMvF/58EY97f+Gzu7rx7l+HmbX2OKfS8vj4zi51nh2zMjLzrUxfcZSeTf25rl0I/RZMLyfwAVg0GzOSNjAqN0XGKgMTBrUg22Ll6w0n8TYbePraVlWWZbXZGTd/J9tOpjHjts7lBT6A9R9IK9+4LSVjgRJ0OinwAXiFQr8XYcc38P2d4N1AeiX0fASO/ln6LvMKkfsm7INxmyGwuXQRdXAo9RCT1k+iY1BHXuv1Wsn7qrlfc5aOWFqjdgSY0mtKja2ElwpK6LvMOJ4iB8PNgmr2MjIb9QxoXZ9VhxLZG5uJTkBBkZMH73mi/lkxYwVFdjLzrHVedpHNzmPzd3IiJZdvx/YgIqheJYOjC2DJqcxFs3Ev+O1piN1amtzE7Cvj20Lag3dDcGqhdNR7wWjpztTpDuk24VZ38xSV1N2ZABG1EubfDEYPqZ27cly5eJ9i8toMY9zRL0nITeAxv1Ai2wzDA2mlOZp+lG0J29iasJUdiTvILpRzPzXxbsL1TaXl99djv2Kxlbp0mPVm/K238NHfUdzSrSGeprp9vL23/AiZ+VamjGh/0dxzaoWzvrb0Ual1vWocDHwNBr1e8SV8FtGWNCa6pXO7/3HmRrUmKimHlsFe563aa44mU9/LRD2zkeiwdkwMDiqnmdU0jZ1JO1l0ZBErTq3AarfSLbgbT3R5gkGNB+Gmd6ugme0X3o/7O9wPwKgWowBIzE1kZ9LOEiFw5k4Zp6MXejS0Estdgb2A1TGrWR2zGneDO13qd2Fo06F0C+5G+8D2uOlLXbh7hvZkW8K2CmVXR+ArrtuGuA0lx+uEjiKtqMSS6DK+C2ovOBUL1FXEDFvtVmbumMncg3PpGNiR9/q+R2i9UAA+HfhpiSZ9zv45zNk/h6FNh9LMt1mdlH0+jp++Y3o5IRvAImCGPpdRBxbz8Z0jGfbResbN38lPj/Y6J3fJX/bEsXR3HBMGtaRzqDt8dAuEdmSBvwfBHsF8MvATnv/ned7r8x4exvKJvCJ8Iph4xUSe7Pokq06v4sejPzJz50w+2fUJ/cL7kW5JL1VQ6HW8FrecaV3H8NLQNqyLSuaZxXtY9tQ1tZpKYF9sJu8vP8INHUK5pZvjfXPFA+Stmsy44CASDHopcCak4GHwhO9GSa+BAZNcnrNSd71/3pf3svUwuOkLvov6kSVRS3iww4PcEHFDhXN1rt+ZwY0H8/X+r7m15a2lrrBJh2DLZ3KKlZGfgcGN+t5mvN2NJQlBKgicCfulwFeUL12gHaw4tYLdybuZfNVkPIweJQJEga2Av079xQc7PmBBt5cQ6Sfgp7HoxizlxevbEBHoyaTI/dzy2Ua+uveKOkvUVhUf/x1Fel4hrwyT1rrxhhDeLowiv8y4y6w3MSE5DjZ9DIMmAzIL9Cs3tCXHUsSHf0fjZTbyYJ8Il+XY7BrPLNrD34eTeHNUe0Z0PktBmhUnM8x2uqPKdw31gqDf81JJdXQZbPtKKpO9QmDZc6XPtux4+Wk7Sib1KkNKfgpPrn4SbzdvZvafiUlf+/kHPYwe5Z5tZ/9HL2WUe+dlxhfrjvPmH4fY9cpgp/FjVfHwd9vZdTqD+l4mOjfyO39JIpww+vONbD2ZzlMDmzNv82lScwt54frWPNynipibGvDqz/v5dtMp3rmpA7f3aCRXVjZ9QJsbnacTrysqc0N6bAt8OVgmGQjvKdMNB7SQmrCq6t3hVlj7P9g9X2bCM3rK5A1XPADhV9S+3s7K1rvBiE/klAubP4Ou98i5xlxQdiBu1Blp5tuMBvUasD1xe0mgdLhXOD1CenBFyBVcEXIF9T3qlzt++cnl0g3Kbqd/+ADubDmFUZ9uZMKgljw1qEXtr9PBvthMbvxkvcPFpV2dnfeC4jJjqj88f6Jap8iz5jFy6Y0k5CYQbKhH1IHnmXR9p0oHAbWhyGan29SVXNs2mNfP3M1IfzcSNCuhnqF8N/S7Epe36IxovIxeDG82nNGtRlcQKvKseSWa2VDPUCJHRFb5ok63pLMzaScvrHuhnHKhGG83b9bctgajrnJt/bmU7er4EM8Q+ob35fvD3zMsYhhTrp5Sefm1nPagKpLzkpm4diI7k3ZyR+s7eLb7sxj1zuuTZknjxqU30synGV8P+bpygfUikF+Uz5z9c/hi7xfYNFu5bWa9mUkFBkZmZMATO1h1LJuxc7dzZ89GvFXD2PO4jHyGzFhHs/r1WPzwVRi2fwHLnuPYTZ8yctc7PNX1KR7o8ECNznky8yRLopbw/eHvybeVF1jNCF7q9TqjWoxi8/FU7vhiM2OubMyUEefm2ZFXWMSwD9eTb7Wx7Klr8PVwjDN+e5qJp39mtYc7hUJg0jT6+7Vj2g3fyqQnTXrLGM6049KVvFHP6heafAT2fA8DXuafuI08/vfj9A/vz/R+0132o1NZpxi5dCSjWozi1ascig5Nk/GAKydDRD+4bR6YvFyHOZz4R1qY3OrB3T+VuLoX2goZsXQE7kZ3Fg9bjF5XXvCPjIrk1Y2v8n7f97k2PRl+Hie9JwbIqVg2RqfwyLwduBl0zL6nOw193c9rAruTKbkM/mAto7o04H+3dJKJlN5rzsTwCJYXpaGhoRd6BjcezLTkVJnEafy+cu9vm13jiYU7+WNfAm/f1IE7isdPZdA0jZeX7mf+ltM8N6QV4/o5cZn8bQLs/A6e2EE01pq7V+alSZfParhvF9oKuf+v+zmSdoS518+lbUDdxOJe6lTm3qmfPHnyBa5O3TN79uzJDz300MWuxiXBjztiiMuwMH5QFRoUFwzvFMb+uExOpOQy74GeDO8UVsc1dI3JqGfZ/gSmjGjPhMEtOZWay5wNJ4lJy6Nvq6Ba+8DP3XiSmauieKhPBI8WP4wsmVKQCmwBcbtlBjSfcBjyjhSgFtwOzQeWuDrUOX+5mDS3IFsO0K4YC62HyuxTnoHl3WOC28lYnLPr3XG03K9Jb+kGEdFPWvwO/Sb3b9QTCvOky+ix1bDgNlmPXfNkGcFnCTV2m3xJZ8bI1PQA3wyTZZZFs8m6XP0kNLqy0mQJkVGRzD80v2QwbdfspOSnkJ6fTv9G/bmn7T280OMFHu70MP3C+9HSryWexvKWyt4NejP/0Hysdiv1i2zManozjdr04khCNkt2xnLbFeF1MjGu3a7xyLwdFNk0Ph/jSF+9d1HV7Xap4aqvFVnk/HbV4MX1L3Iw7RA2zY5VK8LTK5PstLbcfJ7iG3fFpPPtplOM69+Mr03RHMqLo0izkVuUy7yD81h3Zh2hnqE80eUJplw9hQGNBuBvrqhoMOqNXBV6FdsStzGz/0yCPavOouducCfCJ4Ig9yA2x2+mqEziIbPezAs9X6BdQNX3/FzKruz4Ec1GYNAZmHdoHkfSjjCg0YAKMYglBLeTVtx+L8hlHfbRbQnbeHD5g8TnxjP16qmM7TC2wuC3LO4Gd/xMfiw4vIBgj+BLZgCmaRorTq3gyb+fZE3MGoY0HUKDeg1IyE0oEf76hfdjfPdnwDsUwnsQEexDvtXGNxtO0iTAk9ah3tUqy27XePi7HcRnWpg3tif+xiJYNAYadONTb0+i0qN455p3cDfUbFoIX7MvV4VdxXeHvsNyVobdImBP0m7+r/3/0dDPg2xLEd9sPEn3xv40Cqi5heLVnw+wPjqFWWO60aLYwp+fQeTyp5jn402B4xVlE4JYazZBniG06XhX6btjxasyq2LsdghoLhNzOSMnGbbPkdkcPQMhoh/HM0/y6MpHaerTlI8HfFzOql6hTUy+pFvS+fHoj1zb+Fr8zH7yvdjoKvmu3DJLup12uoPhXRoxoHV9grxMDGhdX457igrg2xFyGqH/+13Ooelg3qF5/HnyT97u/TaNfSp6sbT0a8nK0yvZHL+ZW/tOQZ8VLy2MDbpDQDPC/T0Y3DaEZfsTmLvxJPvPZLL5RBr5BTYGnIcMn8/9uJe4jHxmj+kuPWCiV8Gu7+g9aBpz4tdIxanexNzr52IMbieVtjoDRPQtOYdOCAa3DWFvbCZzNpygWVA9WoWU9/CY9tcR5mw4ySN9mzFhsIsxqLs/hLQnr/GVjFk2hpjsGNbFruPmFje7VBiVw+he+bjJ8S7TNI3XNr7Guth1vNPnHa4Ku6pabXU58Prrr8dPnjx5trNtyr3zMuNYUm6NXTvPJtTHncSseOx2rcbZmmqD55GfWO82gwafpyJ8GvLRwFdpFdyF91cc5URqLrPGdDtnLdjqI0m8/usBBrcN5vkhZWKV1k2TGsQnd1XUgCfskwPiOUOklq9sWu+6wugB1tyK66vrWtpxdOWaeyGg8VXyc/3/SjNmHvoVIh9CTinusPaXdS3NSYL9P8oJi3MSpUBn8oEXT8vt+c7nq6luHOQHOz6o4EIFoNPpeOPqN6p1Dg+jB/e1u49P93zKq3kaHsfWQLf7mXhdK5YfTOTjv6OZfGPtB7k/7ohld0wG00d3kjEYtciaCpx360sJ+enyPp/ZId2SaunGHBkVydqYtRTaZIa6AjT0pgPsSPiL3ILude5OC7DmSDI6AVn6jaxL3UOBIwbFrtnRCz0Pd3yYx7s8Xq1znWv8xtkuljV10axN2a6Of6jjQ/i4+fDmljd5dOWjfDTgI+q51Tvn89cETdP45sA3zNw5k3CvcL689stqa+lHNh/JL8d+YfqO6fQL71cxA+UFJjo9mne2vsOWhC209GvJW73fontI9xLranyujDFvG9BWKrIaXVly7LPXtmLXqQxeXLKPdmHepQJQJczZcIJNx1N59+YOMg7un/chN5msm77gl83PMzRiqBROzpEJXSc4zyzY/enSel/XitVHknjuxz38OaFPjeLK/jqQwMKtp3mkbzN6NStz73bNY4aPBxbKJy+y2CzM2DmjxIUakIpJ/wjYMBO+GACthsq4reTDZWK0gmVMemEOtLweApuTYcng8b8fx6Q38WH/D6tlLX+408P8cuwXPtjxAR8N/Kh0Q5e7ZSKjpIMy/vVsNE2uv2sR1AsuZ/HKsGQwa+8srg67ml4NKiaHAdDr9DzZ5UmeXP0kkVGRjB46TSpEo1bI8Aygef16xGdaKCyysz5aZiudt+U087acrtMEdhujU1h+MJFnr2tF/eJJ4k/+AyZvjM0HwjZpBfU3+8s2rd9GunY2qZhB182g4/O7u3HvnK1M+GE3niY97cN8eHzhLno29efTNce4s2cjnh9SSdxfeA8I78GrayeSZpFWxtT8VF7b+Fr1k6FU410298Bcfjn2C492epTrmlxXvfP+B7i0/CsUteZ4Sg4RgbV7+Yf5mrHaNFJyXaXsPg/sXcQ1h96goS5FzseVGYP49UmeCNrF53d35XB8NiM+3sD+MxXnR6mKIwnZPLFgF61DvJlxW+fS2MFi3/Lmg2W667MJ6QD3/ymTj8wdDsfraA4Wa74UqgAGvy7noitLTeJuaoLRLK8FpPXS7EuFbGvFKbaFTmrkmg2A3hNk2uORn5TO2O7jPElQdQSIdEt6BasdnFva4z7hfQDIaz0UmsrvzYLqMbp7Q+ZvOUVMWsW5wmpCRl4h7/x5uHzyllVTXKQmdwRzp5+UWcdSomUfy8+Q0x1AqcCYGQOOfs6vT8r1dUFhHuxfAgvvhGkt5ITqJ/6RAuDAVytaX2vQ12bsnFHBzdFGAfrAZWx2pFiva9YeTaZrIz9m7a2oJLBpNhYfXXxeyj2bKb2m4G/2RyAumcD921rfxjvXvMPupN3c/9f9LicOri3R6dGM/Hkk0enRZBdmM371eKbvmM7ARgP5ftj3Ncp6J4TglatewVJk4X/b/nde6lsdsgqzeHfru9zy6y0cSjvEpJ6T+GHYD3QPkR5RxTE7zXyb0T6gPQsOLyhNBLT/J1j9Nga9jo/u7IKHm55x83eSV+hkCpoyHIrP4n9/HuHatsEyyZqmwcn10HIISy0x5Bflc2frO2t1XaNajKJPwz4I5DuuREHRbITMioiM358+ujMJWRam/naw2udOzLLwwk97ad/Am6fPtuK4+zLevblTV2OTzkRUelTpCjdP+U55ai/0fxlObYCVU8o/F7MTIC9F7hfYHKvNytNrnyYhN4EZ/WeUxIxWhb/Zn7EdxrImdg3bEraV39hqCFzjEIbXvAPvtYTJvvB2Q+nFAVIAOitEYdbeWeRac3mme+XJkPqF96NL/S58vudz8tCktfD6d8vts/65/gzrGIreMSTRCxjeMZR/nu9freurCptdY8pvB2ng687Y3k1LNwyeAuM2E5ufhA0bYZ5hJOQllPbx3uOhYTen53R30/Plfd1pHerFo/N2MilyH9tOpPHR39EM6xjKG65i3pMOw69PQW5KxWRJ9gLWxKwhMiqyehdWxbtsXew6pu+YzuDGg3mkk6s5gf+bKKHvMiIzz0pKTiERdWDpA4jPqNn8YbVi1RTctLOETIcAMqR9KD8+ehUCuPXzTSwrM89gVaTkFHD/N9vwcNPz1X1nWSPWTZOui/2ed32CgGZw/3LpFjn/FjndQG2I2wWz+sKP98uXfo8HYcTHDiFKyGU1pi2oNZ6B0rXVGZmx0hVszBIp6A18RdazzfBS99JzFCBismIYs2wMyfnJdA7qXBJUfS7WE4AWvi0wCAOHAhvLeEUHTw1siU4Ipq+o3f16f/nRislbXFkzMx1ZYP+ZLucZ+7gbTG8D7zZ2TJqMa4Fx2XOlv22VDx7Zu0jG5032lcvdC0vPeXAp/Ph/ELcTej4MD66GJ3ZIF6WOo2XfOse+Nr7r+JLBZDFmvRnSbmDNESdz6NWSlJwC9sZm0rdlEOO9O2Cyl7ciXMi5kYqFgAjfCD4Z+MklE7g/NGIoMwfM5Hjmce5ddi/xOdV/NlaHPGse41aN43jGcR5a8RCjfx3Nuth1PHfFc7zX9z2nypuqiPCJ4IEOD7DsxDLWF2djPU+UFVhBWoiXRC1heORw5h+az80tbub3Ub9ze+vbK7jIFltXJ3SbQEp+CkuilsgNMVth3f8g6RDB3mZm3t6F6OQcJkXux1WOBIvVxvjvd+PtbuTtmzrIZ4kQcPcSbCM/Y+HhhXSt35U2AbXPVj2l15SSaTEC3AN4ve0D8OlVMhGGg87hvozr15xF22NZdSjR1alKsNs1Ji7eQ77Vxszbu1ScEqfL3Yy64xfCvUqVgSadic5BnSmwFzD6t9HM3ju7nIs0Zm85B+hTeyH5YMXnIsCueWiaxltb32JbwjZe7/U6net3rlF73N3mboI9gnl/+/vOp1HZu0gKfTmJgCZdBI+tdKqIWcdgIAAAIABJREFUO5V1iu8Pf8+o5qNo4Vd5zLgQggndJpCcn8z8Q/PB7FM6ZdF6OUdnfW8zPu5G7IBeJ7BpsOFYKnYn1TwXFm+P4XBCNi8ObV0+4ZAQ4NOAk5knARjceDB2zc6JzDKx3Zmxck7d/IrTbXibjRxNzKGgyM6KQ0klquPf9sbT9tU/nVdm9Zuw7ydAMGPnjIrJkmwW3tv+XvUurJJ32bGMYzy37jla+7dm6tVTL7nY4YuNao3LiGMlmTtrZ+kL9ZEvjPhMJw/h84XLgbRc3y7Mh6WPXy21S/N38uGqKJcv2GIsVhsPfbud1NwCvry3e4kwC8hg8p3fyvS/fk0qr5t3KPzfHzLNc+A5JgexFcGad+HLQfKlcs3TpQJUx9Ey+Hhyhlyeb4GvGFdWueq4+52DALE/ZT93L7ubjIIMvrz2S2YNnlVr64mb3o1mvs04lHZIWrMSZBB3iI+Z+3s3ZenuMxyMy6rxeQH2n8lk3pZTjLmyMW3KxuxU1W49HpIJAm76AobNkP2mOGudq35enJ0VZObTac2l69Oie2H5K7DXYdFyZilc+gj84nAvbT0M7v0NJhyA696ULsllta616GttA9qilbEMm3Qm+jXsy1X1h7DmaFKV/8easu6oFCT7tarPqEJBxzLjxXNVEtSGYiGgJpatC0Gfhn2YPXg2qfmp3PPnPeUHbrXk1Y2vlrhgJecnk5yfzJwhcxjTdkytkms90OEBmng3YermqU7dvOuCsgLrY6seY0v8Fu78/U5e2/gajb0b88OwH3jlqlfwNftWep4rQq6gS/0uzNk/B6vNCn2fBzcvGZsG9G4RyPiBLYncdYaFW524nAHv/XWEI4nZTLu1IwH1TDIZRW4qCMH61H3E5sRyR5s76uS6PYwe3NxCTmz9ft/38fBvBpYMGR9XhicHtqB1iBcvLNlHem5hpeecs+HE/7N33uFN1usb/yRp2nSPdFGgg+6yK4ogUoYKeBRw4RHFc8TjcU9cx3FUjh4Vwa3noPz0uBfKUFmKbASRIpvu0kJX0pHuZv7++DZp0yRt2qalaD7XxcXVvG/epCG87/t8n/u5b3bkqPnnZcNt7y9yfxTzb0BcYBwyicxyTl9+8XJWzVnFtKHTeOPAG9yw7gZLAW7BO0goIuyhOcWnJz5lZfZKbh5xM5fHX+78B9GKwkPB3WPv5mjlUTYWbrTdYfNibBQvRkPr49a8sv8VPGWeTkvKx4aPZcrQKbx35D1qmluLp4Ofwo9PweGVQJtr6bd3TWJKchiaRi2z39zJgaLqTo7cNXXNOpZuymJcTDB/GtmuM7rxcVgvFroLawsBuCjmIgDrjmxjFexbAXv/a/f4Ox+eyiVpEZZlQIVcypwxUfa7lCUH4PhamHAn+Cq5L/0+u3OrtdpaFm5cyJ7SPV1fT+xcy2qaa7j7p7tRyBS8Ps05CfAfDXfR9zsiX9XAbOlOMtZNbesCdFc2duhLUr+YSL7XfCZ9N9V1srOucKIACfdX8Nkt53Nl+mBe/iGbuz87QJPWYPdpJpOJh1ceIrOohlfmjWHUkA4X9mNrQSqHyW3B0h1Xha3wDhbdLolEZNDtsX8itIvmFLx3CWz9t8iru2O3kE2eaXop9+tOAbH91HYWblyIt4c3H836iDHhY1zWPUkJSeFE5QnRPV250PL4bRnxBCjkLNl4otvHNBpNPLnmCEpfL9uB9On/hI4B0+0/t8gRois6ah6Mu0lc6Ca2ZpM5+p4HtLO1Tpsj5ly8AsRc6d7/woEPxTZ7nUKA/K3ib0UAxF0InRhq9JS1eWuRSWR4SoV5glLbyDOBY8lICqO4qonCyt5JaTuyLVtFqJ8nw6MCoDKXOXJhBDGQJJYDhfSIdN6b+R5ag5a/rP8Lxyqdl+054pNjn/BT0U9W+YYSJJbuQG/wlHnyzwn/5HT9af57sBvn0m7QvmAtayzjb5v+RkVjBc9f+DwfzPzA6a6aRCLh1lG3UtZQxpq8NULuN/lByNkkjLCAu6clcGFiKE9/e9RmBGFXrpoVOwtYcH4MU5Nb3Ye3Pg9vpENLHZ8c/4Rwn3CmR0932e+eHiHmzz1kHiKrNf0vwryjutCyj6eHlJfnjaGmUcs/1x51eKxjJUKWenFaBNed10HWX3FcxBn8IjwjSupLGBU2yuqcHqIIYdmUZSzNWEpJfQnzvpvHisMrrLt+Ds6Lu5VDWLJvCVOHTuWe9Ht69mEAlw27jOTgZF7LfM0yk2yhiwVnM7+W/crmos0sHLGwW7Oo9469l0Z9I+8ebg1rn/o4DD1fLNSpsli+YBzPzh1BWlQA/7vpPNbfNxmFXMa17+xh1YGeZwW/tSUPdb2Wf7YPVDfohKN36yJjgaaAEEUIw0OHI5fKLXmjAAwaBcl/gj1v21UFhQcoCPP3AonIVuw04uunZ8U91IQ7Afsy5IuiL+KhcQ9RqCnklk23cMP6G9hWvM3pxUSdUceibYssEuBI38hufFp/HNxF3+8Iz2MreUG+Ann9aXo0L9TaRZDVnkIqAb/mUtfOG3WCbuoTNJs6zANI5TYFiEIuY9k1o/nHrBS+P1zKvOU/U6axlaG+vjmXtQdLeGhGMrNG2tH/T7pPRCL4ixNDx1XhRl0nN7D7/wcbHoEfnmqbcesMRaBwz7z6fbhqhTj5DQR6KfdzlpXZK7n7p7uJC4zj40s/Ji6wbbbAFd2TVGUqlc2VqGIngjrLcmMT6C3njinxbM1S8XNe92bOVu4/xYGiGv4xK4VA7w7fy1HzYPYbPfvcHBXaFz3d9vO4hTD7dbhxNdyTCY+Xw7WfiG2OblAaXC+vbI/eqOf7/O+ZMnQKU4dORSqR8lZZBT41RWQkiRvZbVkVLns9g9HE9mwVkxPDhJlUZQ4aXzFbExsQO6AklgOFlJAUPpz1Id4e3izcuNB2hqkLjCYjxyqP8c6hd1iwbgEv7HsBXcfw5lZjDldwbuS5zE2Yy4dHPyS7upey+Q50nBkymox4SDz4+6i/c9mwy7rdpZwYNZERyhGsOLxCfCbjbxWS/01PgNGAVCrh1WvHEOLjyZ2fZlLbLD43TaOORV8eZFiYL49d2lpkVhfCr+/D8LnkN6n4ufRnrk2+tsvoj+4Q7S/s9M2dHNJvFAuW+z+w2i8tKoB7pyfy7cESvjtk221r1hm49/MDBPrIefGqUbaf297lYgFs9HxMJhOFtYWkKdPsntNnxM5g1ZxVTBk6hdcyX2PBugXk1eSJjXbOi/nefjwYrCAhKIEXLnyhVzI9mVTGA+Me4HT9aT478Zn1RicWnI0mI0t/XUq4Tzg3Dr+xW6+dEJzA7PjZfHbiM0rqS0RY/TXvi9/3yxtBa23klhThz5o7L+Cc6GDu/+Igz687jsHYPRVFUWUj7+0s4Mr0wdYL3gXbRMGXJhQShbWFxAbEIpfKiQuMsy76ADIeFgXfXrtGkJYu5ao7LuD68TGo6u34QJzcLbrBF9zXFroOPDOhbdFO6a3kuUnPcePwG1l/1XqePP9J1I1q7vrpLuZ9N49NhZvsS3Pb8eIvL/JL2S88PfHpbkuA/0i4i77fERecfAsfSYdVLLMxh8kEx9bY/mmVw2HQwYZHHRhU2MocXE3p0Nl8ajB3vyTiQmICosba7CuRSLg1I54VN44jX1XP7Dd38ltxDRW1zcxb/jMf7znJKz9mc1X6EO6YYicEuEEt/m4XGN5+VdjsJOWQmc+LG/Ndr4qi2Gin21hbIoLVdc3CJOaWLTDiSuc/kP6iD6WlJpOJNw+8yTM/P8PEqIm8P+P9PnHrSwkRbqzHla25QTk/WLb9ZWIsgwIVvLDhhNMrhppGncW85cr0wbY7bFsCfuE9+9x6UmhLpW0Xy95IcnvB7pLdVDZXcnn85cQHxWM0GYnxHQSVuUQrfRgW6svWbNcVnodO1VDdqCMjOUzIjJqqUXv54Cn1ZM3cNQNOYjlQiAmI4YNZHxDhE8FtP9zGlqItnSoY6rR1bCrcxJO7nmT6V9O59rtreePAG+iMOqYMmWITZOzqOcpF5yzC39OfZ35+psubuu5gL2Bdb9Lzn4P/6dHxJBIJt46+ldP1p1mXv044O166TGSvtRYjSj8v3pw/llPVTTz01UHKNU1MXbYFVV0zr107Fm/P1u771hfFczIe4dMTn+Ip9eTqpKt79ft2JDpAnAuLalvdlgMHQ9JMES+jt75HuC0jntFDAnly9RFUddY37c+vO05ORT3LrhlNSMfc36Zq4Xw98hrwVaJqUtGkbyImwDbCwIzSW8nLU17mpYyXOFV/imu+vUZ0/UZcCZe/Tm7IUOYOjuRAyFDujklALvfljWlvuGSBZ2LURC6IuoB3Dr1jyYEFnFK8rCtYx9HKo9ybfm+34zQA7hxzJxIkvPXbW+KBgCixAKzKEi6mHQj29eTDm89jwfkxLN+ez98+2GdZSHCGFzYcRyaV8PCMFOsNR1cLaXKr0qhQU2hZhE0ISrA9R0SNEd+bn9+0jWgCqy7ls3NH2M90DhwK590qxh7aUaOtwYSJMO8wq0U8L5kX85Ln8d2V3/HsBc/SrG9m0bZFXLHmCr7N+9aqQ2w+t72R+QZfZH3BTcNvYnb8bKc/pz8i7qKvjzAXIBV1/WeGEqx3cNNl7g58eaPtn4Otq166Jmh00A1x0oa/N5Romjhpam3HP5gD9/wGCj/45u9tzocdmJ4awTd3XICXXMq1y3/m/i9/Y19BFf9cc4TzYkP495V2XKRO/SrMNXJ/tDzUbScpqQz+9LK44Gd+KOb0XhneJqn9fhG8fb74bEsPiue4KFz+bEFn1PHEridYfmg5VyZe2af6+uRgYQ99Qq8RVuDZbXMbCrmM+y9K4mBxDRuPljl1vKWbsqhp1PLMbDvfn9pS2PJvsXrZU3pTaPdWkttD1uSuIcgriMmDJ6P0VgJQFRIrsh6ByUlh7MmvpFlnX27dXbZlq5BIYHJimJDTPVyAOiCSUO/QXs2S/RGI9I3kfzP/R1JwEvdtuY+bNt5kUTA0aBvIrs7m/w7/H3/d8Fcu/PxCFm1bxOaizYyLGMdzk55jy7wtfH7Z57wx/Q2mDJ3Sa7OlzghSBPHQuQ9xSHWIr7Jc48T6U9FPNll10PuCNWNIBikhKaw4vAKD0QBJl8DwuVbn9nGxITw6M4WNR8u54j+7qWrQMXJIICOHBIodKk7Aoc/hvFuoU/izNm8tM+Nm2s2W7A3eHt5E+ERwsvZk24OT7hcLlh3+/3jIpCybN4ZGrYF/fHPYsji25UQFH/x8kpsnxTE5Kcz2RTI/An2T6HqC5bVi/B0XfWZmxs5k9ZzVlq7fjetv5GjUcO6IiiLf04u/hXhRoq3l1amvEuXnuqzg+8+5nzptHe8eerftwS4W4pr1zbyW+RqpIalcNuyyHr1upG8k81Pn823et21d7fipMP9L4SnQ3pirVVkll0n519wRPDt3BDty1Fz59m4K1XbinTqwN7+SdYfLuH1KPJGB7aSWBh2c+A6SZ4FcgaZFQ3VLtaXoSwxOpKShhHptvfUBMx4R33N953OfDgkaCpcuAU/r6392lfgcXp7yst1FPLlUzpyEOayes5qXJr+ETCrjsZ2Pcfmqy1mZvRJNi8aiznrn8DtcEHUB96bf27P3+AfCXfT1Ea9vzmFfYRWv/5jT9c4uQG8wUmpS2t9o7gLcvtv2j3nWyNNX5NF09vw+pFTTxF5jKqoLngIfpTBPuewV4UK43bGjU3KkP+W1LbTojezKrcQEGE3wS2EVo57eZPuEzYvFrNTQ8ZaHHDlJdSpjkkhg2hMwch6U/tZaGLdKavetAG8l3LZTBKEPcDqdZewBDboG7tp8F2vz1nLH6Dt4esLTLpUudcTP049o/2hOVJ2AxBkig0jbJs+9Mn0wCeF+LNmYhd7QeTfhyGkNn+w9yY0TYkmLshO4fGQlYIKR81z+uTlFP0ly26Np0bC1eCuXxl2KXCYnzFvcAKoDB0FlHphMZCSH0awz8kuBa2IDtmapGD0kiGBzd8EnBLW29oznup0tBCuCWTFjBUFeQdS01Fjm2jK+zOCqtVfxauarNOgaWDhiIR/M/IDt127npYyXmB0/2+oz7o+oisuGXcb4QeN5NfNVKhp7LhFuMbTw773/5t4t9xIXGMekwZNcWrBKJBL+PurvFNYWthmCmEzCBXrX65b9lm7KAqCk1f36t2INsY9+T/IT6yH3B/D0g0kPsDp3tYhpSO1dTIMjYgJirIu+oefByKuFvLADCeF+PDQjmR+Pl/O/XYVc8fYuFn35GymR/jw0w0Hm2sldEDNJRBvRruizE1ZuD6W3kmUZy1gyeQnFdcVc9/11VDRWYMKE1qAlRZnC2HBbpU9vSA5JZnb8bD498Smn6totZneyEPfx8Y8payjjoXMf6pXE9G8j/4af3I/XM9u+KzTXwPcPdBrhc8P5MXx083gq61uY89YuduaoHb6G0WjiX98fIypQwS0XDrPeqG2AUX+GMcIwyGz4FBsQC4hOH0CeJs/6eYPTRc6rr4P7S0eYTLD+URFfZIes6iwkSEgKdhDi3opMKmNm3ExWXr6S16e+TpBXEM/8/AzTv5pu+b5IkODt4Y2sD2bZf2+4iz4Xk/zEemIf/Z6P9xZhMomwTcsJvw8prm7iRd089DIHBhMSCUQMt/3TOtOGVAaXPGvTRTD1QxcBxAXyhCka34x7hJwNhOnJ6PliHq4Tdj48lctHDbLk73l5OHCRyt8qNO0XLrLK5bsv/T4bu26nV4WLfrb//oxaEfcwwOnWLKMTqBpV/HXDX9lbupfFExdz+5jb+6UzkxKSwvHK43D+7aLYbvc99pBJeWhGMvmqBlbud9y1Npu3hPh62pq3mDn0BUSl0xgY5dLPrVv0s9vrxsKNaI1aZicI2Yy5KFBHpglzI4OW8+OUeHpIXRLdUNWg5eCpGqYkt3YXjnwNO5ahalK5i75usKlwE42Gtu+l0WREZ9QxJ34OP179I19d/hX3pN9DekS6zfnPTH9EVUgkEp48/0m0Bi0v/vJi10+wQ74mn+u/v57PTnzGgrQFfHzpxyzLWObygnV69HQSghJ49/C7Qo4qkYib2q0viGw5YMfDU5k1ItJyPbJyNZx4N9ydidEnmM9OfMaYsDEMVw7v9fuyh03RB8J+f8fLYrGmAwsviBMKmXXHOVBUQ02TjtevG2tt9d+e6z6Haz+y/Hiy9iSeUk8ifZw30JBIJMyKm8Uto25BIpFgMLUpBXKrc53PbesGd429C5lExusHXu9y38qmSlYcXsGUoVM4N/LcXr1uoFcgC0cuZNupbewv3y8edJj5aj1SMyFeydq7JhEZoOAv7//C+7sK7I4rfJ15iiOna3lkVkqbnNiMdxDMesEi7bQUfYGxQFvR53ARs/gXMRLkLCe+g73/gTL7RV92dTZD/Yc6fU6RSqRMjZ7Kp3/6lBtTb0Rn1Fm+LyZM7Dy9s0++L7833EWfi9nx8FRmj4myhG12amPrQvJV9aw1TqLoghdaQ7fpfhfA3EXwi8CEhFPGUGqmL+2XCIFSTRPnKErwaThtvWHu22129w4ID1AQ4C3HaDLh5SFFa7DjImUyweZ/CZfEcQutnn95/OUWm2kzg/0GO7cq3FVm2wCnW7OMdmjf7cqvyeeGdTdwsvYkb05/kysSr+ijd21LqjKVU/WnqPUNFrEaHQrNS9IiSI8O4pUfsx06vq7MFOYtj85KtTVvAeFUV3YYRl3b68/tbGJt3loSghJIC0kDIMxHFGOq4CFw0VPg4YW3p4zzhynZlt17M5cdOSpMJhHVAIiw+YOfU9lU6S76usGrma/ayByNJiM7Tu8gwteBqsMO/RFVERMQw99H/Z1NJzex/dR2p59nMplYnbuaP3/3Z8oby3lr+ls8fO7DeMo8+6RglUqk3DLyFnJrctlctFk8eNHTYNAK2TfiehTi62m5HllcDQ2t/zf8wth5eifFdcVcn3p9r9+TI2ICYqhpqbGeX9O3iLy0DvENAKn/3MAvhVXoWk1DjCa45JXt9hestY3iHNsuuPxk7UmG+g/tUbfl/w7/n81MpytNg9oT6RvJgrQFrC9Yz1G1Y9dSgLd/e5sWfQsPnPOAS177+tTrCfcO55X9r4iizUnnUIChIT58fcdEpqWE88y3x/jHN4fR6ts+s4YWPUs2ZjE2OojZoztIYg16MZLQzn+gsLYQD6kHg/3E3HqUXxTeHt62Zi5mtr4gfAq0XUtMMRrgp+dAmSi6i3bIqsoiOcRBF7kTJBIJ3xV812/fl98b7qLPxYQHKPD38sBsttSpja0LyVeJ/4gh518Pj56EpzU96wKMmgcPZrPlz1lM0r5OQdSf+uDd2lJa08yLsretQ6qh7eY9f6s46TigSxepssMiKybjEZBb/1vsPL2TFkMLQV5BFplAniaP3SVOzG2dIWMNV7AqZxXbirdZzTJuKtzEgnUL+M9v/+HbvG85UHEAVaPK7qpi+y7h3zb9jRvW3UCLoYX3Z77PpMGT+vV3SQ0RznhZVVni4rbuIStnVYlEwiMzUyivbeF/uwttnq9p1PHC+hOMiwnmyrF2zFsA6kohZBir/Ly7NwN6FlOoKeSg6iCXx19u6dgqFULmo25Si+H+RiHpzEgKI0/VQHFV77qe27JUBPvIGTm4dQ6qMg9dyDCqW6oJ9XEXfc5iLwurPwPtu8vCEQsZFjiMZ/c861TnvEHXwD92/oMndz3JiNARrLx8JZOHTLbapy8K1hmxM4gJiOGdQ++I86IyXnS8D3wk4nywvR4Fq/bAa6MhR8ySf3r8U8K9w5ke47qYho6YDVUsZi4A/hEiy/O3T4TJWDvMC9aeMjsdyvaUH4WlSZa4CjMna092auLSGf39XV04YiEhihCW/rrUocFXXk0eK3NWck3yNVaO073B28Ob28fczkHVQX4q/qnb9w9+Xh4sv+Ec7pqawOf7irl+xR7U9S1U1DZz8cvbUNW18ORlabbqmsLt8P4syG4LTi/UFBLtH23p8kslUhKCEsipcTCSlPEINKrtLhjYcORrUB2Hqf8QkSEdaNQ1UlxX3KW00xFn27ltIOEu+voAdX0LF6eJldSpSWH2bWxdTJ6qHqWvJ0GeJqivAGMvnNBKD5JW9ClgorSmf4xoSjXNhFIjXBHtkfODyDZq58zYni5dpAaNgrv2wRjb+YmV2SsJ9Q7lnUveYVjQMN6b8R4JQQn8Y8c/KG8o7/yNu8hY40zMh72a+SrNhg6dAIwcVB3kPwf/w2M7H+PG9Tcy7atpnPvJucxZPYc7N9/J83uf56NjH3HbD7dR2VQpul3NlRhMBj6+9OM+kyt1hsXBs/I4qLNFbpTKOp9v/DAl01LC+c/WXDSN1uZAy34Q5i2L54wQEQH2iJ8Gd2fy6tH3uj8D6mL66/uyNm8tUonUysBALpMT5BWEulElQuR3vgyIog+ECUtPMRpNbMtWMTkpTMjjjAaoyqcyWOSDuTt9zmPOwupLIxZXIpfJeWrCU5Q2lPL2b293uu9R9VGu+fYa1hes564xd/Huxe92q3vZG2RSGX8b+TdOVJ1g26lt4sHJD4mRgR+eBDpcj+YMZ5H0C/CLhNgLKNAUsKtkF/OS5/XprLPZwdMS22Bm3ELhvNlBqmdesNYZTZ3nru1dDkY9DBptechgNFBcV+z0PF9H+vu76ufpx22jb+PX8l/b/g078PL+l/H18OX20be79LXnJswlNiCW1zNfRz/tCdv7B4ms0/sHqVTCgzOSeeO6sRw6pWHOm7t4aOUhSjTNxCp9SI+2Ewl1dLWYJW2XD2yOa2iPXQdPM9HjYdgUMb+q7WRRxqATXe+IkZBmX+2TXZ2NCZPFhK27nG3ntoGEu+jrA5YvGMfSeeKEOC4uxL6NrYvJVzUwLMwXSn6DpYlW7pTdP9g2Inc/jT9NlGrsBEH3AeU19QQYasSF0R7TnoTwNFhzp6Wz4DTNteJvZbzNEHtZQxk7Tu/gioQrSAlJYfWc1YwIHcGyKcto1jfz0PaHbLKqrHCBsYar5+qcZepQW8mxQqZg8QWL2XfDPtbOXcvb09/msfGPcW3ytcQGxFLeUM7q3NUs2beEA6oDaI1tjl5Gk7Hb2WCuQumtJNw7XJi5JFwsHmzn4mnm4ZnJ1LXoeXtb24XtyGkNH+/pxLwFxHfOoAOJpHczoC6gv74vRpOR7/K/Y8KgCYT7WC/GhHqHom6uhOA4y3xQfJgvQ4K9e1X0HS2ppbJB2zbPpykGQwtqP1HsmU1k3DhHfxixuJL0iHSuSryKj49/LBZwOmA0Gfng6AfcsP4GtAYt7814j1tH39rvBg5/GvYnBvsNbuv2+YSI8/7Ux2x3zt4Ap/bBlEdA7s1nJz5DLpW7PKahI0P9hiKVSCmqK7LeEDcZQuLtdmy6VMw0VgmTkVHzrKSdpQ2l6Iw6p5w7HdHf39Wrk64mNiCWl/e/bB0UD+wp3cP2U9u5ZdQtBCtcm6vrIfXg3vR7ydfks9bH0/r+wcsfwlJgeNfxTpePjsJoMnG6pslyzi2sbLT1kDDoxXxd0gxLgak36imqK7LM85lJCEqgsrmSqmYH91gZj0BDhcgqdoRRD6OuFbJnqf0Sw+xg2hN5p5mz7dw2UHAXfX1EgELOkGBvjpXU9svr5anqiQ/zg7rWgNWAXtgc+4sw86HyWosDWV/SpDUgbapEitFxp0+ugCvfERedb+91LhQdxI368smw6Um7m7/J+QaTycSVidYn2WGBw3hqwlMcqDjAG5lvdP4avTTWOBPzYUW1RawvWE+QV5Dd1TIvmRdxgXFcOORCrku5jofOfYjXpr3Gytkr2TN/D8FethfCM62pT1GmcLzquMikihgJObburSmRAVwxZjD/21VIqaYJo9HEP7sybwHY/Ay8ng5GA1ckXkGgZ6Blk6fUs19XGfvr+/Jr2a+UNpTazT0K9Q4V8k5lvCW2QSKZ9HjIAAAgAElEQVSRkJEUxu5ctdWsSXfY2hrwfmFia3FXWwoeCtQ+gZbXdeM8/WHE4mruP+d+Ar0CeebnZ8iqyrJ0tKuaq7hr810s/XUpkwdP5uvZX3NOxDln5D3KpXJuHnkzh9WH+bnkZ/Hg8LkwuMP7MRrFLHnIMBhzPfXaetbkrmFW3CxL9EmfvUeZnCjfKE5qOpi5SCRw7s2iwOhgItKlYibzQ6uYBjMW584eyjuh/7+rcqmc+9Lvo0BTwKrcNmm+wWhg6b6lDPYb3GfOqtOjpzMqdBRv/fYWzWmz2+4fHi6EO3bblUTaY9cj05gxPAJpZx4SJ3eKOK60OZaHTtefRm/U23b6WmXQeTW2Rj8AxEyE1NngwPgJEIXl1H9A4kUOd8mqysLf059BvoM6/f0642w8tw0E3EVfH5I2KIDjpX1f9NU0aqls0IpOX20puXI5c3/+R8+lX/5CJpPi19Avnb5STRPhktZhc/9OnL8iRwpTl+NrIXezcwc/8BFUF0Cs7YyZ3qjnm5xvmBg1kSH+thr6S4ddyrXJ1/L+0ffZUrTFZrsrsDdX19fzYVqDlge3PYiH1IMPZ33Y7dUyiUTC/efcP+A09SkhKRRoCoR5RdIlULRHyJg6cP/FSZhM8Pz3J5j+8jYyOzNvAWF+cHQVRJ8PUhkGo4EmfZNFmuUj9+m3VcZuZ0r2gjV5a/CT+zEteprNtraiLwGqCsRqMkLi2aA1sP+k7efuDFuzVYwaEkioX2soeMwEeKwUlW+I5XXddI/+MGJxJYFegTxy7iMcrTzKwo0LLTPDV625ir2le3ls/GO8OvVVAr0Cuz5YHzInfg4RPhEsP7S8bS5M2whr7oIDn4ifyw+LRZGpj4NMzpq8NTTqG5mf0jfFREdiAmI4WXfSdsP5d8ANK22lhZ1hNIg4otgLhfN3O1xR9EH/f1enRU9jbPhY3jrwFkdUR5i7Zi4rDq8gqzqL+9LvsyyIuhqJRMJ959xHRWMFn574tG2DudirKYLT+7s8TniAglA/L0zgWJKbtR7kPm0KGMQ8H2Azq5gYlAhATnUnUWPXfgTj/25/2+GVcPzbLhfls6qzSApO6rWz99l2bhsIuIu+PiQtKoB8dQONWn3XO/eCvFYTl/gwPxprTnJHZDj5tSd7Lv1qlVgmeNdTqun7Tl+ppplTplCyJr8JQ7qwRZ54D1z9npU23SG6Jti2RGTyJV5is3nX6V2UN5ZzTdI1Dg/x8LkPk6ZM4/Fdj1vn+riIpb8utZmr6+uO2Sv7X+F41XH+dcG/iAuM69Fq2UDU1KeGpGIwGYT7WOIlIvS2pthmv6EhPtxwfgxrD5VQoG4g1M/TsXkLiDnSZo2QrCAuWI36Ru4aI6y/k4KT+m2VsUeZkj2gUdfIDyd/YEbsDBQetiZUYd5hqJvUmELiwagDjZCQTUwIRS6TsLUHLp6aRh0HiqqZ0jEIWipF3SLkRmYTGTe/b2bFzUKpUFKrrbXMDDfqG/n0T59yXcp1/RID0xWeMk8WjlhIZkUmv5b/Kh6Ue4MqCzY8Ci+nwfIMkTtrNGA0Gfn0+KeMDhvN8ND+mXs2xzbYmJWYP7+a4s7ns9ojlcGfP4GLF9tsOll7Eh8Pn7NuUUYikfDAOQ9Q2VzJLT/cQn5NPm8ffJsRoSOYETujT1/73MhzuXDwhaw4vMLaYdVkgs/nwzd/F0qlLuhSknvJs3DzJqtwdPOcZ8dOX6h3KIFegY4dPM0YjXBsrbUZUEs9rH8EfnnXxj3b6qkmI9nV2T2e53PTO9xFXx+SOigAkwmyyur69HXyVfUADAvz45+VP1Mlk/VK+pVrbGTu4Ei8PE/3S6evpKYJDX4oRl/ZeacPxIVnxFVCK15X1rlhzb4VwnHRnFPYga+yvyLUO5TJQyfbebLAU+bJsoxlYIJF2xahNWgd7ttdvs37lgZtg1VUBAjZSV91zLYWb+Xj4x8zP2W+pYPT09Wygaapt5i5VB0Xhf49mcLApwPJT6znvV0Flp/V9VqGPbbOcZbmoS/AN0wMsQOZ5ZlAWyf4oOpgv81h3pd+HzKJ9fxSX3RYfyz6kSZ9E5fHX253u9JbSYuhhbohY+DSpdDadfHz8mBcTAjbepDXtyNXhdEEGcntir6Nj8OOZagb1QR7BSO3Eyzt5vfH6tzV1OvqrR4zmowcqzx2ht6Rfa5MvJJQ71CWH1wuHpBIxIJkSy3UngZMYuTiu3vZtfMFiuqK+q3LB8LMpUHXQGVzpe3GiuPw2ig4stL5Aw4aLcK6O2B27hwIxXh3GRM+hgifCOp19ZgwYTQZ8fXw7Zff5d70e6nX1rP016VtxlwSiegMV+bCr+93eYwuJbkyuVBJtaNAU0CwVzBB5nivViQSiTBz6aroK94DXy4QSioze/8r3D27MLE7VXeKJn1Tr+b53PQcd9HXh6QNEqYQx0v7tujLUzUgl0nYX7mR7aYGWlrPVS3GFn48+SNP7HyCnad3klWVRU1zjUOLYmg1idjxMPmeXqzwPUZFfR06Qy+cQJ2gVNNMkqSYQZV7nZ/Vqy6Et86DPW/Z3240wr7/ExdgO9LO9gYuXTmoDfEfwrOTnuVY5TGW7Fvi3PvrBIPRwCv7X+GxnY+RHpnOlKFTLB0zKVJ0Rl2fuLqVNZTxxK4nSA1JZdG4Rb0+3kDT1A/2G4y/p78wgDBfsA16m4UBp63JQYQZZ28QCw2t0pv95fsZ7DeYSN9IpkdPp8XQwq6SXX36u5m5LP4yJBIJ0tZTtwQJk4dMdnmHdW3uWob4DSE93PYGD9oFtCv8hF29b1sHLiM5jBNldZTXdk8lsDVLRaC3nDFD282LHlsDqizUTeo+n4FyM3B4NfNVi4TZzJmeGbaHwkPBX4f/lb1le/mt4jfx4G+f2O6oa+KTE58S5h3GxTEX227vI8ydHKvYBjNhKRCa7FRhQdkR+OZWh/mzvYlrONOsyllFTUuN1WMHVQf7JYInOSSZmbEzWZ272tqYK2mmMNzZ+ry4BvWUbS/B1hdtHi6sLbQxcTFjdvDs7D6R6Akw9HzY+YoYf2iqgd2vQ9IsGNK5cWFWdRaAu9N3hnAXfX3IkGBv/BUeHCvVdL1zL8hX1ROj9OWN316jyWjdidKb9KzJW8PtP97O1d9ezYVfXMi4j8cx8+uZ/GX9X3hw24Ms2beE/x35H+vy13HX5rssNvzNJg1ekSu7ffPWXUo1Tdys2ILnN3/tVBZgRVCMmC3YvFjkBnVEKoVbfoLLXrH79FU5q+wauDhiWvQ0/jr8r3yR9QXrCxx0hJygXlvPvVvu5b0j7zEvaR7LL17OCxe+YOmYRfhGkB6ezhM7n2DHqR09fp2O6I16Htn+CFqDliWTl+Ap83TJcQeSpl4ikZAakiocPEHM9L0UL/IZ2+G0NTmAVwAsWA3niRkGk8lEZkWmxUAiPSKdIK+gtqDmPmZPyR70Rr1lnsmEyZJR6CpK60v5pewXZsfPdrjabXbRVDeqQZ0rbgpbsUQ3dKPbZzKJqIYLE0NFVAMI2ZmmGJSJqJvUbufOPxBnUw7XNUnXEOwVzH8P/Vc8YCdYu9DDg12eEq5JvqZfu9Xm2AbzzJ0VEgmMuwlKMm3OkTbs/a9YgLEzA6gz6ChpKDlri74zvcDQoBfjOVbqLIkELnlOzKTvWNqzAxsNIrqowvb+qEBTYCPtNJMYlEidro7yxk7iqiQS4UZbexqWJcOLMWIEYmgX4zkIExepREp8ULyzv4kbF+Iu+voQiURC6qCAPnfwzFPVMyzUl/vS78W7w+CxQqbggXMe4KNZH7E0YykPn/sw16dez+iw0cikMk5UnWBl9kqW7V/GIzseYV/5PosNv96kw8P/OF+e+KZP339JTTND5LXg142cJYkELn8NFIFC+65vd9LWNYvujk+ImOvqgN6o5+ucrx0auDjinvR7GBs+lqd2P0W+Jt/599pKcV0xC9YvYOfpnTw+/nGenPAkcqncqmP29vS3eWv6WyQGJ/LA1gfaVo97yTuH3iGzIpMnz3/S4Qrf74GUkBSyq7OFBXdokpBZ5dhGN3Q5B2FGKoXYC4RLJVBQW0BVc5Wl6POQepAxJIPtxdvROTF/0Vs2FG7A39Of5RcvJz4wnrHhY1lxZIUwVXER3+V/hwmTQ2kntOv0Nalh1d9h0+OWbSmR/kQEeHUruuFYaS2quhZLwQhAVauDnDIedZP6rJsXctNzBuLMsCN85D7cOPxGdp3exRH1EbvB2p8F+ONhMnU6P94XRPlG4SH1sF/0gZhT9vDuvNvXUAmHv4LR11rFNJgpri/GaDKetUXfmVxgWJWzyirmyMqYa9AoSF8A+h6OlJzcLeIV0qz/z9Rqa6lqrnLc6WtdwO1S4tmgBonU2ixt+0si0qMTsqqziA2ItTsr7qbvcRd9fUzaoABOlNVhNDopW+wmeoORoqpG4sP9uCIqg8maKsuEmPlCedOImxgTPoYZsTNYkLaAB8Y9wIuTX+S9Ge/x3RXfsXf+XnZft9vKht6MRKrjy/zlffLezQj3zm4WfQC+oTD7TSg/Aluea3t8y7OwYprDk6XZwKW7OUlyqZwlk5egkClYtHWRjaFGZ/xS+gvXfX8dFY0V/Pfi//LnlD9bbW/fMfPz9OPti94m3CecOzff2esA7n1l+1h+aDmz42d3eiP/eyAlJIUWQ4twJ/MJEcZAdqIbupyDALFiv/4RKzMY8zxfe9nj9Ojp1Onq+jyjsMXQwuaizVwcczGpylRWz13N4omLaTG08Mp++x3t7mIymVibt5ZzIs7pdEEk1EcUYKomlXDwrGyz+DZHN+zIUaF3Uhq+tbUraDXP1xoFYQqJR9Wksrymmz8GA21muDP+nPxnAjwDWH5ouZhpatcRq5dIWOPvy8yQUf2+cCGTyhjqP9Rx0ecdBCOvgmOrrRdO25P5Aeib4bxb7W42R0KcrUXfmVxg6NKY6/LX4dIejpQcWy0K+iRrQxqLc2dAnJ0nCXkn0PV9x+bFYOpwftc1icc7IbvKbeJyJnEXfX1MWlQAjVoDJ6v6xuihuLoJncHEsFBfqCthsboKWes/a3cs+P09/Vk0bpHNipfJKOf8wAV98t7NlNY0ozRVdW3iYo/kmXDOX6F4H7wyAp4Ogt1vgEwBHvYljCuzVxLqHUrG0Ixuv1ykbyQvXPgCeTV5PLvn2c517618mfUlt/5wK0qFks/+9BnnDzq/y+eEeoey/OLleMm8uPXHWympL+n2ewWoaq7i0e2PEu0fzePjH+/6CWc5Zqnj8arWYOfEi4V0qa4TqYojDq8UsiZjWwdvf/l+lAql1Q3OhKgJeHt497nEc+epnTToGpgZO9PyWGxgLDem3cjavLUu6QofUh+isLbQbjZfe/zl/nhKPalsqgRlopBhtnMBzEgKp7ZZz2/Fzs2jbMtWMTwqwFpiazRASDy1AeHojDpCFe6i74/EQJsZ7gw/Tz9uSL2BrcVbyRoy2ipwe03YEBqkUq6faCe4vR+I8XcQ22Bmyj/gzn3gYSeewKAXhmhxkyEize7TXRXXcCY5UwsMXXYZzfL64n1Q0o3zu9Eg3DUTLwZPX6tNFudOB52+QK9Awr3DyanpJLYB7MqYO30c0WUsaSghKaSTTFw3fYq76OtjzGYufSXxzKsQDmfx4X5QW4KPyUSA3A9/T/9uXyjNK15mN0kvoxGPxmSCjBf0yXsHqGvWUdeiw19fBX4R5FbntrlYOUv0BCg9IG48aS3CSg/YlRmUNZSx/fR2pwxcHDFx8ERuHX0ra/PWsjp3tcP9dEYdz+55ln/t+RcToibw8aUfW2YsnGGI/xD+e/F/adI3cesPt1LVXNWt92kymXhi5xNUt1TzUsZLA/qmyVXEBsbiJfNqm+tLbF3lzP2x+wc79CUMOU8EK7eSWZ5JekS61aybwkPBpMGT+Kn4J4wdVz5dyPrC9YQoQjg30npu4tZRtxLuHc7zvzyPwWjo1WuszV2LQqbgkhjbiJP2SCQSwnzCWjt9rbMZVW2S50mts3nOSDxrm3XsP1ltLe0EGHk13JOJWi+KyTAf90zfH42BNDPcFfNT5+Mr9xXdvlHz4P4jGJ+q4vNBcYwKHcWI0BFn5H3FBMRQVFvk+NwUOMSSzWuDvgmGXwET73V4/JN1JwnyCjrjuYm94UwtMHTsMkqR2nYZ9Vr48kb4fpHzRnfNGjGW0Boz1J5CTSEeEo9OlRwJwU44eNqRMXf6OKLLB24TlzOJu+jrYxIj/PCQSvrMzCVf3Vr0hYqiD6DR2MJViVf16EK5eOJiSzGkNBgZ2TiTkpq+i20w5wD+POk9GsfM547Nd1i7WDnDT88KWUF79M12ZQarclZhNBmdNnBxxG2jbmP8oPE8t/c5sqqybLbXNNdw2w+38UXWF/x1+F95Y9ob+Hv6d/t1koKTeHPam5Q2lHLHj3fQoGtw+rkfHvuQHad38OC4By1xBr93PKQeJAUntRV9kSNh8sPCarw7lB0RA/Cj5lkeKq0vpaShxDLP157p0dNRN6k5pDrUm7fvkEZdI9uKt3FJzCV4SD2stvnIfXhg3AMcqzzGqtyeO85pDVrWF65nWvQ0/Dz9utxf6a1sC2gHixwTINBbztihQU4Vfbty1BiMJqYkh9vdbp5XdM/0uRnIBHoFMj9lPj+e/JG8GiF33l2ym8LaQuan9l9MQ0eiA6JpMbRQ0dhJdmZVPvzvMij+xfpxL3+Y8RwkXuTwqWezc2d7ztQCg7nLCIAE2y6jhydMewJO/wpHvnbuoD4hcM3/IPUym00FmgKG+A/pdNE7ISiB/Jr8zhcRO8iYAfFzJ5ENFudOd1zDGcNd9PUxXh4yEsL9+iy2Ia9ChEsH+sihrpRmiZRmQ0uPV9185D6MHzQeD4mMt8pVJPsa+jSgXRxbgvewCfzzxPtUNld2P2PQSZmBwWjokYGLPWRSGS9c+AIBngEs2raIQ6pDlg5lXk0e89fN50DFAZ694FkWjVuETCrr+qAOSI9I5+UpL3Oi6gT3/nSvU1mBR9RHeDXzVaYNncZ1Kdf1+LXPRlJCUjhedVxIbyUSmPY4RHZzlf3wlyD1gOFtiwP7K/YD2C36Jg+ZjIfUo88knluKt9BsaGZW3Cy72y+Nu5T08HRez3zdOui3G2wt3kqdto458XOc2t8c0E5oIlz3uU00SkZSGIdOaVA7MslpZVu2Cn+FB+nR7TKjTCb47yTYu1x0E3EXfW4GPgvSFqDwUPDOoXfIrc5l0dZFBHkFddk570vMLo0O5/pA5JCW/Aa/vtf2mDoH8n7qsrt0UvP7KPrOFOYuo1KhxGgy2vcKGH2dWMD88RnrQHR7GI1WM9Yd6SyuwUxCUALNhmZO19uP6ADEgmg7GTOBQ8XP7RZKO5JdnU2wV7DbifkM4i76+oG0PnTwzFfXMyy0dVU+bS61lwl73wDPgB4fMyYgBoWHNwkPn6I+YlyfBrSX1jQRhZqs4/9me/E2S0Fj5WLVFU7KDHaVCAMXVzmohXqHsmTyEopri7l5483k1+Rz86abmf/9fBp1jbw34z3mJDh3A90Vk4dMZvEFi9lbtpdHdzza6Qpcvbaeh7Y9RKh3KIsvWHxWBub2hpSQFOq0dW0XLIMeCnd2eiG0wVzwtcuf21++Hz+5H4lBiTa7+3v6Mz5yPJuLNjs159ldNhRsIMIngjHhY+xul0gkPDb+MTRaDW/95iC7sgvW5q0l3Duc8YPGO7V/qHeoKPrk3pA8SxgrtcPcuduR47jbZzKZ2JqlYlJCKB6ydpejxkooOwwmo5gbxF30uRn4BCuCuTb5WtYXrGfhpoU06hvRG/XojH3v7OuITmMbzHj5i5v1o6ugsXWMYNer8Pn1QirogEZdIxVNFe6ir5ckBCfwUsZLAByrPGa7g1QqIhw0RbD3P50frHgPvJEOWRtsNhmMBopqixyauJhJDBbXuC7n+lplzDxdI/7upOADEdeQFJL0h7snGUi4i75+IHVQAGW1zVR2seLdE/JUDcSHtw7qRqShGTYZoFf6+mBFMPW6erRSKYMCvVHXa2nR925WyBElmmbOkWXzbuk6mgzWK1hOZ+U4KTP4KvsrlApljwxcHDEuchzxQfE0G5oxYaKquQqZVMbnl33u8Aa9p8yOn82D4x7kh5M/8Nze5+wWFyaTicU/L6a0oZQlk5ec1XMWPSVNKQwHLBJPfTN8ONd6Fbsrpv8TrnrX6qHM8kzGho912LWdFj2N4rriri+U3UTTomFnyU5mxs5EKnF8yk4OSeaapGv4IusLu5LjzlA3qdl5eid/iv+T011ppbeSmpYaEVVxOlPcMLZjeFQASl/PTvP6ssrrKKttZkpyh5VfdetnqExA1ahCIVPgJ+9acurGzZnmL8P/ggQJ1c3Cyl5r0DqvWukDwn3CUcgUnRd9IDL79M1w8HMR03DoKxj9Z+Hw6YCiOhH67i76eo/ZhOxopZ3cYYBhGTByHnQ1b3h0NXgobJQXACUNJWiN2i47fcMCxRx7b53D26M36smtyXXP851h3EVfP5AWJbpurpZ4VjdoqWrQtnX68reiUQnXwt4WfQBVPzzO+HphgFGucX3BCqLTN0xRz31VNZZhZjNeMi/nsnKckBmUNZSx/dR2rkjsuYGLPVblrOJUvbWMVGfQ8XPJzy57jfb8ZfhfuHnEzXyV/RVv/vam7fvJXcX6wvXcOeZOxoaP7ZP3MNBJDE5EJpG1OXh6+Ymh9pwfnDuA5pSNpKmquYp8TT7pEekOniSKPgkSl0s8fyr6Cb1Rz6xh9qWd7bl77N0EeAbw/C/Pd6vjuC5/HQaTwWlpJ7QFtFc2V8KBj+C7B6y2S6USJieFsT1H7TCyxlwQZiR1mOczzwcqE1A3q1F6K92rw27OCnac2mH1XdUatc6rVvoAqURKdEB010Vf5EgIjoMfnoSXhoGhBYI6L+YsTpAOgr7dOI+fpx+xAbH2O31mrnoXxtuPzgCEtPP4Wki4SFz3OlCgKQC6/vfykfswxG9I12Yu3aCotogWQ4t7nu8M4y76+oHUQeaiz7UST7OJy7Cw1k7fl39Bc+QrALuZe84S4iWGiqtzNhBXI4qXkj6SeJZqmonxrOeKJj2jQkdZbZNJZFwcc7FzB+pCZrAq1zUGLh3pMmenD7g3/V6uTLySdw69wyfHP7E4nm4p2sLze59n/KDxLByxsM9ef6DjJfMiLjCurdMHwsVTnQXVhZ0/WdcMb0+AH61X5g+UHwBgXISdPL9WQr1DGRM+hp+KfurpW7fL+oL1RPtHkxZi3zK9PYFegdw99m72l+9nfcF6p1/j2/xvGa4cTnxQvNPPsQpoVyZAU1WbNKyVKclhVDVoOXzavkRsa5aKlEh/IgM7BPVW5oDME4KiUTeq3TMgbs4aXs18FYPJWhnT19eErogJiOm66Dv0JdSeBqO+7bFtL3Qatl1UKzp9Q/2HuuJt/uFJU6Z1XvSBWJA8vBJUdtQcp36BulLhuGoHc0ZfV50+cNLBsxtYTFzcnb4zirvo6wdCfD2JDFBwzMVFX55KODnGh/mJjKzmGmq9ROu/N52+EO/Wos8nGH+dcM7rq7m+Ek0Tgz004BfBlUmiIJMgQalQ0qRv4undT/d6RspgNPBNzjdMjJro8otTlzk7fYBEIuHJ859k2tBpvPDLC9y08Sbya/J5YOsDeHt48/yk53tlHPN7IDUklROV7Yo+c0BtV92+7A3QUgvDplo9/Gv5r3jJvBiuHN7p06dHT+dE1QlO1TnOKuoO6iY1e8v2MjNuptOdrqsSryI1JJVlvy5zygE3qyqLE1Unuszm64i5EFM1quw6eAJMSghFIsGui2d9i55fT1ZZB7KbCYqBkdeAVIa6Se2e53Nz1nAmrgldERMQw6m6U+jbF3Qd2bwYOpqEdRG2fbL2JOE+4X+IOKD+IE2ZRnljucWx2C5N1UJVsekJ223H1oDMyyaQ3UxhbSGBXoEEewV3+V4SgxIp1BQK+b4LyKrKwkPiQVxg5/OEbvoWd9HXT6RFud7MJV/VgFwmYUiwt1jdAWpbZ9t6Je9sPSFUefujaBY3ayU1rnfwNJlMlNY0E4oG/MJp0IoiNiYghhWXrOCe9HvYULiBL7K+6NXr7CrZRVlDGVcnXe2Kt21Fx5wdL6mXbc5OH+Ah9WBJxhKCvYKpaanBhAm9Sc+woGHuPDOEmUtFU4XFBARlPITEQ8G2zp946EvwixRhxO3IrMhkVNgo5LLOpcHToqcBuEzi+cPJHzCajMyK7VraaUYmlfHY+MeoaKrgnUPvdLn/2ry1eEg9uDTu0m69N6W3MLlRN9uPbQBQ+nkxanCg3aJvd64ancHElI7SToBzb4a5bwOgalK5iz43Zw1n6prQGdH+0ehNekrrSx3v1IOw7d9LXMNAwbyo2Gm3zycEJi+CnE3CXbU9GQ/D9V8KYx47FNYWEhsQ69QCYkJQAnqT3iLh7S1Z1VnEBcXhKfN0yfHc9Ax30ddPpA0KIE9VT7POdYYoeap6YpW+wvWuNaNP4yHDQ+KBj0fPV94sM32ePkgbygn0llPWB7ENmiYdTToDe0Y/C1e+Ywkf/2bONyQEJ7BwxEImDZ7Ekn1LOKp2MNzsBGYDlylDp7jonVtjztmRIEHprbTN2ekj1uWvs5GWHlUfPWOzIwOJVKUYireSeN7wNVz9vuMnNVaJC+nIq6Fdp7ReW8+JqhOkhzue5zMz1H8oScFJLpN4bijYQEJQQrezo8aEj2F2/Gw+OPZBp7IuvVHP9/nfkzEkgyCFY8MGe1iKvkY1BEULx9NKWzlQRlIYB4qqqWm07iJszVbh6ynjnJgOq84mE7S602oNWmq1te6iz81ZxZm6JjjCLOc7WdeJxLMHYdvuos+1pCpTkSBxbOZi5rxbhRpi4xOWcyUA3sEwbIrDpxVqCp2evzRfc1wl8cyuynZLOwcA7qKvn0iLCkBvNJFbUe+yY1w7uDMAACAASURBVOar6tvm+Vo7fRoJBHgF9Mr0IMAzAA+JB9VyT5B6EB0g6xN5p7l7GBQeDcp4KpsrCfQKtBitSCVSnp/0PEpvJYu2LepR/lh5Q3mfGLi0x5yzMyxoGG9Nf6vfpC6vZr5Kc08dT3/nmIfFLWYuACFx0Fmn7thqMOps5kEPqg5iNBnt5vPZY3r0dA5UHOhcouMEZQ1lZFZkOszm64r7z7kfL5kXL/7yosN9dpfsprK5ksvjL+/28eVSOcFeweL3lMnhtp0w6QGb/TKSwzGaYGdu2+dhMpnYlqXigoRQPD06XIaqC+C5SDi62vIZurvXbs4mztQ1wRHR/k7ENnQzbFvToqGmpcZt4uJCfOW+xAZ2YeYCIFfARU9DxVH47RPx2C/vwr7/c/iUem09qiaVU/N8AHEBcXhIPMip7r0bdXVzNRVNFe6ibwDgLvr6CbOZi6sknjqDkZOVjWKeD8QM0g3foMHYa5t+iURCkCKI6tAEeDif0CD/PpF3lmqakKNnbOEKKD1IVXMVIYoQq32CFEEszVhKeUM5T+56stvzfX1l4NKRhOAEVs9Z3e2OTG8YiLMjA4UAzwAG+w3meOVx6w1b/g3bl9p/0qg/w58/g0hrQ6H95fuRSWSMDhvt1GtPj56OCRNbi7f24J23sbFwIwAzY2f26Pmh3qHcPvp2dpzewbZi+7LWtXlrCfIKYvLgyXa3d/kaPqGW8HTCU+06xo0eEkigt9wquiFPVc/pmiZLlp8V6lwxW+QfaSn63J0+N2cbZ+Ka4IgQRQh+cr/Oi75uhm2bj2UuKN24hjRlGsfUXRR9IMxaRlwFPqHCtXPnK53OrJtlml1l9JmRy+TEBMS4pNNnNnFJCknq9bHc9A530ddPxIT44OMpc5mZS3FVI3qjiWHmos8vDBKmo9HV9yqY3UyIIoSqFpEzNCjIu286fRoxzxeVuRROZ1LdXG1T9AGMDhvNA+MeYEvxFj489qHTxzcYDXyd8zUTBk34XbqLDcTZkYFEmjLNWt4JUHYE9n9gE8kAgKcPpFwKHbrk+8v3k6ZMc3q1Pik4iSF+Q3o917e+YD0jlCMs4co9YX7KfOIC43hx34u0GKxjVzQtGrYUbeHSuEu7nFV0RKgitG1u8tR++PEZcQPSDg+ZlEmJoWzLVlkWbbaaoxrsmbi0i2swF5Tuos+Nm54jkUicc/DsRti2+VgxgW55pysZrhxORVOFMMjqDIkErn4PtPWwLFk4rxbvdei2ao5r6I6RiqscPM25se5O35nHXfT1E1KphNRBAS4r+tqcO1vlnTk/iJy+Fo1LArmDFcFUN5TDFws413SE6kYdTVrXBrSX1jQRKWuVbPpH2u30mbkh9QamR0/nlf2vcKDigFPHNxu4XJN8jave8oBjoM2ODCRSQlIoqiuiXttOUp10CWiKQNWhGDzwsVgp7VAMthhaOKw+7NQ8nxmJRML06OnsKd1DnbZn2Zwna09ytPIoM+N61uUzI5fJefS8RymuK+bDo9YLJhsLN6I1apmd0D3XzvaE+YS1dfrKDsHOl8XNRwemJIVRUddiySrdmqUiMdyPwUHeNvtSmQOKIPBRWgpKd9Hnxk3vcCqrrxsU1hYilUgZ6vf7W1A9k6QpRTRPlxJPEAXe2nugoUL83FQF395jt/ArrC1EJpF1awE8ISiBU3WnnHKB7ozs6mxCvUMtc+Buzhzuoq8fSR3kz/GS2l5HEICY5wPaOn1bX4Cdr1Crre1VRp+ZEK8QqrQaOL6WOIO4UJTVulbiWaZpJslHFK/4hXda9EkkEhZfsJhBvoN4cNuDFtOXzliZvbJPDVwGAgNtdmQgkRKSArRJSwBIvET8nb2x7TGTCXa9Jh7r0OU7oj6Czqhzep7PzPSY6eiNenac2tGj976hYAMAM2LtW293h4lRE5kePZ13D79LWUOZ5fG1eWtJCEpwKv/PEUpvJeomtTinOXDwBGHmAiK6oVGr55eCKqbY6/KZn69MAIkEVZMKCRKH5wU3btw4R2xALKUNpWg7xjL0kKLaIqJ8o3qsEnBjn9QQYebiVNG3eTF0MHNzFLNRqClksN/gbv17JQYlYsJk6RL2lKyqLHeXb4DgLvr6kbRBgdS16DlV3XupZJ6qnlA/TwK9W/8D15aAf5RrO30ttSD1IBQh8yytca3Es0TTRJxCFK96nzBqWmpQKhyvBAV4BrBsyjJqmmt4bMdjGE1Gh/uaDVzmJsztMwOXgcJAmh0ZSKSG2HHwDIiCyJHWsw+lB0GdbVfKtL98PwBjw8d267VHh41GqVD2WOK5oXAD6eHpRPpG9uj5HXno3Icwmows+3UZIG4ADqoOcnn85b0yfQpVhKIz6qjV1nZa9IUHKEgdFMC27Ap+zqtEazCSYS+qASB1Noy9ARA5hcGKYDykHj1+j27cuBGdPqPJ6LIM0ZO1J93Szj7AR+5DXGBc1w6e0K2YjYLaAqdNXMyY7ylyanpu5qIz6MjT5Lnn+QYI7qKvH0mLajVzcYHEM1/V0NblMxqgvhydfyT1unoCvHo/0xesCKZOV4fOL4JAg+iqlbg4tqFU08xQT1H01XiIm7quVvTTlGk8ct4j7CrZxbuH3nW436rcVRhMBq5KvMp1b9jNWUWYTxhKhdLWzGXE1RA4uE3KefgrkMohzXYWMrM8k4SghG7HGUglUqZGT2Xn6Z02s3RdkVOdQ25Nbo9dO+0x2G8wN4+4mQ2FG1iVs4ob19+IBAmXDbusV8c1u2qqm9TgHwlyX6jMs7vvlOQwfi2s5vtDpfh4yjg3zkFA8Hm3wLibxHEb1ZYQeDdu3PQcs8umKySeJpNJFH3+7qKvLxiuHO5cp8/JmA2jyUhRbVG3nVaH+A3BS+ZFbnXP5/ryNfnojXp3p2+A4C76+pHkCH+kEtc4eOarG9rm+eorwGSgzlfcRLlE3tlafFX7heLTIvTiruz0mUwmSjXNHB52MyzKplInZn3MGYGdcU3SNVwadylvH3ybvaV7bbYbjAa+yflGGLgEuOcN/sikKFNszVwm3QdXviOknEaDKPoSLxGht+3QG/UcqDjQbWmnmenR02nUN7KnZE+3nre+YD1SiZSLYy7u0es64qYRNzHIdxCLf15MdUs1njJP/OS2bpvdwTxrp25Si89TGQ+19lefM5LC0BtNfHPgNOfEBOHlIbPdqaUe6sotBbm6Se2e53PjxgWYDaFcUfSpm9Q06hvdGX19RJoyDVWTiorGis53dDJmo7ShlBZDS7dMXABkUhnDAof1yswluzobcJu4DBTcRV8/4u0pIy7Ut9edvuoGLVUN2ra4hrrWYHaFKPZcIe+0FH3KOGTeQYT4erq001fZoEWrNxIZ5Af+EZYZPWdmdyQSCU9NeIqYgBge2f6IjcvV7pLdlDaUcnXS1S57v27OTlJDUsmrybM/x9JYJf5EjoIx19lszqrOolHf2OOib3zkePzkft2SeJpMJjYUbmB85HiXD70rPBRE+ESgN+kBsTjy1O6nenVMc0FmMXNZuAHmfWR33/ToYDykQkrarHMgzc7ZCMuSoOKY5bjuos+Nm94T4BlAsFdw5wHtTmK2/3dn9PUNTpu5OBmzUagpBHr275UYnNgreWd2dTaeUs9uS0vd9A3uoq+fSYsK5Hgvi758tdnEpbXTFzEC7vwFTVgi4Jqiz9xxq5xwK8z7gEGBCspcGNtQ2pr7N7HsIzj0ZVvR5+2cYYOP3IeXM16mQdfAw9sfRm/UW7Z9lf0VIYoQpg6d6rL36+bsJCUkBb1Jb3vR2vAYvHUe+CjhhpWQahtOvr9MzPN1x7mzPXKZnMlDJrOleIvV97MzjlYepbiu2KXSTjOrclZZmdroTXq2Fm9lVc6qHh/TXJBZYhs8fW3McACSn1hP0hPr0RtFB29fYTWxj35P8hPrrXc0S0NDhmE0GalsrnQXfW7cuAinYhucoKi2CKBXcTJuHJMSkoJUInVO4ulEzIalSO9B4ZUQlEBFYwWaFk23nwvCxCU+KN49lz1AcBd9/UzaoABOVTehadL1+Bh5/8/encc3epf33v/8tFiLLVmbx2PPjGezkxlPCJCkpASeEJguLA0zoT0cllI45Sl9nT5Ak/Y5LC0lJKXP0x6eQ6fQpAVaek7bQ2loOyEpJJQACQlhSTKE7LPbnhnvkqzF2qX7+ePWLUveF+mWxr7er1detm/dkn+QZKKvrt/vuiaNcQ3lSp/NAV1XEkd/Y1mP7Z1G6ItmyrP6Ol2M1bHSN1oOkHvOfxXOfrcS+pZr5DJfv7+fT/z8J3hy4knufvpuzkTP8CvHf4VHLjzCLf23SFcxUelM+VJ43hbPQhZmp+BOP/z5VYu2uD4xeYKdHTvpbu9e9+8/3HeYmezMqseMPHD+AWwWG4d3H17371zKsRPHSM/r9JYpZjh24ti6X7PD3oHT6pyrto8/B//6f0Kkttvbox95PW99RS9tNv0/OU67hSOv6OXRj877YGb6tP5ptd1FLBujUCpUzg0KITamXmMbhuPD2C12etp76rAqMZ/b7mavd5XNXFbhfOw8HrtnTe+vDP0+vZnL2ZnFz2qv5GT0JFcGZGtnq5DQZ7KDPR6ADVX7zk4nabNa2Okvt+c//RA88TeVT2Lqsr3TUd7eefHH8KXDXOmKM1rHM336+UANe3oKOrYRzUSxKRueNs+aXudI/xHeNvA2vvTsl3j/f7yf4fgwJUq8Ze9b6rZWcfna4dlBh72DFyNVzVyeuQee/t9zP8cuLJhtpGkaJyZOcE33+qp8htfueC1tlrZVbfEsaSUeHHqQ1+54Ld62jTdjmu/Wa27FZas9/+G0Ornt2tvW/ZpK6fMhpzPT+oVCVj8jOVn7CfU2rxOPw0a+WMJhs5AtlPA4bGzzOGtfMHy60gV0Oq2/psx2EqI+9nj3MJma3PDctaH4EH2ePqyWRc7liro4FNKbudRjxNdQbIi9nXvX1al5wK/vIFvPub7p9DSRTETO87UQCX0mq3Tw3EAzl7OTs+wJubGWz8fw7NfgsWN1DX1ehxershLJxeHSk+xzxIhnCsxmV7dNbSVjsQxd1jSqmIMOfTC73+nHotb+j+THX/VxOuwdlWqhQvHFZ79Yl3WKy5tFWbgycGVtM5dVzDY6HztPNBvluu7rNvT73XY3N/TewHdHvrvif7x/OvlTJlOTvGlP/bd2AtwycAs37rwRh9UBgMPi4KZdN3G0f2HX0rXocnUxnSqHvuB+/esiYxumk1neff1ujv/Oa3j39buZSs7raqpp+vbOcugzzglK904h6sPYjnkhcWFDrzMSH5GtnQ02GBxkOj29cjOXVVjPuAZDt7ubDnsHp6NrP9d3MqIfJ5BKX+swPfQppd6olDqplDqjlPrYIo/3KaW+p5T6qVLqGaXUm81eYyNt8zgJdTg2VOk7N51kX6iq615iFDw9xHIxFGrDHflAf7Psc/iIojdc2GnT1ztWp3N9o7EMBz3lTxs7thHOhNc9gPmB8w+QL81tl9XQNnxWSWweBwMHORU9RbFU1C+sYrbRkxNPAmy40gfwhr43MDY7xguR5c9nPHD+AZxWJzftumnDv3Mpd95wJwFnAIVeobvjhjs2/JohV6hSlcPlg/auRUPfF95zHZ8+ehWDvV4+ffQqvvCeeYFaK8Gb/gxe9p+AuXOCcqZPiPqox9iGYqnISGLt7f/F2hwKHgJW0cxlBal8isnU5Lr/fiml6Pf1r6vSZ5whv8IvM/pahamhTyllBe4C3gQMAu9USg3Ou+0TwD2apr0SeAdwt5lrNMPBHs+6O3jmiyVGwqm5Ji6gD2b36oPZPW2eum258Dv9REp618NtqjygvU7n+sZm0uxzZ0FZwaNX+tYb+o6dOLZgFtpGzyqJzeNA4ADpQnqua90qZhudmDxByBWiz7PxT7Nv2nUTVmXlO8NLb/EslAp8e/jbvG7X63Db3Rv+nUtx293cffhu9vn2cdfhu+ryu4Ku4Fz3TtArddPraPFtscIr3gV91wNS6ROi3nZ59BFGGwl946lx8qW8VPoa7MrAlViUZcPn+jbSxMXQ79dD31q3mp6MnGR7+/a67D4T9WF2pe9VwBlN085pmpYDvgocmXePBhgHWjqBURPXZ4rBXi+nJ5Lki0u0LV/GSCRFoaTNNXHRNIiPVUJfPf/lCjgDRAuzoCwEtHLom6lT6ItliHW/Cv5oGvpeTSQdWXXnzvkacVZJbB4HAgcA5oa0r2K20VMTT3HNtmvWdQZiPr/Tz7Xd1y57ru8n4z8hkok0bGtntX5/P/ceuZd+f39dXq/L1UU8F58bi9F9FVjX0aktfBYuPQUl/c/F6fQ0LpuroSFYiK3EbXezzbVtQ6FvOKY/V2b0NZbL5mJf574NV/rOx/SmWhupzPb7+pnJzhDOhNf0vFPRU3Ker8WYHfp2ANWbyS+Wr1X7FPDrSqmLwDeBDy32QkqpDyilnlRKPTk1NbXYLc3zzD16R8BP+RbtDDjY4yVXLHF2Krnmlz43pXfurFT6snHIz+qhLxerS+dOg9/pJ5qdgX2vxx3YDsx13dyIYkljIp6hp9MJFgtYrPqZPsfKg9kX06izSmJz2OfbR5ulbe5c3wqzjUaTo4zPjq97Pt9i3tD3Bs7FzlX+AzzfA+cfoMPewWt3vrZuv9MsC8Y2vOX/g/fev/YXeurv4MtvQv/cD6ZT01LlE6LOdndubGyDzOgzz2BwcMPNXIbiQ1iUZUOV2QGf3sxlLef6ssUs52PnZWtni2nFRi7vBP6npmk7gTcD/6DUwu4emqZ9UdO06zRNu66rq4XeGDxzj94JMHYB0BbtDDjYs/5mLkZQ3GdU+pyd8PGLcO37iGfjda30+R1+vTnKe/4N+6v/K6EOR10qfdPJLIWSxmuTD8IDHyVdSJMqpDbUpa8RZ5XE5mC32On399d28FxmttFTE/p8vnqGvsN9+giGxap9uWKO7wx/hzf0vaHywcXlxBipULPFcz3CZ/VGMOXt6dOZaTnPJ0Sd9Xn6GEmMrPv5I4kR3Da3/LtpgsHgIOFMmInUxLpfYyg2RG9774b+22LsClnLub6zM2cpakVp4tJizA59l4BdVT/vLF+r9n7gHgBN034IOIHL50+X79ypdwKsNq8z4N5QOw6bZV2h79xUklCHg05X1Qw6hwccHmLZGF5H/Vq9B1wB4rl4pUlKr89Zl0qfMfphX/xJOPVgZRbges/0QWPOKonN42DgIC9FXlrVJ6ZPTTyFp81TmU9UD9vbt3MoeIjvjnx3wWM/uPQDEvlEQwaym8H4sKbSzCUxrlfsXvrG2l5o+vRc909gKjUlbyyFqLM93j1EMhHiufX1FRiKD7Hbu7suW9/F8urRzGUoPrSh83ygvzcLOANrCn2Vzp2yvbOlmB36ngAGlFJ7lVJt6I1a7pt3zwhwGEApdRA99LXY/s1lrKIzoM1q4cB2Dy+Or6fSN8v+6iYuQ4/Bf/wRZBN1395pzOqbefj/hb9+LT2dTsbr0MjFaAbjKYQr4xpgY6EP6n9WSWweBwMHiWVjjM+Or3jvUxNP8cptr6z7DKrDfYd5dvrZBWt4YOgBfA4f1/dcX9ffZxZjC2Yl9Dl9MPJDGHtm9S9SLED0PAQHKpfC6bCEPiHqzNjmNxJfX7VvODYs5/lMstFmLiWtxHB8uC5bcQd8A5yJrj70nYqewmVzVZoHidZgaujTNK0AfBD4FvAiepfO55VSdyql3lq+7feB31JK/Qz4J+B9Wj2mU5plFZ0BQW/m8sJofM17tc9NJee2dgIMPw6Pf46SstZ/e6dTP2MXySdg/Dl2eO116d5pVPqcGX0we71CnxBLORAsN3Op3uK5iHA6zFB8qK5bOw2Hd+tbPKurfal8iocvPMwv7v5F7Bb7Uk9taca26kroszvB17fo2IYlzQxDqQAhPfRlChkS+URl66gQoj6MwLaec335Yp7R2VHp3GkSl83Fft/+dVf6JmYnSBfS7O3cu+G1GB08S9rqGhCejJ5kwDdQ9w9PxcaYfqZP07Rvapp2haZp+zVN+5PytU9qmnZf+fsXNE17jaZpL9c07RWapv2H2WvckFV0BgQ42OMlmsozHl99iIrM5oim8rWVvvgouIMktDwaWkNCX7TNDWjsdaVIZgvEM/nln7iCsVgGl92KJTVZGdcAEvpE41zhvwKLstQOaV/EickTAFyzbePz+ebb17mPvZ17a0Lf9y99n3Qhfdlu7QSwWWz4nf6FYxvCaxjm6+mB37gP9r8BmKsaBp3rP+crhFhol2cXCrWu0HcheYGSVpImLiYaDKy/mcv5uN44rC6hz9dPqpBibHZsxXs1TeNk5CRXBKSJS6tpxUYulzejM6C33JTU4a3pDGhYTzOXc+UmLvurK32JMfD0Es/qr1PvkQ0A0Tb9AHBfWwLY+NiGsVianZ02lMMLnTsl9ImGc9lc7PHuWbHSd2LiBE6rs3KWot4O9x3myYknmcnMAPDAuQfocnU1JGSaqWZAO5RD31l9pMxqtLlh3+vAo3cJNl5LKn1C1FebtY3ejt51hT5jS6hU+sxzKHSISCayrmYuQ7EhoD6dVo0z7qvZ4jmRmiCei8t5vhYkoa8Rrn47/N4L4N0JV/3qgsAHcKAc+l5cw5D2uc6d8wez9xDLxQDqe6avHMIiFv0fk16r/kZ1o81cRmcybPN16F0TX/O7RNIRmcclGu5A4MDcrL4lPDXxFFd3XY3d2pitlof7DlPUijxy8RESuQSPXnqUX97zy5f9FpguVxfTqarQt/M62H0D5GZX9wJnHoLTD1V+NEKfnOkTov52e9c3tsF4jlT6zDMYHATg+em1n+sbig/Rbm+vy5+jRug7PbPyDo5KExfp3NlyJPQ10gefgJuPLfpQh8PG7qCbF9YQ+s5NzdJmtbDTXxWOcknw9DSk0tfp6MSiLEQtCq58C76A/ql7PSp9PZ1zW2AjmYhU+UTDHQwcZCI1UekWO18il+Bk9GRDzvMZDgUP0e3u5r6z93H03qPkS/nLemunIegKMp2pCn1Xvx3e/TVwdCz9pGqPHYNH/qzyo7FVVEKfEPXX5+ljJD6y5i2DQ/EhfA5fXd9niOVd6b8Sq7Kuq5nLUGyIPd49dem02tHWQU97z6o6eJ6M6qHPmO8nWoeEvkZqW75yNdjjXdP2zrNTs+wJubFaqv4F/vBP4S2frVT66jmywaIs+Bw+IqUsvPMr+A68DouC8Q1U+vLFEpOJLNfzHPzjr0Hs4oYGswuxWis1c3l68mlKWolruhu31VIpxY07b+Qn4z9hMj2JVVnZ37l/5Se2OGN754I3kat9Uzl9utLEBfRKn0VZ5M8FIRpgT+ceEvlE5WjFao3ER2Rrp8mcNue6m7mcj5+vy3k+Q7+vf1XbO09GTrKjYwcdbav80E+YRkJfI118Eu75DUhOLvrwYI+X4YjeHGU1zk0la8/zGaw2Ytly6GurX+gDfUC7URmxWxRdHgejG+jgORHPoGmwh0tw5ttgseuVPpdU+kRjHQwcBFiymcuJyRPYlI2rQ1c3dB3GOQvDp374qYb+PjN0uboolAqVP4fQNPjLV8FDn1r5ydkEJMdrZvRNp6cJOoOX/bZXIVpRn6c8tmGNQ9qH4kOytbMJDgUPrbmZSyqfYnx2vK5/v/r9/ZyLnaNQWv4966noKTnP16Ik9DVSNg4vfB2mFn+TebDHi6bByVXM68sXS4xEUrXn+Saeh395P0yfrrzZqueZPtA7eEYyEfhfN8NX301Pp4uxDVT6jJEPXURBWaA9RDgTlu2douE6HZ30tvfyUnjxfx+fmniKweBgQ8+WHj99nOfCz1V+LmpFHr7wMMdPH2/Y7zSDsQ2z0sFTKbBYYfrUyk82RjsEayt9srVTiMZYz9iGVD7FZGpSZvQ1wWBwkGg2uqo5swYj0G90MHu1Ad8A+VJ+2Q8LUvkUw/FhOc/XoiT0NZLxJmZ68YOvg72r7+A5HE5RKGm1lb6pk/Dcv0AxRywXw21z170BRcAZIJqNgrUN4pfo9Tk3dKbPCH3+YgTau9CURc70CdMcCBxYdHtnppDhuennGnqeD+DYiWOkC7UfmmSKGY6dWPzs7+XCCGi1HTz3L/lnX43w2fL9/ZVLU6kpCX1CNEhvRy82ZVtT6LuQuABI585mqDRzWcO5vnp27jSspoPnmZkzaGhS6WtREvoaybsDbK4lhxT3dDrxue2rauZyrtK5syr0xUfLv6eXWDbWkMPVfmd5e2fHdkhO0NPpYjSWXtfMGICx8mD29nwYOraRyCcolAoS+oQpDgQPMBwfJpVP1Vx/dvpZ8qV8Q8/zAdx6za24bLVzPJ1WJ7dde1tDf2+jLR76+iF6HoorbF8fPAIffKrmTF84HZbQJ0SD2Cw2dnp2rin0DcWHAOnc2QxX+K/ApmxrOtd3Pn4ehaprZXZv514syrJsMxejiYvM6GtNEvoayWLR3/gs8Wm3UoqD2728MJZY8aXOTumtz2u2dybG9FDp9BHPxhsS+gLOADPZGQod2yA5Sa/XTiZfIpZe34D2sVgGj8OGzdsNPS+vnBeU0CfMcDBwEA2NU9HabYcnJk6gULxy2ysb+vtvGbiFG3feiMOqz750WBzctOsmjvYfbejvbTRjnt6C0FcqwMwKbyytdgj161+BYqlIOCOhT4hGWuvYhsqMPo9U+sxmNHNZS6XvfOw8vR29OG3Ouq6jz9O3fOiLnKTd3s6Ojh11+72ifiT0NVrvy8HmWPLhwV4vL43FKRRLy77MuakkXR4HXmfV9s3yjD6UIpaL1f08H+iVPoAZtw+0IrudeqVudJ1bPEdn0vT4nHDkLjhyV6V7WNAZrM+ChVjGgYDewXP+J6ZPTTxFv7/flFbkd95wJwFnAIUi6Apyxw13NPx3Nprb5sZlc9WGvt5r4Nr36Wf7lvPoZ+Glb1Z+nMnOUNSKEvqEaKA+bx8X29TxhQAAIABJREFUEhcoacu/9zAMxYfY5tom83Sb5FBobc1cjHEN9dbv6+d0dOlt+6eip7jCfwUWJfGiFcnflUY7che8438v+fBgj5dsocRQePkhxmenkuwLtddetLZBl/4mNpaN1XVcg8EIfdHAbnjVB9hWnq+33mYuY7FM7Yy+tB76pHunMEO3uxu/w1/TwbNQKvD01NNcu62x5/kMbrubuw/fzT7fPu46fNemeBOllCLkCs01cgHoHoSb/wL8e5Z+oqbBo/8Dzj9SuWQER6N6KISovz3ePaQLaSZTi3cXn284PszuTmni0iyDgUFmsjOMzo6ueK+maQzHh+vaxMXQ7+9nJDFCtphd8FhJK1VCn2hNEvqarNLMZZktnpqmcXZqlv3b5o1reNsX4J3/BNCwM31GBS7a2Qtv/gzbtu8CWPfYhrFYmoGONNx9A7z0DcKZMIDM4xKmUEpxIHCgJvS9FHmJdCHd8CYu1fr9/dx75F76/f0r33yZCLlChNPh2oulIqSjSz8pMQ65ZE0TFyP0SaVPiMYxGrIY2zZXMhIfka2dTXQodAhYuEtlMZOpSVKFVMMqfSWtxPnY+QWPXUpeYjY/K507W5iEvkabGYG/+QU49R+LPry/qwO7VS3bwTMymyOWzi+s9JVpmta47Z3lMBbJRqCYp8tZxGZRlYYsa5EtFJlO5tjnSMLk81DMV7Z3ypk+YZaDwYOcnjlNvqifS31q4imAhjdx2ewWVPoA/u7NcM97l35SuLxNqGpGn/EaEvqEaJzdnvLYhsTK5/pi2RjRbFSauDTRgH8Am7Lx/PTK5/oqTXcaUOkb8OkNtxbb4nkqop+Vl86drUtCX6M5fXDxCZh4dtGH22wWBrZ5lu3geW5a3/pZU+mbDcOX3winHyJdSFMoFRrWvRMgMjsFf9yF9Ud30+11Mr6OSp/xnJ32clXTs51IJoKnzVP3URNCLOVg4CCFUoGzMX1UwFMTT7HLs4tt7m1NXtnlLeQK1Z7pA31rpzGSYTFLzOgzXk8I0Rjd7d04rA6GYyuHPqPhi8zoax6H1cGAf2BVlT6jCrfXu7fu69jl3YXdYl+0mcvJ6EkUqjLaQbQeCX2N5vTq4w6ml+52NNjrXbbSZ4xr2B+qHtdwEUZ+CPnU3GD2BoQ+n8OHQhHNx8Hlg+Q42zudjK7jTJ/R/GW7ZUa/0NFNJBORJi7CVEYzlxfDL1LSSpyYPGHq1s7NKuQKkcglyBSqPhAK9ut/VuVSiz8pMQF2tz7epmw6PU2HvWPBaAshRP1YlIVdnl2rqvRJ6GsNg8FBng8/v2Izl6H4EG6buyEfZNotdvZ07lk89EVOstu7e1OcU9+sJPSZITQwt41pEQd7vEwns0wmFq+enZ2apc1mYYe/6k1QfEz/6u0lliuHvgZs77RarHQ6Oudm9SXG6el0Voasr4XR/CVIbeiTrZ3CTH3ePtw2Ny9FXuLczDli2RjXbJOtnRvV5dIbrxjndIG5bZuRc4s/6fUfh4+c08fblE2np6XKJ4QJ9nj3rGpsw3B8GIuysNOz04RViaUMBgeJ5+JcSl5a9r6h2BC7vbtRSjVkHf2+/kUHtJ+MnpQmLi1OQp8ZjFl9S3w6M9ijN3N5cYlmLuemkuwNtmO1VP0LnCh3cPL0VCp9jejeCfp5u0gmAh499PX6XIzFMmse0G4ERU9oB+w/DG1uImkJfcJcFmXhysCVvBh5sXKe77ru65q8qstf0KVX7KdSVef6jIHr4aV3OmCvrehNpaYk9AlhAmNsQ7FUXPa+kfgIPe09tFnbTFqZWMyh4OqauQzFhxpyns8w4BtgdHaUZC5ZuZbMJbmUvCRNXFqchD4z9L0adr8GCgtb3MJc6Ftqi+fZqdnaoeygV/qUBTq6G7q9E/RzfdFMVA99yQl6Op3kCiXCs7k1vc7oTBqf207bde+B9/wbANFsVEKfMN2BwAFORk7y5MSTdLm65BPsOqhU+qo7eAb2w+v/sDJapkYhB199N5x5qOayDGYXwhx7vHsolAorjgEYijdm5ptYmwH/ADaLbdkh7ZlChtHkaEPO8xmMM3vGuXiA0zP6bjZp4tLaJPSZ4eX/Gd75FbA7F324021nh8/Fi4s0c8kVSoxEUuzvmjeuweWDPa8Fq62h2zuhqtI3eASu/+3KnL21NnMZnzejr1gqEs1EZUafMN3BwEFShRTfGvoWV/ivaNg2mK3ECGo1zVza3PC6j8C2RULfzDC89O8wW9v8RSp9QphjNWMbjJlvcp6v+dqsbQz4lm/mMhwfRkNjb2cDQ1951FD1Fs+TkZMAUulrcRL6zLTMdsiDPd5FO3iORGYplrSFlb5X/1/w3vsBGl/pc5QrfVe+CW74ED2dengdXePYhtFYht5OJ3zx9fCtP2QmO4OGJpU+YTrjU2sNjeemnyOVX6LRiFi1gDOARVkWjm2YDcPYMwufMG2Ma5jr9JbKp0gVUhL6hDCBEeSWO9cXzoRJFVKVgCiaazA4yAvhF5Y8XtPIcQ2GHR07cNlcNc1cTkZP4m3z0u3ubtjvFRsnoc8MmgafeyV8+5NL3jLY6+XcVJJMvnZv/dmp8riG+ZW+KvFsHIfVgdO2eCVxo/xOPzPZGYr5DESH6WnX17jWZi5jsTQ9PidMnwKtVJnRZ4yFEMIs//DiP1S+TxVS3P747U1czeZgtVjxO/wLxzZ879Pw929d+IRFZvQZz+1ydzVqmUKIsqAzSLu9fdnQNxQbApDtnS3iUOgQ8Vyci8mLiz5u/P3q8zQupFuUhf2d+ytbOkGf0Xdl4ErZNdPiJPSZQSmwOuY+2V7EYI+XkgYnx2ubuZwtj2tYUOn7q9fA458HaNhgdkPAGUBDIzb0ffiLqwmFn8ZuVWsa25DOFZlJ5enr0CCXhI5tldAnIxuEmY6fPs5jlx6r/Jwv5Xn4wsMcP328iavaHLrcXQtDX7Af0lFIRWqvh8+AOwSuuQ99KjP6nFLpE6LRlFL0efqWHdsg4xpay2BwEGDJc31D8SG2t29v+NiEfv9cB89iqcjpmdNynu8yIKHPLKH+Zcc2VJq5zNvieW5qlm0eBx5n1fDy3CxMPAdFvZFKLBtrWOdOoLL9Mtqmd+6ylGf1jc2svtJnBMQ9jnKo7dheCX2yvVOY6diJY6QLtR9YZIoZjp041qQVbR5BV3Dx0AcLO3ha7LCztmuqsTU05JbQJ4QZ9nj3LDugfTgxjN1ip6e9x8RViaUM+AawW+xLnus7Hzvf0CYuhn5fP+FMmEgmwoXEBdKFtIxruAxI6DNL6AqIDkExv+jDO/0uPA7bgg6e56aSi3fuhMpA41g21rDzfDC3/TJiseoXkuP0dLrW1MjFCIg7bOX/fVWVPgl9wky3XnPrgsHfTquT2669rUkr2jxCztDCM31G6Ju/0+FXPgvv+ueaS5VKn5zpE8IUfd4+RmdHyS/x3mQ4Nswuzy6sxn//RVO1WdsY8A/wwvTC0KdpWsPHNRgGfPo4nrMzZzkZlSYulwsJfWYJDkCpoAe/RVgsakEzF03TODs1u/A8X9WMPmj89s5K6CumwNEJCX1sw1q2dxr3BvwhuOrXILCXcDqMRVkaGliFmO+WgVu4ceeNOKwOABwWBzftuomj/UebvLLLX5e7i0g6QkkrzV307QaLbflZfWXT6WlsyobP4WvgKoUQht3e3ZS0EheSFxZ9fCQxIk1cWsyh4CFeiCxs5jKdnmY2P2vK+Uujg+fp6GlORk5iVVb2+/av8CzRbBL6zLLjGnjVB2CZ4aYHezy8NBanVNL/RY7M5oil8+ybH/ri5dBnUqWvsr0zEwVPNyTG6Ol0MRHPVNa6EqPSF+i/Bn7tbyGwj0gmgt/hx6LkH0NhrjtvuJOAM4BCEXQFueOGO5q9pE0h5ApR0ArMZGfmLlpt8Kt/Cy9/x9y1sZ/BF14Hl07UPH86PU3AFZA/E4QwiXFWb7GxDcVSkZH4iDRxaTGDwUESuQQXE7XNXMzo3GnocnXhbfNyZuYMp6Kn2Nu5t/JBqmhd8l9Ws3RdCW/+DPiXPgw92OtlNldkJKK3jzc6dy7Y3ukOQv8vglev9MWz8YaGPuO1o5ko3PQxuPZ99Pqc5Isa08nFB87PNxZLE+pw4Kj6J05m9Ilmcdvd3H34bvb59nHX4bsafuh9q1h0Vh/AoaP6n4GGqZMw9jTM+/99Kj1VGfIuhGi85cY2jKfGyZVy0sSlxSzVzOV87DyAKWf6lFL0+/o5M3OGk9GTcp7vMiGhz0zFPCSnlnx4sEcPV8YWz3Plzp398yt9A78Iv/4v0NZOppAhU8w0NPTZLXY6HZ36GbyrfhX6D1eGrI+u8lzfaCyjz/e770Pwlz8HQCQTkfN8omn6/f3ce+TeyjYVsXGV0JeaF/pmLsCz/wKl8rbP6dOgLBCofXMSToflPJ8QJup0dOJz+BYNfdK5szUt1czlfOw8LpuL7nZzZuUN+Ad4Mfwi47Pjcp7vMiGhz0x/fxTuec+SDw90d2C1KF4sh76zU0nabBZ6fbVNJ6qHvMdz+r3etsZ174TygPZsFGanYfiH9Hj1Mv74Ks/1jc2k9dCXGIc2PcRK6BNiczGqdNOZeaHvzEPwr++H+CX95/AZ/ayfrXY70FRqSkKfECbr8/Ytur1TQl9rslvtXOG/YkGlbyg+xG7vbtO2x/f7+skU9Q/+PXaPKb9TbIyEPjMF9y07q89pt7K/q73SwfPc1Cz7Qu1YLfOGXX75jfCvvwXoWzuBho5sAP1cXyQTgZ99Ff7ujfQ69W2do6sc2zAWy+jhNTkJHfqnUBL6hNhcltzeWRnbcHrua7C2wlosFYlmoxL6hDDZHu+eynmwasPxYVw2l2y5bkGHgod4MfxiTdOsodiQqecvd3Xsqnz/hWe+QCqfMu13i/WR0Gem4ACkpvVBxUsYrOrgeXaxcQ0AM8OVhjCxXAygod07Qe/gqTdy2a7/XIrgsFkYW0WlL5HJk8wW9Epfchw83WSLWZL5pIQ+ITYRt92N2+ZmKjVvG3tIb+9N+Kz+tftlsPfGmlsiGb3rp7zBFMJcfZ4+JlITC+aXDseH2e3djVJqiWeKZhkMDpLIJ7iQ0Luu5oo5RmdHTWniYvjnU3Mjd2YyM9z++O2m/W6xPhL6zGS88ZleunX5YK+XsViGyXiGC9H0wnENxQIkJypNXGLZcuhr8NgDv9OvV/rKVTqVNMY2rFzpGyvf0+u169tDO7brARKZ0SfEZhNyhQinw7UXO7r1bd3G2IZb/gpe8+GaW2RGnxDNsVQHTyP0idZzKHQIoHKubyQ+QkkrmVbpO376OD8a+1Hl52wpy8MXHub46eOm/H6xPhL6zBQ0Pu1eeovnwR59m+aDz49TLGkLK32zk6CV5mb0mRT6As4AsWyMUjn0kdAHtI/NrFzpGy3fs8NjhRs+BLtvIJwJV15XCLF5hFyLDGhXCoL79dCnLT7mxXhOyC2hTwgzVUJfYi705Yt5RpOj9HlkRl8r2u/bT5uljeen9XN95+Plzp2dje/cCXDsxLEFleFMMcOxE8dM+f1ifST0mcm/B37hU9D7yiVvMULf/T/TZ/EtqPTFx/Sv3l79x3Ijl0Zv7ww4AxS1InFHucV6Ypwen5PxNVT6ukMB+KU/hn2vI5KOVF5XCLF5hFyhhWf6AI7+Nbz18/Dk38Jn+mG2thpoVAel0ieEuYzh69UdPC8mL1LUiqZuFxSrZ7fYuTJwJS9E9ErfUGwIwLRK363X3IrLVttk0Gl1ctu1t5ny+8X6SOgzk9UGr70Nth1c8pZQh4Nur4MnhvTtj3tD8yp9Dg9c+z4I6TNRYtkYVmWl3b7I2b868jv8AERKOfi1L8PBm+npdDKRyFJcYUD72Ewai4JuZxEycdA0fasoEHQGG7puIYS5lgx93YP6h1XTZyA3C+7aD3wqlT4JfUKYqt3eTperqyb0SefO1jcYHOSF8AuUtBJD8SG2ubeZNnP2loFbuHHnjZWB7A6Lg5t23cTR/qOm/H6xPhL6zJacggs/WfYWo9q3zePA47TXPth1Bdz8F/pWKfTQ1+nobPhBa7+zHPqMWX3B/fR0uiiWNCYTy1f7RmMZtnmc2J7/V/jTXRC7UAl9MpxdiM2ly91FMp9csPWH2EV47Bic/77+59e8P7Om09N42jyVNxFCCPPMH9tQCX0eCX2t6lDwELP5WUbiIwzFhkwZyl7tzhvuJOAMoFAEXUHuuOEOU3+/WDsJfWZ7/HPwP98CpeKStwyWQ186X1wYqHKzNc+N5WINn9EHc9swo9kojD0D5x6m1+cEVh7bMBZL0+Nz6g1oADq6iWaiOKwO3DZzPpUSQpjDqN4vqPYlJ+Ch22Hy+QXjGoz7pXOnEM0xf2zDcHxYH9zu9DVvUWJZg8FBAJ4PP8/5+HnTt+K67W7uPnw3+3z7uOvwXaZVGcX6SegzW2gAijl97MISBnv1EJfIFPjcQ/Oavvz7bfD5ays/GpW+RquEvkwUHvtz+Pfb6OnU93OvNLZhbCZDb6dLf9Pn9IHNQTgT1j8hklbQQmwqXW49uC3o4Fkd9IymVlWm09OytVOIJunz9hHJREjkEoB07rwc7PPto83SxmOXHiORS5jWxKVav7+fe4/cS79/4Qd5ovVI6DNbcPmxDVd+4gE++JWfVn7+xx+PsOdj3+DKTzygX4iPVsYmgHmhz/i0L5KJ6LP6EhN6kEMPdUvRNI3RWLo8o2+iMucvkolUtowKITYPI7gt6OB56ltz3z/1d/DMPTUPT6WmJPQJ0STGNk5ji+dwfFi2drY4u8XOgcABvjPyHcC8Ji7i8iWhz2zlBixLjW149COv562v6KXNqv+tcdotHHlFL49+9PX6DfHRyow+0Lt3NrpzJ+h/uHjaPHOhLz+L15LGZbdWunMuZiaVJ5Mv0eNzQWKiElgjmYh07hRiEzKCW832zmfugfur5vLNTuk/l4OfpmmEM2EJfUI0iVHVG44Pky6kmUhNSKXvMnAweHDu/LRsnBIrkNBntvYguPwwvXjo2+Z14nHYyJdKOGwWsoUSHoeNbR6nPt8qMQae3sr9ZlX6QN/iGc1EoUOv1qnkJD0+57LbO0fLj/V0OuG634Rr3wtI6BNis/I7/FiUhalUVaXvO3dCft6fE/m0fh2Yzc+SLqTlTJ8QTbLLuwuFYjgxXKn2SehrfQO+ua3ydzx+B6l8qomrEa3O1uwFbElv+xvwLT3wdDqZ5d3X7+Zdr+rjKz8ZYcpo5pKZgXyqMqMvX8qTzCfxOhrfyAX0N3PRTBQ85e2lyXF6O12MLlPpM7Z+9nQ6oe+dgP6pfiQdkXENQmxCVouVoDNIOFN1pi92cfGby9eNqmDQJX8mCNEMDquDnvYehuPDMq7hMmJs7QSIpCPc/vjtfOZ1n2niikQrk9DXDAO/sOzDX3jPdZXvP330qqpHFLz+D2H3DQCVA9dmbO8EvdJ3IXlBHy7/Xx6E7kP0dJ7n1KmpJZ8zFtdDX6/HClMnoXMXs5TIlXJS6RNik1owq69zJ8QuLLyxcycwd/7PaAIjhDCfMbZBQt/l4fjp4zw99XTl52wpy8MXHub46ePcMnBLE1cmWpVs72yG2CV4+iv6+IW1cPngdR+BHdfoL5ONAZi2vdPvLFf6nJ2w+9Xg9NLjczGVzJIrlBZ9zthMGptFEcqPwV2vgpe+ITP6hNjkgq5g7fbOw58Eu6v2JrtLv85cp8+QU870CdEsu727GYoPMRQfosvVJS34W9yxE8cWzEPNFDMcO3GsSSsSrU5CXzOMnoB7/6te+VqL2TDEx/SzfZgf+owzfSWtBM8fh/OP0tPpRNNYckD7WCxDt9eJdbY8o8/TPRf6pNInxKbU5eqqHdlw9dvh5s9B5y5A6V9v/px+Han0CdEKdnt3k8gleGbqGanyXQZuveZWXLbaD9OcVie3XXtbk1YkWp2EvmYwxjaEFx/bsKQn/gY+ewCKeUDv3Anmbe/0O/0UtaK+rfTbt8OJ/6Wf1YMlO3iOzqT1Ie4JYzD7dgl9QmxyIVeIcCZMsVScu3j12+G25+BTM/rXcuAD/Uyf3WLH22bO+WQhxEJG0BuKD0nouwzcMnALN+68EYfVAYDD4uCmXTdxtP9ok1cmWpWEvmYI7AVlWbKD55ISo+AOga0NaM72TjBm9fVAYpxen/4p0+jM4h08x2IZfYh70gh92yT0CbHJhVwhilqRmezMqu43BrMrJT3HhWiW6qDXbm9v4krEat15w50EnAEUiqAryB033NHsJYkWJqGvGWwO8O1eclbfkuJjNTP6mrG9E5jr4JmcWLbSVyppjMcy9PickBwHmxOcnRL6hNjkFp3Vtwwj9Akhmsfv8Fe+v//c/dL+/zLgtru5+/Dd7PPt467Dd8k5TLEsCX3NEhqA6TVu70yM1s7oy8VQKDrsHXVe3OJqQl/HdkhM4HHa8ThsjC1S6QvP5sgVS/R2uuDQLfArfw5KEclE6LB30GZtM2XdQghzGWfzVhv6ptJTEvqEaLI7f3Rn5ftkLsntj9/exNWI1er393PvkXvp9/c3eymixUnoa5Y3/Xd4z7+t7Tnx0QWVPk+bB6vFWufFLc74FDCcCeuVvmwMcim2dzoXrfSNVQ9m33EtvOJdgD5LRqp8QmxeRhdOo0HLSsLpsIQ+IZro+OnjfP/i9ys/50v5Svt/IcTmIKGvWQJ7oWPb2p7zS5+Gq/9z5cdYNmba1k6YO9MXzUThmvfC7z4DNic9PteioW+0PJi91+eCkR9BVJ/9E8lI6BNiMzOGrK+m0pcv5YlkInS5pHOnEM0i7f+F2Pwk9DXLbBge+QxMPL/657ziXZXB7KBv7zSrcydAm7WNDnsH0WwU2kPg3w0WC72dzkpVr1pNpe+f3gE/+AtArxRK6BNi83Lb3bTb21cV+iJp/YyvERSFEOaT9v9CbH4S+pqllIfvfRqGHlvd/bNhuPgk5OfCVTwbx+swt8W53+nXG7FkYvDoZ2HsZ/R0uphO5sgWijX3jsUytNksBBwapKPg2Q6UK30ymF2ITa3L1bWq0GfcI5U+IZpH2v8LsflJ6GuWjm5o86x+bMO578HfHIbI+cqlWNbcSh/MDWinVITv3AHDj+vdOYHxeVs8R2fS9HQ6UbPlcz0d3ZS0EjPZGan0CbHJBV3BNYU+OdMnRHNJ+38hNjcJfc2ilN7Bc7VjGxJj+ldvbffOplX6XH6wtkFifMmxDeOxjP5YclK/0NFNLBujpJUk9AmxyYVcoVWFPqPZi9HxUwjRHNL+X4jNTUJfM61lbEN8DOxucOqVvZJWIpFLmNrIBaoqfUrp1crEuD58HRac6xuLZfRxDclx/YKnuzKjL+iU8ztCbGZr3d4pfyYI0XzS/l+IzUtCXzMFByA1DfmFnS8XiF/Sq3xKAZDMJylpJdO3d/odfqLZKJqm6aEvOU5veXun0a0ToFjSGI+XB7Pv/Dl4xz9BsL8S+oxOoEKIzSnoCjKbn11xwPN0ehqfw4fdajdpZUIIIcTWI6GvmW74IPzBKNidK9+bGANP7Yw+wPRKn9/pp1AqkMgn9MYsiQncbTY6XfaaSt9UIkuxpOlVwI5tcODN4PDoM/5AtncKsckZjVnC6fCy902np+U8nxBCCNFgtmYvYEuzu1a+x/CLfwzaXHfMeDYOmB/6jLAWzUTxHrlL33KKPpZhrKrSN1oOgL0+J4z8GIpZ2HtjpT27hD4hNjcjyE2lp9jl3bXkfVPpKQl9QgghRINJpa+ZNA3+/Tb42VdXvrfv+toZfU2q9FWHPlw+sLUB5dBX1cjFCIA9nS547M/hwT8A9HENCoXP4TN13UIIcxlBbqVzfeF0WEKfEEII0WAS+ppJKTj9EJx5aPn7skl44euQGK9ciuXKoc/sM33ls3jhTBjGn4Vv/jdITtLjc9Vs7zS+1xu5TOhbPNFDn9/px2qxmrpuIYS5qit9S9E0janUlMzoE0IIIRpMQl+zhfpXntUXPgP3/AZcfKJyyaj0mT2yoabSFx+Dn3wRokP0djqJpvKkc/oW1NGZDO42K16XrRz6uoHyYHbZ2inEpud3+rEq67Jn+hL5BLlSjqBLOncKIYQQjWR66FNKvVEpdVIpdUYp9bEl7nm7UuoFpdTzSqmvmL1GUwUH9FCnaUvfY8zo81TN6Ms2t9IXzUT1Ri7l9c0f2zAWKw9m1zQ99Hkk9AmxlViUhaAzuGylbzqlb/2USp8QQgjRWKaGPqWUFbgLeBMwCLxTKTU4754B4OPAazRNOwTcauYaTRcagFyyZuvmAvFR/au3qntnLobb5ja9zbnD6qDd3q6PXqiEvgl9NANzA9pHYxl6fS5IR6FUqFT6opmohD4htoigK7jsmT7jMTnTJ4QQQjSW2ZW+VwFnNE07p2laDvgqcGTePb8F3KVpWhRA07RJk9dortAV4OvT5/UtJT4KygLt2yqXYtmY6U1cDMasPtwhUFZIVg9o10Pf2Ixe6cPphQ88AoduAfSzgDKjT4itocu9/IB2owoYckvoE0IIIRrJ7NC3A7hQ9fPF8rVqVwBXKKV+oJT6kVLqjYu9kFLqA0qpJ5VST05NLb19qOXtex3c+ixsf9nS9yTGoGM7WOcmbMSz8aaFvoAzoI9esFj02YG5WT3goYe9XKHEVDLL9k4XWO3Q+wrwbCdfzJPIJaTSJ8QWEXKFpNInhBBCtIBWnNNnAwaAm4CdwPeVUi/TNG2m+iZN074IfBHguuuuW+ZA3CZw08fh595fcymWi5l+ns/gd/qZSE3oP9z6LFgsOIFAexujsQyTiQyaBr2dThh/Di49CS97O5F8ApAZfUJsFSFXiEgmQrFUXLRj73R6GofVgcfuacLqhBBCiK3D7ErfJaB6Su/O8rVqF4H7NE3La5p2HjiFHgI3rwc+Bvd9aOnHfbtgx7U1l2LZmOmdOw1+p18/0wd6ta9Mn9WXrmzx7PG59HEU9/8NGIL1AAAgAElEQVQuaKXKc4JO6dQnxFYQcoUoaSV9O/giptPThFwhlFImr0wIIYTYWswOfU8AA0qpvUqpNuAdwH3z7rkXvcqHUiqEvt3znJmLNN3sJJx7ZOnHf/IlGPtZzaVmnukLOANEM1E0TYNnvgb/9tuAPoh9bCbD6Iwxo88JyUmwt4OjoxL6Ai6p9AmxFRhdOZfa4jmVnpKtnUIIIYQJTA19mqYVgA8C3wJeBO7RNO15pdSdSqm3lm/7FhBWSr0AfA/4b5qmLT3oaTMIDsDMCOQzCx/LJuGb/zec/W7lkqZpTd3eGXAGyJfyzOZn9XETz3wVivnFK33J8ZpxDcbzhRCbX2VAe2rxc9fhdFhCnxBCCGEC08/0aZr2TeCb8659sup7Dfi98l9bQ2gA0CByDroHax9bZEZfupCmUCo0r3tnuftmJBOhwxjbkJykx+cknilwZjKJx2mjw2GDxITehAYJfUJsNUagW67Sd233tYs+JoQQQoj6MX04u1hEsF//Gj698LHFZvQZg9mbOLIBmDerb5ze8tiGEyPRyvckJ6BDHzURzoSxW+x02DtMX7MQwnzLhb5cMUcsG5NKnxBCCGGCVuzeufUE+6Hv1WBzLnysEvrmJlvEcuXQ18TtnaAPWjeGruuz+vYAcG5qlpuu1M/y8JsPQjFfuT/gDEjTBiG2CKfNicfuWTT0hdP6rn3j3J8QQgghGkdCXytwdOjhaDGJcujzLKz0Nat7ZyX0ZaPQfUBfWzFPr89VuccY1k773Kf4kUxEtnYKscUEXcHKEPZqMqNPCCGEMI+EvlaiaTC/CvbzvwODR6HNXbnU9O2dVWf68PbA778EwLZCsXKP3rlzCp74ErzsP0FogEhaQp8QW03IFapU9aoZQTDkltAnhBBCNJqc6WsV3/8MfPagHvyq2V0Q3F9zqdnbO502Jy6ba25WX5nDZiXU4QDKnTsjZ+GRP4OZYUAqfUJsRV2uruUrfU4JfUIIIUSjSehrFW0deqfO2XlnX354N7x4f82lZm/vhLlZfQA8+HF48A8A6PK0AeC2WyAxrj/e0Y2maRL6hNiCgq7gomf6ptPTKJTM7RRCCCFMIKGvVQQH9K/Tp2qv/+Av4FTteb94Nk6bpQ2ndZHGLybxO/xzoS98BoYfAyCZKQDw4PPj+mB2gI7tpAtpMsWMvMETYovpcneRLqRJ5VM116fT0/idfuwWe5NWJoQQQmwdEvpaRWiRsQ3FAsxO1szoA317Z6ejs6ldMAOuwNz2zo5uJkdH2POxb3Ahmgbgvp+N8Zf3P0ZBs4A7QDijn+mRSp8QW0tlQPu8LZ5T6Slp4iKEEEKYREJfq+jcBVYHTFeFvuQEaKWaGX2gb+9sVhMXg9/h17t3Ani202WJceTl3Ths+j9STruFa4IFVEcXWKwymF2ILWqpWX3hdFhCnxBCCGESCX2twmKF638bel85d22RGX2ghz5vW/PO84Ee3iLpCJqmgWc7SivRY02QK5Zw2CxkCyW+uftjWD/0JACRtB76gs5gM5cthDCZVPqEEEKI5pORDa3kl/649ufZ8pk4z7xKXy7Gzo6dJi1qcX6nn1wpR6qQoj2wD3ZcRyKZ4N3X7+Ndr+rjKz8ZYSqRAaceTqXSJ8TWZAxfrx7boGka0+lpCX1CCCGESST0tZrZMLh8euXvwFvgD8bA2lZzSzwbpzPY5O2dVbP62ve/Afa/gT+pevzTR6/Su3q+OAYHb65sBTWeJ4TYGjodndiUjanUXKUvlo1RKBUqgVAIIYQQjSXbO1vJs/8Cn9kH4bNz19rcYK3N5vFcvGkz+gxGxa7SwXO+UhF+/Ncw9jNA/5TfbXPjtDWv46gQwnwWZVkwtqEyo08qfUIIIYQpJPS1Ev9e/avRwfPHX4RH/0fNLdlilnQh3fRGLjWhr1SEL9yozxQ0zE7rTWg6ugEZzC7EVhZyhWpCn3G+T0KfEEIIYQ4Jfa3EGNtgdPB84etw+ts1t8SzcYCmh77q7Z1YrBC7WDtuIjmhf60OfTKjT4gtaX7ok0qfEEIIYS4Jfa3E2Qnt2+bCU2J0YROXbAwAr6O53Tv9jqrQB9CxHRLjczcYoc+zvXKfVPqE2JpCrlBN904j9HW55UyfEEIIYQYJfa0mNADTZ0DT9JEN3oWD2YGmn+lz2904rc65M32e7trQl02AzQkd2wA99Mm4BiG2ppArRDQTpVAqAHroc9lcuG3uJq9MCCGE2Bqke2eruf63oZiHdBQKmYWhr1zpa/b2TtC3eFYGtHdsh6mTcw9e9TY4dAsAJa1ENBOVSp8QW1SXqwsNjWgmSpe7qzKjTynV7KUJIYQQW4KEvlYzeET/Gj4LrkBLh76AMzC3vXPXz+khtVr5DV08E6OoFSX0CbFFVQ9o73J3yYw+IYQQwmQS+lpNIQdTL+lh76Pn9W2eVeK5ciOXJm/vhHKlz9jeed1v6n8Zvv8ZyKXgF26XwexCbHEhtx7wjLN80+lp+n39zVySEEIIsaXImb5WE7sAX/g/4NS39J/nbX+KZWNYlZV2e3sTFlerptI33+mH4OITwFyzFxnMLsTWZFT1KqEvJZU+IYQQwkxrDn1KqTcppf5IKfVFpVRf+dqNSqnelZ4rVsG3Gyx2eOhT8LX3Laj0xbIxOh2dLXEWxu+oqvSNPQP/fR+ceUj/OTlRM64BpNInxFZVHfoyhQyJfIIul3TuFEIIIcyy6tCnlOpWSv0YuB94L/B+wPio9r8Af1T/5W1BVhsE9sLsJAw9trDSl4vhbWvuuAaD3+knU8yQyqfA6YVUWO/gqWmLhr6gS7p3CrEVOawOPG0eplJTMqNPCCGEaIK1VPo+D3QAB8p/VaeRh4DDdVzX1hYc0L/Om9EHc5W+VmBU7qLZqN69EyAxBrkk5FP6GAfmQp/P4WvKOoUQzRdyhQhnwhL6hBBCiCZYS+h7I/AJTdPOANq8xy4CO+q2qq2uPMuK8Wfgz6+CZ+6pPNSSoS8TBbtTHy6fmIBMHAL7oHMXoIc+n8OHzSJ9g4TYqrpcXVLpE0IIIZpkre/CC0tcDwHpDa5FgB7wzj8y93PsAtz/Yf37q99OPBdvma53RmOWSjOXju2QHIfOHfDhn1bui2Qicp5PiC0u6Ary7NSzldDX5ZYzfUIIIYRZ1lLpexT4sFLKWnXNqPj9JvDduq1qK/vOnQvn3eXT+nVaq9K3IPS97Ndg92sW3BdOhyX0CbHFdbn0+XxT6SksyoLfId18hRBCCLOspdL3UeAx4DngOHrg+y2l1CHgZcDP1395W1Ds4pLX86U8yXwSr6M1GrnUbO8EeN1H9K8/+2d4+h/hnf8MbW4imQhX+K9o0iqFEK0g5AqRKWYYjg8TcAawWqwrP0kIIYQQdbHqSp+mac8B1wFPAu8DisDb0M/zXa9p2qlGLHDL6dy55PVELqF/2wKD2QHcNjcOq2Mu9AHkMzD5PIz8COwuQLZ3CiHmzvC9FHlJzvMJIYQQJlvTnD5N085omvYeTdN6NU1r0zRtu6Zp79Y07XSjFrjlHP5kJSxV2F1w+JPEsjGAlqn0KaXwO/1z2zt/8iX4k26YOqmPa1CKfClPPBeX0CfEFmcEveH4sIQ+IYQQwmRrmdO3Syl1zRKPXaOU2lW/ZW1hV78dbv5cufOl0r/e/Dm4+u2V0NcqlT7QB7RXQp+7HOzGfgYd2wCYycwAMphdiK2uehi7hD4hhBDCXGs50/dXwCngxCKPvQu4Eri5Hova8q5+u/7XPPFcHKBlGrmAHuYq2zurZ/X16p8PGIEw4JLQJ8RWVh30qgOgEEIIIRpvLds7f56lO3R+D2nk0nCVSl8LhT6/068PZwfwbJ97oOflAIQzYUAqfUJsdZ2OzsqszqAr2OTVCCGEEFvLWkKfm4VD2au1b3AtYgWtuL0z4AxUzenr1r/+wqfgpo8CVZU+CX1CbGlKqcoHVsVSscmrEUIIIbaWtYS+Z4F3LvHYO4HnN74csZxYTg99njZPk1cyx+/0ky6kSRfS4OiA19xa2doJEElL6BNCQCqfIp7Vt6h/+bkvk8qnmrwiIYQQYutYS+j7U+BdSqmvKaXeUm7e8hal1D3ooe9PGrNEYYhlY3jaPC0132rBrL6r3gbf+D0Y/iGgV/psyoa3rTU6jgohmuOTj3+SQqkAQCKX4PbHb2/yioQQQoitYy1z+o4D7wVeDdwPPFH++mrg1zVNu7chKxQVsWyspbZ2gt69E8qh75l74B9ugfAZ+NpvwDP3VGb0KaWavFIhRLMcP32c71/8Plr5hECulOPhCw9z/PTxJq9MCCGE2BrWOqfvH4BdwCBwY/lrn6Zp/9SAtYl54rl4SzVxAX17J0Dkxa/D/R+GlN64heQk3P9hIpPPVe4RQmxNx04c07eAV8kUMxw7caxJKxJCCCG2ljWFPgBN95KmaT8of12uuYuoo3i29UJfZXvnM1+BfO2bOvJpopFTcp5PiC3u1mtuxWVz1VxzWp3cdu1tTVqREEIIsbUsO6dPKfU7wNc0TZsqf78cTdO0v6rf0sR8sVyMHR07mr2MGpXQV+4sOl9YK7JLZvQJsaXdMnALPxj9AQ9feJhsMYvD4uCmXTdxtP9os5cmhBBCbAkrDWf/S+BJYKr8/XI09AHuokFi2RheR2s1RGm3t2O32Am7OyEeX/B4xGaVSp8QgjtvuJOjXz/K+Ow4QVeQO264o9lLEkIIIbaMZbd3appm0TTtJ1XfL/dX67SU3IRKWqklz/QppfQB7TteAfba7VupNhdppST0CSFw293cffhu9vn2cdfhu3Db3c1ekhBCCLFlrLS988Y1vJamadqjG1yPWEIyn6SklVqueyfoWzyjbh/c/Dn4zp0QuwidO4m+9sPwwucJOoPNXqIQogX0+/u594g0ehZCCCHMttL2zofRt20a/farm7aoeT8DSLWvQWLlM3OtVukDfWxDNBOFq9+u/1UWmXoWXvi8VPqEEEIIIYRoopVC38uqvu8Bvgw8CPwbMAlsA34V+GXgNxuxQKGLZ/Xzcq0Y+gKuABcSFxZcj2Qi+uMS+oQQQgghhGiaZUOfpmnPG98rpf4f4O81TfvEvNseVEp9GrgVeKj+SxRwGVT6stEF1yuhT7p3CiGEEEII0TRrmdN3GHhkicceAW7a8GrEkmK5cuhr0TN9s/lZssVszfVwRh/U7nfIcHYhhBBCCCGaZS2hLwIcWeKxW8qPiwYxKn2tNrIBwO/UQ100U1vti2QiuGwu6dInhBBCCCFEE610pq/anwJ/qZTaA9zH3Jm+I8CbgA/We3FiTmV7ZwtW+ozQF8lE2N6+vXI9monKeT4hhBBCCCGabNWhT9O0u5VSl4A/AO5C79RZBH4KvE3TNOnD3UCxXAy3zY3dam/2UhYwRjIsVumT0CeEEEIIIURzraXSh6ZpXwe+rpSyAiFgWtO0YkNWJmrEsrGWbOICtZW+apFMhG53dzOWJIQQQgghhChby5m+Ck3TipqmTUjgM088G7/8Ql9aKn1CCCGEEEI027pCnzBfLBdryfN8AB67B5vFVrO9U9M02d4phBBCCCFEC5DQd5mIZWMt2bkTQCm1YFZfPBenoBUk9AkhhBBCCNFkEvouE7FsDG9ba4Y+0Gf1VW/vlMHsQgghhBBCtAYJfZcBTdP07Z0teqYP9HN91ds7K6HPIaFPCCGEEEKIZpLQdxlIF9IUSoWWD31S6RNCCCGEEKL1SOi7DLTyYHZDwBmoqfQZ38uZPiGEEEIIIZpLQt9lIJYrh75WrvQ5/CTzSXLFHADhTLhyXQghhBBCCNE8EvouA5VKXwuHPmMbp1Hhi6QjeNu82K32Zi5LCCGEEEKILU9C32XACH0t3b2z3LDFGNsgM/qEEEIIIYRoDRL6LgOXxfZOp76NM5LWG7hI6BNCCCGEEKI1mB76lFJvVEqdVEqdUUp9bJn7flUppSmlrjNzfa3octjeWQl92bnQF3QFm7kkIYQQQgghBCaHPqWUFbgLeBMwCLxTKTW4yH0e4HeBH5u5vlYVz8Zps7ThtDqbvZQlGVW9ypk+qfQJIYQQQgjREsyu9L0KOKNp2jlN03LAV4Eji9z3x8CfARkzF9eq4rk4nY5OlFLNXsqSPG0ebMpGNBOlUCowk52pVP+EEEIIIYQQzWN26NsBXKj6+WL5WoVS6hpgl6Zp31juhZRSH1BKPamUenJqaqr+K20hsWyspbd2AliUBZ/TRyQTYSY7A8iMPiGEEEIIIVpBSzVyUUpZgM8Cv7/SvZqmfVHTtOs0Tbuuq6ur8Ytrolgu1tKdOw1+p59IJkIko5/rk9AnhBBCCCFE85kd+i4Bu6p+3lm+ZvAAVwEPK6WGgJ8H7tvqzVwuh0of6GMbopmohD4hhBBCCCFaiNmh7wlg4P9v796jrKzve4+/v3uAGVBAwEujo3EQFDEQNUhtyIVI2pK2UWypJy57VrTtac9RV6LHs1JjlhA5rU1Se+QkNWuZNMQ21cZLDoqedDXGxhrkNISIiRdI2EMmMt4g4N4jl7kxv/PH7CEDjkYzs59nz573ay3W3s9l7/3drt9y8+H7e35PRLRExATgI8C6gYMppXJK6diU0qkppVOB/wAuTCltyrjOmjJaQt+0pmm80vXKods2zGhy9U5JkiQpb5mGvpRSL3A18K/AFuCelNIzEbEqIi7MspbRpKO7g6kTRkfoc3qnJEmSVFvGZf2BKaVvAt88Yt+K1zl3cRY11bKug10c6D0wKjp905um82r3q+zcv5OGaGBKY+1fhyhJkiTVu5payEWv1dHVAdT2jdkHDHT2WsutTGuaRiEcXpIkSVLe/Ft5jSt3lQFGRdds4L58xVeKTu2UJEmSaoShr8aVu/tD36i4pq+xP/S9sO8Fb8wuSZIk1QhDX40b6PSNpumdRz6XJEmSlB9DX40braHP2zVIkiRJtcHQV+M6uisLuYyC6Z1TGqfQEA2AnT5JkiSpVhj6aly5q0xDNHDU+KPyLuWXKkThUEfS0CdJkiTVBkNfjSt3lZkyYQoRkXcpb8pAOO062JVzJZIkSZLA0Ffzyt3lUXE9H8D+nv28vO9lAL781JfZ37M/54okSZIkGfpqXLmrPCru0QewYsMKevt6gf6byq/csDLniiRJkiQZ+mpcuas8KhZxWbttLY+1P0YffQB093Xz6I5HWbttbc6VSZIkSWOboa/GdXR3jIrpnaufWM2B3gOH7es82MnqJ1bnVJEkSZIkMPTVvHLX6Lim75pzr2HiuImH7WtqaOLad12bU0WSJEmSwNBX03r6etjbs3dUTO+8ePbFvK/5fTQ2NALQWGhk8cmLWTZrWc6VSZIkSWOboa+Gvdr9KsCoWchl1btXMb1pOkEwY+IMbnr3TXmXJEmSJI15hr4aVu4qA4yK6Z0Ak8ZP4otLvsjMY2Zy25LbmDR+Ut4lSZIkSWPeuLwL0Os7FPpGwfTOAbOmzeL+i+7PuwxJkiRJFXb6alhHdwcwejp9kiRJkmqPoa+GjbbpnZIkSZJqj6Gvhh3q9I2i6Z2SJEmSaouhr4YNdPomT5iccyWSJEmSRitDXw0rd5WZPGEyDYWGvEuRJEmSNEoZ+mpYubvs1E5JkiRJw2Loq2HlrrKLuEiSJEkaFkNfDevo6jD0SZIkSRoWQ18Nc3qnJEmSpOEy9NWwcleZKY1T8i5DkiRJ0ihm6KtRfamPjm6nd0qSJEkaHkNfjdrbs5e+1Of0TkmSJEnDYuirUQM3ZrfTJ0mSJGk4DH01qqOrA4ApE7ymT5IkSdKvztBXo+z0SZIkSRoJhr4aVe429EmSJEkaPkNfjbLTJ0mSJGkkGPpq1EDo85o+SZIkScNh6KtR5e4yE8dNZELDhLxLkSRJkjSKGfpqVLmr7NROSZIkScNm6KtRHV0d3phdkiRJ0rAZ+mpUudtOnyRJkqThM/TVqF37d/HM7mcovlLMuxRJkiRJo5ihrwbt79nPC3tfYF/PPq565Cr29+zPuyRJkiRJo5ShrwateHwFffQBsPvAblZuWJlzRZIkSZJGK0NfjVm7bS3/3v7vh7a7+rp4dMejrN22NseqJEmSJI1Whr4as/qJ1XQe7DxsX+fBTlY/sTqniiRJkiSNZoa+GnPNudcwoXD4DdmbGpq49l3X5lSRJEmSpNHM0FdjLp59MW+f8vZD242FRhafvJhls5blWJUkSZKk0crQV4NaprZQiAJBMGPiDG569015lyRJkiRplBqXdwF6rZ91/Ix3HvdOOro7uOV9tzBp/KS8S5IkSZI0StnpqzG9fb38tPxT5h87n/svup9Z02blXZIkSZKkUczQV2N2vLqD7r5uTjvmtLxLkSRJklQHDH01prXUCsDsabNzrkSSJElSPTD01ZhiqQjAzKkzc65EkiRJUj0w9NWY1lIrJx19kou3SJIkSRoRhr4aUywVvZ5PkiRJ0ogx9NWQnr4e2jraDH2SJEmSRoyhr4bs6NhBb18vs47xNg2SJEmSRoahr4YMLOJi6JMkSZI0Ugx9NaRYKhIELVNb8i5FkiRJUp0w9NWQYqlI8+RmJo6bmHcpkiRJkuqEoa+GtJZaXcRFkiRJ0ogy9NWInoM9PNfxnNfzSZIkSRpRhr4a0dbRRm/qtdMnSZIkaURlHvoiYmlE/DgiihFx/RDH/3tEPBsRP4qIRyLi7VnXmIfWUivgyp2SJEmSRlamoS8iGoDbgA8Bc4FLI2LuEadtBhaklOYD9wGfy7LGvBRLRQpRcOVOSZIkSSMq607fQqCYUtqeUuoGvg5cNPiElNJ3Ukr7K5v/ATRnXGMuiqUip0w+hcaGxrxLkSRJklRHsg59JwE7Bm23V/a9nj8B/mWoAxHxZxGxKSI27dq1awRLzIcrd0qSJEmqhppdyCUi/ghYAPzNUMdTSl9KKS1IKS047rjjsi1uhHUd7OK5V58z9EmSJEkaceMy/rzngZMHbTdX9h0mIj4IfAp4f0qpK6PactNWbqMv9bmIiyRJkqQRl3Wn7/vA7IhoiYgJwEeAdYNPiIhzgNuBC1NKOzOuLxfFUhHATp8kSZKkEZdp6Esp9QJXA/8KbAHuSSk9ExGrIuLCyml/AxwN3BsRT0bEutd5u7rRWmplXIyjZYord0qSJEkaWVlP7ySl9E3gm0fsWzHo+QezrilvxVKRU6acwviG8XmXIkmSJKnO1OxCLmNJsVR0aqckSZKkqjD05exA7wHaX213ERdJkiRJVWHoy9lPyz8lkez0SZIkSaoKQ1/OWkutAHb6JEmSJFWFoS9nxVKRcYVxnDLllLxLkSRJklSHDH05ay21cuqUUxlfcOVOSZIkSSPP0JezYqno1E5JkiRJVWPoy9H+nv08v/d5F3GRJEmSVDWGvhxtL28HXMRFkiRJUvUY+nJULBUB7PRJkiRJqhpDX45aS62ML4zn5Mkn512KJEmSpDpl6MtRsVRk5tSZjCuMy7sUSZIkSXXK0Jej1lKrUzslSZIkVZWhLyf7evbx4r4XXcRFkiRJUlUZ+nLSWmoFXMRFkiRJUnUZ+nIysHKnnT5JkiRJ1WToy0mxVKSxoZGTjj4p71IkSZIk1TFDX05aS63MnDqThkJD3qVIkiRJqmOGvpwUS0WndkqSJEmqOkNfDjq6O9i5f6eLuEiSJEmqOkNfDraXtgMu4iJJkiSp+gx9ORhYudNOnyRJkqRqM/TloFgqMnHcRE48+sS8S5EkSZJU5wx9OSiWisycOpNC+J9fkiRJUnWZOnLQWmr1ej5JkiRJmTD0ZazcVebnB35u6JMkSZKUCUNfxlzERZIkSVKWDH0Zay21At6uQZIkSVI2DH0ZK5aKHDX+KH7tqF/LuxRJkiRJY8C4vAsYa4qlIqdNPY2IyLsUSZIkaVTq6emhvb2dzs7OvEvJXFNTE83NzYwfP/5Nv8bQl7HWUivvb35/3mVIkiRJo1Z7ezuTJ0/m1FNPHVPNlJQSu3fvpr29nZaWljf9Oqd3ZmhP5x72dO7xej5JkiRpGDo7O5kxY8aYCnwAEcGMGTPecofT0JchF3GRJEmSRsZYC3wDfpXvbejLkLdrkCRJkpQ1Q1+GWkutTB4/meMnHZ93KZIkSdKYsrOjk0tu/3/sfHXsLf5i6MtQsVTktGNcuVOSJEnK2ucf2cb32/bw+W9vy7uUzLl6Z0ZSSrSWWllyypK8S5EkSZLqxk0PPsOzL3S87vGNbXtI6Rfb//S95/in7z1HBCw8dfqQr5l74hRWfvisN/zctrY2li5dyvnnn8+GDRs477zzuOKKK1i5ciU7d+7kzjvvBODjH/84nZ2dTJw4ka9+9aucccYZ3HrrrTz11FOsWbOGp556iksvvZSNGzcyadKkt/4f4E2w05eR3Z27KXWVmD1tdt6lSJIkSWPG2c3HMOOoCRQqk+0KATOOmsDZzccM+72LxSLXXXcdW7duZevWrdx1112sX7+eW265hZtvvpk5c+bw3e9+l82bN7Nq1SpuuOEGoD8IFotF1q5dyxVXXMHtt99etcAHdvoy4yIukiRJ0sj7ZR05gE+tfYq7Nj5H47gC3Qf7+NA7fo2/vHjesD+7paWFefP63+ess85iyZIlRATz5s2jra2NcrnMRz/6UbZt20ZE0NPTA0ChUOCOO+5g/vz5/Pmf/zmLFi0adi1vxE5fRrxdgyRJkpSPn+/t4rJffztrr1zEZb/+dnbt7RqR921sbDz0vFAoHNouFAr09vZy44038oEPfICnn36aBx988LD7623bto2jjz6aF154YURqeSN2+jJSLBWZ2jiVGU0z8i5FkiRJGlNu/88LDj3/y2XvyOxzy+UyJ510EgB33HHHYfs/9rGP8dhjj3H11Vdz3333sXz58qrVYacvI62lVk6b6sqdkiRJ0ljxiU98gk9+8pOcc8459Pb2Htp/7bXXctVVV3H66afzla98heuvv56dO3dWrY5Ig5eyGaUWLFmyc6YAAA89SURBVFiQNm3alHcZryulxKKvL+JDp36IG3/jxrzLkSRJkka1LVu2cOaZZ+ZdRm6G+v4R8YOU0oKhzrfTl4FdB3bxaverLuIiSZIkKXOGvgwUX+lfudPbNUiSJEnKmqEvA96uQZIkSVJeDH0ZaC23Mr1pOtObpuddiiRJkqQxxtCXgWKpaJdPkiRJUi4MfVWWUmJ7aTunTTX0SZIkScqeoa/KXt7/Mnt79jLrmFl5lyJJkiSNTT+6B259B3z6mP7HH92Td0WZGpd3AfVuYBGXWdMMfZIkSVLmfnQPPPgx6DnQv13e0b8NMP+S/OrKkJ2+Khu4XYOdPkmSJKlKvvq7r/2z8cv9x7590y8C34CeA/Avf9H/fN/u1772TWhra2POnDlcfvnlnH766Vx22WV8+9vfZtGiRcyePZuNGzeyb98+/viP/5iFCxdyzjnn8MADDxx67Xvf+17OPfdczj33XDZs2ADAo48+yuLFi1m+fDlz5szhsssuI6U07P88dvqqrFgqcuzEY5naODXvUiRJkqSxp+P5ofcf2DPsty4Wi9x7772sWbOG8847j7vuuov169ezbt06br75ZubOncsFF1zAmjVrKJVKLFy4kA9+8IMcf/zxPPzwwzQ1NbFt2zYuvfRSNm3aBMDmzZt55plnOPHEE1m0aBGPP/4473nPe4ZVp6GvylpLra7cKUmSJFXTFf/39Y9Nbe6f0vma/Sf3Px41441f/wZaWlqYN28eAGeddRZLliwhIpg3bx5tbW20t7ezbt06brnlFgA6Ozt57rnnOPHEE7n66qt58sknaWho4Cc/+cmh91y4cCHNzc0AnH322bS1tRn6allf6qO13Mrvz/79vEuRJEmSxqYlKw6/pg9g/MT+/cPU2Nh46HmhUDi0XSgU6O3tpaGhgW984xucccYZh73u05/+NCeccAI//OEP6evro6mpacj3bGhooLe3d9h1ek1fFb2470UO9B6w0ydJkiTlZf4l8OHPVzp70f/44c9nsojLb//2b/OFL3zh0HV5mzdvBqBcLvO2t72NQqHA1772NQ4ePFjVOuz0VVFrqRVwERdJkiQpV/MvyWWlzhtvvJFrrrmG+fPn09fXR0tLCw899BBXXnklf/AHf8A//uM/snTpUo466qiq1hEjsRpM3hYsWJAGLnysJWueXsOtP7iVxy99nCkTpuRdjiRJklQXtmzZwplnnpl3GbkZ6vtHxA9SSguGOt/pnVXUWmrl+EnHG/gkSZIk5cbQV0XbXtnm1E5JkiRJuTL0VUlf6uOn5Z+6iIskSZKkXGUe+iJiaUT8OCKKEXH9EMcbI+LuyvHvRcSpWdc4EjY8v4HOg51O7ZQkSZKUq0xDX0Q0ALcBHwLmApdGxNwjTvsT4JWU0izgVuCzWdY4Evb37OeG9TcAcPeP72Z/z/6cK5IkSZI0VmXd6VsIFFNK21NK3cDXgYuOOOci4B8qz+8DlkREZFjjsK3YsIKO7g4AOro6WLlhZc4VSZIkSWNb8ZUiyx5YRvGVYt6lZC7r0HcSsGPQdntl35DnpJR6gTIwI5PqRsDabWt5rP0xDqb+Gyx293Xz6I5HWbttbc6VSZIkSWPT/p79XPnIlWwvbeeqR67KbCbeunXr+MxnPpPJZ72RUbuQS0T8WURsiohNu3btyrucQ1Y/sZoDvQcO29d5sJPVT6zOqSJJkiRpbFuxYQV7OveQSOw+sDuzmXgXXngh11//mmVMMjcu4897Hjh50HZzZd9Q57RHxDhgKrD7yDdKKX0J+BL035y9KtX+Cq459xr+euNfHxb8mhqauPZd1+ZYlSRJklSfPrvxs2zds/V1j+/av4sde3fQl/oA6Orr4ltt32LL7i0cN+m4IV8zZ/oc/mLhX7zh57a1tbF06VLOP/98NmzYwHnnnccVV1zBypUr2blzJ3feeSfPPvssmzZt4u/+7u+4/PLLmTJlCps2beKll17ic5/7HMuXL//Vv/hbkHWn7/vA7IhoiYgJwEeAdUecsw74aOX5cuDfUko1E+p+mYtnX8z7mt9HY0MjAI2FRhafvJhls5blXJkkSZI09jy/9/lDgW9AH308v/fI3tNbVywWue6669i6dStbt27lrrvuYv369dxyyy3cfPPNrzn/xRdfZP369Tz00EOZdgAz7fSllHoj4mrgX4EGYE1K6ZmIWAVsSimtA74CfC0iisAe+oPhqLLq3atY9sAyXtr3EjMmzuCmd9+Ud0mSJElSXfplHbm129YOORPvU+d/atiNmZaWFubNmwfAWWedxZIlS4gI5s2bR1tb22vOX7ZsGYVCgblz5/Lyyy8P67Pfisyv6UspfTOldHpK6bSU0l9V9q2oBD5SSp0ppT9MKc1KKS1MKW3PusbhmjR+El9c8kVmHjOT25bcxqTxk/IuSZIkSRqTqjkTr7Gx8dDzQqFwaLtQKNDb2/uG52c5mXHULuRS62ZNm8X9F93PrGmz8i5FkiRJGtNWvXsV05umE8SYnIln6JMkSZJU18b6TLwYRWukvK4FCxakTZs25V2GJEmSpAxs2bKFM888M+8ycjPU94+IH6SUFgx1vp0+SZIkSapjhj5JkiRJqmOGPkmSJEmjTj1cpvar+FW+t6FPkiRJ0qjS1NTE7t27x1zwSymxe/dumpqa3tLrMr05uyRJkiQNV3NzM+3t7ezatSvvUjLX1NREc3PzW3qNoU+SJEnSqDJ+/HhaWlryLmPUcHqnJEmSJNUxQ58kSZIk1TFDnyRJkiTVsaiHFW8iYhfws7zrGMKxwM/zLkJjgmNNWXK8KSuONWXFsaYsVWu8vT2ldNxQB+oi9NWqiNiUUlqQdx2qf441Zcnxpqw41pQVx5qylMd4c3qnJEmSJNUxQ58kSZIk1TFDX3V9Ke8CNGY41pQlx5uy4lhTVhxrylLm481r+iRJkiSpjtnpkyRJkqQ6ZuiTJEmSpDpm6KuSiFgaET+OiGJEXJ93PaofEbEmInZGxNOD9k2PiIcjYlvlcVqeNao+RMTJEfGdiHg2Ip6JiI9X9jveNKIioikiNkbEDytj7abK/paI+F7lt/TuiJiQd62qDxHREBGbI+KhyrZjTVUREW0R8VREPBkRmyr7Mv8dNfRVQUQ0ALcBHwLmApdGxNx8q1IduQNYesS+64FHUkqzgUcq29Jw9QLXpZTmAucDV1X+X+Z400jrAi5IKb0TOBtYGhHnA58Fbk0pzQJeAf4kxxpVXz4ObBm07VhTNX0gpXT2oHvzZf47auirjoVAMaW0PaXUDXwduCjnmlQnUkqPAXuO2H0R8A+V5/8ALMu0KNWllNKLKaUnKs9fpf8vSCfheNMIS/32VjbHV/4k4ALgvsp+x5pGREQ0A78L/H1lO3CsKVuZ/44a+qrjJGDHoO32yj6pWk5IKb1Yef4ScEKexaj+RMSpwDnA93C8qQoq0+2eBHYCDwOtQCml1Fs5xd9SjZTVwCeAvsr2DBxrqp4EfCsifhARf1bZl/nv6Lhqf4CkbKWUUkR4LxaNmIg4GvgGcE1KqaP/H8X7Od40UlJKB4GzI+IYYC0wJ+eSVIci4veAnSmlH0TE4rzr0ZjwnpTS8xFxPPBwRGwdfDCr31E7fdXxPHDyoO3myj6pWl6OiLcBVB535lyP6kREjKc/8N2ZUvo/ld2ON1VNSqkEfAf4DeCYiBj4B2p/SzUSFgEXRkQb/ZffXAD8bxxrqpKU0vOVx530/4PWQnL4HTX0Vcf3gdmVlaAmAB8B1uVck+rbOuCjlecfBR7IsRbVicp1Ll8BtqSU/tegQ443jaiIOK7S4SMiJgK/Sf81pN8BlldOc6xp2FJKn0wpNaeUTqX/72f/llK6DMeaqiAijoqIyQPPgd8CniaH39FIyVk51RARv0P/nPEGYE1K6a9yLkl1IiL+GVgMHAu8DKwE7gfuAU4BfgZcklI6crEX6S2JiPcA3wWe4hfXvtxA/3V9jjeNmIiYT/9iBg30/4P0PSmlVRExk/5uzHRgM/BHKaWu/CpVPalM7/wfKaXfc6ypGirjam1lcxxwV0rpryJiBhn/jhr6JEmSJKmOOb1TkiRJkuqYoU+SJEmS6pihT5IkSZLqmKFPkiRJkuqYoU+SJEmS6pihT5KknETE4ohIEfGOvGuRJNUvQ58kSZIk1TFDnyRJkiTVMUOfJGnMiYj3RsS/R8T+iNgdEV+OiMmVY5dXplyeFxHfjYgDEfGTiLh4iPe5OiK2RURXRBQj4tohzpkfEQ9GRCki9kbExoj4zSNOOzYi7q0c3x4RV1bpq0uSxiBDnyRpTImIRcC3gZeA5cA1wO8AXz3i1LuBB4DfB54C7o2Idw56n/8CfAFYB3wYuBf424i4ftA5c4DHgbcB/xW4GFgLnHzEZ30Z+GHl+KPAbRGxcPjfVpIkiJRS3jVIkpSZiPgu0JtS+sCgfRcAjwDzgAX0B8BPpZRurhwvAM8CT6aUPlLZ3gF8K6V0xaD3+SJwGXBCSqkzIv4ZeC8wO6V0YIhaFgPfAf5nSmlFZd944AXgKyml6498jSRJb5WdPknSmBERk4DfAO6JiHEDf4D1QA/wrkGnrx14klLqo7/rN9B9awZOpL+7N9jdwBT6wyPABcDdQwW+I3xr0Gf1ANsqnyFJ0rAZ+iRJY8k0oAH4Iv0hb+BPFzCew6dd7jzitTvpn6bJoMeXjzhnYHt65XEG8OKbqKt0xHY30PQmXidJ0i81Lu8CJEnKUAlIwKeBbw5x/AXgtyrPjwd2Dzp2PL8IcC8O2jfYCZXHPZXH3fwiIEqSlAs7fZKkMSOltA/4D+CMlNKmIf68MOj0Q6t1Vq7huwjYWNnVTn9A/MMjPuISoIP+hV+g/zrBSyLCrp0kKTd2+iRJY80ngEciog+4D3gVOAX4XeBTg87704joBp4G/hSYBVwK/df4RcSngdsjYjfwMPB+4L8BN6SUOivvcRPwfeCxiPhb+jt/5wC7U0prqvotJUmqsNMnSRpTUkrrgfcBxwFfAx6kPwju4PBr9D5Cf7fvfuCdwH9KKW0e9D5fBj5eOech+gPhdSmlzww658fAe4CfA39P/+Iwy4GfVenrSZL0Gt6yQZKkQSLicvpv2TA5pbQ353IkSRo2O32SJEmSVMcMfZIkSZJUx5zeKUmSJEl1zE6fJEmSJNUxQ58kSZIk1TFDnyRJkiTVMUOfJEmSJNUxQ58kSZIk1bH/D5hyjLJSSFIIAAAAAElFTkSuQmCC\n"},"metadata":{"needs_background":"light"}}],"source":["plt.figure(figsize=(15,7))\n","plt.plot(np.max(Dict_DL[0], axis=1), '-*', label='max')\n","plt.plot(np.mean(Dict_DL[0], axis=1), '--o', label='mean')\n","plt.plot(np.min(Dict_DL[0], axis=1), '-d', label='min')\n","plt.title('dice change in training process')\n","plt.xlabel('epoch', {'size':15})\n","plt.ylabel('dice', {'size':15})\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":465},"executionInfo":{"elapsed":693,"status":"ok","timestamp":1647736747443,"user":{"displayName":"胡浩宇","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06402011815971394657"},"user_tz":-480},"id":"TNlJJz0zbLPY","outputId":"0f141acc-c10f-4672-f34e-81d02af86359"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Text(0, 0.5, 'loss')"]},"metadata":{},"execution_count":8},{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x504 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["plt.figure(figsize=(15,7))\n","plt.title('loss change in training process')\n","plt.plot(Dict_DL[1], 'rd-')\n","plt.xlabel('epoch', {'size':15})\n","plt.ylabel('loss', {'size':15})"]},{"cell_type":"markdown","metadata":{"id":"lxD8SZeybLPZ"},"source":["#### plot the testing results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ac6oKshbbLPZ","outputId":"b1a32cc3-42a3-444f-87da-3cd770127e69"},"outputs":[{"data":{"text/plain":["[0.9437307879496948,\n"," 0.9326509143396567,\n"," 0.9325968150484968,\n"," 0.9405126394458315,\n"," 0.9245356325920436]"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["list(dice_dict.values())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kUJXhZcDbLPa","outputId":"9a7ea45a-99d7-4468-cf66-a612b1f700e8"},"outputs":[{"data":{"text/plain":["[0.9594531603821137,\n"," 0.9595811966158684,\n"," 0.9437467516837585,\n"," 0.947463483081991,\n"," 0.8613989191649729,\n"," 0.9599963992431486]"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["y2"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"3FHFN-AFbLPa","executionInfo":{"status":"ok","timestamp":1647738429008,"user_tz":-480,"elapsed":468,"user":{"displayName":"胡浩宇","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06402011815971394657"}}},"outputs":[],"source":["li_di = list(dice_dict.values())\n","y1 = li_di[:13]\n","y2 = li_di[13:38]\n","y3 = li_di[38:]\n","y_temp = {}\n","id_in = 1\n","for i in y1:\n","    y_temp['kind'] = 'Adult Mice'\n","    y_temp['Dice Coefficient'] = i\n","    if id_in:\n","        y_result = pd.Series(y_temp)\n","        id_in = 0\n","    else:\n","        y_result = pd.concat((y_result, pd.Series(y_temp)),axis=1)\n","\n","for i in y2:\n","    y_temp['kind'] = 'Baby Mice'\n","    y_temp['Dice Coefficient'] = i\n","    y_result = pd.concat((y_result, pd.Series(y_temp)),axis=1)\n","for i in y3:\n","    y_temp['kind'] = 'Rats'\n","    y_temp['Dice Coefficient'] = i\n","    y_result = pd.concat((y_result, pd.Series(y_temp)),axis=1)\n"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"z9lL7SpkbLPb","executionInfo":{"status":"ok","timestamp":1647738429737,"user_tz":-480,"elapsed":12,"user":{"displayName":"胡浩宇","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06402011815971394657"}}},"outputs":[],"source":["y_result = y_result.T"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":554},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1647738429738,"user":{"displayName":"胡浩宇","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06402011815971394657"},"user_tz":-480},"id":"F4jjyrkjbLPb","outputId":"52f8f56a-a2ca-4720-b712-4a792cae6d1c","scrolled":false},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/matplotlib/cbook/__init__.py:1376: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  X = np.atleast_1d(X.T if isinstance(X, np.ndarray) else np.asarray(X))\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 720x576 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["all_data = [y1, y2, y3]\n"," \n","fig = plt.figure(figsize=(10,8))\n"," \n","# bplot = plt.boxplot(all_data,\n","#             notch=False,  # notch shape\n","#             vert=True,   # vertical box aligmnent\n","#             patch_artist=True)   # fill with color\n","\n","bplot = plt.boxplot(all_data, \n","            patch_artist=True, \n","            showmeans=True, \n","            boxprops = {'facecolor':'#9999ff'}, \n","            flierprops = {'marker':'o','markerfacecolor':'red','color':'black'}, \n","            meanprops = {'marker':'D','markerfacecolor':'indianred'}, \n","            medianprops = {'linestyle':'--','color':'orange'}) \n"," \n","colors = ['pink', 'lightblue', 'lightgreen']\n","for patch, color in zip(bplot['boxes'], colors):\n","    patch.set_facecolor(color)\n"," \n","plt.xticks([y+1 for y in range(len(all_data))], ['Adult Mice', 'Juvenile Mice', 'Rats'])\n","plt.ylabel('Dice Coefficient')\n","t = plt.title('Box plot')\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U-_EL1SebLPc","outputId":"259dd9bc-23aa-4e71-c214-f05d4e330467"},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-50-5e1df10ab325>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"whitegrid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtips\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tips\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/konglab/apps/anaconda3/lib/python3.8/site-packages/seaborn/utils.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(name, cache, data_home, **kws)\u001b[0m\n\u001b[1;32m    594\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mget_dataset_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"'{name}' is not one of the example datasets.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m             \u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0mfull_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/konglab/apps/anaconda3/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0murl_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_splittype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/konglab/apps/anaconda3/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/konglab/apps/anaconda3/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'urllib.Request'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;31m# post-process response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/konglab/apps/anaconda3/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[0m\u001b[1;32m    543\u001b[0m                                   '_open', req)\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/konglab/apps/anaconda3/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/konglab/apps/anaconda3/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36mhttps_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1396\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mhttps_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1397\u001b[0;31m             return self.do_open(http.client.HTTPSConnection, req,\n\u001b[0m\u001b[1;32m   1398\u001b[0m                 context=self._context, check_hostname=self._check_hostname)\n\u001b[1;32m   1399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/konglab/apps/anaconda3/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1356\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# timeout error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1358\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1359\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m             \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/konglab/apps/anaconda3/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1345\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1347\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1348\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/konglab/apps/anaconda3/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/konglab/apps/anaconda3/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/konglab/apps/anaconda3/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/konglab/apps/anaconda3/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1239\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1241\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1242\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/konglab/apps/anaconda3/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["sns.set(style=\"whitegrid\")\n","tips = sns.load_dataset(\"tips\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b3Tx6gifbLPd"},"outputs":[],"source":["tips"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":368},"executionInfo":{"elapsed":570,"status":"ok","timestamp":1646966497241,"user":{"displayName":"胡浩宇","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06402011815971394657"},"user_tz":-480},"id":"q98ynRgXbLPd","outputId":"40e1a9d4-700f-43c9-f506-ad32dd6ea5d6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Text(0, 0.5, 'dice')"]},"metadata":{},"execution_count":27},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 1080x504 with 1 Axes>"]},"metadata":{"needs_background":"light"}}],"source":["plt.figure(figsize=(15,7))\n","plt.plot(list(dice_dict.values()), 'yo-')\n","plt.title('dice on each test image')\n","plt.xlabel('input_image', {'size':15})\n","plt.ylabel('dice', {'size':15})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8kMotE-1bLPe","outputId":"54db269f-cb92-4aef-f2af-30937809a973"},"outputs":[{"name":"stderr","output_type":"stream","text":["/konglab/apps/anaconda3/lib/python3.8/site-packages/nilearn/plotting/img_plotting.py:498: RuntimeWarning: overflow encountered in short_scalars\n","  ptp = .5 * (vmax - vmin)\n"]},{"ename":"ValueError","evalue":"minvalue must be less than or equal to maxvalue","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/konglab/apps/anaconda3/lib/python3.8/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    339\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/konglab/apps/anaconda3/lib/python3.8/site-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(fig)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'png'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'retina'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'png2x'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mretina_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/konglab/apps/anaconda3/lib/python3.8/site-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mFigureCanvasBase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'svg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/konglab/apps/anaconda3/lib/python3.8/site-packages/matplotlib-3.4.2-py3.8-linux-x86_64.egg/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2228\u001b[0m                        else suppress())\n\u001b[1;32m   2229\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2230\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2232\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbbox_inches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/konglab/apps/anaconda3/lib/python3.8/site-packages/matplotlib-3.4.2-py3.8-linux-x86_64.egg/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdraw_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rasterizing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_rasterizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/konglab/apps/anaconda3/lib/python3.8/site-packages/matplotlib-3.4.2-py3.8-linux-x86_64.egg/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/konglab/apps/anaconda3/lib/python3.8/site-packages/matplotlib-3.4.2-py3.8-linux-x86_64.egg/matplotlib/figure.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   2778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2779\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2780\u001b[0;31m             mimage._draw_list_compositing_images(\n\u001b[0m\u001b[1;32m   2781\u001b[0m                 renderer, self, artists, self.suppressComposite)\n\u001b[1;32m   2782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/konglab/apps/anaconda3/lib/python3.8/site-packages/matplotlib-3.4.2-py3.8-linux-x86_64.egg/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/konglab/apps/anaconda3/lib/python3.8/site-packages/matplotlib-3.4.2-py3.8-linux-x86_64.egg/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/konglab/apps/anaconda3/lib/python3.8/site-packages/matplotlib-3.4.2-py3.8-linux-x86_64.egg/matplotlib/_api/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*inner_args, **inner_kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m                          \u001b[0;32melse\u001b[0m \u001b[0mdeprecation_addendum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m                 **kwargs)\n\u001b[0;32m--> 431\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minner_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minner_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/konglab/apps/anaconda3/lib/python3.8/site-packages/matplotlib-3.4.2-py3.8-linux-x86_64.egg/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, inframe)\u001b[0m\n\u001b[1;32m   2919\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_rasterizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2921\u001b[0;31m         \u001b[0mmimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_draw_list_compositing_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2923\u001b[0m         \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'axes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/konglab/apps/anaconda3/lib/python3.8/site-packages/matplotlib-3.4.2-py3.8-linux-x86_64.egg/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/konglab/apps/anaconda3/lib/python3.8/site-packages/matplotlib-3.4.2-py3.8-linux-x86_64.egg/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/konglab/apps/anaconda3/lib/python3.8/site-packages/matplotlib-3.4.2-py3.8-linux-x86_64.egg/matplotlib/image.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    638\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 640\u001b[0;31m             im, l, b, trans = self.make_image(\n\u001b[0m\u001b[1;32m    641\u001b[0m                 renderer, renderer.get_image_magnification())\n\u001b[1;32m    642\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/konglab/apps/anaconda3/lib/python3.8/site-packages/matplotlib-3.4.2-py3.8-linux-x86_64.egg/matplotlib/image.py\u001b[0m in \u001b[0;36mmake_image\u001b[0;34m(self, renderer, magnification, unsampled)\u001b[0m\n\u001b[1;32m    924\u001b[0m         clip = ((self.get_clip_box() or self.axes.bbox) if self.get_clip_on()\n\u001b[1;32m    925\u001b[0m                 else self.figure.bbox)\n\u001b[0;32m--> 926\u001b[0;31m         return self._make_image(self._A, bbox, transformed_bbox, clip,\n\u001b[0m\u001b[1;32m    927\u001b[0m                                 magnification, unsampled=unsampled)\n\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/konglab/apps/anaconda3/lib/python3.8/site-packages/matplotlib-3.4.2-py3.8-linux-x86_64.egg/matplotlib/image.py\u001b[0m in \u001b[0;36m_make_image\u001b[0;34m(self, A, in_bbox, out_bbox, clip_bbox, magnification, unsampled, round_to_pixel_border)\u001b[0m\n\u001b[1;32m    542\u001b[0m                                        \u001b[0mvmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ms_vmax\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m                                        ):\n\u001b[0;32m--> 544\u001b[0;31m                     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresampled_masked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    545\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/konglab/apps/anaconda3/lib/python3.8/site-packages/matplotlib-3.4.2-py3.8-linux-x86_64.egg/matplotlib/colors.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, value, clip)\u001b[0m\n\u001b[1;32m   1191\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# Or should it be all masked?  Or 0.5?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mvmin\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mvmax\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"minvalue must be less than or equal to maxvalue\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: minvalue must be less than or equal to maxvalue"]},{"data":{"text/plain":["<Figure size 1800x2880 with 88 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["file_msk = sorted([v for v in os.listdir(args_test.out_dir)  if '.nii' in v])\n","file_test = sorted(os.listdir(args_test.test_t1w))\n","file_test_msk = sorted(os.listdir(args_test.test_msk))\n","\n","plt.figure(figsize=(25, 40))\n","for index in np.arange(len(file_msk)):\n","    plt.subplot(len(file_msk), 2, index*2+1)\n","    ax1 = plt.gca()\n","    msk = nib.load(os.path.join(args_test.out_dir, file_msk[index]))\n","    nii = nib.load(os.path.join(args_test.test_t1w, file_test[index]))\n","    nil.plotting.plot_roi(msk, nii, alpha=0.5, axes=ax1, dim=2)\n","    if index == 0:\n","        plt.title(\"Model result\")\n","    \n","    plt.subplot(len(file_msk), 2, index*2+2)\n","    ax2 = plt.gca()\n","    msk_test = nib.load(os.path.join(args_test.test_msk, file_test_msk[index]))\n","    nil.plotting.plot_roi(msk_test, nii, alpha=0.5, axes=ax2)\n","    if index == 0:\n","        plt.title(\"Test result\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mp3Rl0habLPf"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Model Procedure_model_3_model.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"nbformat":4,"nbformat_minor":0}